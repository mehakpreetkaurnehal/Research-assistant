Decentralized Gaussian Process Classification and an Application in
Subsea Robotics
Yifei Gao
Virginia Tech
yifeig22@vt.edu
Hans J. He
Virginia Tech
hjh2bs@vt.edu
Daniel J. Stilwell
Virginia Tech
stilwell@vt.edu
James McMahon
US Naval Research Laboratory
Acoustics Division, Code 7130
james.mcmahon@nrl.navy.mil
Abstractâ€”Teams of cooperating autonomous underwater ve-
hicles (AUVs) rely on acoustic communication for coordination,
yet this communication medium is constrained by limited range,
multi-path effects, and low bandwidth. One way to address the
uncertainty associated with acoustic communication is to learn
the communication environment in real-time. We address the
challenge of a team of robots building a map of the probability
of communication success from one location to another in real-
time. This is a decentralized classification problem â€“ communi-
cation events are either successful or unsuccessful â€“ where AUVs
share a subset of their communication measurements to build
the map. The main contribution of this work is a rigorously
derived data sharing policy that selects measurements to be
shared among AUVs. We experimentally validate our proposed
sharing policy using real acoustic communication data collected
from teams of Virginia Tech 690 AUVs, demonstrating its
effectiveness in underwater environments.
I. INTRODUCTION
We address challenges imposed by teams of cooperating
autonomous underwater vehicles that seek to coordinate their
actions underwater. Communication between AUVs is neces-
sary to support coordination, but the acoustic communication
channel available to AUVs is extremely low-bandwidth often
unreliable [1] [2]. To maximize the ability of AUVs to
collaborate in real-time, we seek a method for the AUVs
to cooperatively build a map that can be used to predict the
probability of communication success. We address this goal
as a Gaussian process classification problem that seeks to
estimate the probability of successful or unsuccessful com-
munication from a transmitting AUV at a specific location to
a receiving AUV at another location. Implementing Gaussian
process classification across a team of agents that have only
very low bandwidth communication channels between agents
requires novel advances in decentralized Gaussian process
classification, which is the principal contribution of our work.
In practice, a team of AUVs can be pre-programmed
with the maximum range at which they can successfully
communicate. This range is necessarily conservative. Instead,
we seek an approach to a team of AUVs developing a map of
This work was supported by the Seale Coastal Zone Observatory and the
Office of Naval Research via grants N00014-23-1-2345, N00014-24-1-2267,
and N00014-25-1-2224.
Yifei Gao, Hans J. He, and Daniel Stilwell are with the Bradley Depart-
ment of Electrical and Computer Engineering, Virginia Tech, Blacksburg,
VA, USA, James McMahon is with the Acoustics Division, Code 7130, US
Naval Research Laboratory, Washington, D.C.
Fig. 1: Virginia Tech 690 AUVs
the probability of communication success in real-time based
on their current experience attempting to communicate in the
environment. A conservative range can be used as a prior,
and the possibility of communicating at longer ranges can
be learned in real-time. In past work, we have investigated
Gaussian process classification in a centralized manner for
this application [3]. In this work, we explicitly address the
challenge of decentralized Gaussian process classification
(GP) so that the team of AUVs can learn communication
success maps in real-time. Toward this goal, we develop
fundamental advances in GP classification that can be applied
to other application.
In the literature, most of the work including our prior
work [2], [4]â€“[9] use Gaussian process regression to predict
received signal strength (RSS) or use a manifold learning
algorithm to predict signal to noise ratio (SNR) [10]. In [3],
we address the application using GP classification, which
allows us to explicitly learn and predict the variable of
interest, which is the probability of communication success,
rather than a proxy variable, such as SNR. Our prior work
[3] employs Gaussian process classification to generate a
communication map among two AUVs in a centralized
manner suitable for post-processing. We also take advantage
of the properties of PÂ´olya-Gamma random variables in [11]
to make prior conjugate, which is not otherwise the case for
GP classification.
We use sparse Gaussian process classification to represent
the communication map. Due to the low-bandwidth commu-
nication channel, the map should be represented with as few
measurements of virtual measurements as possible.
arXiv:2511.15529v1  [cs.RO]  19 Nov 2025

Gaussian process classification is used in a variety of appli-
cations that range from image classification [12] to collision
prediction for robot motion planning [13]. GP classification
is widely used in active learning for label prediction [14]. A
computation challenge of GP classification arises from the
prior being Bernoulli and therefore non-conjugate. Recent
work that addresses this challenge with clever use of PÂ´olya-
Gamma random variables [15] [16] [17], which we adopt in
our work. Our approach to sparse GP classification, which
builds upon the seminal works [15] [16] [18]. The use
of sparse GP classification is because we seek a minimal
representation of the environment due to the low bandwidth
communication channel available underwater.
The contributions of this work are
â€¢ We provide a rigorously derived policy selecting data
to share among agents for decentralized sparse GP
classification under limited communication bandwidth.
â€¢ For an application in underwater communication, We
show empirically the efficacy of our data sharing policy.
We conduct numerical experiments using real data from
field trials with teams of small AUVs that communicate
acoustically.
The remainder of the paper is as follows. In Section II we
review sparse GP classification with PÂ´olya-Gamma random
variables. In Section III, we derive an upper bound for the KL
divergence between the estimated posterior and true posterior
which we use to establish a data sharing policy. In Section IV
we illustrate and evaluate our results on real communication
data set acquired by a team of AUVs.
II. PRELIMINARIES
In this section, we briefly review sparse Gaussian process
binary classification with PÂ´olya-Gamma random variables.
For further details are found in [19], [15], [11], [20], [21].
A. Gaussian Process Binary Classification
In standard Gaussian process (GP) classification, we con-
sider learning a latent function f : Rd â†’R from measure-
ments {x, y} where x âˆˆRd and binary label y âˆˆ{0, 1}.
We assume this latent function is sampled from a Gaussian
process GP(Âµ(x), k(x, xâ€²)) where Âµ denotes the latent mean
function and k : X Ã— X â†’R is a kernel function. The pos-
terior closed-form formula for Gaussian process regression
(i.e., learning f) is covered in [19].
For GP binary classification, we want to learn the proba-
bility distribution
p(yâˆ—= 1|y, xâˆ—) =
Z
p(yâˆ—= 1|fâˆ—)p(fâˆ—|y, xâˆ—)dfâˆ—
(1)
where y is the set of all training measurement labels and xâˆ—is
the test input. The conditional likelihood p(yn|fn) is defined
as a logistic function
p(yn|fn) =

efn
1 + efn
yn 
1 âˆ’
efn
1 + efn
(1âˆ’yn)
which is non-Gaussian. The logistic function maps the range
of f (R) to the interval [0, 1], inducing a valid probabilis-
tic interpretation. We want to predict the probability of
a successful communication event given any test location
xâˆ—. The latent posterior p(fâˆ—|y, xâˆ—) on (1) is non-Gaussian
since the likelihood is non-conjugate. However, the logistic
likelihood can be made conjugate by introducing a PÂ´olya-
Gamma random variable [11].
B. Gaussian process classification with PÂ´olya-Gamma (PG)
random variable
As proposed in [11], PÂ´olya-Gamma random variable w âˆ¼
PG(b, c) with probability density function
p(w|b, c) =
1
2Ï€2
âˆ
X
k=1
gk
(k âˆ’1
2)2 + ( c
2Ï€)2
where b > 0, c âˆˆR and gk âˆ¼Ga(b, 1) are independent
Gamma random variables. We assume an independent obser-
vation model conditioned with PG random variables w,
p(y|w, f) =
N
Y
n=1
p(yn|wn, fn)
with
p(yn|wn, fn) = 1
2 exp

âˆ’wn
2

f 2
n âˆ’2fn
Îºn
wn

where wn âˆ¼PG(1, cn) and Îºn = yn âˆ’1/2. Based on the
Laplace transform property of PÂ´olya-Gamma random variable
[11], the joint conditional likelihood with PG variables is
proportional to a Gaussian distribution
p(y|w, f) âˆN(â„¦âˆ’1Îº|f, â„¦âˆ’1)
where â„¦= diag(w) is a diagonal matrix of PÂ´olya-Gamma
variables and Îº = [Îº1, ..., ÎºN]T âˆˆRN. The prior of latent
function is assumed p(f) = N(f|0, KNN) where KNN is
the Gram matrix taking total measurements X as input.
Conditioned on y and the PG random variables w, the
posterior over the latent function values is also Gaussian
p(f|y, w) = N(f|Âµ, Î£)
with
Âµ = Î£Îº,
Î£ = (Kâˆ’1
NN + â„¦)âˆ’1
C. Sparse Gaussian process with PÂ´olya-Gamma Variables
Computing the posterior distribution p(f|y, w) requires
the O(N 3) procedure of inverting the Gram matrix KNN.
The author of [20] proposes sparse Gaussian process to
reduce computational complexity to O(M 3), introducing M
inducing points to summarize total data set with size N where
M â‰ªN. Denote the inducing locations as Z = [z1, ..., zM]T ,
zi âˆˆRd, and let fM = [f1(z1), ..., fM(zM)]T be the inducing
output vector. The conditional prior on latent function values
f given fM is
p(f|fM) = N(f|KX,ZKâˆ’1
Z,ZfM, KX,X âˆ’KX,ZKâˆ’1
Z,ZKT
X,Z)
where kernel matrix KX,Z âˆˆRNÃ—M and KZ,Z âˆˆRMÃ—M.
Note that now we only need to invert kernel matrix KZ,Z. In
[20], the author shows that we can estimate the latent pos-
terior as p(f|y) â‰ˆ
R
p(f|fM)q(fM)dfM using the variational

distribution q(fM). However, in standard GP classification,
the variational distribution for inducing outputs does not
have a closed-form expression. In next section, we show that
while conditioning on PÂ´olya-Gamma random variables Ï‰,
we can compute a closed-form expression for q(fM|w) =
N(fM|Âµw, Î£w) where Âµw and Î£w are the mean vector and
covariance matrix conditioned on w. This yields a closed-
form formula for the latent posterior conditioned on PÂ´olya-
Gamma variables p(f|y, w),
p(f|y, w) â‰ˆ
Z
p(f|fM)q(fM|w)dfM = N(f|m, S)
where
m = KX,ZKâˆ’1
Z,ZÂµw
S = KX,X âˆ’KX,ZKâˆ’1
Z,ZKT
X,Z + KX,ZKâˆ’1
Z,ZÎ£wKâˆ’1
Z,ZKT
X,Z
III. SELECTION OF INDUCING POINTS
In this section, we first derive the upper bound for KL
divergence between the estimated and true posterior inspired
from [18]. Then we show this upper bound can be used
as to select inducing points to share. To see the detailed
derivation of the Kullback-Leibler(KL) divergence upper
bound in Gaussian process regression, please refer to [18].
A. Lower and Upper bound for Log Marginal likelihood
We assume that we are given a set of known measurements
of communication success acquired by an agent (an AUV, in
our case) that consist of locations X = [x1, ..., xN]T and a
corresponding labels yn âˆˆ{0, 1}. By assuming independent
observations, the conditional likelihood can be written
p(y|w, f) =
N
Y
n=1
p(yn|wn, fn) = CN(â„¦âˆ’1Îº|f, â„¦âˆ’1)
where C is a unknown constant that arises from the joint
conditional likelihood being proportional to a Gaussian dis-
tribution. The marginal likelihood conditioned on PÂ´olya-
Gamma random variables w is
p(y|w) =
Z
p(y|w, f)p(f)df = CN(â„¦âˆ’1Îº|0, â„¦âˆ’1 + KNN)
(2)
Similarly, we follow the derivation in [20] and [21] to derive
a lower and upper bound for log p(y|w) which is used to
compute upper bound of KL divergence between estimated
and true posterior. We compute directly
log p(y|w) â‰¥log C âˆ’N
2 log(2Ï€)
âˆ’1
2 log |Î£| âˆ’1
2(â„¦âˆ’1Îº)T Î£âˆ’1(â„¦âˆ’1Îº)
âˆ’1
2tr(â„¦ËœK) = Llower
(3)
where Î£ = â„¦âˆ’1 + KNMKâˆ’1
MMKMN and ËœK = KNN âˆ’
KNMKâˆ’1
MMKMN. Note that Llower lower bounds the log
marginal likelihood, and if X = Z, then tr(â„¦ËœK) = 0 and
Llower = log p(y|w). We can show that while conditioning
on PÂ´olya-Gamma variables w, the optimal closed-form dis-
tribution for inducing variables q(fM|w) can be written as
q(fM|w) = N(fM|KMM ËœÎ£âˆ’1KMNÎº, KMM ËœÎ£âˆ’1KMM)
(4)
where ËœÎ£ = KMM + KMNâ„¦KNM and Îºn = yn âˆ’1
2 is one
of the entries in vector Îº with yn âˆˆ{0, 1}.
Following the derivation in [21], we can write an upper
bound for log marginal likelihood conditioned on PÂ´olya-
Gamma variable,
log p(y|w, X) â‰¤log C âˆ’N
2 log(2Ï€)
âˆ’1
2 log |â„¦âˆ’1 + KNMKâˆ’1
MMKMN|
âˆ’1
2cT (â„¦âˆ’1 + pI + KNMKâˆ’1
MMKMN)âˆ’1c
= Lupper
(5)
where c = â„¦âˆ’1Îº and p = tr(ËœK). If X = Z, then we again
arrive at Lupper = log p(y|w).
B. KL Divergence Upper bound
Following the analysis in [18], an upper bound is
derived for KL divergence between the estimated pos-
terior q(f, fM)
=
p(f|fM)q(fM|w) and true posterior
p(f, fM|w, y) = p(f|fM)p(fM|w, y) which is denoted as
KL[q(f, fM)||p(f, fM|w, y)]
â‰œ
KL[Qw||Pw]. We donâ€™t
know the true inducing posterior conditioned on PÂ´olya-
Gamma variables w denoted as p(fM|w, y) but we can use
q(fM|w) to approximate it. This KL divergence quantifies
how close we are to the true posterior conditioned on w.
Let log marginal likelihood conditioned on w denoted as
L = log p(y|w). From (3) and (5), we can have Llower â‰¤
L â‰¤Lupper and we can also directly show that log p(y|w) =
KL[Qw||Pw]+Llower from which we write the upper bound
KL[Qw||Pw] â‰¤Lupper âˆ’Llower. Note that the constant C
in the joint conditional likelihood p(y|w, f) is eliminated in
the difference between upper and lower bound of the log
marginal likelihood log p(y|w). Therefore, we can write
KL[Qw||Pw] â‰¤1
2tr(ËœK)Î»max(â„¦) +
1
2Î»min(â„¦)||Îº||2
2
where
ËœK
=
KNN âˆ’KNMKâˆ’1
MMKMN. Î»max(â„¦) and
Î»min(â„¦) are maximum and minimum eigenvalue of diagonal
matrix â„¦hence largest and smallest corresponding entries
in the diagonal. Since we assume that we are given PÂ´olya-
Gamma random variables w to form â„¦, only tr(ËœK) and ||Îº||2
2
affect the KL divergence upper bound. We cannot control
||Îº||2
2 since it scales with size N of the current data set.
Selecting inducing points which minimize tr(ËœK) is non-
trivial but [18] [21] [22] [23] propose determinantal point
process and ridge leverage scores to select m â‰¤M number of
inducing points in an acceptable amount of computation time.
However, in this paper, we are only interested in selecting
small number of inducing points where m â‰¤2 due to limited
communication bandwidth.

C. Communication Among Multiple agents
In this section we examine precisely what information
should be shared among agents. We assume that each agent is
within its own locality region and denote local measurements
obtained by agent i be {Xi, yi}. We assume each agentâ€™s
locality region is independent, and that each agent selects
inducing points directly from their measurement locations
Xi. Each agentâ€™s set of inducing points Zi are therefore
independent with respect to those of other agents. Suppose
agent i chooses mi inducing points from its current inducing
set Mi in its locality region which minimizes tr(ËœKi),
tr(ËœKi) = tr(KNiNi âˆ’KNimiKâˆ’1
mimiKmiNi)
where Ni denotes the total number of local data set size
obtained from agent i in the locality region. After se-
lecting mi number of inducing points, agent i computes
the variational distribution q(fmi|wi) = N(fmi|Âµmi, Î£mi)
using (4) with KMM and KNM replaced by Kmimi and
KNimi. The information package which each agent needs to
share is

Zmi, q(fmi|wi)

, where Zmi denotes mi inducing
locations selected to minimize tr(ËœKi). Based on the upper
limit of the KL divergence derived Section III-B, agent i
selects mi inducing points and computes q(fmi|wi), which
minimizes the upper bound of KL divergence in its locality
region.
D. Prediction
For the case of n agents, each agent summarizes its
locality region with q(fmi|wi), which it shares with other
agents. We denote the complete set of inducing variables
as fM = [fm1, ..., fmn] where fmi = f(Zmi) are the latent
function vectors taking inducing locations Zmi as input. Then
we can write the joint variational distribution after each agent
i receives information packages

Zmj, q(fmj|wj)

where
i Ì¸= j as q(fM|w) = N(fM|ÂµM, Î£M) where
ÂµM =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Âµm1
...
Âµmi
...
Âµmn
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
, Î£M =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
Î£m1
Â· Â· Â·
0
Â· Â· Â·
0
...
...
...
...
...
0
Â· Â· Â·
Î£mi
Â· Â· Â·
0
...
...
...
...
...
0
Â· Â· Â·
0
Â· Â· Â·
Î£mn
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£»
(6)
where ÂµM
âˆˆRMÃ—1 and Î£ âˆˆRMÃ—M where M
=
m1 + m2 + ... + mn. Note this decentralized formulation
(block-diagonal structure) is due to assuming independence
between the locality region for each AUV, which we arrive at
by noting that the effect of measurements outside of a locality
region are vanishingly small. Therefore, the distribution for
the value of the latent function at test location xâˆ—is
p(fâˆ—|y, w) â‰ˆ
Z
p(fâˆ—|fM)q(fM|w)dfM
= N(fâˆ—|knâˆ—MKâˆ’1
MMfM, knâˆ—nâˆ—âˆ’knâˆ—MKâˆ’1
MMkMnâˆ—)
where p(fâˆ—|fM) is the Gaussian conditional prior over fâˆ—
which has a closed-form expression and knâˆ—M is a vector in
R1Ã—M. Then the probability of success on test location xâˆ—
given training data y is approximated as
p(yâˆ—= 1|y, w) â‰ˆ
Z
p(yâˆ—|fâˆ—)p(fâˆ—|y, w)dfâˆ—
which can be computed by 1-D Gaussian quadrature.
IV. NUMERICAL RESULTS
We evaluate and illustrate our approach to decentralized
sparse GP classification using data acquired from previous
field trials with teams of Virginia Tech 690 autonomous
underwater vehicles (see Figure 1). The 690 AUV displaces
approximately 43Kg and is 2.23 meters long. It communi-
cates acoustically using the WHOI Micromdem 2, which
operates at 25KHz. Experiments where performed with two
AUVs in Claytor Lake, near Dublin, Virginia, USA, and with
two and three AUVs in Massachusetts Bay near Boston,
Massachusetts, USA. The AUVs communicate using time-
division multiple access communication; each AUV broad-
casts to all other AUVs sequentially, with each broadcasting
AUV assigned a sequence of known time-intervals. Therefore
if a receiving AUV does not receive a data packet during a
time when it is expected, the occurrence of an unsuccessful
communication event can be noted and recorded.
The location of a communication event is represented by
x âˆˆR4, where the first two dimensions indicate the easting
and northing coordinates of the broadcasting vehicle in the
horizontal plane, and the last two dimensions contain the
easting and northing coordinates of the receiving vehicle.
For the data presented herein, all AUVs operated within a
few meters of the same depth, so depth variations are not
considered. Distance between AUVs is not explicitly included
as a parameter since the relative positions of transmitting
and receiving AUVs implicitly encode this information. To
reduce computational complexity and enable straightforward
visualization of communication events on 2D plots, environ-
mental factors such as multipath effects and water current
speed are not incorporated in this model. These factors will
be addressed in future work.
We note that communication success measurements are
obtained by all AUVs that receive or fail to receive a commu-
nication packet from a broadcasting AUV. The broadcasting
AUV cannot assess success or failure. The broadcasting
AUV can be informed of a receiving AUVâ€™s communica-
tion measurements, but only via direct communication of
the measurement from the receiving AUV. Of course, the
acoustic communication channel available to the AUVs is
very low bandwidth, and we presume that all communication
measurements cannot be shared among the team of AUVs.
This motivates our development of a data-sharing policy
(see Section III) for decentralized sparse Gaussian process
classification.
We evaluate our decentralized sparse GP classification
approach using three datasets as detailed in TABLE I. The
column Agent indicates the specific AUV involved in each
dataset. For each AUV, the Unsuccessful Events and Suc-
cessful Events columns represent the total number of failed

and successful communication attempts when that particular
AUV acted as the receiving vehicle. These measurements
were obtained by the AUV, and are therefore stored locally
onboard the AUV. For instance, in Dataset 3,AUV 1 suc-
cessfully received 119 data packets from AUV 2 or AUV 3,
and it failed to receive 132 data packets from AUV 2 and
AUV 3.
Using the data acquired from the field, we implement
decentralized GP classification by sharing inducing points
(i.e. virtual measurements in each broadcast data packet, and
we choose which measurements to share using three different
strategies. We select inducing points that minimize tr(ËœK) as
described on Section III, and we compare this approach to
randomly selecting inducing points, and to selecting inducing
points that maximize tr(ËœK), which our analysis suggests
should be the least useful inducing points. We address two
cases: in the first we share one inducing point, and in the
second we share two inducing points. While an acoustic
modem data packet may have room for more inducing points,
the data packet is usually needed for communicating other
missions-specific data. Thus we believe our experiments with
one and two inducing points is representative of real-world
constraints.
We evaluate the classification prediction accuracy with the
ration ACC =
Ncorrect
Ntotal
where Ncorrect is the number of
correct prediction. Ntotal is the total number of test points xâˆ—.
If the true label is y = 1 , and prediction gives a probability
larger than 0.5, we treat it as a correct prediction and y = 0 is
addressed similarly. We assess the certainty of the prediction
using
NLL = âˆ’
1
Ntotal
Ntotal
X
i=1
yi log(pi) + (1 âˆ’yi) log(1 âˆ’pi)
For all three data sets, we use the squared exponential
kernel function with unit signal variance, for any two inputs
x and xâ€²
k(x, xâ€²) = exp

âˆ’âˆ¥x âˆ’xâ€²âˆ¥2
2l2

We initially standardize all communication data sets to
make them have zero mean and unit variance. Then we use
GPflow [24] to train on all standardized data sets and de-
termine an optimal length-scale parameter l = 0.289, which
we then fix for all experiments across all agents. Note that
real-time hyper-parameter learning is not explicitly addressed
in this work. For each decentralized communication data
set, we establish a reference radius r based on the squared
exponential kernel function with the fixed length-scale. This
radius is defined:
r = l Â·
p
âˆ’2 Â· log(Ïµ)
with Ïµ tolerance where we have k(x, xâ€²) < Ïµ if ||xâˆ’xâ€²|| > r.
We use this radius to define a locality region within each
agentâ€™s local data set. Each region is centered at one of
the local measurements stored in the agentâ€™s data set and
encompasses all points within distance r of this center. Note
that locality region R(r, c) with center c âˆˆR4 which we
defined is a ball containing all locations l âˆˆR4 which is
R(r, c) = {l âˆˆR4 : ||l âˆ’c||2 â‰¤r}
and c is one of the measurement point obtained by AUV. Only
data points inside this locality region are considered when
training the sparse Gaussian process classification model.
Our approach assesses how effectively one or two inducing
points can summarize the information provided by all mea-
surements within a locality region. Before evaluating how our
data sharing policy enables an AUV to make communication
predictions in locality regions containing data points it has
not yet encountered, we first conduct two local simulations
by selecting inducing points within locality regions in each
agentâ€™s local data set. If the selected good inducing points
yield better predictions on test points within the locality, we
can be confident that when an agent shares these inducing
points with other agents, those received agents can make
better predictions on this locality region even if they have
not yet visited that region themselves.
For Dataset 1 (see Table I) there are two AUVs. We
partition the data corresponding to which agent received or
failed to receive a communication event and label partitioned
data Agent 1 local data set and Agent 2 local data set. We
then perform 100 random permutations for train-test splits on
each agentâ€™s local dataset. In each permutation, 65 percent of
the data is allocated for training and 35 percent for testing.
For every permutation, we identify a small locality region
defined by a radius r = 0.4 with a fixed center c and we select
inducing points from this fixed locality region. Itâ€™s important
to note that since the communication data was previously
standardized, this radius r has no unit. We observe that
increasing r would necessitate more than one or two inducing
points to adequately summarize the information within the
locality region. In subsequent visualizations, we revert to
the raw communication events data, where coordinates of
each AUV are expressed in meters with northing and easting
positions.
Since it is a random permutation, each locality region en-
compasses distinct training data points. Within each locality
region, each agent selects m = 1 and m = 2 inducing points
that effectively summarize the data within that locality. We
classify inducing point selection strategies as follows: good
inducing point locations are selected by minimizing tr( ËœK),
where we constrain selection to existing data point locations
within the locality; bad inducing point locations maximize
tr( ËœK); and random inducing point locations are randomly
selected from data points x within the region.
After selecting the inducing locations Zmi, each agent
computes q(fmi|wi) based on equation (4) and uses this to
predict the unseen 35 percent test data points within each
identified locality region. We assume that wi is available for
each agent, we compute it via the Gibbs sampling technique
described in [16], which involves sampling from the PÂ´olya-
Gamma posterior p(wi|fi) and p(fi|yi, wi) for a specified
number of iterations to produce a sampled wi.

TABLE I: AUV Communication Events Across Different Datasets
Dataset
Location
Agent
Unsuccessful Events
Successful Events
Dataset 1
Massachusetts Bay
1
248
198
2
219
227
Dataset 2
Claytor Lake
1
366
197
2
336
227
Dataset 3
Massachusetts Bay
1
132
119
2
105
130
3
103
109
Results for each agent selecting inducing points and eval-
uate on its test points inside locality region before sharing
those inducing points are presented in Tables II and III. Note
the values shown on the table are averaged over 100 since we
have 100 random permutations. We observe that for Agent 2,
selecting one or two good inducing points offers substantial
prediction advantages over random or bad inducing points.
This advantage is more pronounced compared to the Agent
1, as prediction performance is also strongly influenced by
the distribution of local data within each locality region.
TABLE II: (Dataset 1: Agent 1 local data set)
SGPC-PG
ACC
NLL
Good: select one / two
0.6498 / 0.6552
0.6685 / 0.6280
Random: select one / two
0.6458 / 0.6489
0.6546 / 0.6444
Bad: select one / two
0.6427 / 0.6455
0.6558 / 0.6499
TABLE III: (Dataset 1: Agent 2 local data set)
SGPC-PG
ACC
NLL
Good: select one / two
0.7152 / 0.7272
0.5902 / 0.5588
Random: select one / two
0.5747 / 0.6249
0.6706 / 0.6353
Bad: select one / two
0.4838 / 0.4835
0.7004 / 0.7029
We visualize all training measurements on one locality
region (see Figure 3) from Agent 2 local data set where
Fig. 2: Partial trajectories of two AUVs.
AUV 2 acts as the receiving vehicle. Note the locality region
R(r, c) is not shown in the Figure 2 since it is a ball
in R4. The arrows in the figure represent communication
events where AUV 1 is broadcasting to AUV 2 so all arrows
are pointing from a point along AUV 1 trajectory to a
point along the trajectory of AUV 2. Since AUV 2 is the
receiving AUV, all of these communication events are only
stored onboard AUV 2. Green arrows indicate successful
communication events perceived by the agent, while black
arrows represent unsuccessful communication events. The
agent selects two good/random/bad inducing locations based
on the data inside this locality region. We observe that the
selection of two good inducing points that minimizes tr( ËœK)
tends to identify locations near data cluster areas, enabling
better representation of all data within this region. In contrast,
bad inducing locations typically fall in areas that poorly
represent the local data set within this region.
Having shown that selected good inducing points improve
local prediction performance, we extend our analysis to
multi-agent contexts. Results demonstrate that sharing good
inducing points among decentralized agents consistently out-
performs sharing random or bad inducing points. We evaluate
performance when agents concatenate received inducing lo-
cations and variational distributions to form the decentralized
posterior q(fM|w) Section (III-D), then predict on test points
across all locality regions.
Our results clearly demonstrate that in the decentralized
scenario, agents benefit from sharing good inducing points
with each other under limited communication bandwidth,
achieving increased accuracy and decreased negative test
likelihood (NLL), see tables IV V and VI. We visualize
test communication event predictions covered by two locality
regions visited by both agents in the Dataset 1 (see Figure 4).
In these figures, green solid triangles indicate correct predic-
tions (predictive probability p > 0.5 if y = 1 and p â‰¤0.5 if
y = 0, while red solid triangles denote incorrect predictions.
Although the two locality regions in Figure 4 are actually
partially overlap, our prediction results still demonstrate that
sharing good inducing points provides significant advantages
over alternative approaches.
TABLE IV: (Dataset 1)
SGPC-PG
ACC
NLL
Good: share one / two
0.6844 / 0.6941
0.6315 / 0.5815
Random: share one / two
0.5972 / 0.6504
0.6674 / 0.6296
Bad: share one / two
0.5606 / 0.5620
0.6765 / 0.6765

Fig. 3: Two AUVs trajectories (upper figure), Good(red
arrows), random (yellow arrows)and bad (blue arrows )in-
ducing points chosen by local agent inside a locality region.
TABLE V: (Dataset 2)
SGPC-PG
ACC
NLL
Good: share one / two
0.7442 / 0.7442
0.5020 / 0.4918
Random: share one / two
0.7485 / 0.7493
0.5756 / 0.5327
Bad: share one / two
0.7433 / 0.7437
0.6320 / 0.6111
TABLE VI: (Dataset 3)
SGPC-PG
ACC
NLL
Good: share one / two
0.7101 / 0.7270
0.6267 / 0.5976
Random: share one / two
0.6979 / 0.6967
0.6433 / 0.6273
Bad: share one / two
0.6947 / 0.6981
0.6550 / 0.6532
V. CONCLUDING REMARKS
We present a decentralized data sharing policy that lever-
ages the theoretical advantages of sparse Gaussian process
classification with PÂ´olya-Gamma random variables. Our re-
sults demonstrate that inducing points selected through this
policy offer significant advantages over random and bad
inducing points when constructing communication maps un-
der bandwidth limitations. Future research directions include
developing more sophisticated sharing policies for scenarios
with overlapping locality regions and optimizing the utiliza-
tion of previously received inducing points to enhance pre-
diction accuracy. These advancements will further improve
coordination capabilities among autonomous underwater ve-
hicles operating in challenging communication environments.
REFERENCES
[1] M. Stojanovic and J. Preisig, â€œUnderwater acoustic communication
channels: Propagation models and statistical characterization,â€ IEEE
communications magazine, vol. 47, no. 1, pp. 84â€“89, 2009.
[2] D. Horner, A data-driven framework for rapid modeling of wireless
communication channels.
PhD thesis, Monterey, California: Naval
Postgraduate School, 2013.
[3] Y. Gao, H. Yetkin, M. James, and D. J. Stilwell, â€œPrediction of acoustic
communication performance for auvs using gaussian process classi-
fication,â€ in 2024 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pp. 1244â€“1251, IEEE, 2024.
[4] J. Zhao, X. Gao, X. Wang, C. Li, M. Song, and Q. Sun, â€œAn efficient
radio map updating algorithm based on k-means and gaussian process
regression,â€ The Journal of Navigation, vol. 71, no. 5, pp. 1055â€“1068,
2018.
[5] A. Quattrini Li, P. K. Penumarthi, J. Banfi, N. Basilico, J. M. Oâ€™Kane,
I. Rekleitis, S. Nelakuditi, and F. Amigoni, â€œMulti-robot online sensing
strategies for the construction of communication maps,â€ Autonomous
Robots, vol. 44, pp. 299â€“319, 2020.
[6] X. Wang, X. Wang, S. Mao, J. Zhang, S. C. G. Periaswamy, and
J. Patton, â€œDeepmap: Deep gaussian process for indoor radio map
construction and location estimation,â€ in 2018 IEEE Global Commu-
nications Conference (GLOBECOM), pp. 1â€“7, 2018.
[7] L. Clark, J. A. Edlund, M. S. Net, T. S. Vaquero, and A. akbar Agha-
mohammadi, â€œPropem-l: Radio propagation environment modeling and
learning for communication-aware multi-robot exploration,â€ 2022.
[8] G. P. Kontoudis, S. Krauss, and D. J. Stilwell, â€œModel-based learning
of underwater acoustic communication performance for marine robots,â€
Robotics and Autonomous Systems, vol. 142, p. 103811, 2021.
[9] G. P. Kontoudis and D. J. Stilwell, â€œA comparison of kriging and
cokriging for estimation of underwater acoustic communication per-
formance,â€ in Proceedings of the 14th International Conference on
Underwater Networks & Systems, pp. 1â€“8, 2019.

[10] P. Kazemi, H. Al-Tous, C. Studer, and O. Tirkkonen, â€œSnr prediction in
cellular systems based on channel charting,â€ in 2020 IEEE eighth in-
ternational conference on communications and networking (ComNet),
pp. 1â€“8, IEEE, 2020.
[11] N. G. Polson, J. G. Scott, and J. Windle, â€œBayesian inference for
logistic models using polya-gamma latent variables,â€ 2013.
[12] Y. Bazi and F. Melgani, â€œGaussian process approach to remote sensing
image classification,â€ IEEE transactions on geoscience and remote
sensing, vol. 48, no. 1, pp. 186â€“197, 2009.
[13] J. MuËœnoz, P. Lehner, L. E. Moreno, A. Albu-SchÂ¨affer, and M. A.
Roa, â€œCollisiongp: Gaussian process-based collision checking for robot
motion planning,â€ IEEE Robotics and Automation Letters, vol. 8, no. 7,
pp. 4036â€“4043, 2023.
[14] G. Zhao, E. Dougherty, B.-J. Yoon, F. Alexander, and X. Qian,
â€œEfficient active learning for gaussian process classification by error re-
duction,â€ Advances in Neural Information Processing Systems, vol. 34,
pp. 9734â€“9746, 2021.
[15] F. Wenzel, T. Galy-Fajou, C. Donner, M. Kloft, and M. Opper,
â€œEfficient gaussian process classification using polya-gamma data
augmentation,â€ 2018.
[16] I. Achituve, A. Shamsian, A. Navon, G. Chechik, and E. Fetaya,
â€œPersonalized federated learning with gaussian processes,â€ Advances
in Neural Information Processing Systems, vol. 34, pp. 8392â€“8406,
2021.
[17] J. Snell and R. Zemel, â€œBayesian few-shot classification with one-vs-
each p\â€™olya-gamma augmented gaussian processes,â€ arXiv preprint
arXiv:2007.10417, 2020.
[18] D. R. Burt, C. E. Rasmussen, and M. Van Der Wilk, â€œConvergence of
sparse variational inference in gaussian processes regression,â€ Journal
of Machine Learning Research, vol. 21, no. 131, pp. 1â€“63, 2020.
[19] C. E. Rasmussen, C. K. Williams, et al., Gaussian processes for
machine learning, vol. 1. Springer, 2006.
[20] M. Titsias, â€œVariational learning of inducing variables in sparse Gaus-
sian processes,â€ in Artificial intelligence and statistics, pp. 567â€“574,
PMLR, 2009.
[21] M. K. Titsias, â€œVariational inference for gaussian and determinantal
point processes,â€ in Workshop on Advances in Variational Inference
(NIPS), p. 14, 2014.
[22] M.-A. Belabbas and P. J. Wolfe, â€œSpectral methods in machine learning
and new strategies for very large datasets,â€ Proceedings of the National
Academy of Sciences, vol. 106, no. 2, pp. 369â€“374, 2009.
[23] N. Anari, S. O. Gharan, and A. Rezaei, â€œMonte carlo markov chain
algorithms for sampling strongly rayleigh distributions and determinan-
tal point processes,â€ in Conference on Learning Theory, pp. 103â€“115,
PMLR, 2016.
[24] A. G. d. G. Matthews, M. van der Wilk, T. Nickson, K. Fujii,
A. Boukouvalas, P. LeÂ´on-VillagrÂ´a, Z. Ghahramani, and J. Hensman,
â€œGPflow: A Gaussian process library using TensorFlow,â€ Journal of
Machine Learning Research, vol. 18, pp. 1â€“6, apr 2017.
Fig. 4: Two agents decentralized prediction on test points
from two different locality regions using good (upper
figure)/random (middle figure)/bad (bottom figure)inducing
points shared among each agent
