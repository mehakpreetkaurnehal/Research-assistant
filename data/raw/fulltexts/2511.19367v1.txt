An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage 
Classification 
Saniah Kayenat Chowdhury1, Rusab Sarmun2, Muhammad E. H. Chowdhury3*, Sohaib Bassam Zoghoul4, Israa Al-Hashimi5, Adam Mushtak6, Amith Khandakar7 
1Department of Robotics and Mechatronics Engineering, University of Dhaka, Bangladesh. Email: kayenat945@gmail.com (SKC)  
2Department of Electrical and Electronics Engineering, University of Dhaka, Bangladesh. Email: rusabsarmun@gmail.com (RS)  
3,7Department of Electrical Engineering, College of Engineering, Qatar University, Doha 2713, Qatar. Email: mchowdhury@qu.edu.qa (MEHC), amitk@qu.edu.qa (AK) 
4,5Department 
of 
Radiology, 
Hamad 
Medical 
Corporation, Doha, Qatar. 
Email: 
sohaibzoghoul@gmail.com 
(SBZ), 
Ialhashimi@hamad.qa 
(IAH), 
adamrads94@gmail.com (AM) 
6Department of Biomedical Technology, College of Applied Medical Sciences in Al-Kharj, Prince Sattam Bin Abdulaziz University, Al-Kharj 11942, Saudi Arabia. 
Email: ama.alqahtani@psau.edu.sa (AA)   
*Correspondence: Muhammad E.H. Chowdhury (mchowdhury@qu.edu.qa). 
 
Abstract  
Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it 
remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial 
and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends 
on multiple quantitative criteria, including the tumor size and its proximity to nearest anatomical structures, 
and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that 
performs staging by explicitly measuring the tumorâ€™s size and distance properties rather than treating it as 
a pure image classification task. Our method employs specialized encoderâ€“decoder networks to precisely 
segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. 
Subsequently, we extract the necessary tumor properties i.e. measure the largest tumor dimension and 
calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis 
of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. 
This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior 
performance compared to traditional deep learning models, achieving an overall classification accuracy of 
91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical 
evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds 
explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that 
operate on an uninterpretable â€˜black boxâ€™ manner, our method offers both state-of-the-art performance and 
transparent decision support. 
 
Keywords: Lung Cancer, Tumor Stage Classification, Medical Image Segmentation, Deep Learning, 
Computed Tomography Imaging, Anatomical Aware Framework 
 
1. Introduction 
Artificial Intelligence has revolutionized medical diagnosis and prognosis, providing AI-driven 
systems capable of delivering outcomes comparable to expert clinicians [1], [2], [3]. In oncology, especially, 
deep learning methods have demonstrated remarkable potential for early cancer detection, precise disease 
classification, and improved treatment strategies [4], [5], [6]. Lung cancer, in particular, remains a major 
global health concern and one of the deadliest diseases worldwide [7]. A study of 2024 by Zhou et al. shows 
that it ranks as the leading cause of cancer-related deaths and approximately 2.48 million new cases are 
reported every year [8]. It is the most frequently occurring malignancy in men and the second most 
commonly diagnosed cancer in women [7]. Therefore, early detection and accurate classification of lung 
cancer are crucial to improve survival rates for patients [9], [10]. Lung cancer classification primarily 
follows the Tumor-Node-Metastasis (TNM) staging system, which divides cancer classification into three 
primary categories [11]: 

â€¢ 
T-stage: Describes the size and extent of the primary tumor. 
â€¢ 
N-stage: Tells whether cancer has spread to nearby lymph nodes. 
â€¢ 
M-stage: Indicates whether cancer has metastasized to distant body parts. 
Precise staging of lung cancer is essential for assessing the tumor's extent and guiding clinical decision-
making. The Tumor Stage (T-stage) is the first staging to be done, and it categorizes tumors into four main 
classes: T1 (tumor size â‰¤3 cm), T2 (tumor >3 cm but â‰¤5 cm), T3 (tumor >5 cm but â‰¤7 cm), and T4 (tumor 
>7 cm) [11], [12], [13]. However, T-stage determination is not limited to tumor size alone; it also depends 
on tumor location, and involvement with surrounding anatomical structures such as the mediastinum, 
thoracic cavity, carina, diaphragm etc. Moreover, whether the tumor is surrounded only by lung tissue or is 
nearing the lung walls is a crucial consideration. Thus, the tumor staging becomes an extremely complex 
clinical task. 
 

Figure 1.1 Comparing conventional CNN approaches and proposed segmentation-based pipeline for lung 
cancer tumor staging, emphasizing on the importance of calculating tumor properties for clinical T-staging. 
 
Currently, medical imaging techniques such as Computed Tomography (CT), Positron Emission 
Tomography (PET), and PET/CT scans are commonly used by experts to diagnose and classify lung cancer, 
providing detailed anatomical and metabolic information [9], [14]. However, during staging, clinicians 
often face limitations in accurately capturing the complex anatomical relationships between the tumor and 
surrounding structures, leading to potential misclassification. Additionally, the reliance on manual 
interpretation introduces variability and can be time-consuming. These challenges highlight the need for 
automation through deep learning methods in order to streamline and enhance the classification process. 
Existing deep learning approaches for T-stage classification face extreme challenges. Many of these 
methods treat staging as a pure image classification problem, relying on conventional classifiers to output 
a stage label. Figure 1.1 illustrates this approach, highlighting its limitations. This approach has fundamental 
limitations. A pure Convolutional Neural Network (CNN) tnbased classifier does not explicitly explore the 
anatomical context and quantitative criteria that are essential for T-staging. For example, according to the 
International Association for the Study of Lung Cancer (IASLC) guidelines, a tumor with the largest 
dimension of 6cm is generally labeled as a T3 stage. However, if there is any invasion with mediastinum, 
diaphragm, esophagus, vertebral body, carina etc., the T-stage is classified as T4 regardless of its size [12]. 
These conditions are explicitly defined in the IASLC guidelines (described in Section 3.9), but a generic 
CNN may struggle with such fine-grained distinctions, failing to recognize multiple organs and the 
distances between them from a single input [15], [16].  Furthermore, CNN-based classification models offer 
limited interpretability regarding their decision-making criteria, reducing their trustworthiness for 
borderline cases. Motivated by these limitations, we propose reframing tumor staging as a localization and 
measurement problem, rather than solely a classification task. This involves distinct segmentation networks 
tailored for segmenting key regions, including the lungs, mediastinum, diaphragm, and tumor. By explicitly 
extracting clinically relevant tumor properties (its size, distance to and invasion of adjacent structures) 
utilizing these segmentation masks, our method directly adheres to IASLC-defined criteria for T-stage 
determination. This approach moves beyond end-to-end CNN classifiers and provides an interpretable, 
reproducible staging pipeline that is clinically relevant. The results achieved via our system highlight the 
potential of our approach to accurately classify lung cancer T-stage, which could significantly impact 
clinical decision-making and treatment planning. The comparison in performance between conventional 
CNN methods and our proposed framework in Section 5.3 further clarifies the need to incorporate medical 
context to T-staging. 
 
2. Literature Review 
This section provides a comprehensive review of existing literature pertinent to deep learning-based 
classification of lung cancer and T-staging. Initially, we discuss prior works focused broadly on 
classification of lung cancer subtypes. Subsequently, we narrow the scope specifically to the literature 
related to T-stage classification, emphasizing the necessity and potential impact of our proposed 
segmentation-driven staging framework. 
 

2.1 Lung Cancer and Nodule Classification 
The application of CNNs has demonstrated remarkable success in various aspects of lung cancer 
analysis, particularly in the classification of pulmonary nodules. Multiple studies have shown the potential 
of CNN-based models to accurately distinguish between normal and nodular lung tissue on CT scans 
[17],[18]. Moreover, the classification among lung cancer classes has also witnessed massive success in 
detecting cancer classes such as - ADC, SCLC, SCC, and LCC cancer types [19], [20],[21]. Khan et al. 
proposed a VGG-19 [22] based scheme for both segmentation and classification of lung nodules, achieving 
a high classification accuracy on two popular lung tumor datasets [23] and [24]. Their use of combining 
deep features with hand crafted features benefitted the overall performance of the network.  Beyond this, 
deep learning has also proven to be exceptionally useful to the classification of lung cancer subtypes. Wehbe 
et al. [25] utilized a very popular detection model YOLOv8 (You Only Look Once) [26] for lung cancer 
subtype classification, achieving a mean Average Precision (mAP) score of 96.8% at IoU = 0.5. Similarly, 
Barbouchi et al. [21] adopted a Detection Transformer (DeTr) [27] model for lung cancer detection and 
histologic classification using integrated PET/CT images. It results in a mean IOU of 83% in the testing 
data. Additionally, the classification results of this work also show remarkable numbers, providing an F1-
score of 93.66%. This works are testament to the success of using deep learning in the classification of lung 
cancer classes.  
 
2.2 T-Stage Classification 
While deep learning has excelled in classification tasks, achieving precise T-staging based on these 
spatial criteria remains a significant challenge and an area of research still relatively unexplored. Barbouchi 
et al.'s DeTr transformer [21] aimed to predict both T-staging and histologic classification using PET/CT 
images, classifying T-stages into T1, T2, and T3/T4. The term â€˜T3/T4â€™ indicates that this work could not 
differentiate between T-stage 3 and 4. This work reports a high overall F-1 score (95.63%) without 
highlighting class specific performances. Moreover, the extent to which the model explicitly analyses tumor 
localization and invasion for T-staging is not detailed. The authors also note that their work aims to bring 
together tumor localization and staging within a single model, leveraging the attention mechanism of the 
transformer to establish relationships between pixel features. Another work by Sathiyamurthy et al. [28] 
proposed an automated technique for lung cancer T-stage detection and classification using an improved U-
Net model with an Advanced Residual Network (ARESNET). Their method involves automated lung 
nodule mask generation and utilizes an extended mobius augmentation technique for data balancing, and 
achieves an overall accuracy of 94% across all the classes. This approach incorporates segmentation, 
suggesting a reliance on tumor size derived from the segmented mask, but the explicit analysis of 
localization and invasion is not thoroughly described. Again, this research lacks a more reliable 
performance metrics such as F1-score and fails to deliver class wise performance. More research to classify 
T-staging has been done in [29], [25],[30], all having one thing in common â€“ depending solely on traditional 
CNN models to output a class of T-stage, without taking the size or location of the tumor in consideration. 
Therefore, the question of whether the studies are in accordance to the clinical approach of staging decision 
remain unsolved. The absence of extensive performance breakdown for each stage class further indicates 
the necessity for more work. While some studies report high accuracies in T-stage classification, the 
methodologies often lack explicit details on how the models analyze the spatial relationships critical for 
determining the T parameter according to the TNM system. 
Recognizing the challenges of current deep learning models in T-stage classification, this research argues 
for the critical incorporation of the discussed clinically relevant tumor properties. By developing a model 

that is sensitive to both the visual patterns within the tumor and its specific location and size relative to key 
anatomical structures, we aim to create a more clinically meaningful and accurate T-stage classification 
system. The following are the major contributions that have been presented in our study:  
â€¢ Design and implementation of three dedicated segmentation networks â€“ LungNet, MediNet, and 
TumorNet for accurate segmentation of the lung, mediastinum, and tumor, respectively. 
â€¢ Applying the trained segmentation networks on the Lung-PET-CT-Dx dataset [23] as external validation 
set to generate precise ground truth segmentation masks of the regions.  
â€¢ Extracting crucial tumor properties: its largest dimension, spatial relationship to the anatomical structures, 
and potential invasion - using a unique contour-based distance computation method.  
â€¢ Implementing a novel automated pipeline for T-stage classification of lung cancer patients based on 
extracted tumor properties, aligned with IASLC guidelines.  
The following sections are structured into five cohesive parts. Section 3 offers a deep dive into the models 
implemented, and the full detail of our experimental methodology. Section 4 and 5 demonstrates the 
experimental setup and reveals the study's results, presenting a comprehensive analysis of model 
performance. This section also sheds light on the limitations and future work prospects of this study. The 
paper culminates in this Section 6, offering concluding remarks and insights. 
 
3. Methodology 
The proposed methodology for this research involves a multi-stage pipeline designed to classify 
the T-stage of lung cancer patients, as illustrated in Figure 3.1.  In the initial stage, we collect and pre-
process all the datasets incorporated in this study. This is followed by data augmentation and train-test 
splits. There are three separate and unique Encoder-Decoder (E-D) CNN architectures designed and trained 
in this work, namely: 
1. LungNet â€“ performs segmentation of the lungs. 
2. MediNet â€“ is dedicated to segmenting the mediastinum. 
3. TumorNet â€“ generates precise segmentation masks for the lung tumor. 
The three models are collectively referred to as AnatomicalNets. The primary objective is to obtain the 
segmentation masks and utilize them to extract the necessary tumor properties for T-stage classification. 
However, the primary dataset, Lung PET-CT-Dx [23] does not inherently include ground truth masks, 
providing only bounding box annotations for lung tumors. To resolve this limitation, we train the 
AnatomicalNets on multiple publicly available datasets [24, 31-35], each of which possesses high-quality 
ground truth masks. Following this, these models are applied to the Lung-Pet-CT-Dx dataset to generate 
the ground truth segmentation masks. Additionally, a dedicated detection model is trained on the primary 
dataset to precisely localize lung tumors within CT slices. Region of interest (ROI) identified by this 
detection model are cropped from CT images when they are used as external validation set in the TumorNet. 
A detection followed by segmentation approach makes sure that all the redundant information is eliminated 
and the model only focuses on the region thatâ€™s important. All the networks within AnatomicalNets 
achieves high dice similarity scores, ensuring reliability and robustness of the generated masks. The last 
step of the initial stage evaluating the tumorâ€™s proximity to the diaphragm. For that, a contour-based distance 
estimation technique is applied. This completes the first stage of the multi-stage pipeline. 
In the second stage, the tumor properties required for T-staging are obtained. The properties evaluated in 
this study are: 
1. Tumor size or the largest tumor dimension. 
2. Distance between tumor and lung walls. 
3. Distance between tumor and mediastinum. 
4. Distance between tumor and diaphragm. 
These properties are calculated using the primary datasetâ€™s generated segmentation masks. The distance is 
quantified by measuring the maximum difference between the contours of the respective masks. To obtain 

the tumor size, maximum distance between the contour points of the tumor mask is evaluated. This 
completes the first two phases of the pipeline, effectively localizing the tumor and providing quantitative 
measures. 
In the third and final stage, we leverage the obtained tumor properties and perform automated T-stage 
classification according to the IASLC conditions [12]. The tumor properties provide information on the 
tumor size and whether its invading other key structures, and T-stage guidelines are based on such criteria. 
The following sections of the paper further detail the data acquisition and preprocessing strategies, model 
architecture specifics, and procedures for extracting tumor properties. 
 
 
 
Figure 3.1 Overview of the proposed methodology. The tumor detection module is trained on the Lung 
PET-CT-Dx dataset to localize the tumor region. Corresponding databases are used to train the 
AnatomicalNets. During inference, either the full CT image or the detected ROI is used as input to generate 
segmentation masks. The Diaphragm position is estimated from lung segmentation masks. Final T-staging 
is performed based on the tumor properties extracted. T-staging: T1 (â‰¤3 cm and surrounded by lung tissue), 
T2 (>3 cm but â‰¤5 cm), T3 (>5 cm but â‰¤7 cm), and T4 (>7 cm or invasion into critical structures). 
 

3.1 Datasets Acquisition 
This study uses Lung PET-CT-Dx Dataset [23] for T-stage classification among lung cancer   
patients. This dataset contains 355 subjects with CT, PET, and PET-CT scans. Among them, 220 patients 
include individual CT scans; totaling 31,717 CT slices, each annotated with a bounding box for lung tumors. 
Patients are classified into four lung cancer types: Adenocarcinoma, Small Cell Carcinoma, Large Cell   
Carcinoma, and Squamous Cell Carcinoma. However, our work focuses specifically on classifying the T-
stage, with diagnoses falling into one of the following stages: T1 (49 patients), T2 (42 patients), T3 (34 
patients), or T4 (95 patients). These labels were not directly usable from the original dataset. Upon 
reviewing the provided ground truth labels, three expert radiologists independently re-evaluated and re-
annotated the T-stage classifications, as discrepancies were observed in the original annotations that did not 
align with clinical imaging features. This dataset, hereafter referred to as the primary dataset, does not 
provide any ground truth segmentation masks for the lung and adjacent regions. Therefore, in order to 
generate segmentation masks, we use this dataset as an external validation set on the proposed segmentation 
networks. To generate segmentation mask for the lung, we train LungNet on three datasets collectively 
referred to as the lung database. The first dataset [33] includes 20 labeled COVID-19 CT scans that results 
in 3520 individual slices with ground truth lung masks. The second dataset [32] contains 9 patients and 829 
CT slices with ground truth lung masks created by expert radiologists. The final dataset [35] for the lung 
segmentation is a popular dataset sourced from Kaggle with 267 CT slices and their corresponding lung 
masks. Together, these datasets provide a total of 4,616 CT slices. The tumor database consists of one 
dataset: the Medical Segmentation Decathlon (MSD) [31] dataset, comprising 64 CT volumes and among 
them, 1224 CT slices have ground truth tumor segmentation masks generated by experts. Finally, the 
MediNet is trained on the SAROS database, which is a dataset for whole-body region and organ 
segmentation in CT imaging named SAROS dataset [34]. The SAROS dataset is a collection of 725 patients, 
with a total of 5,513 CT images annotated for 13 body regions and 6 body parts, including the mediastinum. 
This research is able to generate segmentation masks for the primary dataset [23] using multiple datasets 
and the large number of collected data ensures the efficacy and reliability of the masks. A summarization 
of the secondary and primary datasets is shown in Table 1. 
 
Table 1 Summary of the datasets utilized in this research 
 
 
Availability of Ground Truth Masks 
Dataset 
Number of 
Patients 
Number of 
CT Slices 
Lung 
Tumor 
Mediastinum 
Lung PET-CT-
Dx Dataset 
220 
31,717 
 
 
 
COVID-19 CT 
lung and 
infection 
segmentation 
dataset 
20 
3520 
âœ” 
 
 
COVID-19 CT 
segmentation 
dataset 
9 
829 
âœ” 
 
 
Finding and 
Measuring 
Lungs in CT 
Data 
N\A 
267 
âœ” 
 
 

Medical 
Segmentation 
Decathlon 
(MSD) 
64 
1224 
 
âœ” 
 
SAROS 
725 
5513 
 
 
âœ” 
 
3.2 Data Pre-Processing 
             All CT images in the primary and secondary datasets are provided in DICOM format, except for 
the CT studies provided in SAROS database [34], which are in Neuroimaging Informatics Technology 
Initiative (NIfTI) format. The CT scans span a vast range of window levels, labeled in Hounsfield Units 
(HU). All the CT slices are set to the lung window (window width: 1400 HU, window center: -700 HU), 
and converted into Portable Network Graphics (PNG) format images. The image intensities have been 
normalized and mapped to pixel values in the range of 0-255. To ensure consistency across datasets, the 
intensity interval is adjusted for each dataset, thereby creating uniform image content. All PNG images are 
subsequently resized to 256x256 pixels for segmentation and detection networks. A key image enhancement 
technique, Contrast-Limited Adaptive Histogram Equalization (CLAHE), is applied to all images. CLAHE 
enhances soft-tissue contrast by locally redistributing pixel intensities without amplifying noise. For this 
study, the CLAHE parameters used are clipLimit = 1.0 and tileGridSize = (16,16). It is important to note 
that the tumor database [31], includes one additional pre-processing step. To create highly accurate tumor 
segmentation masks, the ground truth masks are superimposed on the CT to extract a rectangular region 
enclosing the tumor. Padding is applied for convenience, and the resulting region is cropped from the CT 
slice, then resized to 256x256 pixels before being passed to the tumor segmentation network. This approach 
has been exceptionally beneficial for tumor segmentation where it is imperative to capture even the smallest 
detail.  
 
 
 
Figure 3.2 Representative CT slices from the datasets incorporated in this study. Lung database is compiled  
from three datasets, while the tumor and SAROS database contain one source each. 
 

3.3 Data Augmentation & Split 
             To ensure the robustness of our study, each of the three databases is partitioned into identical 5-
folds cross-validation splits. 80% of the data is used for training and 20% for testing. The validation sets 
contain an additional 20% from the training sets. Within the 5 folds of the lung database, the train, 
validation, and test splits comprise approximately 2953, 739, and 924 images respectively per fold. In order 
to expand the size, we utilize geometric augmentation approaches. The training set is augmented by 
incorporating horizontal flips and small translation operations, resulting in a total of 8859 images. Similarly, 
for the tumor database [31], the train, validation, test splits stand at 783, 196, 245 images respectively. Due 
to the relatively smaller number of data, a broader range of geometric augmentations is applied to this 
database, including horizontal and vertical flip, translation operations, and rotation; making the total count 
of images in the training set 3915 in each fold.  Finally, the SAROS database [34], includes 3528, 882, and 
1103 images in its training, validation and test splits per fold. Applying geometric augmentations such as 
are horizontal flips and translation operations with a probability of 0.20, increase the number of images in 
the training set to 5233 images. 
 
3.4 Segmentation Models  
             In this section, we elaborate the architecture of our three deep learning networks â€“ LungNet, 
MediNet, and TumorNet, designed and trained to generate segmentation masks for the lungs, mediastinum, 
and tumor. The networks follow a similar base model architecture, inspired by U-Net [36]. The U-Net 
architecture [36] is one of the most impactful architectures widely adopted for biomedical image 
segmentation. The U-shaped structure of U-Net divides the network into two primary paths: the encoder 
path for contraction and the decoder path for expansion. The encoder is composed of multiple convolutional 
layers followed by the activation function Rectified Linear Unit (ReLU) and max-pooling operations. As 
the input progresses through the encoder, the spatial dimensions of the feature maps decrease, while the 
number of channels increases, enabling the network to capture a hierarchical representation of the image. 
After the encoder, the bottleneck stage processes the image in a compressed form, allowing the network to 
interpret the global context of the image. The decoder path then works to restore the feature map's original 
spatial dimensions. U-Net's use of skip connectionsâ€”linking feature maps from each encoder stage to the 
corresponding decoder stageâ€”enables the integration of high-level structural information and detailed 
features, which improves segmentation accuracy. The U-Net architecture has been further enhanced by 
researchers using more complex encoder and decoder networks [37], [38], [39]. For example, ResNet [40] 
and DenseNet [41] are often chosen as encoders. ResNet is a transformatory architecture that introduces 
residual connection to train the deeper layers of the network more efficiently and minimize the problem of 
vanishing gradients. DenseNet, on the other hand, incorporates dense connections between layers, allowing 
each layer to acquire feature maps from all preceding levels. For the decoder part, the standard U-Net 
decoder structure is widely used due to its effectiveness in progressively restoring the resolution of feature 
maps. The decoder works by expanding the feature maps back to the input imageâ€™s original spatial 
dimensions through up-convolutions, while incorporating skip connections from the encoder. This process 
ensures that the decoder is enriched with detailed features that help in reconstructing high-quality 
segmentation outputs. Another popular decoder is the Feature Pyramid Network (FPN) [42]. FPN utilizes 
a hierarchical framework comprising encoder and decoder components arranged in a pyramid-like structure, 
generating intermediate segmentation predictions at various spatial resolutions along the decoder pathway. 
Ultimately, these intermediate feature maps are resized to match spatial dimensions, combined, and 
processed through a convolutional layer with a 3 Ã— 3 kernel. Finally, a SoftMax activation function is 

applied to yield the segmentation mask. Further advancing the U-Net paradigm, UNet++ [43] has been 
proposed to address some limitations of the original U-Net, particularly in capturing finer-grained details 
and improving segmentation accuracy for objects of varying scales. Unlike the simple skip connections in 
U-Net, UNet++ redesigns these pathways to connect the encoder and decoder through a series of nested, 
dense convolutional blocks. The dense skip connections enable the aggregation of features from multiple 
scales within the decoder, leading to more precise delineation of object boundaries and better performance 
on complex segmentation tasks. The following sections describe each network in great details. 
 
3.4.1 LungNet 
For accurate T-stage classification, an important criterion is to assess whether the tumor is 
completely surrounded by lung tissues, or approaching the lung walls. A tumor that is fully encased within 
lung parenchyma and measures less than 3 cm in its largest axis is categorized as T1. Hence, LungNet is 
implemented for producing high-quality lung segmentation masks, which are to be used to evaluate this 
condition. Lung masks are also utilized to approximate the diaphragm position, as diaphragmatic invasion 
reclassifies the tumor stage as T4. The E-D CNN network in LungNet uses DenseNet-121 as its encoder. 
This works efficiently as each layer in the network is connected to all the other layers during the feed-
forward phase. As a result, each layer accepts all its previous layers as inputs and enhances feature 
reusability. For the decoder stage, U-Netâ€™s decoder has shown great performance in the way it increases 
and restores the resolution of the encoder output as well as refines it by using skip connections in our 
proposed model, followed by the final 1x1 convolution. Hence, crucial spatial details and fine-grained 
features are maintained during boundary delineation in the lungs. This architecture demonstrates robust 
performance in generating reliable lung masks during inference on the primary dataset. 
 
3.4.2 MediNet 
The second segmentation module in the AnatomicalNets framework is MediNet, developed and 
trained for mediastinum segmentation. This is a significant step to classify T-stage as tumor invasion into 
the mediastinum will directly assign the stage to T-4, irrespective of the tumor size. For this model, we 
again utilize DenseNet121 as the encoder. But unlike in LungNet, the decoder component is based on the 
UNet++ architecture. This combination of a DenseNet121 encoder and a UNet++ decoder allows our model 
to leverage the strengths of both architectures: the feature representation power of DenseNet and the 
improved feature aggregation of UNet++. The effectiveness of this design is further validated by its strong 
performance on both annotated and primary datasets. 
 
3.4.3 TumorNet 
             Finally, tumor segmentation masks are acquired through the implementation of TumorNet. The 
encoder backbone consists of a ResNet-152 model, chosen for its depth and ability to extract detailed 
hierarchical features. The decoder utilizes a Feature Pyramid Network (FPN), designed to effectively 
reconstruct segmentation masks by combining multi-scale feature maps generated by the ResNet-152 
encoder. TumorNet operates on cropped tumor ROI from CT images, enabling focused segmentation of 
tumor structures. Deploying task specific encoder-decoder architecture enables us to achieve the highest 
level of performance which is evident in the results section of our experiments.  
 
3.5 Lung Tumor Detection 
As illustrated in figure 3.1, a dedicated lung tumor detection model, based on the YOLOv11 
architecture is trained on the primary Lung-PET-CT-Dx dataset. This model detects tumor regions in CT 
images using a bounding box. These serve as inputs to TumorNet during segmentation inference. The 

detection model is an integral part of our pipeline. While training TumorNet, the tumor region in the 
annotated tumor database is cropped and padded to generate the tumorâ€™s segmentation mask. However, 
since there is no ground truth tumor segmentation mask in the primary dataset, we deploy a detection model 
to generate tumor ROIs for each CT slice. The detected bounding boxes are used for mask generation in 
our cross-dataset inference strategy. For the detection model, YOLOv11 architecture has been implemented. 
The architecture of YOLOv11 represents a significant enhancement over previous versions. YOLOv11 
incorporates new layers, blocks, and optimizations that enhance both computational efficiency and 
detection accuracy. The convolutional layers assist in gradually decreasing the spatial resolution, as it 
increases the feature map depth. Another special aspect of YOLOv11 is that it uses C3k2 block instead of 
C2f block. It is a more efficient block based on the Cross-Stage-Patrial (CSP) network. Our lung tumor 
detection model shows high mean Average Precision (mAP) scores, providing accurate bounding boxes 
across CT slices. 
 
3.6 Mask Inference  
T-stage classification depends heavily on the tumorâ€™s size and distance properties. The primary 
dataset [23] does not include ground truth masks for any of the desired anatomical structures. The modules 
within the AnatomicalNets are therefore trained to be applied to the primary dataset. The LungNet, 
MediNet, and TumorNet generates ground truth segmentation masks for the lung, mediastinum, and tumor 
respectively by treating Lung-PET-CT-Dx as an external validation set. The inferred masks serve as 
surrogate ground truth data, enabling further analysis and research on the dataset, where manual annotations 
are otherwise unavailable. A randomly selected subset of the generated masks are reviewed and approved 
by three expert clinicians for qualitative assurance. In figure 3.3, sample CT images and inferred ground 
truth masks are shown, demonstrating the precision of the generated lung, mediastinum, and tumor masks. 
 
 
 

Figure 3.3 CT images (first column) alongside corresponding lung masks (second column), mediastinum 
masks (third column), and tumor masks (fourth column) generated through cross-dataset inference. 
 
3.7 Diaphragm Estimation 
As discussed earlier, extracting diaphragmatic invasion is critical for T-stage classification. 
However, publicly available datasets specifically annotated for diaphragm segmentation are limited or 
unavailable. To overcome this challenge, we develop a specialized estimation technique leveraging the 
inferred lung segmentation masks of the Lung-PET-CT-Dx dataset. Our method involves a pixel-based 
approach along the lung mask contours. Specifically, the two lowest pixels at the inferior boundary of the 
lung masks are first identified. From these points, an upward region extending approximately 10% of the 
lung mask's height is extracted. The choice of 10% is not random but rather obtained from a rigorous trial 
and error process, indicating that this method provides the best results. Additionally, insights from studies 
regarding the estimation of the diaphragmâ€™s position [44] has aided in making this strategy. This strategy 
enables reliable estimation of the diaphragmâ€™s position without requiring additional segmentation models, 
offering a practical and efficient alternative. Figure 3.4 illustrates the step-by-step procedure for estimating 
the diaphragm position. 
 
 
 
Figure 3.4 (a) A sample CT image from the primary Lung-PET-CT-Dx dataset. (b) The corresponding lung 
segmentation mask generated from the CT image. (c) The estimated diaphragm position derived from the 
lung segmentation mask, used for further anatomical analysis. 
 
3.8 Quantitative Measurement of Tumor Properties           
             The second phase of the proposed pipeline involves computing the tumor properties essential for 
classifying T-stage. In clinical practice, to determine the T-stage, experts evaluate both the size and location 
of the tumor - particularly in relation to potential invasion of adjacent structures. To reflect this clinical 
context, we calculate the following quantitative metrics from each patientâ€™s CT scan: 
â€¢ 
Maximum tumor dimension. 
â€¢ 
Distance between the tumor and the lung walls. 
â€¢ 
Distance between the tumor and the estimated diaphragm. 
â€¢ 
Distance between the tumor and the mediastinum. 
These distances are measured by first extracting the contours of the generated ground truth segmentation 
masks. Next, we compute the maximum distance between the tumor contour and the corresponding 
anatomical region in pixels. Finally, the pixel distance is multiplied with the pixel spacing of the CT slices 
to get the distance in practical units. This process ensures that the greatest spatial separation between the 

tumor and its neighboring structures is captured, which is crucial for determining the existence of any 
invasion. A distance of zero is interpreted as direct invasion of the corresponding structure. In addition to 
this, the tumorâ€™s largest dimension is determined. Considering that it can be either in the width, heigh, or 
depth dimension, two different approaches have been adopted. First, the contour outline of the tumor 
segmentation mask is analyzed to measure the largest euclidean distance between contour points, multiplied 
by the pixel spacing of the CT slice. Additionally, to estimate the tumorâ€™s depth, the number of slices 
containing visible tumor regions is multiplied by the slice thickness of the CT scan. Finally, the two 
dimensions are compared and the larger measurement is assigned as the tumor size. These extracted 
properties contribute to the overall assessment of the tumor's volume and extent. Figure 3.5 illustrates a 
rough visualization of the contours. 
 
 
 
Figure 3.5 Visualization of the approach used to measure the key distances. The first row illustrates the 
contours obtained from the generated masks. In the single image in the second row, the contour images are 
superimposed and the maximum distances between the tumor and key anatomical structures are indicated 
using a dashed line (â€˜redâ€™ for mediastinum, â€˜blueâ€™ for diaphragm, â€˜greenâ€™ for lungs). 
 
3.9 T-Stage Classification 
The final phase of our proposed workflow is the classification of lung cancer T-stage among 
patients. In this step, we utilize the extracted tumor properties to perform a condition-based T-staging. 
Following the previous phases, we have the tumor size, the knowledge of any invasion of tumor into the 
lung walls, mediastinum, or the diaphragm. Now we constrain them into conditions set by the IASLC 
guidelines. The conditions and the decision-making process is detailed in the flowchart 3.6. All these 

conditions are in accordance with the IASLC guidelines. By adhering to the medical guidelines, our method 
is not only highly accurate, but also medically reliable and reproducible. 
 
 
 
Figure 3.6 A flowchart describing the step-by-step decision-making process for T-stage classification on 
the basis of tumor size and invasion into other anatomical structures. 
 
4. Experiment 
This section outlines the experimental framework utilized in our study. We talk about the 
experimental setup of the architectures in AnatomicalNets, as well as the detection model using YOLOv11. 
 

4.1 Experimental Setup 
To achieve the best performance in all of the networks, task specific training parameters are used. 
The training parameters look similar for all the architectures except for the loss function, which has been 
adapted depending on the networkâ€™s purpose. Experiments are conducted on a local workstation equipped 
with an IntelÂ® Coreâ„¢ i9-14900KF CPU at 3.20 GHz and 32 GB of RAM, running a 64-bit x64 architecture. 
The software environment comprises Python 3.10.6 and PyTorch 2.6.0+cu118. A number of state-of-the-
art deep learning models are implemented using the Segmentation Models PyTorch (SMP) package 
(Iakubovskii). For AnatomicalNets, all advanced encoders used are initialized with pretrained weights 
trained on the ImageNet dataset [45]. The optimization of the loss function is performed using the Adam 
method, with a constant learning rate (Î± = 0.00001) in the segmentation networks. In the detection model, 
Stochastic Gradient Descent (SGD) has been used as the optimizer. During the training phase, an early 
stopping criterion is employed: if no significant improvement in validation loss was observed for 20 
consecutive epochs, training was immediately terminated. Hyperparameter and training parameter details 
are provided in Table 2 and 3. 
 
Table 2 Segmentation network training parameters 
 
Training Parameters 
Parameter Value 
(LungNet) 
Parameter Value 
(MediNet) 
Parameter Value 
(TumorNet) 
Batch size 
64 
64 
64 
Learning Rate 
1e-4 
1e-4 
1e-4 
Number of folds 
5 
5 
5 
Max epochs 
100 
100 
100 
Epoch Patience 
5 
5 
5 
Epoch Stopping 
Criteria 
20 
20 
20 
Encoder Weight 
ImageNet 
ImageNet 
ImageNet 
Optimizer 
Adam 
Adam 
Adam 
Loss Function 
DiceLoss 
DiceLoss 
0.5*DiceLoss + 
0.5*JaccardLoss 
Encoder Depth 
5 
5 
5 
 
Table 3 Tumor Detection Network (YOLOv11) training parameters 
 
Training Parameters 
Parameter Value 
Batch size 
16 
Image Size 
640x640 
Momentum 
0.9 
Mosaic 
True 
Label Smoothing 
0 
Max epoch 
100 
Optimizer 
SGD 
 
4.2 Loss Function 
Our framework describes three modules for the segmentation of the lung, mediastinum, and lung 
tumor in the CT slices. In LungNet and MediNet, to segment the lung and mediastinum, DiceLoss has been 

used. For TumorNet, we optimize our model with a richly descriptive, overlap-based loss that combines 
two complementary measures of mask similarityâ€”Dice and Jaccardâ€”in order to both counteract class 
imbalance and sharpen boundary delineation. The Dice component derives from the Dice Similarity 
Coefficient (DSC) originally formulated to quantify volumetric overlap in medical imaging; it naturally 
ranges between 0 (no overlap) and 1 (perfect agreement), making it particularly effective when the region 
of interest occupies only a small fraction of the scan and models tend to over-predict the background. Since 
the classical DSC is non-differentiable, we employ the probabilistic variant, defined for a binary mask as: 
ğ¿ğ·ğ‘–ğ‘ğ‘’=  1 âˆ’
(2 âˆ‘
ğ‘¦ğ‘—ğ‘ğ‘—
ğ‘
{ğ‘—=1}
+  ğœ–)
(âˆ‘
ğ‘¦ğ‘—
ğ‘
{ğ‘—=1}
+ âˆ‘
ğ‘ğ‘—
ğ‘
{ğ‘—=1}
+  ğœ–) 
 
(1) 
where ğ‘ğ‘—âˆˆ[0,1] is the modelâ€™s predicted probability at pixel ğ‘—ğ‘—, ğ‘¦ğ‘—âˆˆ{0,1} is the corresponding ground-
truth label, N is the total number of pixels, and Ïµ is a small smoothing constant to guard against division by 
zero. To further penalize false positives at object boundaries and reinforce overall region overlap, we 
augment this with the Jaccard (IoU) loss: 
ğ¿{ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘} =  1 âˆ’
(âˆ‘
ğ‘¦ğ‘—ğ‘ğ‘—
ğ‘
{ğ‘—=1}
+  ğœ–)
(âˆ‘
ğ‘¦ğ‘—
ğ‘
{ğ‘—=1}
+ âˆ‘
ğ‘ğ‘—
ğ‘
{ğ‘—=1}
âˆ’ âˆ‘
ğ‘¦ğ‘—ğ‘ğ‘—
ğ‘
{ğ‘—=1}
+  ğœ–) 
 
(2) 
 
By weighting both terms equally, our final overlap loss becomes 
 
ğ¿{ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘™ğ‘ğ‘} =  0.5ğ¿{ğ·ğ‘–ğ‘ğ‘’} +  0.5ğ¿{ğ½ğ‘ğ‘ğ‘ğ‘ğ‘Ÿğ‘‘} 
(3) 
 
This balanced formulation ensures that the network receives strong gradient signals both where regions 
should coincide (via the Dice term) and where their intersection over union must be maximized (via the 
Jaccard term), leading to robust and precise tumor segmentation in challenging, low-contrast medical 
imagery. 
 
4.3 Evaluation Metrics 
In this section, the evaluation metrics used in each step of the workflow are described. In evaluating 
segmentation performance, this research utilizes a comprehensive array of measures. In addition to familiar 
indicators such as precision, Intersection over Union (IoU), recall (sensitivity), we also apply the Dice 
Similarity Coefficient (DSC), overall accuracy, False Negative Rate (FNR), False Positive Rate (FPR), and 
specificity. Among these, IoU and DSC serve as the primary metrics for this portion of the analysis. The 
metrics can be defined as follows: 
 
ğ¼ğ‘œğ‘ˆ = 
ğ‘‡ğ‘ƒ 
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ+ ğ¹ğ‘ 
 
 
 
(4) 
ğ·ğ‘†ğ¶ = 
2ğ‘‡ğ‘ƒ 
2ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ+  ğ¹ğ‘ 
 
 
(5) 
ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ = 
ğ‘‡ğ‘ƒ+ ğ‘‡ğ‘ 
ğ‘‡ğ‘ƒ+ ğ‘‡ğ‘+ ğ¹ğ‘ƒ+ ğ¹ğ‘ 
 
 
(6) 
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› = 
ğ‘‡ğ‘ƒ 
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ƒ  
 
 
(7) 

ğ‘†ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘–ğ‘¡ğ‘¦ = 
ğ‘‡ğ‘ƒ 
ğ‘‡ğ‘ƒ+ ğ¹ğ‘  
 
 
(8) 
ğ‘†ğ‘ğ‘’ğ‘ğ‘–ğ‘“ğ‘–ğ‘ğ‘–ğ‘¡ğ‘¦ = 
ğ‘‡ğ‘
ğ‘‡ğ‘+ ğ¹ğ‘ƒ 
 
 
(9) 
ğ¹ğ‘ğ‘… = ğ‘‡ğ‘ƒâˆ—ğ¹ğ‘
ğ‘‡ğ‘ƒ+ ğ¹ğ‘ 
 
 
(10) 
ğ¹ğ‘ƒğ‘… = 
ğ¹ğ‘ƒ
ğ¹ğ‘ƒ+ ğ‘‡ğ‘ 
 
 
(11) 
 
Here, TP, FP, FN refers to true positive, false positive, and false negative respectively. IoU and DSC are the 
most impactful metrics to evaluate the consistency between ground truth mask and predicted mask. There 
is a slight difference in the way these two metrics are calculated. IoU evaluates the overlap by determining 
the ratio of the intersection to the union of the predicted and ground truth seg mentation masks. Conversely, 
DSC measures overlap by considering the proportion of the intersection relative to the total aggregate area 
of the predicted and ground truth segmentations. 
To rigorously assess the performance of our tumor detection module, several quantitative evaluation metrics 
were employed, computed individually across each fold and averaged across all five folds for robust 
analysis. The metrics utilized include Precision, Recall, F1 Score, mean Average Precision (mAP) at IoU 
thresholds of 0.5 (mAP50) and between 0.5 to 0.95 (mAp50-95). 
The F1 Score represents the harmonic mean of precision and recall, providing a balanced measure between 
false positives and false negatives: 
 
ğ¹1 ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ = 2 âˆ—ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™
ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ 
 
(12) 
Additionally, the mAP was computed at two standard thresholds. The mAP50 is defined as the mean 
Average Precision at an IoU threshold of 0.5, serving as a commonly used benchmark in detection tasks. 
The \ mAP50-95 metric averages the precision across a range of IoU thresholds from 0.5 to 0.95, penalizing 
localization inaccuracies and providing a comprehensive performance perspective: 
 
ğ‘šğ´ğ‘ƒ= 1
ğ‘âˆ‘ğ´ğ‘ƒğ‘–
ğ‘
ğ‘–=1
 
 
 
(13) 
 
5 Results and Discussion 
This section presents an extensive analysis of the performance of the segmentation and detection 
experiments conducted in the study, as well as the size and invasion-based T-stage classification, offering a 
detailed report of the results. We also compare our results with results obtained from implementing most 
common CNN architectures for the classification of T-stage. 
 

5.1 Results of the AnatomicalNets 
Table 4 presents an extensive analysis of performance metrics regarding LungNet, MediNet, and 
TumorNet. This incorporates a comparison among five different deep learning networksâ€” UNet, 
DenseNet121-UNet, DenseNet121-UNet++, ResNet50-UNet, and Resnet152-FPN. The assessment criteria 
comprise Accuracy (AC), Dice Score (DSC), Intersection over Union (IoU), Precision (P), Sensitivity (SN), 
Specificity (SP) , FNR, and FPR. This comparative analysis aims to determine which deep learning network 
and preprocessing technique is the most effective in segmenting the lungs from CT images. Applying the 
same five encoder-decoder structures keeps the consistency among different models and also provides 
information on which design is most suitable for the intended task. 
 
5.2 Results of Tumor Detection Module  
Table 5 presents an extensive analysis of performance metrics regarding the tumor detection model. 
Thos is trained to predict the tumor ROI in the Lung PET-CT-Dx dataset. YOLOv11 has shown the best 
detection results in terms of the most important metric mAP50-95 and other metrics. YOLOv10 and 
YOLOv8 have also been utilized to show the difference in performance and to further emphasize the impact 
of using YOLOv11. The evaluating metrics highlighted in this analysis are: Precision (P), Recall (R), 
mAP50, and mAP50-95. This analysis indicates to the efficacy of the YOLOv11 detection model to localize 
the tumor region from CT images. Similar to the segmentation models, these are averaged results from the 
described five folds.  
 
5.3 Results of T-stage Classification 
The core objective of this study is accurate classification of the T-stage of lung cancer patients. To 
keep this classification process aligned with the clinically acclaimed guidelines, we opt for a method that 
does this classification based on tumor size and its invasion into other significant structures. This novel 
approach achieves high metrics among all significant metrics. The metrics evaluated in the classification 
task are Accuracy (Acc), Precision (P), Recall (R), and F1 Score. The classification is done according to 
the procedure described in Section 3.9. The classification report is summarized in Table 6 and a confusion 
matrix is provided in Figure 5.1. The proposed methodology achieves an overall accuracy of 91.36%. The 
detailed breakdown of precision, recall, and F1 score for each T stage (T1,T2,T3,T4) further highlights the 
efficacy of the methodology. All classes achieve high F1 score indicating the robustness of the model in 
classifying the stages.  The confusion matrix illustrates a pattern in the cases the approach failed to make 
the correct predictions. To further validate the proposed methodology over traditional image classification 
networks, a comparison in performance is demonstrated in Table 7 between our approach and most widely 
adopted CNN architectures. These differences indicate to the credibility and necessity of the adopted 
approach even more. 
 
Table 4 Model performance comparison for the three anatomical segmentation models. 
 

Model Name Network 
Acc 
(%) 
IoU 
(%) 
DSC 
(%) 
P 
 (%) 
SN 
(%) 
SP 
(%) 
FNR 
(%) 
FPR 
(%) 
LungNet 
UNet 
97.26  
91.59  
93.56  
94.00 
93.76  
98.30  
6.24 
1.70 
DenseNet 
121-  
UNet++ 
97.60  
92 
94.28 
84.04 
95.64 
98.59 
4.36 
1.41 
Densenet 
121 - Unet 
98.96 
96.07 
97.82 
97.27 
96.85 
99.37 
3.15 
0.63 
ResNet50-  
UNet 
97.19 
92.87 
95.02  
94.58 
94.0  
98.50  
6.0 
1.50 
ResNet152-   
FPN 
97.88 
92.79 
95.98 
95.64 
94.56 
98.79  
5.44 
1.21 
MediNet 
UNet 
97.62 
88.76 
91.89 
90.58 
89.64 
94.09 
10.36 
5.91 
DenseNet 
121-  
UNet++ 
98.06 
90.46 
93.39 
91.78 
92.80 
96.63 
7.2 
3.37 
Densenet 
121 - Unet 
97.71 
89.13 
92.17 
90.92 
91.98 
96.21 
8.02 
3.79 
ResNet50-  
UNet 
96.78 
85.92 
88.47 
88.21 
88.76 
95.03 
11.24 
4.97 
ResNet152- 
FPN 
97.01 
86.58 
89.14 
88.95 
89.39 
95.41 
10.61 
4.59 
TumorNet 
UNet 
97.76 
80.10 
85.74 
87.82 
84.56 
98.55 
15.44 
1.45 
DenseNet 
121-  
UNet++ 
97.90 
81.22 
86.63 
88.04 
84.26 
99.51 
15.74 
0.49 
Densenet 
121- Unet 
97.88 
81.76 
86.45 
88.39 
84.85 
99.53 
15.15 
0.47 
ResNet50-   
UNet 
97.66 
79.09 
85.25 
85.33 
87.33 
99.52 
12.67 
0.48 
ResNet152- 
FPN 
97.93 
83.43 
89.68 
89.40 
88.15 
99.55 
11.85 
0.45 
 
 
Table 5 Model performance comparison for the tumor detection model. 
 

Network 
P (%) 
R (%) 
mAP50 (%) 
mAP50-95 
(%) 
YOLOv8 
85.80 
83.87 
86.0 
45.37 
YOLOv10 
91.0 
91.0 
92.3 
58.6 
YOLOv11 
93.0 
92.3 
94.3 
60.0 
 
Table 6 Classification results of lung cancer T-stage 
 
T stages 
Acc (%) 
P (%) 
R (%) 
F1 Score 
T1 
91.36 
96.0 
90.0 
93.0 
T2 
90.0 
88.3 
89.9 
T3 
97.0 
96.0 
96.0 
T4 
88.0 
90.0 
90.0 
 
Table 7 Comparison of Performance with Traditional CNN Approach. 
 
Network 
Acc (%) 
P (%) 
R (%) 
F1Score (%) 
ResNet152 
37.29 
39.04 
37.29 
36.71 
DenseNet121 
40.35 
39.80 
39.8 
39.81 
SwinTransformer 
39.28 
40.08 
39.28 
39.04 
Proposed Model 
91.36 
96.0 
90.0 
92 
 
 
 
 
Figure 5.1 Confusion Matrix illustrating the performance of the proposed novel T stage classification 
approach 
 
Table 6 portrays the robustness and reliability of the proposed framework, highlighting consistently high 
F1 scores ranging from 89% to 96% across different tumor stage categories. This indicates the modelâ€™s 
strong capability in accurately distinguishing each class Table 7 provides a comprehensive comparative 
analysis with existing CNN-based approaches. These methods exhibit a maximum average F1 score of 

39.81% and an accuracy is 40.35%, significantly inferior to our results. These findings support the approach 
of adopting a clinically informed framework to tumor staging.  
 
5.4 A Deeper Look into the Wrong Predictions 
It is observable from Table 6 and Figure 6.1 that stage T4 has the lowest precision and the second lowest 
F1 score among all the stages. This can be explained using clinical rationale. Key points to consider include:  
â€¢ 
Multifocal Disease as a T4 Criterion: A key criterion for T4 classification in lung cancer is the 
presence of separate tumor nodules in a different ipsilateral lobe than the primary tumor [46], [47] 
. A patient with such tumor nodules is classified as T4, regardless of the individual nodule sizes or 
the extent of invasion of contiguous structures by any single nodule. In our proposed pipeline, 
which primarily assesses individual nodule characteristics (size) and their distances to anatomical 
landmarks, a patient with, for instance, a 2 cm nodule in the upper lobe and a 1.5 cm nodule in the 
lower lobe could be erroneously classified as T1 or T2 based solely on these individual 
measurements. This limitation significantly contributes to the observed misclassification of true T4 
cases as earlier stages. 
â€¢ 
Limited Scope of Invaded Structures in T4 Definition: The misclassification of T4 is also 
significantly influenced by the limited range of invaded structures considered by our model. 
According to the AJCC 8th Edition TNM classification, a primary lung tumor is classified as T4 if 
it directly invades any of the following structures: diaphragm, mediastinum, heart, great vessels 
(e.g., aorta, superior vena cava, pulmonary artery/veins), trachea, recurrent laryngeal nerve, 
esophagus, vertebral body, or carina [46], [47], [48]. Our model's features are restricted to distance 
measurements from only the diaphragm, mediastinum, and the lung walls. While our method 
presents a novel approach and is among the first works to address staging in a clinically relevant 
manner using such features, this narrow focus represents a significant limitation causing these 
misclassifications. 
These points collectively highlight the inherent complexity of diagnosing the T4 stage and directly 
indicate the limitations of this study's current feature set. Figure 6.2 illustrates a few examples of these 
misclassified T4 cases. 
 
Table 8 Examples of Misclassified CT Images in T4 Stage 
 
CT Image 
Predicted Stage 
Ground Truth 
Rationale for 
Misclassification 
 
 
 
T2 
 
 
 
T4 
 
 
 
Misclassification 
possibly due to 
the presence of 

 
 
 
T3 
 
 
 
T4 
secondary tumor 
nodules or 
invasion into 
anatomical 
structures not 
considered by 
the model 
 
 
 
 
 
T1 
 
 
 
T4 
 
 
 
 
 
 
5.5 Limitations & Future Work 
In this study, we have demonstrated a novel hybrid framework that integrates deep-learningâ€“based 
segmentation of key thoracic structures with intuitive, rule-based measurements, yielding interpretable T-
stage classifications at a high accuracy. However, there are some limitations of this approach. Addressing 
the limitations could be the used as a motivation for the future work in this domain of research. Our 
diaphragm boundary relies on a simple heuristic using the lowest 10 % of lung mask points, due to the lack 
of datasets related to diaphragm segmentation. It may misalign in atypical anatomies. Also, our evaluation 
remains confined to a single 2D CT dataset. By focusing solely on two-dimensional slices, we may overlook 
volumetric tumor characteristics that could refine staging. Again, according to the IASLC guidelines, the 
staging of lung tumor also depends on some more regions visible in the CT scan such as the trachea, 
esophagus, carina etc. In future work, we plan to curate or annotate a true diaphragm segmentation set to 
replace our heuristic, extend the pipeline to full 3D segmentation for more accurate tumor depth assessment, 
and perform segmentation of more regions in order to broaden validation across multi-center cohorts. 
Finally, we aim to expand our approach to complete TNM staging by integrating lymph node and metastasis 
detection models to further enhance clinical applicability. 
 
6 Conclusion 
This study proposed a novel, transparent, clinically informed framework for lung cancer T-stage 
classification that synergizes deep learningâ€“based segmentation with rule-based measurement protocols. 
By explicitly modeling anatomical context and adhering to IASLC criteria, our method attains 91.36% 
accuracy and robust per-stage performance across four tumor stages. The approach bridges the gap between 
â€œblack-boxâ€ classifiers and clinical practice, offering interpretable outputs that can readily integrate into 
radiological workflows. Future enhancementsâ€”such as refined diaphragm segmentation, 3D modeling, and 
expansion to full TNM stagingâ€”will further elevate its utility and generalizability in real-world settings. 
 
Declaration of generative AI and AI-assisted technologies in the manuscript preparation process 
During the preparation of this work the authors used ChatGPT 4 in order to refine language, check grammar 
and punctuation, and enhance the overall readability and conciseness. After using this tool, the authors 

reviewed and edited the content as needed and take full responsibility for the content of the published 
article. 
 
References 
[1] 
S. J. Olshansky, B. A. Carnes, and D. Grahn, "Confronting the Boundaries of Human 
Longevity: Many people now live beyond their natural lifespans through the intervention 
of medical technology and improved lifestylesâ€”a form of" manufactured time"," 
American Scientist, vol. 86, no. 1, pp. 52-61, 1998. 
[2] 
A. Leiter, R. R. Veluswamy, and J. P. Wisnivesky, "The global burden of lung cancer: 
current status and future trends," Nature reviews Clinical oncology, vol. 20, no. 9, pp. 
624-639, 2023. 
[3] 
W. Sun, B. Zheng, and W. Qian, "Computer aided lung cancer diagnosis with deep 
learning algorithms," in Medical imaging 2016: computer-aided diagnosis, 2016, vol. 
9785: SPIE, pp. 241-248.  
[4] 
S. Lakshmanaprabu, S. N. Mohanty, K. Shankar, N. Arunkumar, and G. Ramirez, "Optimal 
deep learning model for classification of lung cancer on CT images," Future Generation 
Computer Systems, vol. 92, pp. 374-382, 2019. 
[5] 
L. Wang, "Deep learning techniques to diagnose lung cancer," Cancers, vol. 14, no. 22, p. 
5569, 2022. 
[6] 
T. L. Chaunzwa et al., "Deep learning classification of lung cancer histology using CT 
images," Scientific reports, vol. 11, no. 1, pp. 1-12, 2021. 
[7] 
J. A. Barta, C. A. Powell, and J. P. Wisnivesky, "Global epidemiology of lung cancer," 
Annals of global health, vol. 85, no. 1, p. 8, 2019. 
[8] 
J. Zhou, Y. Xu, J. Liu, L. Feng, J. Yu, and D. Chen, "Global burden of lung cancer in 2022 
and projections to 2050: Incidence and mortality estimates from GLOBOCAN," Cancer 
Epidemiology, vol. 93, p. 102693, 2024. 
[9] 
V. Ambrosini et al., "PET/CT imaging in different types of lung cancer: an overview," 
European journal of radiology, vol. 81, no. 5, pp. 988-1001, 2012. 
[10] 
W. J. Petty and L. Paz-Ares, "Emerging strategies for the treatment of small cell lung 
cancer: a review," JAMA oncology, vol. 9, no. 3, pp. 419-429, 2023. 
[11] 
R. U. Osarogiagbon et al., "The International Association for the Study of Lung Cancer 
Lung Cancer Staging Project: overview of challenges and opportunities in revising the 
nodal classification of lung cancer," Journal of Thoracic Oncology, vol. 18, no. 4, pp. 410-
418, 2023. 
[12] 
K. H. Weerakkody Y, Worsley C. "Lung cancer (staging - IASLC 8th edition)." 
https://radiopaedia.org/articles/lung-cancer-staging-iaslc-8th-edition (accessed. 
[13] 
Y. Zhong et al., "Deep learning for prediction of N2 metastasis and survival for clinical 
stage I nonâ€“small cell lung cancer," Radiology, vol. 302, no. 1, pp. 200-211, 2022. 
[14] 
Ã–. Ã–nal, M. Kocer, H. N. EroÄŸlu, S. D. Yilmaz, I. EroÄŸlu, and D. KaradoÄŸan, "Survival 
analysis and factors affecting survival in patients who presented to the medicaloncology 
unit with non-small cell lung cancer," Turkish Journal of Medical Sciences, vol. 50, no. 8, 
pp. 1838-1850, 2020. 
[15] 
I. M. Israel, S. A. Israel, and J. M. Irvine, "Factors influencing CNN performance," in 2021 
IEEE Applied Imagery Pattern Recognition Workshop (AIPR), 2021: IEEE, pp. 1-4.  

[16] 
H.-C. Chu, H.-C. Lai, M.-Y. Wang, and S.-J. Wu, "CNN Distance Estimation Based on 
Received Signal Strength Indicator," in 2024 IEEE International Conference on Systems, 
Man, and Cybernetics (SMC), 2024: IEEE, pp. 2103-2107.  
[17] 
M. A. Heuvelmans et al., "Lung cancer prediction by Deep Learning to identify benign 
lung nodules," Lung cancer, vol. 154, pp. 1-4, 2021. 
[18] 
W. Hendrix et al., "Deep learning for the detection of benign and malignant pulmonary 
nodules in non-screening chest CT scans," Communications medicine, vol. 3, no. 1, p. 
156, 2023. 
[19] 
T. Adams, J. DÃ¶rpinghaus, M. Jacobs, and V. Steinhage, "Automated lung tumor detection 
and diagnosis in CT Scans using texture feature analysis and SVM," in FedCSIS 
(Communication Papers), 2018, pp. 13-20.  
[20] 
M. A. Khan et al., "VGG19 network assisted joint segmentation and classification of lung 
nodules in CT images," Diagnostics, vol. 11, no. 12, p. 2208, 2021. 
[21] 
K. Barbouchi, D. El Hamdi, I. Elouedi, T. B. AÃ¯cha, A. K. Echi, and I. Slim, "A transformer-
based deep neural network for detection and classification of lung cancer via PET/CT 
images," International Journal of Imaging Systems and Technology, vol. 33, no. 4, pp. 
1383-1395, 2023. 
[22] 
K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image 
recognition," arXiv preprint arXiv:1409.1556, 2014. 
[23] 
P. Li, Wang, S., Li, T., Lu, J., HuangFu, Y., & Wang, D. . A Large-Scale CT and PET/CT 
Dataset for Lung Cancer Diagnosis (Lung-PET-CT-Dx) [Data set], doi: 
https://www.cancerimagingarchive.net/collection/lung-pet-ct-dx/#citations. 
[24] 
S. G. Armato III, McLennan, G., Bidaut et al. LIDC-IDRI | Data from The Lung Image 
Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed 
reference database of lung nodules on CT scans, doi: 
https://www.cancerimagingarchive.net/collection/lidc-idri/#citations. 
[25] 
A. Wehbe, S. Dellepiane, and I. Minetti, "Enhanced Lung Cancer Detection and TNM 
Staging Using YOLOv8 and TNMClassifier: An Integrated Deep Learning Approach for CT 
Imaging," IEEE Access, 2024. 
[26] 
D. Reis, J. Kupec, J. Hong, and A. Daoudi, "Real-time flying object detection with 
YOLOv8," arXiv preprint arXiv:2305.09972, 2023. 
[27] 
X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang, "Dynamic detr: End-to-end object 
detection with dynamic attention," in Proceedings of the IEEE/CVF international 
conference on computer vision, 2021, pp. 2988-2997.  
[28] 
B. K. Sathiyamurthy and V. K. Madhaiyan, "Automated lung cancer T-Stage detection and 
classification using improved U-Net model," International Journal of Electrical & 
Computer Engineering (2088-8708), vol. 14, no. 6, 2024. 
[29] 
R. Fan et al., "T-stage diagnosis of lung cancer based on deep learning in CT images," 
Digital Medicine, vol. 10, no. 4, p. e00017, 2024. 
[30] 
J. Zhang and H. Zhang, "A Bayesian neural network model based on CT images for 
staging non-small cell lung cancer," in 2023 8th International Conference on Computer 
and Communication Systems (ICCCS), 2023: IEEE, pp. 884-893.  
[31] 
 Medical Segmentation Decathlon (MSD), doi: https://doi.org/10.1109/ISBI.2019.00030. 

[32] 
COVID-19 CT segmentation dataset. [Online]. Available: 
https://medicalsegmentation.com/covid19/ 
[33] 
M. Jun et al., "COVID-19 CT lung and infection segmentation dataset," (No Title), 2020. 
[34] 
S. Koitka et al., "SAROS: A dataset for whole-body region and organ segmentation in CT 
imaging," Scientific Data, vol. 11, no. 1, p. 483, 2024. 
[35] 
K. S. Mader. Finding and Measuring Lungs in CT Data 
[Online]. Available: https://www.kaggle.com/datasets/kmader/finding-lungs-in-ct-data 
[36] 
O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical 
image segmentation," in Medical image computing and computer-assisted interventionâ€“
MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, 
proceedings, part III 18, 2015: Springer, pp. 234-241.  
[37] 
R. Sarmun et al., "Enhancing intima-media complex segmentation with a multi-stage 
feature fusion-based novel deep learning framework," Engineering Applications of 
Artificial Intelligence, vol. 133, p. 108050, 2024. 
[38] 
S. Kolhar and J. Jagtap, "Convolutional neural network based encoder-decoder 
architectures for semantic segmentation of plants," Ecological Informatics, vol. 64, p. 
101373, 2021. 
[39] 
T. Lei, R. Wang, Y. Zhang, Y. Wan, C. Liu, and A. K. Nandi, "DefED-Net: Deformable 
encoder-decoder network for liver and liver tumor segmentation," IEEE Transactions on 
Radiation and Plasma Medical Sciences, vol. 6, no. 1, pp. 68-78, 2021. 
[40] 
K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in 
Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, 
pp. 770-778.  
[41] 
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, "Densely connected 
convolutional networks," in Proceedings of the IEEE conference on computer vision and 
pattern recognition, 2017, pp. 4700-4708.  
[42] 
T.-Y. Lin, P. DollÃ¡r, R. Girshick, K. He, B. Hariharan, and S. Belongie, "Feature pyramid 
networks for object detection," in Proceedings of the IEEE conference on computer vision 
and pattern recognition, 2017, pp. 2117-2125.  
[43] 
Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, "Unet++: A nested u-net 
architecture for medical image segmentation," in Deep learning in medical image 
analysis and multimodal learning for clinical decision support: 4th international 
workshop, DLMIA 2018, and 8th international workshop, ML-CDS 2018, held in 
conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, proceedings 4, 
2018: Springer, pp. 3-11.  
[44] 
T. Suwatanapongched, D. S. Gierada, R. M. Slone, T. K. Pilgram, and P. G. Tuteur, 
"Variation in diaphragm position and shape in adults with normal pulmonary function," 
Chest, vol. 123, no. 6, pp. 2019-2027, 2003. 
[45] 
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale 
hierarchical image database," in 2009 IEEE conference on computer vision and pattern 
recognition, 2009: Ieee, pp. 248-255.  
[46] 
P. Goldstraw et al., "The IASLC lung cancer staging project: proposals for revision of the 
TNM stage groupings in the forthcoming (eighth) edition of the TNM classification for 
lung cancer," Journal of Thoracic Oncology, vol. 11, no. 1, pp. 39-51, 2016. 

[47] 
F. C. Detterbeck, "The eighth edition TNM stage classification for lung cancer: What does 
it mean on main street?," The Journal of thoracic and cardiovascular surgery, vol. 155, 
no. 1, pp. 356-359, 2018. 
[48] 
M. B. Amin et al., "The eighth edition AJCC cancer staging manual: continuing to build a 
bridge from a population-based to a more â€œpersonalizedâ€ approach to cancer staging," 
CA: a cancer journal for clinicians, vol. 67, no. 2, pp. 93-99, 2017. 
 
 
 
 
 
 
 
