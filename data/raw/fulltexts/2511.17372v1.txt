Quantum Masked Autoencoders for Vision Learning
Emma Andrews
University of Florida
Gainesville, FL, United States
e.andrews@ufl.edu
Prabhat Mishra
University of Florida
Gainesville, FL, United States
prabhat@ufl.edu
Abstract
Classical autoencoders are widely used to learn features of
input data. To improve the feature learning, classical masked
autoencoders extend classical autoencoders to learn the fea-
tures of the original input sample in the presence of masked-
out data. While quantum autoencoders exist, there is no de-
sign and implementation of quantum masked autoencoders
that can leverage the benefits of quantum computing and
quantum autoencoders. In this paper, we propose quantum
masked autoencoders (QMAEs) that can effectively learn
missing features of a data sample within quantum states in-
stead of classical embeddings. We showcase that our QMAE
architecture can learn the masked features of an image and
can reconstruct the masked input image with improved vi-
sual fidelity in MNIST images. Experimental evaluation high-
lights that QMAE can significantly outperform (12.86% on
average) in classification accuracy compared to state-of-the-
art quantum autoencoders in the presence of masks.
1
Introduction
Feature learning is a critical task to understand the data
samples during machine learning. This enables the model
to establish relationships between the features within the
data samples and aid the model in its specific task, such as
classification. Autoencoders are a popular model architec-
ture for learning and extracting the features within data,
taking the original data and compressing it into a smaller
data space [2, 8]. If the autoencoder learns the features, it
can then take the compressed data and reconstruct it back
to the original data space, with minimal loss of information.
In classical machine learning, there are concerns with the
scalability of models and data, meaning that as the models
grow in parameters and the data grows in feature size, they
become computationally expensive to train and run. Autoen-
coders are no exception, becoming increasingly costly to
process the input data and handle the reconstruction. Thus,
ensuring that the model can learn the features under the
presence of less data is crucial.
Masked autoencoders (MAEs) are a variant of autoen-
coders that learn features efficiently, even under the pres-
ence of masked, or missing, information from the original
data sample [6]. These are able to achieve feature learning
despite missing 70-80% of the original data sample, such as
an image. The masked autoencoder can successfully fill in
masked information in the original data sample without the
data originally being there during reconstruction.
Quantum machine learning (QML) can produce machine
learning models that can achieve comparable results to classi-
cal machine learning models using fewer parameters. Quan-
tum autoencoders (QAEs) can create models that can com-
press and learn data samples in a fraction of the parameters
compared to their classical counterparts [13]. For traditional
QAEs, masking out information in the original data sample
results in the mask being present after the reconstruction,
not learning the features in the masked regions.
In this paper, we propose quantum masked autoencoders
(QMAEs) to efficiently learn the features and reconstruct
the entire original input, even under the presence of masked
regions. Specifically, this paper makes the following contri-
butions.
â€¢ We develop a QMAE architecture, learning features of
masked information present in image samples in the
MNIST dataset [10]. To the best of our knowledge, this
is the first work establishing masked autoencoders as a
quantum machine learning model. The original images
can be masked up to 25% and the reconstruction can
achieve good similarity with the original image. This
similarity is also better than an equivalent QAE.
â€¢ We showcase that with the reconstructions from QMAE,
they can be used in classifiers and have a greater accu-
racy (12.86%) in label predictions compared to QAEs.
The rest of the paper is organized as follows. Section 2
surveys related work. Section 3 presents the QMAE archi-
tecture, including training considerations such as the loss
function. Section 4 presents the experimental results. Finally,
Section 5 concludes the paper.
2
Background and Related Work
In this section, we describe relevant background and survey
related efforts in both classical and quantum domains.
2.1
Classical Autoencoders
Autoencoders were initially proposed in the classical domain,
featuring an encoder and decoder [8]. The encoder is respon-
sible for processing the original input data ğ‘¥into a smaller
data space, also known as the latent space. The compressed
arXiv:2511.17372v1  [quant-ph]  21 Nov 2025

, ,
Emma Andrews and Prabhat Mishra
result ğ‘§of the original input ğ‘¥from the encoder can be given
as input to the decoder to reconstruct back to the original
data space ğ‘¥exists in. The result from the decoder is Ë†ğ‘¥, and
is ideally as close to the original data sample ğ‘¥as possible.
This architecture is shown in Figure 1.
Encoder
Decoder
Figure 1: Classical autoencoder architecture.
Masked autoencoders (MAEs) [6] are a specific type of
autoencoder that masks out or removes portions of the input
data before encoding the data. During reconstruction, this
missing data is reconstructed from the knowledge of sur-
rounding information and through a learnable mask token.
This learnable mask token is inserted into the logical patch
locations of the masked patches in the latent space. Along
with the encoded visible patches, the learnable mask token
is given as input to the decoder to reconstruct the original
data sample.
2.2
Quantum Machine Learning
Quantum computing operates on data that can be both 0
and 1 simultaneously, whereas classical computing data is
either 0 or 1. The quantum state of a qubit can be expressed
in superposition as
|ğœ“âŸ©= ğ›¼|0âŸ©+ ğ›½|1âŸ©,
(1)
where ğ›¼and ğ›½are the amplitudes satisfying |ğ›¼|2 + |ğ›½|2 = 1.
In a quantum circuit, the gates process this data by apply-
ing a unitary matrix onto the current quantum state value.
A collection of these gates are parameterized gates, where
the exact value of the unitary matrix is determined by the
evaluation of the parameter. Quantum circuits that operate
primarily with parameterized gates are known as variational
quantum circuits (VQCs).
Entanglement is another important property in quantum
computing where two or more quantum states are computed
together such that they establish a relationship between
them. This relationship can be leveraged across different
applications, such as machine learning.
QML trains models on datasets to carry out a specific
task, such as classification [4, 15]. It constructs the models
through VQCs, serving as the ansatz. The parameters in the
gates of the VQCs are taken as the parameters of the model,
which are optimized during the training process towards an
objective via a loss function. This training process is often
done classically, using the same techniques to optimize and
train classical machine learning models.
2.3
Related Work
Similar to classical autoencoders, quantum autoencoders
(QAE) [13] utilize an encoder and decoder. Both are imple-
mented as variational quantum circuits, encoding the input
quantum state data into the latent space then decoding it
back to a reconstruction of the original data sample. Figure 2
provides an overview of this architecture.
Encoder
Decoder
Latent Space
Figure 2: Quantum autoencoder architecture.
For example, QAEs can compress and reconstruct images
to a high quality [17]. This is achieved with a specific ansatz
for the encoder and the decoder that lends itself to entangling
the qubit states. However, if masked data were to be given as
input to a QAE with the goal of reconstructing the masked
image, the mask would be reconstructed as that missing
information is seen as a feature of the original image.
Note that it is not possible to directly use the concepts of
classical MAEs to develop quantum MAEs due to intricacies
with quantum circuits. This is primarily due to limitations
with state preparation and measurements during the middle
of an executing quantum circuit. Therefore, we need to adapt
the classical MAE architecture to quantum to have similar
resulting models in the quantum domain.
There are many applications of QAEs and their variants
that perform feature learning and extraction. This includes
anomaly detection [12, 16], quantum error correction [11],
and quantum circuit processing [18]. However, none of these
applications explore the presence of masked information and
learning features to reconstruct information that is masked.
To the best of our knowledge, there are no prior efforts in
developing quantum autoencoders that can deal with data
in the presence of masks.
3
Quantum Masked Autoencoder (QMAE)
We propose a quantum masked autoencoder (QMAE) that
can reconstruct original data in the presence of masks. Fig-
ure 4 shows the QMAE architecture that consists of four
major components to learn the masked information: image

Quantum Masked Autoencoders for Vision Learning
, ,
embedding (Section 3.1), the encoder and decoder ansatz
(Section 3.2), the learnable mask token (Section 3.3), and
the training including the loss function (Section 3.4). The
remainder of this section describes these four components
in detail.
3.1
Image Embedding
Images consist of pixel values which are classical in nature.
Thus, these values must be embedded into quantum states
so that quantum circuits can process the data. We utilize
amplitude embedding [14], flattening the image into a one-
dimensional vector of size 2ğ‘›such that
|ğœ“âŸ©=
2ğ‘›
âˆ‘ï¸
ğ‘–=0
ğ‘¥ğ‘–|ğ‘–âŸ©,
(2)
where ğ‘›is the number of qubits required and ğ‘¥ğ‘–is the ğ‘–th
vector element in the one-dimensional vector ğ‘¥of the image
data. It is important to note that the amplitude embedding
will normalize the data if it is not already normalized.
3.2
Encoder and Decoder Ansatz
The encoder and decoder ansatz carry out the main computa-
tion of the model and both variational quantum circuits, pa-
rameterized by ğœƒ. Thus, the encoder is defined as the ansatz
ğ‘ˆ(ğœƒ) and the decoder is its adjoint, ğ‘ˆâ€ (ğœƒ). Note that the
decoder is also parameterized by the same values ğœƒas the
encoder. Figure 2 shows the interaction that the encoder and
decoder ansatz have together. The encoder ansatz takes in
the input state |ğœ“inâŸ©, consisting of ğ‘›qubits. The encoder com-
presses |ğœ“inâŸ©into ğ‘˜qubits, or the latent space, where ğ‘˜< ğ‘›.
The remaining ğ‘¡= ğ‘›âˆ’ğ‘˜qubits consist of the trash space and
are discarded, or reset to |0âŸ©, prior to running the decoder.
Thus, the decoder reconstructs the compressed |ğœ“inâŸ©into ğ‘›
qubits and results in the state |ğœ“outâŸ©.
We utilize the ansatz presented by Wang et al. [17] as the
encoder and decoder ansatz to strongly entangle pairs of
qubits together. This involves a two-qubit interaction circuit
consisting of 18 gates with 15 parameters. The rotation gates
are parameterized and consist of 3 CNOT gates to entangle
the result together. Therefore, the parameters are minimized
while the qubits are entangled together. This two-qubit inter-
action circuit is presented in Figure 3. Each qubit pair within
the data input range processes this two-qubit interaction
circuit. Therefore, the total number of parameters that must
be trained for a data input spanning ğ‘›qubits is
ğ‘ğ‘= ğ‘›(ğ‘›âˆ’1)
2
Ã— 15.
(3)
The overall architecture is showcased in Figure 4 for input
data requiring 4 qubits to process, such as an 8 Ã— 8 image.
Figure 3: Two-qubit interaction circuit, originally pro-
posed for image compression QAEs by Wang et al. [17].
The circuit consists of 9 parameterized ğ‘…ğ‘gates, 6 pa-
rameterized ğ‘…ğ‘Œgates, and 3 CNOT gates.
The architecture first embeds the masked image into a quan-
tum state through an amplitude embedding, as described in
Section 3.1, onto qubits 2-5.
We can denote the different qubits and their functionality
as different subsystems. Subsystem ğ´is the latent space
qubits, subsystem ğµis the trash space qubits, and subsystem
ğµâ€² is the reference space qubits. The reference space qubits
contain a reference to the state that the output is compared
against, such as for fidelity measurements. Thus, the output
of the encoder ansatz can be defined as
ğ‘ˆ(ğœƒ)|ğœ“âŸ©ğ´ğµ= |ğœ“âŸ©ğ´âŠ—|ğœ“âŸ©ğµ,
(4)
where |ğœ“âŸ©ğ´is the compressed state on the latent space qubits,
|ğœ“âŸ©ğµis the trash state on the trash qubits, and |ğœ“âŸ©ğ´ğµis the
complete output from the encoder, including both the latent
and trash qubits.
Ideally, if the encoder learns the latent space represen-
tation of |ğœ“âŸ©ğ´ğµ, then the trash state |ğœ“âŸ©ğµbeing reset to |0âŸ©
results in the decoder outputting the original state |ğœ“âŸ©ğ´ğµ, or
ğ‘ˆâ€ (ğœƒ)ğ‘ˆ(ğœƒ)|ğœ“âŸ©ğ´ğµ= |ğœ“âŸ©ğ´ğµ.
(5)
However, as it must learn this representation, this results in
the density matrix from the full encoder-decoder ansatz of
ğœŒout = ğ‘ˆâ€ 
ğ´ğµâ€² (ğœƒ) Trğµ[ğ‘ˆğ´ğµ(ğœƒ) |ğ‘âŸ©ğ‘ˆğ´ğµâ€² (ğœƒ)] ğ‘ˆğ´ğµâ€² (ğœƒ) ,
(6)
where |ğœ“ğ‘–ğ‘›âŸ©is the input state to the encoder or the ğ´ğµsystem,
|ğ‘âŸ©= |ğœ“ğ‘–ğ‘›âŸ©âŸ¨ğœ“ğ‘–ğ‘›|ğ´ğµâŠ—|ğ‘âŸ©âŸ¨ğ‘|ğµâ€², and |ğ‘âŸ©is the reference state
contained in ğµâ€².
3.3
Learnable Mask Token
As detailed in the ansatz architecture, the encoder input
is the image with masks in place. For QMAE, instead of
masking the data to a zero value, a learnable mask token
is used instead. This learnable mask token is a parameter
of the model of the total patch size and attempts to learn
an efficient representation of the input dataset to be able
to reconstruct information in that logical patch correctly,
according to the original image with no masks. Thus, along
with the parameters for the ansatz, the total model will train
ğ‘= ğ‘ğ‘+ ğ»ğ‘ğ‘Šğ‘,
(7)

, ,
Emma Andrews and Prabhat Mishra
H
H
embedded masked image
SWAP
embedded input image
reconstructed image
Figure 4: QMAE architecture. The original image is
masked and embedded as input to the encoder ğ‘ˆ(ğœƒ).
The decoder takes the compressed representation and
reconstructs the image, with learned features. A SWAP
test is then performed between the reconstructed im-
age and the original input image to get the fidelity.
where ğ»ğ‘is the height of the patch and ğ‘Šğ‘the width of
the patch when the original image is considered as a two-
dimensional image.
Insertion of the learnable mask token during input to
the encoder allows for efficient circuit running, allowing
the masked patches to be traded out for the learnable mask
token prior to embedding the input to the circuit, akin to
the MAE architecture presented in [5]. Thus, the mask to-
ken represents classical space, matching the original input
modality. Additionally, this causes a reduction in mid-circuit
measurement and state preparation that would have to occur
in between the encoder and decoder if the learnable mask
token were to be inserted in the latent space, as is done in
the original MAE architecture [6].
3.4
Training and Loss Function
To train QMAE, including updates and optimization to the
learnable mask token, the loss function needs to represent
how well the model is at producing results that are equiv-
alent to the original data sample, without masked patches.
One way to measure the similarity between two quantum
states is the SWAP test [1, 15]. The SWAP test, shown in Fig-
ure 5, measures the fidelity between two states |ğœ“âŸ©and |ğœ™âŸ©.
An ancilla qubit is used to contain the result of the fidelity
measurement, with the expectation value measured at the
end, resulting in
âŸ¨ğœğ‘âŸ©= |âŸ¨ğœ™|ğœ“âŸ©|2.
(8)
Thus, the resulting expectation value âŸ¨ğœğ‘âŸ©contains how
similar states |ğœ“âŸ©and |ğœ™âŸ©are and can be used as part of the
loss function to optimize the circuit.
H
H
SWAP
Figure 5: SWAP test to measure fidelity of two states
located on qubit wires 1 and 2.
Specifically, in QMAE, the SWAP test measures the fidelity
between the original input image with no masks and the
reconstruction resulting from the output of the decoder. As
such, since the original input image is not embedded into the
circuit as the input to the encoder, extra qubits are required
to perform an additional embedding operation to embed the
original input image into the circuit for the SWAP test. In
Figure 4, these extra qubits can be seen by embedding the
original image into qubits 6-9. With the fidelity measured as
the expectation value âŸ¨ğœğ‘âŸ©, it can be used in a loss function
to guide the optimization of circuit parameters. Thus, the
resulting loss function for training QMAE is
L = 1 âˆ’âŸ¨ğœğ‘âŸ©,
(9)
which is minimized. The loss can be optimized through clas-
sical optimization techniques, such as Adam [9].
The training process for QMAE is presented in Algo-
rithm 1. For each epoch and each sample in the training
dataset D, the masked image ğ‘šis prepared, including em-
bedding it into the ansatz ğ‘„(ğœƒ) with the original image ğ‘‘.
The ansatz ğ‘„(ğœƒ) is executed with this embedded masked im-
age, resulting in the fidelity âŸ¨ğœğ‘âŸ©between the reconstructed
image and the original image. This is minimized in the loss
function L and used to optimize the parameters ğœƒ.
Algorithm 1: QMAE
Data: D: training dataset, ğ‘„(ğœƒ): ansatz, ğ¸: epochs
Result: ğœƒ: trained model parameters
1 for ğ‘’âˆˆğ¸do
2
for ğ‘‘âˆˆD do
3
ğ‘–= get_mask_idx() ;
# Random sampling
4
ğ‘š= insert_mask_token(ğ‘‘);
5
image_embed(ğ‘š,ğ‘‘);
6
âŸ¨ğœğ‘âŸ©= execute(ğ‘„(ğœƒ));
7
L = 1 âˆ’âŸ¨ğœğ‘âŸ©;
8
optimize(ğœƒ, L);
9
end
10 end

Quantum Masked Autoencoders for Vision Learning
, ,
Our resulting QMAE architecture utilizes all four compo-
nents presented to learn the features in images effectively.
Specifically, the images are embedded from classical to quan-
tum data through amplitude embeddings, where the encoder
and decoder process the data with the learnable mask to-
ken inserted into mask patches. This model is trained with
a fidelity loss function to ensure that the output quantum
state from the model is similar to the original input image,
without any masks.
4
Experiments
This section evaluates the effectiveness of QMAE using the
MNIST dataset. We also compare the QMAE results to a QAE
with the same ansatz.
4.1
Experimental Setup
All experiments were run using Python v3.13.5, PennyLane
v0.42.3 [3], and PyTorch v2.8.0. Additionally, torchmetrics
v1.8.2 provided some additional metric implementations. We
tested our QMAE implementation on MNIST [10]. Each im-
age is grayscale and resized to 16 Ã— 16, resulting in 8 qubits
required to embed the images into quantum states. The la-
tent space is chosen to be 7 qubits, leaving 1 qubit for the
trash space. An additional 8 qubits are required to embed
the original input for the fidelity measurement, and an addi-
tional ancilla qubit to measure the fidelity. The trash space
must also be reset to |0âŸ©as input to the decoder, resulting in
an extra qubit to prepare the trash state qubit via a SWAP.
Thus, the resulting QMAE circuit for these datasets contains
18 qubits.
For comparison, we also train a QAE with the same ansatz,
masked input (excluding the learnable mask token), and hy-
perparameters to showcase how the mask token can learn the
missing features and represent them well during reconstruc-
tion. The QAE results produced during training are compared
against the original input images in the loss function. We
analyze the reconstructed images from both models on the
visual fidelity of the images and classification accuracy, as
outlined in the following subsections.
4.2
Comparison of Reconstructed Images
Figure 6 shows the original images (bottom row) as well
as the reconstructed images (top two rows) by our QMAE
architecture and the state-of-the-art QAE architecture under
25% mask. Specifically, the bottom row (row 3) shows the
five original MNIST images (without masking). To enable a
fair comparison between our proposed approach (QMAE)
and state-of-the-art (QAE), we apply a single masked patch
representing 25% of the entire image. The results produced
by QMAE with 25% mask (row 1) are better in image quality
than the results from QAE with 25% mask (row 2). QAE is
not able to learn any features in the masked areas, instead
reconstructing the masked patch. In contrast, the QMAE
model is able to learn the features to produce high-quality
images in the presence of masked patches.
Figure 6: Results from QMAE (row 1) and QAE (row 2)
at 25% mask. The original images are in row 3.
We have also explored the reconstruction quality by vary-
ing the mask percentages in the QMAE architecture, shown
in Figure 7. This mask percentage represents a tradeoff be-
tween the size of the mask and reconstruction quality. Specif-
ically, the reconstruction results with 12.5%, 25%, and 50%
masks are shown by the images in the top row (row 1), row
2, and row 3, respectively. As expected, 12.5% mask provides
better reconstruction quality compared to 25% mask. Increas-
ing the mask percentage to 50% shows that the model is not
able to learn the features in the image and produces noise.
Therefore, 25% mask percentage is the best masking value
for the MNIST dataset.
In addition to visual inspection, we quantify the image
quality with quantum-based and classical-based metrics,
such as fidelity and the structured similarity index measure.
4.2.1
Quantum-based Metrics. Like with the loss function,
the fidelity between the original image embedded into a
circuit and the output from the decoder ansatz can showcase
how similar they are. As all data is processed in quantum,
this can provide the most accurate similarity metric when
the data remains as quantum states. The fidelity is used to
train the model through the loss function, as described in
Section 3.4. Figure 8 showcases the fidelities for both QMAE
and QAE during training. QMAE can converge to 0.25 fidelity
loss, while QAE ranges from 0.2 to 0.8. Note that this value is
the result of Equation 9. Additionally, the fidelity is measured
on 10,000 images from the MNIST test set, where QMAE had
an average fidelity of 0.734 and QAE an average of 0.600.

, ,
Emma Andrews and Prabhat Mishra
Figure 7: Results from QMAE at different mask per-
centages. Rows 1-3 showcase the QMAE results at 12.5%
(row 1), 25% (row 2), and 50% mask (row 3), respectively.
(a) QMAE training loss.
(b) QAE training loss.
Figure 8: Training losses for QMAE (a) and QAE (b).
QMAE was able to converge to around 0.25 fidelity
loss, whereas QAE struggled to converge and ranged
between 0.2 to 0.8 fidelity loss.
4.2.2
Classical-based Metrics. Classical-based metrics in-
clude metrics that examine the two targets as images filled
with classical pixel values. We utilize both cosine similar-
ity (CS) and the structured similarity index measure (SSIM)
to calculate the similarity between the original image and
the reconstruction resulting from the model. Generally, the
images produced by QMAE score higher in similarity with
both metrics to the original image compared to the results
from QAE. For 10,000 samples, QMAE averaged 0.843 CS and
0.446 SSIM, while QAE averaged 0.799 CS and 0.445 SSIM.
4.3
Comparison of Classification Accuracy
We have evaluated the quality of the reconstructed images
with an image classifier to see if the reconstructed images
showcase the features. The image classifier used is a trained
ResNet18 [7] model on 16 Ã— 16 MNIST data. To adapt to the
smaller image sizes, the first maxpool layer is replaced in
favor of the identity layer. The adjusted ResNet18 model was
trained to 99.33% accuracy on the MNIST test set of 10,000
images. The trained model is used to classify reconstructions
from both QMAE and QAE. For each image, both QMAE
and QAE were given the same random mask for the best
comparison. On the MNIST test set of 10,000 images, QMAE
scored an accuracy of 65.06%, while QAE scored only 52.20%,
highlighting the fact that QMAE can produce significantly
higher quality images than QAE. Table 1 summarizes all
obtained metrics for both QMAE and QAE. QMAE performs
better on all metrics evaluated.
Table 1: Summary of metrics from QMAE and QAE on
10,000 test samples.
Model
Fidelity
CS
SSIM
Accuracy
QMAE
0.734
0.843
0.446
65.06%
QAE
0.600
0.799
0.445
52.20%
5
Conclusion
This paper proposed quantum masked autoencoders (QMAE),
a new quantum machine learning architecture to learn the
features in masked out information in the original data sam-
ple. On image datasets, QMAE can successfully reconstruct
missing information that is coherent with the image that
is present. This is evidenced by having greater similarity
metrics in both quantum and classical measurements com-
pared to state-of-the-art methods, such as quantum autoen-
coders. The results also highlighted that QMAE reconstruc-
tions present more features of the original dataset that a
classifier can use to successfully make class label predictions
with greater accuracy compared to QAEs.

Quantum Masked Autoencoders for Vision Learning
, ,
References
[1] Esma AÃ¯meur, Gilles Brassard, and SÃ©bastien Gambs. 2006. Machine
Learning in a Quantum World. In Advances in Artificial Intelligence, Luc
Lamontagne and Mario Marchand (Eds.). Springer, Berlin, Heidelberg,
431â€“442.
[2] Dor Bank, Noam Koenigstein, and Raja Giryes. 2021. Autoencoders.
arXiv:2003.05991
[3] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, Shah-
nawaz Ahmed, Vishnu Ajith, M. Sohaib Alam, Guillermo Alonso-
Linaje, B. AkashNarayanan, Ali Asadi, Juan Miguel Arrazola, Utkarsh
Azad, Sam Banning, Carsten Blank, Thomas R. Bromley, Benjamin A.
Cordier, Jack Ceroni, Alain Delgado, Olivia Di Matteo, Amintor
Dusko, Tanya Garg, Diego Guala, Anthony Hayes, Ryan Hill, Aroosa
Ijaz, Theodor Isacsson, David Ittah, Soran Jahangiri, Prateek Jain,
Edward Jiang, Ankit Khandelwal, Korbinian Kottmann, Robert A.
Lang, Christina Lee, Thomas Loke, Angus Lowe, Keri McKiernan, Jo-
hannes Jakob Meyer, J. A. MontaÃ±ez-Barrera, Romain Moyard, Zeyue
Niu, Lee James Oâ€™Riordan, Steven Oud, Ashish Panigrahi, Chae-Yeun
Park, Daniel Polatajko, NicolÃ¡s Quesada, Chase Roberts, Nahum SÃ¡,
Isidor Schoch, Borun Shi, Shuli Shu, Sukin Sim, Arshpreet Singh, Ingrid
Strandberg, Jay Soni, Antal SzÃ¡va, Slimane Thabet, Rodrigo A. Vargas-
HernÃ¡ndez, Trevor Vincent, Nicola Vitucci, Maurice Weber, David
Wierichs, Roeland Wiersema, Moritz Willmann, Vincent Wong, Shaom-
ing Zhang, and Nathan Killoran. 2022. PennyLane: Automatic Differen-
tiation of Hybrid Quantum-Classical Computations. arXiv:1811.04968
[4] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost,
Nathan Wiebe, and Seth Lloyd. 2017. Quantum Machine Learning.
Nature 549, 7671 (Sept. 2017), 195â€“202.
[5] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jin-
dong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. 2025.
Masked Autoencoders Are Effective Tokenizers for Diffusion Models.
In Proceedings of the 42nd International Conference on Machine Learning.
PMLR, 8145â€“8171.
[6] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and
Ross Girshick. 2022. Masked Autoencoders Are Scalable Vision Learn-
ers. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 16000â€“16009.
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep
Residual Learning for Image Recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 770â€“778.
[8] G. E. Hinton and R. R. Salakhutdinov. 2006. Reducing the Dimension-
ality of Data with Neural Networks. Science 313, 5786 (July 2006),
504â€“507.
[9] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Sto-
chastic Optimization. arXiv:1412.6980
[10] Yann LeCun. 1998. The MNIST Database of Handwritten Digits. (1998).
[11] David F. Locher, Lorenzo Cardarelli, and Markus MÃ¼ller. 2023. Quan-
tum Error Correction with Quantum Autoencoders. Quantum 7 (March
2023), 942.
[12] Maria Francisca Madeira, Alessandro Poggiali, and Jeanette Miriam
Lorenz. 2024. Quantum Patch-Based Autoencoder for Anomaly Seg-
mentation. In 2024 IEEE International Conference on Quantum Comput-
ing and Engineering (QCE), Vol. 01. 259â€“267.
[13] Jonathan Romero, Jonathan P Olson, and Alan Aspuru-Guzik. 2017.
Quantum Autoencoders for Efficient Compression of Quantum Data.
Quantum Science and Technology 2, 4 (Aug. 2017), 045001.
[14] Maria Schuld and Francesco Petruccione. 2018. Supervised Learning
with Quantum Computers. Springer International Publishing, Cham.
[15] Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. 2015. An
Introduction to Quantum Machine Learning. Contemporary Physics
56, 2 (April 2015), 172â€“185.
[16] Kilian Tscharke, Maximilian Wendlinger, Afrae Ahouzi, Pallavi Bhard-
waj, Kaweh Amoi-Taleghani, Michael SchrÃ¶dl-Baumann, and Pascal
Debus. 2025. Quantum Autoencoder for Multivariate Time Series
Anomaly Detection. arXiv:2504.17548
[17] Hengyan Wang, Jing Tan, Yixiao Huang, and Wenqiang Zheng. 2024.
Quantum Image Compression with Autoencoders Based on Parame-
terized Quantum Circuits. Quantum Information Processing 23, 2 (Jan.
2024), 41.
[18] Jun Wu, Hao Fu, Mingzheng Zhu, Haiyue Zhang, Wei Xie, and Xiang-
Yang Li. 2024. Quantum Circuit Autoencoder. Physical Review A 109,
3 (March 2024), 032623.
