Active learning with physics-informed neural networks for optimal sensor placement
in deep tunneling through transversely isotropic elastic rocks
Alec Tristania, Chloé Arsona
aCornell University, Department of Earth and Atmospheric Sciences, Ithaca, NY 14850, USA
Abstract
This paper presents a deep learning strategy to simultaneously solve Partial Differential Equations (PDEs) and back-
calculate their parameters in the context of deep tunnel excavation. A Physics-Informed Neural Network (PINN)
model is trained with synthetic data that emulates in situ displacement measurements in the host rock and at the
cavity wall, obtained from extensometers and convergence monitoring. As acquiring field observations can be costly, a
sequential training approach based on active learning is implemented to determine the most informative locations for
new sensors. In particular, Monte Carlo dropout is used to quantify epistemic uncertainty and query measurements
in regions where the model is least confident. This approach reduces the amount of required field data and optimizes
sensor placement. The PINN is tested to reconstruct the displacement field around a deep tunnel of circular section
excavated in transversely isotropic elastic rock and to determine rock constitutive and stress-field parameters. Results
demonstrate excellent performance on small, scattered, and noisy datasets, achieving high precision for the Young’s
moduli, shear modulus, horizontal-to-vertical far-field stress ratio, and the orientation of the bedding planes. The
proposed framework shall ultimately support decision-making for optimal subsurface monitoring and for adaptive
tunnel design and control.
Keywords:
Active Learning, Physics-informed Neural Networks, Inverse Analysis, Deep Tunnels, Transversely
Isotropic Elastic Rocks, Optimal Sensor Placement
1. Introduction
Data-driven methods are increasingly used in geomechanics to model the behavior of rock materials.
In the
context of physics-informed machine learning, Physics-Informed Neural Networks (PINNs) have emerged as a pow-
erful framework to solve both forward and inverse problems [1]. By seemlessly integrating prior knowledge from
observational data and from Partial Differential Equations (PDEs), PINNs can make accurate predictions with small
datasets, while other traditional deep learning techniques require large amounts of data [2]. The meshless nature of
PINNs makes them suitable for geomechanics applications where data are often sparse and irregularly distributed,
while automatic differentiation [3] enables efficient computation of the derivatives needed to satisfy PDEs. PINNs
have already been successfully applied across a wide range of scientific domains [4] as an alternative to numerical
methods based on discretization schemes. In fluid mechanics, they have shown particular effectiveness [5], for exam-
ple for solving inverse problems of unsaturated groundwater flow [6]. In solid mechanics, PINNs have been employed
to solve both forward and inverse problems in elastic and plastic materials [7], as well as to predict dynamic stress
fields [8].
Despite such promising achievements, a key open challenge in data-driven material modeling is the strong depen-
dence of model performance on the amount of available data. When sufficient data cannot be obtained, one remedy
is to train Machine Learning (ML) models on smaller but more informative datasets that capture the essential
complexities of the underlying mapping, which can be facilitated by data sampling strategies [9]. Furthermore, the
location and distribution of points where PDEs are evaluated strongly influence PINN model performance. Adaptive
sampling has proven to improve PINN accuracy. For instance, Lu et al. [10] suggested adding points to the training
data showing high residuals. Nabian et al. [11] introduced an algorithm based on importance sampling to select
the residual training points that contribute the most to the loss function. Wu et al. [12] compared uniform and
non uniform sampling strategies and developed a refined selection technique to minimize the residual errors. In the
∗Corresponding author
1email: ayt34@cornell.edu
arXiv:2511.20574v1  [physics.comp-ph]  25 Nov 2025

aforementioned studies, the number of observations, i.e. the number of data points where measurements are available,
is assumed to be fixed.
However, decisions on sensor placement and measurement frequency are essential in geomechanical applications,
where acquiring data can often be cumbersome and expensive. Optimal Experimental Design (OED) refers to the
computational frameworks developed to make those decisions [13]. In particular, Active Learning (AL), which is
a term that emerged from the machine learning community, relates to a class of algorithms designed to optimally
choose the data to label and focuses on learning supervised predictive models [14]. The key idea behind AL is that
a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to
choose the training data from which it learns [15]. AL algorithms sequentially select one new data point to label at
a time, within a data-pool. As a special case of OED, AL aims to improve downstream predictions by maximizing
information gain rather than by calibrating model parameters [16]. An important point is to specify the criterion
to select new data - for example, by labeling the data for which the uncertainties of the current model are the
highest [17, 18, 19, 20], by using query-by-committee techniques [21, 22], or by minimizing the loss function residual
[23, 24, 25]. In most cases, AL outperforms traditional random sampling [26].
In tunneling, inverse problems are typically ill-posed, i.e.
the existence and uniqueness of the solution may
not be guaranteed. In that context, inverse problems are challenging, because each rock constitutive relationship
yields a different model formulation, which requires a specific (and often elaborate) numerical implementation. For
instance, Sakurai and Takeuchi [27] developed a finite element formulation in which the rock behavior was assumed
linear elastic to estimate the Young’s modulus of the ground, using displacement measurements obtained at the
cavity wall (from convergence plots) and within the rock mass (from extensometers). When closed-form solutions are
available, least-squares optimization methods are suitable. For example, Lecampion et al. [28] used synthetic data
that emulates extensometer measurements to obtain the constitutive parameters of a Perzyna elasto-visco-plastic
model. Tristani et al. [29] derived an analytical solution for tunnels excavated in fractional viscoelastic plastic rocks
and implemented a least-squares optimization scheme to characterize the rock mass. As an alternative, PINNs can
be used to solve both the forward and inverse problems - for instance, to predict ground surface settlements and
estimate geotechnical parameters from settlement displacement data [30, 31, 32] or to estimate tunnel lining loads
[33, 34].
In this study, we explore the use of PINNs to reconstruct the displacement field and learn the constitutive
parameters of the rock mass, while simultaneously optimizing the placement of convergence and extensometer sensors
to enhance the model’s performance. To illustrate the approach, we focus on a host rock that exhibits an elastic
transversely isotropic behavior. For such behaviors, it is worth emphasizing that analytical solutions to the problem
of a cavity subjected to a biaxial far field stress have been derived in the past from the complex variable theory
[35].
The displacement field was expressed in the form of integer series for unlined [36] and lined [37] circular
tunnels, and for tunnels of any cross-sectional shape (using conformal mapping techniques) [38]. Those analytical
solutions assume that the constitutive parameters are known. Back-analysis approaches have also been proposed.
For the cavity expansion problem, Kolymbas et al. [39] suggested an approximate solution to estimate the material
constants, while Vu et al. [40] developed a semi-analytical solution to model displacement and stress fields around a
tunnel section and to calculate constitutive parameters of the rock from convergence data. The data-driven approach
developed in the present study leverages the mesh-free nature of PINNs to solve the inverse problem from scattered
and noisy displacement measurements. To efficiently acquire data, an active learning strategy is implemented to
sequentially query and label the measurements that are expected to most significantly improve model performance.
In particular, the PINN model is trained using dropout as a stochastic regularization technique [41], which enables
uncertainty quantification at inference time by performing Monte Carlo dropout [42]. In addition, a synthetic dataset
is generated using the high-fidelity finite element code MOOSE [43].
The remainder of this manuscript is organized as follows. Section 2 presents some theoretical background on
PINNs and introduces the excavation problem in transversely isotropic elastic grounds in the form of non-dimensional
equations. Section 3 details the model and formulates the active learning strategy developed in this study to best
acquire the training data. Section 4 discusses the results obtained from a case study with several querying strategies,
and demonstrates the effectiveness of the proposed approach using small, scattered, and noisy data. Finally, in
Section 5, conclusions are drawn, and perspectives for future research are presented.
2

2. Theoretical background
2.1. Physics-informed neural networks (PINNs)
Let us consider the following generic PDE expressed as:
(
D [u (x) ; λ] = 0, x ∈Ω,
B [u (x) ; λ] = 0, x ∈∂Ω
(1)
Here, D [·] denotes a linear differential operator acting on u, the solution of the differential equation; B [·] are
boundary operators, λ denotes the PDE parameters, x is a position input vector in the spatial domain Ω⊆Rd
having boundary ∂Ω, where d ∈{1, 2, 3}.
The PINN aims to approximate the true solution u(x) with a deep feedforward neural network N(x; θ), where
θ = {w, b} denotes the trainable weights and biases. The network N(x; θ) maps the spatial coordinates x to an
approximation of the solution via a sequence of nested layer-wise transformations:
zl = f(wlzl−1 + bl)
l = 1, . . . , L
(2)
where z0 = x and zL = N(x; θ) ≈u(x) and where f corresponds to the activation function which carries the
non-linearity of the system. Usually, f = tanh for all layers except the last one, which is linear.
Neural networks are trained to minimize a loss function L. For PINNs, this loss function is composed of three
terms, as follows:
L(θ) = wpdeLpde(θ) + wbcLbc(θ) + wdataLdata(θ)
(3)
with:

























Lpde(θ) =
1
NΩ
NΩ
X
i=1
∥D [N(xi; θ); λ]∥2
Lbc(θ) =
1
N∂Ω
N∂Ω
X
i=1
∥B [N(xi; θ); λ]∥2
Ldata(θ) =
1
Ndata
Ndata
X
i=1
∥N(xi; θ) −ˆyi∥2
(4)
where Lpde(θ) represents the residual of the PINN solution, i.e., the extent to which the predicted solution fails to
satisfy the governing PDEs that encode the physical constraints; Lbc(θ) represents the deviation from the boundary
conditions (BCs); and Ldata(θ) represents the discrepancy between the PINN-predicted solution and the available
data at observation points.
In equation 4, {xi}NΩ
i=1 are residual points (also called collocation points) located in the domain Ω; {xi}N∂Ω
i=1 are
boundary points; and {xi, ˆyi}Ndata
i=1
are measurements. NΩ, N∂Ω, Ndata denote the total number of collocation points,
boundary points, and data points, respectively. wpde, wbc and wdata are the weights associated with the PDE loss,
BC loss, and data loss, respectively.
The network parameters θ are tuned by minimizing the total training loss L(θ).
This is done during back-
propagation at machine precision, i.e., the highest numerical precision allowed by the floating-point representation
on the hardware (typically float32 or float64), by leveraging Automatic Differentiation [44] and using Leibniz chain
rule. In this study, Automatic Differentiation is implemented in PyTorch [45]. Different optimizers can be chosen:
stochastic optimization schemes such as Adam [44] or Nadam, [46] or determinist optimizers such as L-BFGS [47],
which have also been widely used to train PINNs.
A hybrid approach combining both optimizers can be used
effectively to improve convergence [12]. In this study, the stochastic Nadam optimizer is used. Note that the weights
wdata, wres, wbc are usually set constant prior to minimizing the total loss, which is the case in this work. They can
alternatively be adapted during training, as proposed in the Self-Adaptive PINN method [48].
PINNs can be used to solve both forward and inverse problems. In the forward problem, the governing PDEs,
material parameters, and boundary conditions are known. The objective is to compute the solution field u. The
PINN is trained to minimize the PDE residuals and to satisfy boundary conditions, and no observational data is
required. In the inverse problem, some coefficients of the PDEs are unknown. The objective is to identify those
unknown parameters from scattered and noisy measurements. The PINN is therefore trained to satisfy both the
PDE/BC constraints and to minimize the data loss. In practice, the unknown parameters are treated as additional
trainable variables within the parameter set θ. Inverse problems are often ill-posed in the sense of Hadamard: a
solution may not exist, may not be unique, or may be highly sensitive to the observational data. In the following,
we focus on solving the inverse problem.
3

2.2. Physical governing equations
Rocks naturally exhibit heterogeneities. When the scale of those heterogeneities is similar to that of the tunnel
diameter, the host rock cannot be considered as a continuum, and field discontinuities must be embedded in the
formulation of the problem. Here, we study instead anisotropic rock formations that present heterogeneities that are
at least two orders of magnitude smaller than the dimensions of the tunnel cross-section. We focus on sedimentary
or metamorphic rocks, which develop a layered structure (through deposition of grains and particles in successive
layers in the case of sedimentary rocks, or foliation during metamorphism in metamorphic rocks). This anisotropy
can be accounted for through a transverse isotropic elastic behavior model to describe the response of the tunnel
[36, 49, 39, 38]. In this case, the elasticity tensor depends on 5 independent parameters [50].
In the following, we consider a deep circular tunnel excavated in a transversely isotropic elastic ground. We define
the x-axis and z-axis as the axes that define the plane of isotropy of the host rock, as shown in Figure 1a. In the (x,y)
plane, the angle formed between the bedding direction (x-axis) and the horizontal is noted β. The tunnel under study
is excavated along the z-axis, which corresponds to the most unfavorable case, since the maximum deformations are
expected to occur in the y-direction, which is the direction normal to the bedding plane. Two-dimensional plane-strain
conditions are assumed. The in-situ stress state in a plane orthogonal to the z-axis is assumed to be homogeneous
and anisotropic. Taking the horizontal and vertical axes as first and second directions of space, respectively, the
in-situ stress state is defined as:
σi =
Kσ0
0
0
σ0

(5)
where K is the ratio between the horizontal and vertical stress (lateral earth pressure coefficient) and σ0 is the far
field vertical stress. A rotation by 2β in the stress space (β in the geometric space) yields the in-situ stress state
components in the (x,y) coordinate system (see Figure 1b), as follows:











σ0
h = σ0
2 [1 + K −(1 −K) cos(2β)]
σ0
v = σ0
2 [1 + K + (1 −K) cos(2β)]
τ 0
vh = σ0
2 (K −1) sin(2β)
(6)
R
Kσ0
σ0
x
y
β
β
(a) In-situ loading conditions
σ0
v
R
y
τ 0
vh
σ0
h
τ 0
vh
x
(b) Loading after rotation
Figure 1: Tunnel with circular cross-section or radius R excavated in transversely isotropic elastic rocks.
The total deformation εt is decomposed into two parts: the so-called eigenstrain εi, which corresponds to the
initial deformation of the ground due to the in-situ state of stress; and ε, the deformation of the ground due to
the excavation of the tunnel, which redistributes stresses around the cavity. The excavation deformation ε is thus
calculated as:
ε = εt −εi
(7)
4

Noting S the compliance tensor of the transverse anisotropic rock mass, the stress and strain fields satisfy:
εi = S : σi
(8)
and
εt = S : σ
(9)
where σ is the Cauchy stress tensor which corresponds to the stress field around the cavity.
In plane-strain, using Voigt notation for equation 7 yields:


εxx
εyy
γxy

=









1 −ν2
h
Eh
−νvh(1 + νh)
Ev
0
−νvh(1 + νh)
Ev
1 −νvhνhv
Ev
0
0
0
1
Gvh











σxx −σ0
h
σyy −σ0
v
τxy −τ 0
vh


(10)
where Eh and Ev denote the Young’s moduli in the plane of isotropy and in the direction normal to that plane,
respectively. νh is the Poisson’s ratio for deformation within the plane of isotropy. νhv and νvh represent, respectively,
the Poisson’s ratio for the effect of in-plane stress on the strain normal to the plane, and the Poisson’s ratio for the
effect of normal stress on the strain within the plane of isotropy. These parameters are related by the reciprocity
condition:
νhvEv = νvhEh.
(11)
Lastly, Gvh is the shear modulus in any plane containing the normal direction to the plane of isotropy. For small
strains, the strain tensor is given by:
ε = 1
2

∇u + (∇u)T 
,
(12)
where u(x) = (ux, uy) is the displacement vector at position x. In the absence of body forces, the equilibrium
equation is:
∇· σ = 0
(13)
The boundary condition at the tunnel wall is:
σ · n = 0
(14)
where n is the normal vector at the cavity wall. Note that this condition corresponds to the case of a tunnel that is
fully excavated.
Far away from the cavity, the perturbation induced by the excavation vanishes. Thus, the displacement field
satisfies the far-field boundary condition:
lim
∥x∥→∞u(x) = 0.
(15)
2.3. Non-dimensionalization
To avoid vanishing or exploding gradients during back-propagation, it is good practice to scale input and output
variables. In the following, the domain is bounded by the tunnel and the locations where measurements are collected.
We note R the radius of the tunnel section, and L the length of any extensometer that could be placed around the
cavity to collect measurements. As suggested in [33], we introduce the following dimensionless variables:
˜x = x
la
,
˜y = y
la
,
˜ux = ux
ua
˜uy = uy
ua
(16)
where:
la = R + L,
ua = 10−1m,
and
σa = σ0
la
ua
.
(17)
This normalization serves two purposes: it provides a dimensionless formulation of the governing equations and
ensures that the input and output variables remain within numerically convenient ranges during training. Note that
the scaling coefficient ua was set to 10−1 m because the maximum displacement expected from the numerical analysis
does not exceed 10−1 m.
5

Inserting the dimensionless variables defined in equation 16 into the equilibrium equation 13 yields the following
expanded relationships:









D
"
νhv(1 + νh) ∂2˜uy
∂˜x∂˜y +
 ˜Eh
˜Ev
−ν2
hv
!
∂2˜ux
∂˜x2
#
+ ˜Gvh
∂2˜ux
∂˜y2 + ∂2˜uy
∂˜x∂˜y

= 0
D

νhv(1 + νh) ∂2˜ux
∂˜x∂˜y + (1 −ν2
h)∂2˜uy
∂˜y2

+ ˜Gvh
 ∂2˜ux
∂˜x∂˜y + ∂2˜uy
∂˜x2

= 0
(18)
and inserting the dimensionless variables into the boundary equation 14, one obtains:









˜xD
"
νhv(1 + νh)∂˜uy
∂˜y +
 ˜Eh
˜Ev
−ν2
hv
!
∂˜ux
∂˜x
#
+ ˜y ˜Gvh
∂˜ux
∂˜y + ∂˜uy
∂˜x

+ ˜x˜σ0
h + ˜y˜τ 0
vh = 0
˜yD

νhv(1 + νh)∂˜ux
∂˜x + (1 −ν2
h)∂˜uy
∂˜y

+ ˜x ˜Gvh
∂˜ux
∂˜y + ∂˜uy
∂˜x

+ ˜y˜σ0
v + ˜x˜τ 0
vh = 0
(19)
where:
D = −
˜Eh
2ν2
hv(1 + νh) −
˜Eh
˜Ev
(1 −ν2
h)
(20)
with:
˜Eh = Eh
σa
,
˜Ev = Ev
σa
,
˜Gvh = Gvh
σa
,
˜σ0
h = σ0
h
σ0
˜σ0
v = σ0
v
σ0
˜τ 0
vh = τ 0
vh
σ0
(21)
3. Proposed approach: active learning to optimally place new sensors
3.1. Problem setup
During tunneling, assessing the response of the rock mass to excavation is essential for ensuring safety. Monitoring
the ground behavior can be carried out by installing extensometers and convergence instruments around the tunnel
wall. However, this process may be both costly and time-consuming, and the placement of sensors is often guided
by empirical or observational approaches. Fundamental questions naturally arise: How many measurements are
necessary? Where should the sensors be placed? And, given the available measurements, can we reconstruct the
displacement field and identify the constitutive parameters of the rock mass? Addressing these questions would not
only improve our understanding of in-situ phenomena, but also contribute to more efficient and reliable tunnel design.
As a preliminary step towards this goal, we leverage PINNs to propose an automatic, data-efficient framework based
on their predictive capabilities. Specifically, we aim to reconstruct the displacement field around the tunnel cavity
while simultaneously solving the inverse problem, that is, determining the rock mass parameters, the horizontal-to-
vertical stress ratio, and the orientation of the bedding planes, using the smallest possible amount of measurement
data.
Extensometers consist of radial cables installed around the tunnel cavity. They measure displacements at anchored
points that are typically located every 3–4 meters along a radial cable starting from the tunnel wall. Because the
measurement reflects the relative displacement between the anchored points and the extensometer head, it may
represent either a relative or absolute displacement depending on whether the head itself moves. In this study, we
assume that the extensometer head remains fixed, so that the recorded displacements correspond to the true ground
displacements. In addition to extensometer data, tunnel convergence is usually monitored.
Convergence measurements correspond to the relative displacement between two opposite points of the tunnel
wall at a given section. They can also be obtained as the three-dimensional displacements of individual wall points
measured by all the convergence stations.
Convergence data provide valuable information about tunnel closure,
allowing for a rapid assessment of ground behavior and for design updates if needed [51]. Because convergence is the
easiest measurement to obtain, it is commonly monitored during and after excavation. Convergence and extensometer
measurements at the wall have been shown to display similar trends [52]. Therefore, in this work, it is assumed that
both convergence and extensometer measurements are equivalent indicators of tunnel closure.
6

3.2. Synthetic dataset
In the following, the measurements are assumed to be drawn from a pool of observations P = {(xi, yi)}Ndata
i=1 .
These observations correspond to displacement measurements collected using field sensors. Each sensor, either an
extensometer or a convergence point, provides an observation subset Si ⊂P. When referred to as an extensometer,
a sensor is represented by seven measurement points aligned along a radial direction within the domain and starting
at the tunnel wall, with anchors placed every 4 m. Consecutive extensometers are spaced by an angle of 10◦. When
referred to as a convergence point, a sensor is a single measurement point located at the boundary of the domain,
i.e., at the tunnel wall. For each sensor in the pool, in addition to computing the data loss term, the PINN also
evaluates the PDE and BC residuals.
Additionally, a second grid G = {(xi, yi)}NΩ
i=1 ∪{(xi, yi)}N∂Ω
i=1 of size Nr × Nθ = 10 × 36 is defined, consisting of
uniformly distributed collocation and boundary points. G is independent of the pool of observations P and is used
throughout the training to enforce the PDE and BC in the domain. This ensures training stability and helps the
model to accurately map the true solution around the tunnel. Therefore, the training set T consists of: (i) the n
sensors that the model has already queried {Si}n
i=1, and (ii) the grid G containing the collocation and boundary
points such that T = {Si}n
i=1 ∪G. The dataset is illustrated in Figure 2.
1.0
0.5
0.0
0.5
1.0
x
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
y
1.0
0.5
0.0
0.5
1.0
x
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
y
Tunnel wall
Boundary
Interior
Measurements
Figure 2: Training set. Left: pool of measurements P from which the model can query data, including both extensometers and convergence
points. Right: grid of collocation and boundary points G used to solve the PDE and BC in the domain.
The domain is scaled as described in Section 2, so that all input coordinates and output displacements lie within
the range [−1, 1]. Note that since we are solving an inverse problem, the displacement measurements collected near
the cavity act as Dirichlet-type constraints on the solution. These local observations help constrain the model to
identify the constitutive parameters within the region of interest. Points located far away from the cavity, which
would correspond to prescribed Dirichlet conditions at the far-field, are not included in the dataset, so that the
learning process focuses on the area where observations are available.
Lastly, model performance is evaluated on the full domain and along the boundary using a polar grid with Nr = 50
radial points and Nθ = 200 angular points, both uniformly distributed, totaling 10,000 evaluation points.
The reference dataset is generated using the high-fidelity finite element solver MOOSE. The computational
domain is discretized using a mesh of 10,274 triangular linear elements, with local grid refinement near the tunnel
wall. Collocation points are extracted using the LineValueSampler utility in MOOSE, which samples field values
along predefined lines and interpolates when the target points do not coincide exactly with mesh nodes.
3.3. PINN Model
To improve training stability and accelerate convergence [7], independent neural networks are defined to predict
each output variable, namely ux and uy. In all experiments, the hyperbolic tangent (tanh) is used as the activation
7

function. Each neural network consists of 10 hidden layers, with 40 neurons per layer. The displacement field is
therefore approximated as:
ux ≈Nux(x; θ)
uy ≈Nuy(x; θ)
(22)
where Nux(x; θ) and Nuy(x; θ) correspond to neural networks. This architecture is illustrated in Figure 3.
...
...
· · ·
· · ·
· · ·
· · ·
...
...
· · ·
· · ·
· · ·
· · ·
x
y
ux
uy
λ
L(θ)
Figure 3: PINN architecture used in this study.
Independent neural networks, Nux(x; θ) and Nuy(x; θ), are defined to predict the
displacement components ux and uy, respectively. Each network takes the spatial coordinates (x, y) as input features.
During training, the parameters of the PINN are optimized by the stochastic Nadam optimizer [46], which has
been widely employed in the machine learning community. At each new iteration step, the model parameters, i.e.
weights, biases, and PDE parameters, are initialized as those obtained at the last previous step.
However, the
optimizer is reinitialized, which means that its internal states are not retained across each step. This process avoids
suboptimal solutions that correspond to local minima of the loss functions.
Equal weights can be assigned to each term of the loss function [7]. However, we observed that using different
weights improves the performance of the PINN, a result that has also been reported in previous studies [53, 54].
After a series of trial-and-error experiments, we determined the following weighting scheme for our model:
wpde = 1
wbc = 10
wdata = 100
(23)
3.4. Sequential learning approach
In most machine learning approaches, the model passively relies on a fixed training dataset. In contrast, active
learning enables the model to strategically select the most informative data points or measurements from which to
learn. The fundamental objective is to achieve maximal predictive performance while minimizing the amount of data
required, making active learning closely aligned with the principles of optimal experimental design in statistics [15].
In this work, dropout is used to train the physics-informed machine learning model. The advantages of using
dropout are twofold: i) by randomly deactivating neurons during training, it enables the quantification of epistemic
uncertainty through multiple inference passes, a process known as Monte Carlo dropout, which approximates Bayesian
inference [42]; and ii) it reduces overfitting to the training data, thereby improving model reliability [41].
A fixed dropout ratio equal to 5% is used for the first two layers. Monte Carlo dropout is then performed to
measure epistemic uncertainties. The model selects the sensors that have the most uncertain predictions, following
a logic similar to that presented in [20, 17]. Algorithm 1 explains how a new sensor is selected.
An Active Learning (AL) strategy is then designed to efficiently query the most informative data points within
the domain and thereby improve the robustness and accuracy of the machine learning models, as described in
Algorithm 2. Data points are queried sequentially, and the PINNs are trained iteratively in a stage-wise manner.
New measurements are labeled from the pool of observations based on the epistemic uncertainties of the PINN,
evaluated at the end of each training stage.
8

Algorithm 1 Query Strategy using Monte Carlo Dropout
1: Input: trained model, candidate sensor set {Si}N
i=1, current training set T
2: for each candidate sensor Si do
3:
Perform nMC stochastic forward passes with dropout enabled N (k)
ux (Si), N (k)
uy (Si),
k = 1, . . . , nMC
4:
Compute:
• mean prediction ¯ux, ¯uy of ux, uy over the nMC passes
• variance σ2
ux, σ2
uy of ux, uy over the nMC passes
5:
Aggregate uncertainty of the sensor: ¯σ2(Si) = mean of (σ2
ux + σ2
uy) over all points in Si
6: end for
7: Select highest-uncertainty sensor: S⋆= arg maxSi ¯σ2(Si)
8: Update training set: T ←T ∪S⋆
9: return updated training set T
Algorithm 2 Active Learning
1: Initialization:
2: Select two initial extensometers S1, S2 from the pool of measurements P such that T = S1 ∪S2.
3: Initialize optimizer
4: Train the PINN for N epochs computing Lpde, Lbc, and Ldata.
5: Add the grid G: T ←T ∪G
6: Initialize optimizer
7: Train the PINN for N epochs computing Lpde, Lbc, and Ldata.
8: repeat
9:
Initialize optimizer
10:
Query next sensor as describe in Algorithm 1
11:
Train the PINN for N epochs computing Lpde, Lbc, and Ldata.
12:
Update the training set: T ←T ∪S.
13: until the maximum allowed number of sensor is reached.
Note that in Algorithm 2, the model is first initialized using two extensometers before the full grid is added. This
strategy ensured that the model converged towards the true solution. One extensometer is placed at the crown and
another on the left side of the tunnel, providing the initial set of observational data. This choice is motivated by
the fact that these locations typically exhibit the most significant deformations in tunneling, making them natural
starting points for model calibration. Additional observational data are then progressively incorporated when the
corresponding measurement points are selected by AL.
At each step, the model can query one extensometer or one convergence sensor. We study two possibilities: after
initialization, the model can query i) only extensometer measurements or ii) only convergence measurements. At each
step, when new sensors are queried, the corresponding collocation, boundary, and data points are simultaneously
added to the training set. This ensures that the data, PDE, and boundary condition loss terms are all evaluated
for the newly included samples during subsequent training. The proposed algorithms are greedy in nature, meaning
that they do not guarantee convergence toward a global optimum. Nevertheless, increasing the number of accurate
observations generally provides more information to the model, which helps constrain the solution space and solve
the inverse problem. Figure 4 illustrates the active learning procedure presented in Algorithm 2.
4. Results
To evaluate the accuracy of the predicted displacement field ˆu in reference to the true displacement field u, we
compute the relative L2 error over the test set:
∥ˆu −u∥2
∥u∥2
.
For the inverse problem, the accuracy of the estimated rock mass parameter ˆλ is quantified using the relative
parameter error:
|ˆλ −λ|
|λ|
.
9

20
10
0
10
20
x (m)
20
10
0
10
20
y (m)
0.1
1.0
1.9
2.8
3.7
4.6
5.5
6.4
7.3
8.2
×10
5
(a) Querying extensometers, Step 2
20
10
0
10
20
x (m)
20
10
0
10
20
y (m)
0.15
1.35
2.55
3.75
4.95
6.15
7.35
8.55
9.75
×10
5
(b) Querying extensometers, Step 3
20
10
0
10
20
x (m)
20
10
0
10
20
y (m)
0.1
1.0
1.9
2.8
3.7
4.6
5.5
6.4
7.3
8.2
×10
5
(c) Querying convergences, Step 2
20
10
0
10
20
x (m)
20
10
0
10
20
y (m)
0.015
0.150
0.285
0.420
0.555
0.690
0.825
0.960
1.095
1.230
×10
4
(d) Querying convergences, Step 3
Figure 4: Sequential active learning process where sensors are actively selected by the model and added to the training set. The color
scale represents the total epistemic uncertainty σ2 = σ2
ux + σ2
uy. Current sensors are shown in white, and the next sensor to be queried
is highlighted in black. (a)-(b) Active learning when only extensometer observation points are added sequentially. (c)-(d) Active learning
when only convergence observation points are added sequentially.
where λ denotes a true parameter.
To account for randomness arising from sampling, network initialization, and the optimization process, each
experiment is repeated 10 times, and the mean and standard deviation of the errors are reported.
4.1. Case study
In anisotropic grounds, predicting the response of deep tunnels can be particularly challenging. Accurate predic-
tions require knowledge of the constitutive parameters of the rock, which remains difficult to obtain, as laboratory
and field test results often differ due to scaling effects [55, 56, 57]. A key advantage of the methodology proposed in
this study is its ability to directly perform back-analysis using data collected at the tunnel scale.
To illustrate the methodology, we aim to simultaneously reconstruct the displacement field and solve the inverse
problem of back-calculating rock properties around the Saint-Martin-la-Porte access gallery, located in Savoie, France.
For context, the Saint-Martin-la-Porte access gallery was excavated between 2003 and 2010. During excavation, the
rock mass displayed pronounced anisotropic behavior, primarily associated with the Carboniferous formation (also
known as the Productive Houillier).
In the following, we generate the synthetic dataset using the parameters obtained by Vu et al. [40]: the elastic
parameters are Eh = 620 MPa, Ev = 340 MPa, Gvh = 200 MPa, νh = 0.12, and νhv = 0.2; the vertical stress is
assumed to be 5 MPa, with a lateral pressure coefficient K = 0.75; and the bedding plane angle is equal to β = 45◦.
Here, we aim to simultaneously reconstruct the displacement field and solve the inverse problem of inferring Eh, Ev,
Gvh, β, and K. It is assumed that the tunnel is fully excavated, such that the radial stress at the wall is zero.
10

4.2. Reconstruction of the displacement field
We first compare the active learning strategy based on uncertainty-driven sampling against a random selection
method. The evolution of the relative error, evaluated on the test set, is shown in Figure 5. Two querying methods are
compared: sampling from extensormeter data only (Figure 5a) and sampling from convergence data only (Figure 5b).
Both the active learning and random selection procedures are repeated at least 10 times using different random seeds
to measure the variability due to initialization of the model parameters (weights and biases).
As shown in Figure 5, selecting measurements based on the highest estimated uncertainty consistently improves
model performance compared to random sampling for both querying strategies.
The performance gain is most
pronounced when only a few measurements are available, with an improvement of 6–7% using three extensometers and
2–3% using four extensometers (Figure 5a). This effect is even more significant when only convergence measurements
are queried, as illustrated in Figure 5b. In summary, for a given query strategy (extensometer only or convergence
only), the difference between random and active learning selection decreases as more measurements are included,
since additional data help the model reconstruct the displacement field in regions with the largest uncertainties. This
confirms the usefulness of AL in cases where observation data are scarce.
2
3
4
5
6
7
Number of sensors
101
Test relative error (%)
Active Learning
Random
(a) Querying only extensometers.
2
3
4
5
6
7
Number of sensors
101
2 × 101
3 × 101
4 × 101
Test relative error (%)
Active Learning
Random
(b) Querying only convergence measurements.
Figure 5: Comparison of relative test errors obtained using active learning versus random selection for different types of sensors. The
solid lines correspond to the mean values of the errors, and the shaded regions represent the associated standard deviations.
Figure 6 compares the model performance on the test set, which is divided into four concentric zones around
the tunnel wall. Zone 1 corresponds to points within the interval [R, 2R], Zone 2 to [2R, 3R], Zone 3 to [3R, 4R],
and Zone 4 to [4R, 5R]. Figures 6a and 6b show the model performance for the horizontal and vertical displacement
components, respectively, when only extensometer data are queried. Similarly, Figures 6c and 6d present the results
for the horizontal and vertical displacement components when only convergence data are queried. For both querying
strategies, the active learning selection method enables the model to progressively improve its performance as more
measurements are queried. Overall, querying extensometer data yields better results compared to querying only
convergence data.
In Zone 1, both querying strategies achieve similar performance. This suggests that it is possible in practice
to reconstruct the displacement field around the tunnel with high accuracy up to one tunnel radius, with only
two extensometers (those used for initialization at the crown and at the waist) and convergence data. With three
extensometers, the error is approximately 4–5% for uy, while with the two initial extensometer data and one queried
convergence, the error on uy is 7–8%. These errors gradually decrease to 3% and 5%, respectively, when five sensors
in total are used. In Zones 2, 3, and 4, querying extensometer data yields better accuracy than querying convergence
data: errors generally remain below 10% when using three extensometers and below 6% with five extensometers. By
contrast, querying only convergence data does not sufficiently reduce the prediction error in Zones 2, 3, and 4, which
are located farther from the tunnel wall. This outcome was expected, as no additional measurements from these
zones are included in the training set.
11

2
3
4
5
6
7
Number of sensors
101
102
Test relative error (%)
Zone 1
Zone 2
Zone 3
Zone 4
(a) ux querying extensometers
2
3
4
5
6
7
Number of sensors
101
102
Test relative error (%)
(b) uy querying extensometers
2
3
4
5
6
7
Number of sensors
101
102
Test relative error (%)
Zone 1
Zone 2
Zone 3
Zone 4
(c) ux querying convergences
2
3
4
5
6
7
Number of sensors
101
Test relative error (%)
(d) uy querying convergences
Figure 6: Mean relative test error evaluated in distinct zones. Zone 1: [R, 2R], Zone 2: [2R, 3R], Zone 3: [3R, 4R], Zone 4: [4R, 5R]
Figure 7 shows the true displacement field generated using the finite element software MOOSE. Figures 8 and
9 illustrate the model predictions when only extensometer data are queried and when only convergence data are
queried, respectively. The PINNs are trained using the sequential active learning procedures described in Section 3.
The corresponding displacement predictions along the tunnel wall are shown for several training steps, after which
no further changes are observed. To remain consistent with the available data, the predictions are restricted to a
maximum radial distance of d = R + L, beyond which no measurements are assumed to be available.
20
0
20
x (m)
20
10
0
10
20
y (m)
4
3
2
1
0
1
2
3
4
×10
2 (m)
(a) ux true
20
0
20
x (m)
20
10
0
10
20
y (m)
6.0
4.5
3.0
1.5
0.0
1.5
3.0
4.5
6.0
×10
2 (m)
(b) uy true
Figure 7: Computations made using the MOOSE framework.
12

20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
3.2
2.4
1.6
0.8
0.0
0.8
1.6
2.4
3.2
4.0
×10
2 (m)
(a) 2 extensometers: ux
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
4
3
2
1
0
1
2
3
4
×10
2 (m)
(b) 3 extensometers: ux
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
4
3
2
1
0
1
2
3
4
×10
2 (m)
(c) 4 extensometers: ux
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
6.25
5.00
3.75
2.50
1.25
0.00
1.25
2.50
3.75
5.00
×10
2 (m)
(d) 2 extensometers: uy
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
6.0
4.5
3.0
1.5
0.0
1.5
3.0
4.5
6.0
×10
2 (m)
(e) 3 extensometers: uy
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
6.0
4.5
3.0
1.5
0.0
1.5
3.0
4.5
6.0
×10
2 (m)
(f) 4 extensometers: uy
Figure 8: Sequential active training and progressive reconstruction of the displacement field when querying only extensometer data.
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
4
3
2
1
0
1
2
3
4
×10
2 (m)
(a) 1 convergence queried: ux
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
4
3
2
1
0
1
2
3
4
×10
2 (m)
(b) 2 convergences queried: ux
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
4
3
2
1
0
1
2
3
4
×10
2 (m)
(c) 3 convergences queried: ux
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
6.25
5.00
3.75
2.50
1.25
0.00
1.25
2.50
3.75
5.00
×10
2 (m)
(d) 1 convergence queried: uy
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
6.0
4.5
3.0
1.5
0.0
1.5
3.0
4.5
6.0
×10
2 (m)
(e) 2 convergences queried: uy
20
10 0
10
20
x (m)
20
10
0
10
20
y (m)
6.0
4.5
3.0
1.5
0.0
1.5
3.0
4.5
6.0
×10
2 (m)
(f) 3 convergences queried: uy
Figure 9: Sequential active training and progressive reconstruction of the displacement field when querying only convergence data.
In Figure 8, the displacement field is partially reconstructed by the model using only two extensometers. With
13

three or more extensometers, the displacement field is fully reconstructed for both horizontal and vertical components.
Overall, the accuracy of the predicted displacement fields, both near the cavity and farther away, improves as
additional extensometers are included. Similarly, in Figure 9, a progressive reconstruction of the displacement field
is observed when only convergence data are queried. Using two extensometers and one convergence sensors, the
model already reconstructs both displacement components around the tunnel with good accuracy. When two or
more convergence measurements are queried, the model accurately reconstructs the displacement field close to the
tunnel wall, although the predictions are less accurate in Zones 2, 3, and 4, which are farther from the wall.
These results indicate that the proposed active learning strategy effectively enables (i) the selection of the most
informative measurements to enhance model performance, and (ii) the progressive reconstruction of the displacement
field around a deep circular tunnel. However, the current algorithm is inherently greedy, as it selects only the next
measurement point without considering the overall arrangement of sensors. As a result, it does not optimize a global
sensing strategy and may miss combinations of points that could collectively provide more informative or efficient
coverage of the domain. This direction will be further explored in future work. Here, satisfactory initialization was
achieved using two extensometers, as employing fewer measurements often resulted in model divergence. It would
be interesting to optimize the initialization of data sampling in future research.
4.3. Inverse analysis
We now evaluate the results of the inverse analysis. Figure 10 presents the mean relative errors obtained with
Algorithm 2 when querying either extensometer data or convergence data exclusively. Overall, it can be observed
that the sought parameters, i.e. Eh, Ev, Gvh, K, and β, are successfully determined under both querying strategies.
When querying extensometer data, Eh, Gvh, and K are the most accurately estimated parameters, with errors
below 8% using two extensometers and below 5% with three. In contrast, the errors for Ev and β remain under 15%
with two and three extensometers, and decrease to below 6% and 5%, respectively, once four or more extensometers
are used.
A similar trend is observed when querying only convergence data. The errors for Eh, Gvh, and K fall below 5%
after three convergence measurements are added to the data pool, and continue to decrease thereafter. The error
for Ev remains below 10% and gradually reduces to approximately 6% with five convergence measurements. The
error associated with β remains between 10% and 15% across all steps. This result can be attributed to the fact that
convergence data are concentrated along the tunnel wall, which limits the model’s ability to accurately optimize β,
a parameter primarily influencing the displacement orientation rather than local displacement magnitudes.
2
3
4
5
6
7
Number of sensors
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Relative error (%)
Eh
Ev
Gvh
K
(a) Querying only extensometers.
2
3
4
5
6
7
Number of sensors
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0
Relative error (%)
(b) Querying only convergences.
Figure 10: Constitutive parameters obtained by the model after optimization step after step. The curves correspond to mean results
obtained over 10 different seeds.
Figure 11 shows the values of the parameters optimized by the PINN as a function of the number of training
epochs when using the active learning strategy with extensometer data only.
Step 1 corresponds to the model
initialization. In Step 2, the grid G is added to the training set. At each step, optimization begins with a reinitialized
optimizer having a learning rate of lr = 10−4, and the full batch of training data, that is, all available training points
in T , is provided to the model. Early stopping is applied, resulting in a different number of epochs for each stage.
14

0.0
0.2
0.4
0.6
0.8
1.0
Epochs (cumulative)
×105
0.6
0.8
1.0
1.2
1.4
Parameter value
1e9
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
Eh
Eh (true)
(a)
0.0
0.2
0.4
0.6
0.8
1.0
Epochs (cumulative)
×105
0.4
0.6
0.8
1.0
1.2
1.4
Parameter value
1e9
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
Ev
Ev (true)
(b)
0.0
0.2
0.4
0.6
0.8
1.0
Epochs (cumulative)
×105
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Parameter value
1e9
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
Gvh
Gvh (true)
(c)
0.0
0.2
0.4
0.6
0.8
1.0
Epochs (cumulative)
×105
0
2
4
6
8
Parameter value
1e
1
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
K
K (true)
(d)
0.0
0.2
0.4
0.6
0.8
1.0
Epochs (cumulative)
×105
0.6
0.7
0.8
0.9
1.0
1.1
Parameter value
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
Step 7
 (true)
(e)
Figure 11: Sequential optimization of parameters using active training.
Vertical dashed lines denote the stages at which additional
measurements are queried and incorporated into the training set.
4.4. Model behavior using noisy data
We now analyze the sensitivity of the model to noisy data. It is an important problem in geomechanics and
structural mechanics, because observations or measurements collected in situ are often noisy. To assess the robustness
of the model, synthetic heteroscedastic Gaussian noise was added to the displacement data. The noisy data ˜u were
generated as:
˜u = u + η,
η ∼N
 0, α2u2
,
(24)
15

where u denotes the true displacement vector and α controls the noise amplitude. In this work, α was set to 0.05,
0.10, and 0.15 to simulate noise levels of 5%, 10%, and 15%, respectively.
Figure 12 shows the relative displacement errors evaluated on the test set for different noise magnitudes. For
noise levels of 5% and 10%, the model still achieves a good accuracy for both querying strategies.
Using three
extensometers, the error is approximately equal to 10% (respectively, 20%) for a noise level of 5% (respectively,
10%). With five extensometers, this error drops to 7% (respectively, 10%) for a noise level of 5% (respectively,
10%). At 15% of noise, the performance decreases, with errors around 25% at five extensometers. Furthermore, the
standard deviation of the predictions increases, indicating a reduced confidence. Using two extensometer and one
convergence measurements, the error in the whole domain is about 25 % for 5% and 10% of noise magnitudes. This
error decreases to approximately 17-20% when three convergence measurements are queried. At 15% of noise, the
error is around 30% when three convergences are queried. For both querying strategies, a noise level above 15%
considerably decreases model performance, at which point, the model can no longer converge. Further investigations
are needed to treat inverse problems with high levels of noise - for example, through a Bayesian perspective [58].
2
3
4
5
6
7
Number of sensors
101
102
Test relative error (%)
0%
5%
10%
15%
(a) Query on extensometers only.
2
3
4
5
6
7
Number of sensors
101
102
Test relative error (%)
(b) Query on convergence only.
Figure 12: Comparison of relative error in the test set for different noise magnitudes. The solid lines correspond to the mean values of
the errors obtained over 10 repeated runs with different seeds, and the shaded regions represent the associated standard deviations.
Figure 13 presents the results of the inverse analysis for noisy measurements with amplitudes of 5%, 10%, and
15%. The model provides reasonable predictions with data that contains small to moderate levels of noise: the
overall trends are consistent with those obtained from noise-free data, although the mean relative error increases
with noise level. For 5% noise, the model achieves errors below 15% for Eh, Gvh, and K using three extensometers,
which progressively improves to approximately 5% with five extensometers (Figure 13a). The parameters Ev and
β are also correctly estimated, with errors around 17% and 25% at three extensometers, and errors of 10% at five
extensometers. At a noise level of 10%, a similar trend is observed. Errors for Eh, Gvh, and K range between 15%
and 18% for three extensometers and decrease to 13%-15% with five extensometers (Figure 13c). For Ev and β,
the errors are approximately 30% at three extensometers and reduce to around 20% and 25%, respectively, at five
extensometers. For 15% noise, optimization becomes more challenging. The relative errors for Eh, Ev, Gvh, and
K lie between 15% and 25% with five extensometers, while the error for β remains relatively high, close to 30%
(Figure 13e). When querying only convergence data, similar observations can be made although the direction of the
bedding planes β is more difficult to determine.
16

2
3
4
5
6
7
Number of sensors
0
5
10
15
20
25
30
35
Relative error (%)
Eh
Ev
Gvh
K
(a) Noise 5% querying extensometers
2
3
4
5
6
7
Number of sensors
0
5
10
15
20
25
30
35
Relative error (%)
(b) Noise 5% querying convergences
2
3
4
5
6
7
Number of sensors
10
15
20
25
30
35
40
45
Relative error (%)
Eh
Ev
Gvh
K
(c) Noise 10% querying extensometers
2
3
4
5
6
7
Number of sensors
5
10
15
20
25
30
35
40
Relative error (%)
(d) Noise 10% querying convergences
2
3
4
5
6
7
Number of sensors
10
15
20
25
30
35
40
45
50
Relative error (%)
Eh
Ev
Gvh
K
(e) Noise 15% querying extensometers
2
3
4
5
6
7
Number of sensors
5
10
15
20
25
30
35
40
45
Relative error (%)
(f) Noise 15% querying convergences
Figure 13: Results of the inverse analysis for multiple noise levels, comparing extensometer and convergence data querying strategies.
Solid lines indicate the mean errors obtained over 10 repeated runs with different seeds. Standard deviations are not represented for
clarity.
5. Conclusion
In this study, we propose a methodology to reconstruct the displacement field and characterize the rock mass sur-
rounding deep circular tunnels excavated in transversely isotropic elastic rocks. To this end, we leverage the mesh-free
17

nature of Physics-Informed Neural Networks (PINNs) to back-analyze extensometer and convergence measurements
collected during tunneling.
A key contribution of this work is the introduction of an active learning strategy based on epistemic uncertain-
ties quantified through dropout regularization. This enables the model to sequentially select the most informative
sensors among a set of extensometers and convergence instruments. The proposed approach reduces the amount of
observational data required, optimizes sensor placement, and further improves model reliability.
By incorporating equilibrium equations, anisotropic constitutive laws, and boundary conditions directly into the
loss function, the model can reconstruct the displacement field and identify the elastic parameters of the rock mass,
the horizontal in-situ stress state, and the orientation of the planes of isotropy, even when observations are limited,
scattered, or noisy.
The proposed model can support decision-making for optimal monitoring of ground behavior and for tunnel
design. The framework is computationally efficient, which makes the inverse analysis practical: each active learning
step requires only 2 to 5 minutes to generate predictions on an Apple M4 silicon chip. Future work will focus on
improved sensor-deployment strategies based on active planning computational methods. Future work will consider
time-dependent effects typical of squeezing conditions and non-circular sections.
6. Code and data availability
The code and data used in this manuscript are publicly available at the GitHub repository [59] (see also https:
//github.com/Alec-YT/AL-Tunnel).
7. Acknowledgments
The authors wish to thank Prof. Herbert Einstein from MIT for discussions on sensor positioning and Prof.
Yunan Yang from Cornell for her advice on training optimization.
References
[1] M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for
solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational
Physics, 378:686–707, February 2019. ISSN 00219991. doi:10.1016/j.jcp.2018.10.045.
[2] George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.
Physics-informed machine learning.
Nature Reviews Physics, 3(6):422–440, May 2021.
ISSN 2522-5820.
doi:10.1038/s42254-021-00314-5.
[3] Sifan Wang and Paris Perdikaris. Deep learning of free boundary and Stefan problems. Journal of Computational
Physics, 428:109914, March 2021. ISSN 0021-9991. doi:10.1016/j.jcp.2020.109914.
[4] Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and Francesco
Piccialli. Scientific Machine Learning Through Physics–Informed Neural Networks: Where we are and What’s
Next. Journal of Scientific Computing, 92(3):88, July 2022. ISSN 1573-7691. doi:10.1007/s10915-022-01939-z.
[5] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis.
Physics-informed
neural networks (PINNs) for fluid mechanics: a review. Acta Mechanica Sinica, 37(12):1727–1738, December
2021. ISSN 1614-3116. doi:10.1007/s10409-021-01148-1.
[6] Ivan
Depina,
Saket
Jain,
Sigurdur
Mar
Valsson,
and
Hrvoje
Gotovac.
Application
of
physics-
informed neural networks to inverse problems in unsaturated groundwater flow.
Georisk:
Assessment
and Management of Risk for Engineered Systems and Geohazards, 16(1):21–36, January 2022.
ISSN
1749-9518.
doi:10.1080/17499518.2021.1971251.
Number:
1 Publisher:
Taylor & Francis _eprint:
https://doi.org/10.1080/17499518.2021.1971251.
[7] Ehsan Haghighat, Maziar Raissi, Adrian Moure, Hector Gomez, and Ruben Juanes. A physics-informed deep
learning framework for inversion and surrogate modeling in solid mechanics. Computer Methods in Applied
Mechanics and Engineering, 379:113741, June 2021. ISSN 00457825. doi:10.1016/j.cma.2021.113741.
[8] Hamed Bolandi, Gautam Sreekumar, Xuyang Li, Nizar Lajnef, and Vishnu Naresh Boddeti. Physics informed
neural network for dynamic stress prediction. Applied Intelligence, 53(22):26313–26328, November 2023. ISSN
0924-669X, 1573-7497. doi:10.1007/s10489-023-04923-8.
18

[9] Jan N. Fuhg, Govinda Anantha Padmanabha, Nikolaos Bouklas, Bahador Bahmani, WaiChing Sun, Nikolaos N.
Vlassis, Moritz Flaschel, Pietro Carrara, and Laura De Lorenzis. A Review on Data-Driven Constitutive Laws
for Solids. Archives of Computational Methods in Engineering, 32(3):1841–1883, April 2025. ISSN 1886-1784.
doi:10.1007/s11831-024-10196-2.
[10] Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis.
DeepXDE: A Deep Learning Li-
brary for Solving Differential Equations.
SIAM Review, 63(1):208–228, 2021.
ISSN 0036-1445, 1095-7200.
doi:10.1137/19M1274067.
[11] Mohammad Amin Nabian, Rini Jasmine Gladstone, and Hadi Meidani. Efficient training of physics-informed
neural networks via importance sampling. Computer-Aided Civil and Infrastructure Engineering, 36(8):962–977,
August 2021. ISSN 1093-9687, 1467-8667. doi:10.1111/mice.12685. arXiv:2104.12325 [cs].
[12] Chenxi Wu, Min Zhu, Qinyang Tan, Yadhu Kartha, and Lu Lu. A comprehensive study of non-adaptive and
residual-based adaptive sampling for physics-informed neural networks. Computer Methods in Applied Mechanics
and Engineering, 403:115671, January 2023. ISSN 00457825. doi:10.1016/j.cma.2022.115671.
[13] Xun Huan, Jayanth Jagalur, and Youssef Marzouk. Optimal experimental design: Formulations and compu-
tations. Acta Numerica, 33:715–840, July 2024. ISSN 0962-4929, 1474-0508. doi:10.1017/S0962492924000023.
arXiv:2407.16212 [stat].
[14] Sanjoy Dasgupta. Two faces of active learning. Theoretical Computer Science, 412(19):1767–1781, April 2011.
ISSN 0304-3975. doi:10.1016/j.tcs.2010.12.054.
[15] Burr Settles. Active Learning Literature Survey. Technical Report, University of Wisconsin-Madison Department
of Computer Sciences, 2009. Accepted: 2012-03-15T17:23:56Z.
[16] Tom Rainforth, Adam Foster, Desi R. Ivanova, and Freddie Bickford Smith. Modern Bayesian Experimental
Design. Statistical Science, 39(1):100–114, February 2024. ISSN 0883-4237, 2168-8745. doi:10.1214/23-STS915.
Publisher: Institute of Mathematical Statistics.
[17] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Inferring solutions of differential equations us-
ing noisy multi-fidelity data. Journal of Computational Physics, 335:736–746, April 2017. ISSN 0021-9991.
doi:10.1016/j.jcp.2017.01.060.
[18] Francisco Sahli Costabal, Yibo Yang, Paris Perdikaris, Daniel E. Hurtado, and Ellen Kuhl. Physics-Informed
Neural Networks for Cardiac Activation Mapping. Frontiers in Physics, 8, February 2020. ISSN 2296-424X.
doi:10.3389/fphy.2020.00042. Publisher: Frontiers.
[19] Dongkun Zhang, Lu Lu, Ling Guo, and George Em Karniadakis. Quantifying total uncertainty in physics-
informed neural networks for solving forward and inverse stochastic problems. Journal of Computational Physics,
397:108850, November 2019. ISSN 00219991. doi:10.1016/j.jcp.2019.07.048.
[20] Liu Yang, Xuhui Meng, and George Em Karniadakis. B-PINNs: Bayesian physics-informed neural networks for
forward and inverse PDE problems with noisy data. Journal of Computational Physics, 425:109913, January
2021. ISSN 00219991. doi:10.1016/j.jcp.2020.109913.
[21] H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of the fifth annual workshop
on Computational learning theory, COLT ’92, pages 287–294, New York, NY, USA, July 1992. Association for
Computing Machinery. ISBN 978-0-89791-497-0. doi:10.1145/130385.130417.
[22] Justin S. Smith, Ben Nebgen, Nicholas Lubbers, Olexandr Isayev, and Adrian E. Roitberg.
Less is more:
Sampling chemical space with active learning. The Journal of Chemical Physics, 148(24):241733, June 2018.
ISSN 0021-9606, 1089-7690. doi:10.1063/1.5023802.
[23] Christopher J. Arthurs and Andrew P. King. Active training of physics-informed neural networks to aggregate
and interpolate parametric solutions to the Navier-Stokes equations. Journal of Computational Physics, 438:
110364, August 2021. ISSN 00219991. doi:10.1016/j.jcp.2021.110364.
[24] Zhiping Mao and Xuhui Meng. Physics-informed neural networks with residual/gradient-based adaptive sampling
methods for solving partial differential equations with sharp solutions. Applied Mathematics and Mechanics, 44
(7):1069–1084, July 2023. ISSN 1573-2754. doi:10.1007/s10483-023-2994-7.
[25] Wenhan Gao and Chunmei Wang.
Active learning based sampling for high-dimensional nonlinear partial
differential equations.
Journal of Computational Physics, 475:111848, February 2023.
ISSN 0021-9991.
doi:10.1016/j.jcp.2022.111848.
[26] Daniel Musekamp, Marimuthu Kalimuthu, David Holzmüller, Makoto Takamoto, and Mathias Niepert. Active
Learning for Neural PDE Solvers, March 2025. arXiv:2408.01536 [cs].
[27] S. Sakurai and K. Takeuchi. Back analysis of measured displacements of tunnels. Rock Mechanics and Rock
Engineering, 16(3):173–180, August 1983. ISSN 1434-453X. doi:10.1007/BF01033278. Number: 3.
19

[28] B. Lecampion,
A. Constantinescu,
and D. Nguyen Minh.
Parameter identification for lined tun-
nels in a viscoplastic medium.
International Journal for Numerical and Analytical Methods in Ge-
omechanics,
26(12):1191–1211,
2002.
ISSN 1096-9853.
doi:10.1002/nag.241.
Number:
12 _eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/nag.241.
[29] Alec Tristani, Jean Sulem, and Lina-María Guayacán-Carrillo. Analytical solutions considering face advance and
time-dependent behavior for back-analysis of convergence measurements in deep circular tunnels under isotropic
initial stress state. International Journal of Rock Mechanics and Mining Sciences, 182:105866, October 2024.
ISSN 13651609. doi:10.1016/j.ijrmms.2024.105866.
[30] Zilong Zhang, Qiujing Pan, Zihan Yang, and Xiaoli Yang. Physics-informed deep learning method for predicting
tunnelling-induced ground deformations. Acta Geotechnica, 18(9):4957–4972, September 2023. ISSN 1861-1133.
doi:10.1007/s11440-023-01874-9.
[31] Qipeng Cai, Khalid Elbaz, Xiangyu Guo, and Xuanming Ding. Physics-informed deep learning and analytical
patterns for predicting deformations of existing tunnels induced by new tunnelling. Computers and Geotechnics,
187:107451, November 2025. ISSN 0266-352X. doi:10.1016/j.compgeo.2025.107451.
[32] You Wang, Qianjun Fan, Fang Dai, Rui Wang, and Bosong Ding. A physics-data-driven method for predicting
surface and building settlement induced by tunnel construction. Computers and Geotechnics, 179:107020, March
2025. ISSN 0266352X. doi:10.1016/j.compgeo.2024.107020.
[33] Chen Xu, Ba Trung Cao, Yong Yuan, and Günther Meschke.
Transfer learning based physics-informed
neural networks for solving inverse problems in engineering structures under different loading scenarios.
Computer Methods in Applied Mechanics and Engineering, 405:115852, February 2023.
ISSN 00457825.
doi:10.1016/j.cma.2022.115852.
[34] G. Wang,
Q. Fang,
J. Wang,
Q. M. Li,
J. Y. Chen,
and Y. Liu.
Estimation of load for tun-
nel lining in elastic soil using physics-informed neural network.
Computer-Aided Civil and Infras-
tructure Engineering,
39(17):2701–2718,
2024.
ISSN 1467-8667.
doi:10.1111/mice.13208.
_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1111/mice.13208.
[35] S. G. Lekhnitskii, P. Fern, Julius J. Brandstatter, and E. H. Dill. Theory of Elasticity of an Anisotropic Elastic
Body. Physics Today, 17(1):84, January 1964. ISSN 0031-9228. doi:10.1063/1.3051394.
[36] A. M. Hefny and K. Y. Lo.
Analytical solutions for stresses and displacements around tunnels driven
in cross-anisotropic rocks.
International Journal for Numerical and Analytical Methods in Geome-
chanics,
23(2):161–177,
1999.
ISSN
1096-9853.
doi:10.1002/(SICI)1096-9853(199902)23:2<161::AID-
NAG963>3.0.CO;2-B.
_eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291096-
9853%28199902%2923%3A2%3C161%3A%3AAID-NAG963%3E3.0.CO%3B2-B.
[37] Antonio Bobet. Lined Circular Tunnels in Elastic Transversely Anisotropic Rock at Depth. Rock Mechanics
and Rock Engineering, 44(2):149–167, March 2011. ISSN 1434-453X. doi:10.1007/s00603-010-0118-1.
[38] Huy Tran Manh, Jean Sulem, and Didier Subrin. A Closed-Form Solution for Tunnels with Arbitrary Cross
Section Excavated in Elastic Anisotropic Ground. Rock Mechanics and Rock Engineering, 48(1):277–288, January
2015. ISSN 1434-453X. doi:10.1007/s00603-013-0542-0.
[39] Dimitrios Kolymbas, Peter Wagner, and Anastasia Blioumi. Cavity expansion in cross-anisotropic rock. Inter-
national Journal for Numerical and Analytical Methods in Geomechanics, 36(2):128–139, 2012. ISSN 1096-9853.
doi:10.1002/nag.998. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nag.998.
[40] The Manh Vu, Jean Sulem, Didier Subrin, and Nathalie Monin.
Semi-Analytical Solution for Stresses and
Displacements in a Tunnel Excavated in Transversely Isotropic Formation with Non-Linear Behavior. Rock
Mechanics and Rock Engineering, 46(2):213–229, March 2013. ISSN 0723-2632, 1434-453X. doi:10.1007/s00603-
012-0296-0. Number: 2.
[41] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a
simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–
1958, 2014.
[42] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian Approximation: Representing Model Uncertainty
in Deep Learning. In Proceedings of The 33rd International Conference on Machine Learning, pages 1050–1059.
PMLR, June 2016. ISSN: 1938-7228.
[43] Cody J Permann, Derek R Gaston, David Andrš, Robert W Carlsen, Fande Kong, Alexander D Lindsay,
Jason M Miller, John W Peterson, Andrew E Slaughter, Roy H Stogner, et al. Moose: Enabling massively
parallel multiphysics simulation. SoftwareX, 11:100430, 2020.
[44] Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic
20

Differentiation in Machine Learning: a Survey. Journal of Machine Learning Research, 18(153):1–43, 2018. ISSN
1533-7928.
[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary De-
Vito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural
Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
[46] Timothy Dozat. Incorporating Nesterov Momentum into Adam. February 2016.
[47] Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical
Programming, 45(1):503–528, August 1989. ISSN 1436-4646. doi:10.1007/BF01589116.
[48] Levi D. McClenny and Ulisses M. Braga-Neto.
Self-adaptive physics-informed neural networks.
Journal of
Computational Physics, 474:111722, February 2023. ISSN 00219991. doi:10.1016/j.jcp.2022.111722.
[49] F. Tonon and B. Amadei. Effect of Elastic Anisotropy on Tunnel Wall Displacements Behind a Tunnel Face. Rock
Mechanics and Rock Engineering, 35(3):141–160, August 2002. ISSN 1434-453X. doi:10.1007/s00603-001-0019-4.
[50] Haojiang Ding, Weiqiu Chen, and L. Zhang.
Elasticity of Transversely Isotropic Materials, volume 126 of
Solid Mechanics and Its Applications.
Springer-Verlag, Berlin/Heidelberg, 2006.
ISBN 978-1-4020-4033-7.
doi:10.1007/1-4020-4034-2.
[51] J Sulem, M PANETt, and A Guenot. Closure Analysis in Deep Tunnels. International Journal of Rock Mechanics
and Mining Sciences & Geomechanics Abstracts, 1987.
[52] Frederico Lara, Lina-María Guayacán-Carrillo, Jean Sulem, Jana Jaber, and Gilles Armand.
Time-
dependent modeling of supported drifts excavated in Callovo-Oxfordian claystone considering the excavation-
induced fractured zone.
Computers and Geotechnics,
179:107030,
March 2025.
ISSN 0266-352X.
doi:10.1016/j.compgeo.2024.107030.
[53] Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-
informed neural networks, January 2020. arXiv:2001.04536 [cs].
[54] Sifan Wang, Xinling Yu, and Paris Perdikaris.
When and why PINNs fail to train:
A neural tan-
gent kernel perspective.
Journal of Computational Physics, 449:110768, January 2022.
ISSN 00219991.
doi:10.1016/j.jcp.2021.110768.
[55] E. Boidy, A. Bouvard, and F. Pellet.
Back analysis of time-dependent behaviour of a test gallery in
claystone.
Tunnelling and Underground Space Technology, 17(4):415–424, October 2002.
ISSN 0886-7798.
doi:10.1016/S0886-7798(02)00066-4.
[56] D. Sterpi and G. Gioda. Visco-Plastic Behaviour around Advancing Tunnels in Squeezing Rock. Rock Mechanics
and Rock Engineering, 42(2):319–339, April 2009. ISSN 1434-453X. doi:10.1007/s00603-007-0137-8.
[57] Frederic L Pellet. Contact between a Tunnel Lining and a Damage-Susceptible Viscoplastic Medium. Computer
Modeling in Engineering & Sciences, 2009.
[58] A. M. Stuart. Inverse problems: A Bayesian perspective. Acta Numerica, 19:451–559, May 2010. ISSN 1474-0508,
0962-4929. doi:10.1017/S0962492910000061.
[59] Alec Tristani. Active learning tunnel source code. https://doi.org/10.5281/zenodo.17712809, 2025. Zenodo.
21
