Draft version November 20, 2025
Typeset using LATEX default style in AASTeX63
Advancing Identification method of Gamma-Ray Bursts with Data and Feature Enhancement
Peng Zhang (张鹏)
,1, 2 Bing Li (李兵)
,2, 3, 4 Ren-Zhou Gui (桂任舟),1 Shao-Lin Xiong (熊少林)
,2
Yu Wang (王瑜)
,5, 6, 7 Shi-Jie Zheng (郑世界),2 Guang-Cheng Xiao (肖广成),8 Xiao-Bo Li (李小波),2
Yue Huang (黄跃),2 Chen-Wei Wang (王晨巍)
,2 Jia-Cong Liu (刘佳聪)
,2 Yan-Qiu Zhang (张艳秋)
,2
Wang-Chen Xue (薛王陈)
,2 Chao Zheng (郑超)
,2 and Yue Wang (王悦)
2
1College of Electronic and Information Engineering, Tongji University, Shanghai 201804, China
2Key Laboratory of Particle Astrophysics, Chinese Academy of Sciences, Beijing 100049, China,
libing@ihep.ac.cn,rzgui@tongji.edu.cn,xiongsl@ihep.ac.cn
3School of Astronomy and Space Science, Nanjing University, Nanjing 210023, China
4Guangxi Key Laboratory for Relativistic Astrophysics, Nanning 530004, China
5ICRA, Dip. di Fisica, Sapienza Universit`a di Roma, Piazzale Aldo Moro 5, I-00185 Roma, Italy
6ICRANet, Piazza della Repubblica 10, 65122 Pescara, Italy
7INAF – Osservatorio Astronomico d’Abruzzo, Via M. Maggini snc, I-64100, Teramo, Italy
8Department of Physics, Jinggangshan University, Jiangxi Province, Ji’an 343009, China
ABSTRACT
Gamma-ray bursts (GRBs) are challenging to identify due to their transient nature, complex tempo-
ral profiles, and limited observational datasets. We address this with a one-dimensional convolutional
neural network integrated with an Adaptive Frequency Feature Enhancement module and physics-
informed data augmentation. Our framework generates 100,000 synthetic GRB samples, expanding
training data diversity and volume while preserving physical fidelity-especially for low-significance
events.
The model achieves 97.46% classification accuracy, outperforming all tested variants with
conventional enhancement modules, highlighting enhanced domain-specific feature capture. Feature
visualization shows model focuses on deep-seated morphological features and confirms the capability
of extracting physically meaningful burst characteristics. Dimensionality reduction and clustering re-
veal GRBs with similar morphologies or progenitor origins cluster in the feature space, linking learned
features to physical properties. This perhaps offers a novel diagnostic tool for identifying kilonova- and
supernova-associated GRB candidates, establishing criteria to enhance multi-messenger early-warning
systems. The framework aids current time-domain surveys, generalizes to other rare transients, and
advances automated detection in large-volume observational data.
Keywords: Gamma-ray astronomy (628), Gamma-ray bursts (629), High energy astrophysics (739),
Convolutional neural networks (1938), Dimensionality reduction(1943), Astronomy data
analysis (1858)
1. INTRODUCTION
Gamma-ray bursts are among the most energetic phenomena in the universe, releasing enormous amounts of energy
in the form of gamma rays over remarkably short time frames. Despite decades of remarkable progress in observations
and theoretical investigations, their origins remain elusive (Zhang 2011; Kumar & Zhang 2015; M´esz´aros 2019; Pe’er
2024), making GRB identification and classification a critical focus of contemporary astrophysical research (L¨u et al.
2010; Zhang et al. 2022; Sun et al. 2023; Wang et al. 2024). The rise of multi-messenger and multi-band astronomy
further underscores the need for efficient GRB detection (Margutti & Chornock 2021; Rudolph et al. 2023): timely and
accurate detection enables follow-up observations of afterglows, host galaxy localization, characterization of associated
counterparts, and constraints on GRB intrinsic properties.
GRB light curves often exhibit irregular, multi-peaked structures that encode rich physical information about their
outbursts.
While traditional classification criteria (e.g., duration, hardness) have been widely adopted, emerging
evidence highlights the need for additional discriminants to address the ”iceberg effect”—a phenomenon where low-
signal-to-noise ratio (low-SNR) GRBs are partially submerged in background noise, disproportionately impacting short-
arXiv:2511.15470v1  [astro-ph.HE]  19 Nov 2025

2
Zhang et al.
duration events (L¨u et al. 2014; Moss et al. 2022). Traditional burst search algorithms identify GRBs by detecting
signals in multi-time-bin, multi-energy-band light curves where the SNR exceeds a predefined threshold above the
background (Band 2002; Blackburn et al. 2013; Cai et al. 2021). However, these methods face significant challenges,
including accurate background estimation and optimal threshold selection, limiting their effectiveness in identifying
faint or complex GRB events. These limitations motivate the development of advanced techniques to enhance GRB
detection and classification accuracy.
In recent years, machine learning (ML) has emerged as a experimental tool for GRB detection.
For example,
Abraham et al. (2021) developed an ML algorithm for automated detection of GRB-like events using AstroSat/CZTI
data, combining density-based spatial clustering with dynamic time warping to demonstrate ML’s versatility and
robustness in GRB identification. Among ML architectures, convolutional neural networks (CNNs)—a specialized
deep learning (DL) framework—excel in pattern recognition tasks (Alzubaidi et al. 2021; Taye 2023), and have driven
a paradigm shift in astrophysical data analysis by autonomously extracting features from multi-dimensional, multi-
modal datasets. In GRB research, CNN-based approaches have achieved notable successes. Parmiggiani et al. (2021)
developed a real-time CNN pipeline for AGILE-GRID intensity maps, detecting 21 GRBs at 3σ significance compared
to only two detections via conventional SNR-based searches. Parmiggiani et al. (2023) used a CNN autoencoder to
reconstruct background light curves and identify 72 previously uncataloged AGILE GRBs. Additionally, (Parmiggiani
et al. 2024) and Crupi et al. (2023) leveraged neural networks to predict background count rates and identify faint,
long-duration GRBs.
Notably, GRB identification relies on extracting meaningful features from light curves with complex temporal and
spectral characteristics.
While traditional 1D-CNNs effectively capture local temporal patterns, their inability to
model spectral-temporal correlations limits their utility in GRB analysis—critical GRB features are jointly encoded
in both domains (Guidorzi et al. 2016; Tarnopolski & Marchenko 2021; Zhou et al. 2024). To address this, recent
time series classification advances have introduced architectures that explicitly model frequency information, including
Wavelet-based decomposition (e.g., T-WaveNet (Liu et al. 2020)), 2D temporal representations (e.g., TimesNet (Wu
et al. 2022)), hybrid Fourier-wavelet transformers (e.g., FEDformer (Zhou et al. 2022), WFTNet (Liu et al. 2024)),
and adaptive spectral blocks (e.g., TSLANet (Eldele et al. 2024)). These state-of-the-art approaches confirm that
spectral-temporal modeling enhances feature extraction, underscoring the need for GRB identification methods that
integrate both temporal dynamics and frequency-domain characteristics.
Complementary to classification, dimensionality reduction techniques have provided valuable insights into GRB
diversity. For instance, Jespersen et al. (2020) used t-distributed stochastic neighbor embedding (t-SNE) to categorize
Swift/BAT GRBs to separate supernova-associated and kilonova-associated GRBs into long and short categories.
Garcia-Cifuentes et al. (2023) identified 7 new extended emission (EE) GRB candidates via dimensionality reduction.
Zhu et al. (2024) and Negro et al. (2025) used Uniform Manifold Approximation and Projection (UMAP) or t-SNE to
reveal GRB clusters that transcend traditional duration-based classification, including distinct subgroups of kilonova-
related GRBs. Similarly, Chen et al. (2024) and Chen et al. (2025) demonstrated robust bimodal GRB clustering and
refuted intermediate GRB classes using unsupervised ML and dimensionality reduction. Collectively, these studies
highlight the utility of such techniques for analyzing burst similarities and refining classification schemes.
Despite these advances, DL models face critical limitations in GRB research. First, data scarcity persists: the largest
available GRB dataset (Fermi/GBM, 3900 events) remains insufficient for robust DL training (Zhang et al. 2024), due
to limited labeled observations, skewed distribution (underrepresentation of rare, short-duration, and low-SNR GRBs),
and narrow coverage of physical properties (e.g., redshift, energy bands, progenitors). While data augmentation has
been explored to mitigate this (Zhang et al. 2024; Shorten & Khoshgoftaar 2019), existing datasets still lack the feature
richness needed to avoid overfitting. Second, uncertainty quantification (UQ) is underdeveloped: model uncertainty
stems from limited training data, the ”black-box” nature of CNNs, and observational noise (Nemani et al. 2023),
yet UQ is critical for distinguishing high-confidence detections from ambiguous candidates—misclassification of which
could lead to missed follow-up observations or erroneous inferences about progenitor origins.
To address these limitations, we present an integrated framework that combines physics-informed data augmenta-
tion (to enhance sample diversity) with a novel frequency-adaptive feature enhancement module (to capture spectral-
temporal correlations). In this Letter, Section 2 details data preprocessing, augmentation, and dataset construction.
Section 3 describes the model architecture, feature enhancement modules, training process, and feature analysis frame-
work. Section 4 presents performance metrics and classification results, and Section 5 discusses the implications of our
approach and summarizes conclusions.

Advancing Identification of GRBs
3
2. DATA AUGMENTATION AND DATASET
The Fermi/GBM trigger system has been operational since 12 July 2008 (first detected GRB: GRB 080714B). For
this study, we utilized Fermi/GBM data spanning from 14 July 2008 to 30 June 2024, including 3,905 original GRBs
detected by NaI detectors. These events are manually verified and cataloged in the Fermi/GBM Burst Catalog1.
Adopting the GRB/non-GRB extraction protocol described in Zhang et al. (2024), we updated the dataset by seg-
menting samples into 120-second time windows. To ensure complete coverage of theT90 interval (with background
included on both sides), 217 GRBs failing this criterion were excluded.
For the remaining events, samples were
generated based on the number of triggering NaI detectors, resulting in 6,189 primary GRB samples. Non-GRB sam-
ples—encompassing modulated background, electronic noise, and potential all-sky source signals—were systematically
extracted from detectors’ quiescent (non-trigger) intervals, totaling 108,000. This balance between classes enhances
model generalization to non-burst events.
Light curves for all samples were extracted at a 64 ms temporal resolution across 128 energy channels, then rebinned
into 9 standard energy bands (25–50, 50–100, 50–300, 100–300, 100–500, 100–900, 300–500, 300–900, 500–900 keV).
This segmentation aligns with typical GRB photon energy distributions and Fermi/GBM trigger criteria (von Kienlin
et al. 2020; Cai et al. 2021), approximating the photon deposition ranges of NaI detectors (without energy response
correction). Critically, it preserves energy-dependent features such as burst morphology evolution, light curve shape
variations, and spectral lags—enriching the diversity of discriminative features for model training. To optimize input
for machine learning, we applied per-band standardization to light curves (scaling features to zero mean and unit vari-
ance). This approach preserves spectral information via inter-band flux ratios, empirically outperforming normalization
(scaling to [0,1]). While both methods unify feature scales to facilitate deep learning convergence, standardization is
preferable for GRB characterization as it retains spectral integrity—essential for distinguishing GRBs from non-GRBs
(e.g., via hardness evolution or spectral lags).
Data augmentation: We first compute the full-energy band peak signal-to-noise ratio (peak-SNR) of primary
GRB samples and fit its distribution using a log-normal function. This fitted distribution is then used for random
sampling to generate more synthetic GRB samples. Specifically, we randomly reduce the count rate of original GRB
light curves multiple times. For each GRB, a single reduction factor is applied to all its 9 energy band light curves
in each reduction, generating distinct light curves with unique SNRs—effectively enhancing training set diversity. For
instance, as illustrated in Figure 1, we generated three synthetic light curves (distinct SNRs) for GRB 230307A. This
approach is analogous to resizing or contrast enhancement in affine data augmentation.
Given an input light curve LC ∈RT (R: real numbers; T: temporal length) and a background light curve LCbg ∈RT ,
the augmented light curve LCnew is computed as:
LCnew = LC · (1 −α) + Poisson(LCbg · α),
(1)
where α ∈[0, 1] is the crop factor controlling burst photon deduction proportion. The Poisson(·) function introduces
stochastic noise, restoring matching Poisson-distributed background noise to simulate real astronomical random noise.
This formula reduces burst signal significance while preserving original background levels, ensuring synthetic samples
match real scenarios (e.g., orbit modulation, instrument noise). Notably, this method reproduces different degrees of
the ”iceberg effect” in synthetic GRBs—especially effective for enriching short-duration, rare faint, and low-SNR GRB
samples. Crucially, augmented samples faithfully retain inherent fine-scale temporal/ spectral substructures of original
GRBs, ensuring no excessive loss of key physical features for GRB identification/classification. The data augmentation
process is detailed below, with statistical results shown in Figure 2:
1. Analyze full-energy band peak-SNR of 6,189 primary GRB samples and fit this distribution with a log-normal
function over [0,25] σ.
2. Randomly sample 500,000 values from the fitted distribution and divide by 25 to scale [0,25] σ to [0,1], yielding
crop factors (α).
3. Uniformly sample 500,000 instances from primary GRB samples and pair each with a random α , then generate
500,000 synthetic samples via Equ. 1. Use the same α for all 9 energy bands of a GRB to preserve spectral
consistency.
1 https://heasarc.gsfc.nasa.gov/W3Browse/fermi/fermigbrst.html

4
Zhang et al.
0
20
40
60
80
100
500
1000
1500
Counts
Primary LC
SNR: 52.35σ
Fitted bkg, N: 89.41
T90: 30.98s
0
20
40
60
80
100
500
1000
1500
Counts
Crop 30% with bkg filling
SNR: 42.04σ
Fitted bkg, N: 89.28
T90: 31.55s
0
20
40
60
80
100
500
1000
1500
Counts
Crop 60% with bkg filling
SNR: 28.58σ
Fitted bkg, N: 89.53
T90: 30.34s
0
20
40
60
80
100
Time since trigger 2023-03-07T15:44:06.671
500
1000
1500
Counts
Crop 90% with bkg filling
SNR: 9.77σ
Fitted bkg, N: 89.75
T90: 26.37s
Figure 1. Data augmentation procedure for GRB light curves, using GRB 230307A as observed by Fermi/GBM detector n8
(full energy channel). The top panel presents the original light curve with a peak-SNR of 52.35 σ, where the red dashed line
indicates the polynomial-fitted background level (mean count rate ∼90 counts/bin) and the shaded area marks the T90 interval.
The lower panels show the resulting light curves after applying count rate reductions of 30%, 60%, and 90% to each time bin,
with subsequent restoration of Poisson-distributed background noise at the corresponding levels. The resulting peak SNR values
are indicated for each modified light curve.
4. Remove 4,648 synthetic samples with peak-SNR blew 2 σ, as excessive photon subtraction makes these resemble
background samples.
5. Randomly draw 100,000 samples from the remainder to balance GRB/non-GRB ratios in the training set2.
6. Apply the same augmentation to non-GRB samples to align background noise distributions, ensuring the model
distinguishes GRBs from background (not artificial Poisson noise differences).
Dataset: We partition preprocessed samples into training, validation, and test sets chronologically to ensure tem-
poral generalization. Dataset details are shown in Table 1. This chronological split alleviates temporal overfitting
while preserving the data’s inherent flux distribution. We maintain ∼1:1 positive-to-negative sample ratios across all
splits. Crucially, only the training set undergoes data augmentation, validation/test sets retain original observations
(primary GRB samples and non-GRB samples) for unbiased performance evaluation.
3. MODEL ARCHITECTURE, TRAINING, AND EXTENSION
3.1. Architecture of Neural Networks
2 For this supervised task, final sample size was determined via prior experience with similar models, relying on empirical heuristics.

Advancing Identification of GRBs
5
0
5
10
15
20
25
Peak-SNR
0
2000
4000
6000
8000
Number
25
100
175
250
325
400
475
0
25
50
75
100
125
Primary GRBs, N=6189×6
Log-normal fitting of primary GRBs
Uniform sampling result, N=100,000
Random deduction, N=100,000
Figure 2. Histogram of full-energy band peak-SNRs for GRB samples during the data augmentation process. The solid black
line represents the SNR distribution of the primary sample, scaled by a factor of 6 for visualization. The dashed gray line
denotes the fitted log-normal distribution over the range [0, 25] σ. The solid blue line shows the peak-SNR distribution of
100,000 uniformly sampled instances from the primary GRB samples. The solid red line represents the peak-SNR distribution
of 100,000 randomly selected GRB samples after data augmentation The pink shaded area highlights peak-SNR values below
2 σ.
Table 1. Description of the dataset.
Dataset
Nu. of GRB Events
Nu. of Samples
Period Definition (UTC)
Original GRBs
Primary GRBs
Synthetic GRBs
Non-GRBs
Training set
1,899
6189
100,000
100,000
07/14/2008 - 31/12/2016
Validation set
842
2,774
2,774
4,000
01/01/2017 - 12/31/2019
Test set
947
3,143
3,143
4,000
01/01/2020 - 06/31/2024
Each sample in our dataset consists of light curves spanning 9 energy bands, which are naturally suited to be treated
as time series data. This makes a 1D CNN the appropriate choice for feature extraction and classification. Our model
adopts ResNet (He et al. 2016) as its backbone, which relies on a sequential stack of four convolutional units (Conv
Units) to progressively extract hierarchical features from the input light curves. A key advantage of ResNet is its
integration of residual connections, which effectively alleviate the gradient vanishing problem that commonly arises in
deep neural networks and enable stable training of deeper architectures. To address the limitation that CNNs lack the
ability to distinguish subtle frequency variations, we propose an Adaptive Frequency Feature Enhancement module,
AFFE, which is integrated into the last part of each Conv Unit. This module explicitly enhances frequency-domain
features to improve the model’s GRB identification capability, and it also adaptively weights and filters frequency
components, enabling the model to focus on the most discriminative features for GRB recognition.
This design
is particularly critical for distinguishing GRBs from background noise, as frequency-domain analysis can uncover
patterns that remain obscured in the time domain.

6
Zhang et al.
After the convolutional layers (including Conv Units with AFFE modules) complete feature extraction, the extracted
high-dimensional features first undergo global average pooling (GAP). This step reduces the number of model parame-
ters while preserving the representative information of features across the temporal dimension, thereby mitigating the
risk of overfitting. A dropout layer with a 50% dropout rate is then inserted between the GAP layer and the subsequent
fully connected (FC) layers. During training, this layer randomly deactivates half of the neurons to prevent the model
from over-relying on specific features, further suppressing overfitting. Following the dropout layer, two FC layers map
the pooled features to the final classification space and output the probability of the sample belonging to the GRB or
non-GRB category. The overall architecture of our our ResNet-based model integrated with AFFE modules is shown
in Figure 3, which includes the order of layers and the data flow.
Conv Unit 1
Conv Unit 2
Conv Unit 3
Conv Unit 4
Feature Extraction
(1D Convolution)
FFT
Weighted
Fourier Components
Fourier 
Components
Conv1D
1x3
Conv1D
1x3
Filtered
Fourier Components
IFFT
Sum
FC
FC
Mean
FC
FC
Mean
C × 1
 × 1
C
－8  × 1
C
－8
C × 1
Channel Axis
imag
F
real
F
F
Adaptive Frequency Feature Enhancement (AFFE)
Threshold
Weights
Classifier
(FC)
32
32
32
32
32
32
64
64
64
64
Input
(Light Curve)
GAP
Pre-processing
Dropout
50%
GRB
Non-GRB
1875
9
2
64
16
 25–50 keV
50–100 keV
50–300 keV
100–300 keV
100–500 keV
100–900 keV
300–500 keV
300–900 keV
500–900 keV
Figure 3. Schematic diagram of the ResNet+AFFE model architecture and the feature transformation process across layers.
The input light curves are colored differently to distinguish the nine energy bands (This color distinction is solely for intuitively
distinguishing each energy band to improve the clarity of the diagram, with no additional physical implications.). The numbers
inside each Conv unit indicate changes in feature map size and channel dimension. The yellow modules represent the AFFE
modules, and the dashed box at the bottom details an implementation of the AFFE module.
Here we describe the AFFE module in detail. Given an input feature map X ∈RT ×C (with T as temporal length
and C as channel number), the AFFE module processes it in the following steps:
1. Frequency Transformation: The input feature map X is first transformed into the frequency domain using
the Fast Fourier Transform (FFT):
F = FFT(X),
(2)
where F ∈CT ×C denotes the complex-valued Fourier frequency spectrum. The real and imaginary parts of F
are denoted as Freal and Fimag, respectively.
2. Weighted Frequency: To adaptively emphasize important frequency components, we apply separate learnable
transformations to the real and imaginary parts of the frequency spectrum. Specifically, four FC layers are used
to generate adaptive weights for Freal and Fimag:
Wreal = FC2(FC1(Freal)),
Wimag = FC4(FC3(Fimag)),
(3)
where Wreal, Wimag ∈RT ×C are the learned weight matrices. The number of neurons in the FC1 and FC3 is
set to C divided by 8. This choice is determined after comparing division factors of 2, 4, 8, and 16, and it

Advancing Identification of GRBs
7
significantly reduces the parameter count of the module. The weighted frequency spectrum is then computed as:
Fweighted = Wreal ⊙Freal + i · (Wimag ⊙Fimag),
(4)
where ⊙denotes element-wise multiplication.
3. Filtered Frequency: To further refine frequency features, we perform a filtering operation in the frequency
domain. First, the weighted frequency spectrum Fweighted is summed along the channel dimension to obtain a
channel-aggregated frequency representation Fsum ∈CT ×1. Two one-dimensional convolutional layers are then
applied to Fsum to learn a frequency thresholding function that approximates a low-pass filter:
Ffiltered = Conv1(Conv2(Fsum)).
(5)
The filtered spectrum Ffiltered is then broadcasted back to the original channel dimension.
4. Frequency Feature Fusion: The results of the Weighted Frequency and Filtered Frequency steps are combined
through element-wise addition:
Fenhanced = F ⊙Fweighted + F ⊙Ffiltered.
(6)
5. Inverse Transformation: Finally, the enhanced frequency spectrum Fenhanced is transformed back to the time
domain using the Inverse Fast Fourier Transform (IFFT):
Xenhanced = IFFT(Fenhanced),
(7)
where Xenhanced ∈RT ×C is the output feature map with enriched frequency-domain information.
3.2. Model training and analysis
Model parameters are initialized using the truncated normal distribution proposed by He et al. (2015). A detailed
analysis of hyperparameter choices-including batch size, learning rate, and number of convolutional layers-is conducted
to evaluate their impact on the test set, following the approach in Ma et al. (2023). Further details on hyperparameter
selection, such as the number of convolutional kernels per unit, convolutional kernel sizes, and neuron counts in FC
layers. The training configuration is as follows: batch size is set to 1024; cross-entropy loss is used to measure the
discrepancy between predicted and true labels; and the Adam optimizer Kingma & Ba (2014) is adopted to dynamically
update model parameters and minimize loss during training. The initial learning rate is 1 × 10−4, with a scheduling
mechanism that reduces the rate by half if validation loss does not decrease for 10 consecutive epochs. To prevent
overfitting, early stopping terminates training if validation accuracy plateaus for 20 consecutive epochs. The patience
value was selected after testing ranges of [10, 20, 40] epochs, and the model parameters from the epoch with the highest
validation accuracy are retained as the final optimal parameters. The result of the optimizing choices are highlighted
in bold in Table 2. The efficacy of selected hyperparameters is further validated by training curves, which demonstrate
stable convergence and appropriate model capacity for the classification task.
We conducted a comparative analysis involving the baseline ResNet architecture that commonly used for time series
data, and ResNet variants integrated with existing feature enhancement modules that including the SE (Squeeze-and-
Excitation block; Hu et al. (2018)), CBAM (Convolutional Block Attention Module; Woo et al. (2018)), ECA (Efficient
Channel Attention module; Wang et al. (2020)), and ASB (Adaptive Spectral Block; Eldele et al. (2024) modules),
and our proposed ResNet-AFFE model. All models were trained and tested on our dataset using hyperparameters
consistent with those reported in the original literature for each comparative module. This ensures a fair performance
comparison, and the results of this comparison (for ResNet variants and our ResNet-AFFE model) on the test set are
presented blew.
The models were implemented using the PyTorch framework, with training conducted on a single NVIDIA RTX-
4090 GPU. Performance was evaluated using four standard classification metrics: Accuracy, Precision, Recall, and
F1-score, consistent with the evaluation methodology reported in Zhang et al. (2024). Accurately verifying these
metrics and their associated prediction uncertainties is critical, as they quantify confidence in model predictions and
provide a scientific basis for subsequent decision-making. During testing, we achieved robust uncertainty quantification
for deep learning predictions via Monte Carlo Dropout (MCD, for references see Gal & Ghahramani (2015); Basora

8
Zhang et al.
Table 2. Hyper-parameter selection.
Parameters
Values
Number of ConvUnit
2, 4, 8, 16
Number of Conv in ConvUnit
1, 2, 3, 4
First ConvUnit filter size
16, 32, 64, 128, 256, 512
Second ConvUnit filter size
16, 32, 64, 128, 256, 512
Third ConvUnit filter size
16, 32, 64, 128, 256, 512
Fourth ConvUnit filter size
16, 32, 64, 128, 256, 512
Norm function after Conv
InstaceNorm, BatchNorm
Activation function
Sigmoid, Relu
FC neurons
8, 16, 32, 64, 128
Dropout rate
0.3, 0.5, 0.8
Initial learning rate
1e-3, 1e-4, 1e-5, 1e-6
Batch size
512, 1024, 2048
Patience of reduce learning rate
5, 10, 15
Note—Bold text represents that the model performs optimally on that metric.
et al. (2025)). In this procedure, all model parameters are kept fixed, and dropout layers between FC layers remain
activated. For each forward pass, the dropout rate is randomly sampled from a uniform distribution over the interval
[0.1, 0.5]. This stochastic forward pass is repeated 1000 times, which allows computation of the mean and standard
deviation for each metric—thereby effectively estimating predictive uncertainty. The performance and uncertainty of
all models are summarized in Table 3.
Table 3. Comparison of models’ performance on test set.
Model
Accuracy (%)
Precision (%)
Recall (%)
F1-score (%)
ResNet
97.17 ± 0.13
98.80 ± 0.14
94.71 ± 0.20
96.72 ± 0.15
ResNet+SEa
97.33 ± 0.04
99.08 ± 0.05
94.82 ± 0.08
96.90 ± 0.04
ResNet+CBAMb
97.29 ± 0.07
99.08 ± 0.08
94.72 ± 0.14
96.85 ± 0.09
ResNet+ECAc
97.34 ± 0.07
99.15 ± 0.08
94.76 ± 0.13
96.91 ± 0.08
ResNet+ASBd
97.40 ± 0.07
99.24 ± 0.08
94.83 ± 0.14
96.98 ± 0.09
ResNet+AFFE (Ours)
97.46 ± 0.07
99.39 ± 0.07
94.81 ± 0.14
97.04 ± 0.08
Note— The uncertainty of each metric is evaluated utilizing MCD. Bold text represents that the model performs optimally on
that metric.
Enhanced time domain feature extraction module: a Hu et al. (2018), b Woo et al. (2018), c Wang et al. (2020).
Enhanced frequency domain feature extraction module: d Eldele et al. (2024).
To examine whether ResNet-AFFE prioritizes burst-specific temporal signatures (e.g., T90 intervals or energy-
dependent flux peaks) over noise, we used Gradient-weighted Class Activation Mapping (Grad-CAM)—a mainstream
visualization technique for addressing deep learning’s ”black box” problem. Grad-CAM generates gradient-weighted
heatmaps to identify features most influential to model predictions (Selvaraju et al. 2017), enhancing interpretabil-
ity. For time-series light curves, these heatmaps pinpoint temporal segments driving classification outputs, replacing
ambiguous ”feature importance” with intuitive visual evidence. We employ Grad-CAM to decode our model’s decision-
making process for GRB classification. Comparative visualizations of discriminative temporal features between the
baseline ResNet and ResNet-AFFE are presented in Figure 4.

Advancing Identification of GRBs
9
−40
−20
0
20
40
60
Time since trigger [s]
0.0
0.5
1.0
ResNet
bn171120556, nb
−40
−20
0
20
40
60
Time since trigger [s]
ResNet-AFFE
bn171120556, nb
−20
0
20
40
60
80
Time since trigger [s]
0.0
0.5
1.0
bn180113418, na
−20
0
20
40
60
80
Time since trigger [s]
bn180113418, na
−20
0
20
40
60
80
Time since trigger [s]
0.0
0.5
1.0
Normalized counts
bn180720598, n7
−20
0
20
40
60
80
Time since trigger [s]
bn180720598, n7
0
20
40
60
80
100
Time since trigger [s]
0.0
0.5
1.0
bn191227069, n3
0
20
40
60
80
100
Time since trigger [s]
bn191227069, n3
−40
−20
0
20
40
60
Time since trigger [s]
0.0
0.5
1.0
bn220107793, na
−40
−20
0
20
40
60
Time since trigger [s]
bn220107793, na
0.0
0.2
0.4
0.6
0.8
1.0
Feature weights
Figure 4. Feature visualization for five representative GRBs using Grad-CAM. The color-coded heatmaps indicate the temporal
regions receiving strongest attention from the deep learning model during classification. Left and right panels contrast the feature
extraction patterns between the ResNet (left) and our enhanced ResNet-AFFE architecture (right). Gray shaded regions denote
the T90 for each GRB.
We further mapped deep features extracted by ResNet-AFFE to observed physical characteristics using UMAP, a
nonlinear dimensionality reduction method rooted in graph theory and manifold learning that effectively projects high-
dimensional data to low-dimensional spaces for structural analysis. We analyzed 3,833 GRBs detected by Fermi/GBM
over the period 2008–2024, concatenating light curves from the triggering detectors of each GRB as model input.
Notably, our model uses GAP to aggregate features from the last convolutional layer, enabling compatibility with
variable-length data under uniform batch input. This design specifically supports the inclusion of long GRBs with T90 >
120 s, which would otherwise be excluded by fixed-length constraints.
UMAP takes as input the output results
from the last convolutional layer of ResNet-AFFE and outputs 2D projections that uncover intrinsic correlations
of features between GRBs.
Key UMAP hyperparameters including n neighbors and min dist were systematically
optimized via exhaustive parameter space exploration (over ranges: 2 ≤n neighbors ≤100; 0.01 ≤min dist ≤
0.99).
Manual validation yielded optimal values of n neighbors = 60 and min dist = 0.66, achieving a balanced
preservation of both global data structure and local neighborhood fidelity—two core advantages of UMAP in high-
dimensional signal analysis. The resulting projections shown in Figure 5, visualize the distribution of true positive
(TP) and false negative (FN) GRBs, with each GRB’s peak-SNR and T90 encoded directly in the projection to link
low-dimensional structure with physical observables. While UMAP effectively reveals the structure of high-dimensional
data, its visualization results are sensitive to hyperparameter selection (e.g., n neighbors and min dist). To mitigate
the subjectivity introduced by manual parameter tuning and ensure the robustness of observed clustering patterns, we
implemented a cross-validation procedure.
Following the approach in Dimple et al. (2023), which targets latent clustering patterns in transient burst events, we
further employed the AutoGMM module (Athey et al. 2019) to cluster features extracted by the ResNet-AFFE model.
AutoGMM is an automated clustering tool that leverages Gaussian Mixture Models (GMMs) to infer the optimal
number of clusters, and it assumes data points arise from a mixture of multiple Gaussian distributions, aligning with
the statistical properties of GRB feature distributions. The algorithm first performs GMM clustering over a range of
candidate cluster counts, then infers the Gaussian distribution parameters and cluster assignments that best fit the
data—eliminating subjective manual cluster number selection. The four clusters identified via AutoGMM were visually
distinguished using distinct color mappings in the UMAP projections, and their corresponding representative light

10
Zhang et al.
−5
0
5
10
15
20
UMAP axis 1
−2.5
0.0
2.5
5.0
7.5
10.0
12.5
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
5
6
7
8
Log2(peak-SNR)
−5
0
5
10
15
20
25
UMAP axis 1
−2.5
0.0
2.5
5.0
7.5
10.0
12.5
15.0
UMAP axis 2
GRB of TP
GRB of FN
−2
−1
0
1
2
3
Log10(T90)
Figure 5.
UMAP visualization of feature space representations extracted from the final convolutional layer of the ResNet-
AFFE model. (Left) Points are color-coded according to log2(peak-SNR) values for individual GRBs. The purple contour
represents the kernel density estimate (KDE) of low-significance TP events (peak-SNR < 5σ). (Right) Points are color-coded
according to log10(T90) values from the Fermi burst catalog. The purple contour shows the KDE of short-duration TP events
(T90 < 2 s).
curves were presented around, as shown in Figure 6. Figure 7 further delineates the spatial distribution of physically
distinct GRB subgroups and their proportions within each cluster, including kilonova-associated GRBs (GRB-KN),
supernova-associated GRBs (GRB-SN), extended-emission GRBs (GRB-EE), ultra-long GRBs, and short/long GRBs
with precursors.
4. RESULT
All datasets used in this study are derived from Fermi/GBM observational data, divided into three defined subsets
for training, validation, and testing. The training set incorporates synthetic GRB samples generated via our proposed
data augmentation technique, designed to increase sample size and enhance the diversity of burst signals. The efficacy
of this method is visually validated in Figure 1, which illustrates controlled modulation of burst SNR while preserving
background levels. Focused on sampling and attenuation factors, the augmentation notably enriches low-SNR GRB
representation: as shown in Figure 2, the peak-SNR distribution of GRB samples expands toward lower values,
increasing total GRB training samples from 6,189 to over 100,000. Non-GRB samples—sourced from triggerless time-
tagged event (TTE) data—were similarly augmented to 100,000, ensuring class balance. For validation and testing, we
used 2,774 and 3,143 primary GRBs, respectively, selected based on the number of triggering NaI detectors. Detailed
dataset specifications are provided in Table 1. Each input sample consists of light curves across 9 energy bands (64
ms temporal resolution; Figure 3, left), with per-band standardization applied to reduce energy-specific magnitude
biases. This preprocessing preserved fine-grained features, enhanced the model’s feature extraction and generalization
capabilities, and facilitated subsequent visualization analyses.
Our proposed classification framework employs a ResNet-based 1D-CNN within a supervised learning paradigm,
designed for binary classification (GRB vs.
non-GRB). Its core capability lies in extracting discriminative fea-
tures from multidimensional data and differentiating classes based on these features.
The complete model archi-
tecture—incorporating the novel AFFE module—is shown in Figure 3.
The AFFE module employs a dual-path
filtering architecture (Equation 6): the term F⊙Fweighted performs frequency-adaptive soft-threshold denoising, while
F ⊙Ffiltered implements coherent feature extraction via an optimized matched-filter scheme. This integrated approach
enables adaptive frequency selection without manual filter design, achieving simultaneous noise suppression and signal
enhancement through data-driven spectral weighting. By weighting and filtering frequency-domain differences between
GRBs and non-GRBs, the module optimizes extraction of discriminative features critical for classification.
Model parameters were optimized via backpropagation and gradient-based methods on our large-scale datasets,
enhancing autonomous learning of salient features. Systematic evaluation of hyperparameter configurations identified
the optimal set (Table 2). Training and validation loss curves reflect stable dynamics: the early stopping mechanism
terminated training at epoch 18 (the optimal point), preserving parameters that generalize best to unseen data.
Convergence of both curves—with no divergence between training and validation losses—confirms robust performance

Advancing Identification of GRBs
11
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
14
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
−50
0
50
Time since trigger [s]
GRB141205337
0
50
100
Time since trigger [s]
GRB110125894
−50
0
50
Time since trigger [s]
GRB121211574
−50
0
50
Time since trigger [s]
GRB170915520
0
50
Time since trigger [s]
GRB090730608
0
50
Time since trigger [s]
GRB090926181
0
50
100
Time since trigger [s]
GRB241223506
0
50
100
Time since trigger [s]
GRB160530667
−50
0
50
Time since trigger [s]
GRB160421137
−50
0
50
Time since trigger [s]
GRB220426285
−50
0
50
Time since trigger [s]
GRB210615982
−100
−50
0
GRB110207959
0
50
GRB170723882
0
50
GRB231230062
0
50
100
Time since trigger [s]
GRB090227772
−50
0
GRB190606080
−50
0
50
GRB200415367
0
50
100
GRB120323507
Figure 6. The dimensionality reduction results of UMAP algorithm for the output features of the last convolution layer of the
ResNet-AFFE model, colored by their AutoGMM cluster assignments (four clusters). Surrounding panels display the full-band
light curves of representative GRBs from each cluster.
without overfitting. We conducted a comparative analysis of multiple models on the test set, including: a baseline
ResNet, ResNet variants with existing feature enhancement modules (SE, CBAM, ECA, ASB), and our ResNet-
AFFE. All models were trained with hyperparameters consistent with their original literature to ensure fairness, and
prediction confidence was estimated via Monte Carlo Dropout (1000 stochastic forward passes). Performance metrics
and uncertainties are summarized in Table 3. Incorporating the AFFE module yielded a statistically quantifiable
improvement in classification accuracy. Specifically, ResNet-AFFE achieved 97.46% accuracy—outperforming all tested
variants with conventional enhancement modules—with consistent results across multiple experimental runs. Grad-
CAM visualized the model’s decision-making process, revealing that ResNet-AFFE prioritizes physically meaningful
GRB characteristics. As shown in Figure 4, compared to the baseline ResNet, our model’s attention is more strongly
concentrated in the T90 interval (containing key prompt emission information) rather than random noise—highlighting
a critical difference in feature focus. This visualization enables even untrained users to distinguish model robustness,
even for identical predictions.
We applied UMAP to reduce the dimensionality of features from the last convolutional layer of ResNet-AFFE,
using 3,833 original GRBs as input. Figure 5 presents the 2D projection, encoded with individual GRBs’ peak-SNR
and T90 values, revealing distinct clustering patterns: true positive (TP) and false negative (FN) GRBs occupy entirely
separate regions. Of these GRBs, 593 were classified as FNs—99% with peak-SNR below 5σ and most being short-
duration events. Dimensionality reduction further uncovered organized spatial distributions: low-SNR GRBs clustered
in the upper-left region, while short-duration GRBs formed a narrow band along the upper-right edge, exhibiting a
characteristic nonlinear distribution. Notably, GRBs with similar durations or SNRs showed discernible aggregation.
Automated clustering via AutoGMM identified four distinct clusters from the features extracted by the ResNet-AFFE

12
Zhang et al.
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
GRB-KN
Cluster 1, 0.00%
Cluster 2, 37.50%
Cluster 3, 37.50%
Cluster 4, 25.00%
(a)
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
GRB-SN
Cluster 1, 31.25%
Cluster 2, 43.75%
Cluster 3, 18.75%
Cluster 4, 6.25%
(b)
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
GRB with EE
Cluster 1, 9.09%
Cluster 2, 31.82%
Cluster 3, 29.55%
Cluster 4, 29.55%
(c)
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
Super long GRBs
Cluster 1, 21.21%
Cluster 2, 36.36%
Cluster 3, 30.30%
Cluster 4, 12.12%
(d)
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
sGRB with precursor
Cluster 1, 44.00%
Cluster 2, 8.00%
Cluster 3, 36.00%
Cluster 4, 12.00%
(e)
−5
0
5
10
15
20
UMAP axis 1
−2
0
2
4
6
8
10
12
UMAP axis 2
GRB of TP
GRB of FN
1
2
3
4
Cluster by AutoGMM
lGRB with precursor
Cluster 1, 7.55%
Cluster 2, 50.00%
Cluster 3, 26.42%
Cluster 4, 16.04%
(f)
Figure 7.
The dimensionality reduction results of UMAP algorithm for the output features of the last convolution layer
of the ResNet-AFFE model, colored by their AutoGMM cluster assignments (four clusters). The triangular symbols in each
sub-graph are the same series of GRBs detected by Fermi/GBM: (a) Kilonova-associated GRBs (GRB-KN, Li et al. (2023)),
(b) Supernova-associated GRBs (GRB-SN) come from the GRBSN webtool (Finneran et al. 2024), (c) GRBs with extended
emission (GRB with EE, Garcia-Cifuentes et al. (2023); Li et al. (2024)), (d) Super long GRBs (Ror et al. 2024), (e) Short
GRBs with precursor (Zhong et al. 2019; Wang et al. 2020), (f) Long GRBs with precursor (Coppin et al. 2020; Deng et al.
2024).

Advancing Identification of GRBs
13
model eliminating subjective manual selection. Figure 6 illustrates representative light curves for each cluster, mapped
onto the UMAP-reduced space, directly linking clustering results to observable temporal morphology of GRBs. To
further explore the physical relevance of clustering, we analyzed correlations between model-extracted features and
GRBs of specific origins or morphologies, quantifying the proportion of these GRBs in each cluster to preliminarily
assess associations.
Figure 7 highlights these special GRBs in the dimensionality reduction projection: Kilonova-
associated GRBs are predominantly localized within Clusters 2 and 3, while supernova-associated events are primarily
concentrated in Clusters 1 and 2. GRBs with extended emission are uniformly distributed across Clusters 2 to 4. Ultra-
long GRBs are spatially confined to the lGRB-dominated Cluster 3, suggesting distinct feature space signatures. Short
GRBs with precursors exhibit a concentrated spatial distribution within Cluster 1, occupying both peripheral regions
and transitional zones bridging short and long GRB populations. Long GRBs with precursors contrastingly show half
their population concentrated in Cluster 2 with uniform dispersion. This visualization reveals natural separations in
the learned feature space between GRBs of different physical origins.
5. DISCUSSION AND CONCLUSION
This study develops an integrated deep learning framework for high-precision GRB identification, addressing core
challenges in transient astrophysical event detection through targeted methodological innovations.
The transient
nature of GRBs poses a fundamental obstacle to deep learning applications, marked by limited observable events and
significant observational bias favoring high-significance bursts—two longstanding limitations that our physics-informed
data augmentation framework directly overcomes.
By generating over 100,000 synthetic GRB samples that an order-of-magnitude expansion from the original 6,189
real-world observations, this framework effectively mitigates observational selection bias against low-significance events
and resolves the ”observational iceberg” effect described by L¨u et al. (2014) and Moss et al. (2022), where the majority
of astrophysical transients lie below conventional detection thresholds due to instrumental sensitivity limitations and
background contamination.
As demonstrated in Figure 2, our methodology successfully reduces the observational bias against low-significance
events, which constitute the predominant population of actual GRB phenomena yet remain systematically underrepre-
sented in existing catalogs. Specifically, we bin light curves into 9 distinct energy channels following traditional trigger
search algorithms (von Kienlin et al. 2020; Cai et al. 2021), preserving crucial spectral information while enhancing
signal detectability. We then systematically modulate burst SNR while retaining realistic background characteris-
tics, yielding physically faithful synthetic samples that span the full dynamic range of actual GRB phenomena. This
approach not only reduces model overfitting but also enhances sensitivity to weak transients—critical for capturing
the majority of GRBs that lie below conventional detection thresholds due to instrumental constraints. The pro-
posed augmentation framework offers substantial advantages for GRB identification systems by alleviating several
fundamental limitations in current approaches. Furthermore, the enhanced sample diversity allows the development
of detection algorithms with improved sensitivity to weak transients that typically evade identification in standard
analysis pipelines. As such, the augmentation strategy provides a generalizable solution for transient astronomy, where
limited mission durations and observational biases often hinder deep learning applications.
We design a novel Adaptive Frequency Feature Enhancement module, AFFE, which represents a methodological
advance in addressing the spectral-temporal complexity of GRB signals. Unlike conventional time-domain attention
mechanisms (e.g., SE, CBAM, ECA) that fail to capture frequency-domain correlations, or fixed-frequency filters
lacking adaptability, AFFE employs a dual-path architecture to perform data-driven spectral weighting—enabling
adaptive soft-threshold denoising and coherent feature extraction without manual filter design.
This mechanism
aligns with the complex, variable frequency signatures of GRBs, optimizing the model’s ability to distinguish GRBs
from non-GRB signals by leveraging discriminative frequency components. Integrating AFFE with a ResNet baseline
yields a classification accuracy of 97.46%, representing a statistically significant 3% improvement over the state-of-
the-art 2D ResNet-CBAM architecture (94.46%; (Zhang et al. 2024)). Comparative analysis of these models and
their predictive uncertainty reveal that our AFFE implementation shows superior performance metrics, establishing
a new benchmark for high-precision GRB identification. Comparative analysis further validates that this frequency-
adaptive architecture maintains performance advantages even near theoretical accuracy saturation, demonstrating its
superiority and robustness in handling the unique spectral dynamics of GRB light curves.
Further validation confirms that this frequency-adaptive architecture maintains performance advantages even near
theoretical accuracy saturation, demonstrating its superiority and robustness in handling the unique spectral dynam-

14
Zhang et al.
ics of GRB light curves, highlighting the methodological advancement in alleviating the unique challenges of GRB
identification.
As visualized in Figure 4, the model selectively attends to critical burst characteristics, exhibiting
strong concordance with domain expertise in GRB identification. The incorporation of the AFFE module signifi-
cantly enhances the ResNet architecture’s capacity to extract physically meaningful features, enabling comprehensive
characterization of burst processes. This Fourier-domain enhancement improves classification by explicitly modeling
spectral-temporal correlations—critical for distinguishing GRBs from noise, as key burst signatures (e.g., multi-phase
emission, spectral lags) are jointly encoded in both time and frequency domains. It is important to note that predictive
uncertainty remains a critical consideration in real-world astrophysical applications. while our model achieves high ac-
curacy, the confidence of predictions—especially for low-significance or atypical bursts—requires further quantification,
and future work should integrate uncertainty estimation techniques to better assess classification reliability.
Additionally, Grad-CAM feature visualization and UMAP dimensionality reduction confirm the model’s ability
to focus on physically meaningful T90 regions and reveal intrinsic clustering of GRBs by origin, morphology, and
observational properties (SNR, duration). The geometric structure of the low-dimensional UMAP embedding reflects
the model’s success in capturing fundamental physical differences between GRB subclasses. Misclassified GRBs are
predominantly located in low-SNR regions, suggesting that these faint detections require specialized further analysis.
The model exhibits clear distribution differences between long and short bursts, confirming its ability to capture
discriminative features for GRB classification and successfully capture subtle variations in light curve morphology and
temporal evolution patterns.
We also gain valuable physical insights from model-derived features. The dimensionality reduction and AutoGMM
clustering of model-extracted features uncover meaningful physical patterns in GRB populations. Kilonova-associated
GRBs (GRB-KN) predominantly cluster in Clusters 2 and 3, with long-duration GRB-KN in Cluster 2 sharing multi-
phase light curve morphologies—supporting a common progenitor system as proposed by previous theoretical work
(Zhu et al. 2022), suggesting a common progenitor system for these apparently long-duration events. A particularly
compelling case is presented by GRB 211211A and GRB 230307A, which exhibit nearly identical three-phase emission
structures (precursor, main burst, extended emission) and occupy adjacent regions in the low-dimensional embedding
space (Dimple et al. 2023; Negro et al. 2025). The comprehensive temporal and spectral analyses of their strikingly
similar properties indicate a common mechanism (Peng et al. 2024; Wang et al. 2025). This clustering pattern mirrors
yet remains distinct from the tight grouping of GRB-SN in Clusters 1 and 2, which similarly reflects their shared
progenitor physics (Kumar & Sharma 2024; Kumar 2025). GRBs with extended emission show spatial separation
into three subgroups highly correlated with duration, while ultra-long GRBs are confined to the lGRB-dominated
Cluster 3—hinting at potential subclasses with physical origins independent of standard duration-based classification.
Notably, sGRBs with precursors occupy transitional zones between short and long GRB clusters, indicating hybrid
characteristics and diverse progenitor systems. In contrast, lGRBs with precursors show widespread dispersion, im-
plying precursor activity is an independent physical process rather than a core burst characteristic. These findings
reinforce the limitations of duration-only GRB classification, aligning with prior studies (e.g., (Modak 2021)) that
highlight inherent uncertainties in this paradigm. The model’s ability to aggregate GRBs by physical origin rather
than just observational parameters demonstrates that machine learning-derived features can serve as a complementary
dimension for probing GRB progenitors, with future increases in the sample size of events with confirmed origins
essential for further validating these classification schemes.
Despite achieving high classification accuracy, the model exhibits residual misclassification of low-SNR GRBs (99%
of false negatives have peak-SNR < 5 σ), which may require specialized feature engineering or multi-modal data in-
tegration (e.g., combining light curves with spectral data, energy response-corrected data) for further improvement.
Predictive uncertainty quantification—critical for real-world astrophysical applications—also warrants deeper integra-
tion, as confidence assessment for low-significance or atypical bursts remains underdeveloped. Additionally, validating
the clustering results for special GRB subclasses (e.g., GRB-KN, precursor-containing GRBs) requires larger samples
of events with confirmed origins. Future work will focus on three key directions: (1) integrating Monte Carlo Dropout
or Bayesian neural networks to quantify prediction uncertainty and improve reliability for faint events; (2) expand-
ing the framework to process multi-modal data (spectral, temporal, and polarization information) for richer feature
extraction; (3) adapting the model for real-time analysis pipelines in time-domain and multi-messenger astronomy,
enabling faster detection and characterization of transient phenomena. The adaptive frequency-domain approach and
data augmentation strategy developed here are also generalizable to other transient astrophysical events (e.g., fast

Advancing Identification of GRBs
15
radio bursts, soft gamma-ray repeaters, supernovae), highlighting their broader potential for advancing observational
capabilities.
In summary, this study advances GRB identification through a synergistic combination of physics-informed data
augmentation and frequency-adaptive feature enhancement, effectively addressing core challenges of limited training
samples, observational selection bias, and inadequate discriminative feature extraction in GRB studies. By generat-
ing over 100,000 physically faithful pseudo-GRB samples, via our augmentation strategy, we effectively mitigate data
scarcity, alleviate the ”observational iceberg” effect, and substantially enhance the model’s generalization capability to
faint, low-significance bursts. We further elevate detection performance by developing a ResNet-based deep learning
framework integrated with the novel AFFE module, which adaptively weights and filters frequency-domain compo-
nents to capture key GRB signatures. The framework not only achieves high classification accuracy that outperforming
conventional attention mechanisms and frequency-domain approaches, but also provides actionable physical insights
into GRB populations by capturing intrinsic relationships between observational signatures and progenitor origins.
Through feature visualization and dimensionality reduction, we demonstrate that GRBs aggregated in the model’s
feature space share similar physical attributes, highlighting that machine learning-derived features offer an additional
dimension for probing GRB origins beyond traditional duration-based classification paradigms. We emphasize the
broad potential of adaptive frequency-domain analysis for advancing GRB identification and underscore the generaliz-
ability of our approach to other transient astronomical phenomena. As time-domain and multi-messenger astronomy
rapidly evolve, there is an urgent demand for higher accuracy, improved temporal resolution, and the ability to un-
cover latent feature relationships. Future work will integrate generalized machine learning models capable of processing
multi-band and multi-modal data into real-time analysis pipelines—enabling deeper feature extraction, faster transient
detection, and more precise characterization—thereby laying the groundwork for robust transient detection systems
that leverage the growing volume of data from next-generation astronomical missions.
6. ACKNOWLEDGEMENTS
We would like to thank Prof Rui Luo, Dr. Yi Yang, and Prof Jia-wei Luo for helpful discussion. This study is
supported by the National Key R&D Program of China (2024YFA1611703), the National Natural Science Foundation of
China (grant Nos. 12473044, 12494572, 12273042, 12133007, 41827807 and 61271351) and the Science and Technology
Innovation Plan of Shanghai Science and Technology Commission (22DZ1209500). This work is also partially supported
by the Strategic Priority Research Program of the CAS under grant No. XDA15360300. B. L. acknowledges support
from the National Astronomical Science Data Center Young Data Scientist Program (grant No. NADC2023YDS-04).
The code and data sets are available upon reasonable request.
REFERENCES
Abraham, S., Mukund, N., Vibhute, A., et al. 2021,
MNRAS, 504, 3084, doi: 10.1093/mnras/stab1082
Alzubaidi, L., Zhang, J., Humaidi, A. J., et al. 2021,
Journal of Big Data, 8.
https://api.semanticscholar.org/CorpusID:232434552
Athey, T. L., Liu, T., Pedigo, B. D., & Vogelstein, J. T.
2019, arXiv e-prints, arXiv:1909.02688,
doi: 10.48550/arXiv.1909.02688
Band, D. L. 2002, ApJ, 578, 806, doi: 10.1086/342661
Basora, L., Viens, A., Chao, M. A., & Olive, X. 2025,
Reliability Engineering & System Safety, 253, 110513,
doi: https://doi.org/10.1016/j.ress.2024.110513
Blackburn, L., Briggs, M. S., Camp, J., et al. 2013, arXiv
e-prints, arXiv:1303.2174, doi: 10.48550/arXiv.1303.2174
Cai, C., Xiong, S. L., Li, C. K., et al. 2021, MNRAS, 508,
3910, doi: 10.1093/mnras/stab2760
Chen, J.-M., Zhu, K.-R., Peng, Z.-Y., & Zhang, L. 2024,
MNRAS, 527, 4272, doi: 10.1093/mnras/stad3407
—. 2025, ApJS, 276, 62, doi: 10.3847/1538-4365/ada0b0
Coppin, P., de Vries, K. D., & van Eijndhoven, N. 2020,
PhRvD, 102, 103014, doi: 10.1103/PhysRevD.102.103014
Crupi, R., Dilillo, G., Bissaldi, E., et al. 2023, Experimental
Astronomy, 56, 421, doi: 10.1007/s10686-023-09915-7
Deng, H.-Y., Peng, Z.-Y., Chen, J.-M., & Zhu, D. 2024,
Research in Astronomy and Astrophysics, 24, 035013,
doi: 10.1088/1674-4527/ad0497
Dimple, Misra, K., & Arun, K. G. 2023, ApJL, 949, L22,
doi: 10.3847/2041-8213/acd4c4
Eldele, E., Ragab, M., Chen, Z., Wu, M., & Li, X. 2024,
arXiv e-prints, arXiv:2404.08472,
doi: 10.48550/arXiv.2404.08472

16
Zhang et al.
Finneran, G., Cotter, L., & Martin-Carrillo, A. 2024, arXiv
e-prints, arXiv:2411.08866,
doi: 10.48550/arXiv.2411.08866
Gal, Y., & Ghahramani, Z. 2015, arXiv e-prints,
arXiv:1506.02142, doi: 10.48550/arXiv.1506.02142
Garcia-Cifuentes, K., Becerra, R. L., De Colle, F., Cabrera,
J. I., & Del Burgo, C. 2023, ApJ, 951, 4,
doi: 10.3847/1538-4357/acd176
Guidorzi, C., Dichiara, S., & Amati, L. 2016, A&A, 589,
A98, doi: 10.1051/0004-6361/201527642
He, K., Zhang, X., Ren, S., & Sun, J. 2015, in Proceedings
of the 2015 IEEE International Conference on Computer
Vision (ICCV), ICCV ’15 (USA: IEEE Computer
Society), 1026–1034, doi: 10.1109/ICCV.2015.123
He, K., Zhang, X., Ren, S., & Sun, J. 2016, in Proceedings
of 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR ’16 (IEEE), 770–778,
doi: 10.1109/CVPR.2016.90
Hu, J., Shen, L., & Sun, G. 2018, in 2018 IEEE/CVF
CONFERENCE ON COMPUTER VISION AND
PATTERN RECOGNITION (CVPR), IEEE Conference
on Computer Vision and Pattern Recognition, IEEE;
CVF; IEEE Comp Soc, 7132–7141,
doi: 10.1109/CVPR.2018.00745
Jespersen, C. K., Severin, J. B., Steinhardt, C. L., et al.
2020, ApJL, 896, L20, doi: 10.3847/2041-8213/ab964d
Kingma, D. P., & Ba, J. 2014, arXiv e-prints,
arXiv:1412.6980, doi: 10.48550/arXiv.1412.6980
Kumar, A. 2025, NewA, 116, 102346,
doi: 10.1016/j.newast.2024.102346
Kumar, A., & Sharma, K. 2024, arXiv e-prints,
arXiv:2411.13242, doi: 10.48550/arXiv.2411.13242
Kumar, P., & Zhang, B. 2015, PhR, 561, 1,
doi: 10.1016/j.physrep.2014.09.008
Li, Q. M., Sun, Q. B., Zhang, Z. B., Zhang, K. J., & Long,
G. 2024, MNRAS, 527, 7111,
doi: 10.1093/mnras/stad3619
Li, Q. M., Zhang, Z. B., Han, X. L., et al. 2023, MNRAS,
524, 1096, doi: 10.1093/mnras/stad1648
Liu, M., Zeng, A., Lai, Q., & Xu, Q. 2020, T-WaveNet:
Tree-Structured Wavelet Neural Network for
Sensor-Based Time Series Analysis.
https://arxiv.org/abs/2012.05456
Liu, P., Wu, B., Li, N., et al. 2024, in ICASSP 2024 - 2024
IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 5960–5964,
doi: 10.1109/ICASSP48485.2024.10446883
L¨u, H.-J., Liang, E.-W., Zhang, B.-B., & Zhang, B. 2010,
ApJ, 725, 1965, doi: 10.1088/0004-637X/725/2/1965
L¨u, H.-J., Zhang, B., Liang, E.-W., Zhang, B.-B., &
Sakamoto, T. 2014, MNRAS, 442, 1922,
doi: 10.1093/mnras/stu982
Ma, P. X., Ng, C., Rizk, L., et al. 2023, Nature Astronomy,
7, 492, doi: 10.1038/s41550-022-01872-z
Margutti, R., & Chornock, R. 2021, ARA&A, 59, 155,
doi: 10.1146/annurev-astro-112420-030742
M´esz´aros, P. 2019, MmSAI, 90, 57.
https://arxiv.org/abs/1904.10488
Modak, S. 2021, Astronomy and Computing, 34, 100441,
doi: 10.1016/j.ascom.2020.100441
Moss, M., Lien, A., Guiriec, S., Cenko, S. B., & Sakamoto,
T. 2022, ApJ, 927, 157, doi: 10.3847/1538-4357/ac4d94
Negro, M., Cibrario, N., Burns, E., et al. 2025, ApJ, 981,
14, doi: 10.3847/1538-4357/ada8a9
Nemani, V., Biggio, L., Huan, X., et al. 2023, Mechanical
Systems and Signal Processing, 205, 110796,
doi: 10.1016/j.ymssp.2023.110796
Parmiggiani, N., Bulgarelli, A., Fioretti, V., et al. 2021,
APJ, 914, 67, doi: 10.3847/1538-4357/abfa15
Parmiggiani, N., Bulgarelli, A., Ursi, A., et al. 2023, APJ,
945, 106, doi: 10.3847/1538-4357/acba0a
Parmiggiani, N., Bulgarelli, A., Macaluso, A., et al. 2024,
arXiv e-prints, arXiv:2404.02107,
doi: 10.48550/arXiv.2404.02107
Pe’er, A. 2024, Galaxies, 13, 2,
doi: 10.3390/galaxies13010002
Peng, Z.-Y., Chen, J.-M., & Mao, J. 2024, ApJ, 969, 26,
doi: 10.3847/1538-4357/ad45fc
Ror, A. K., Gupta, R., Aryan, A., et al. 2024, ApJ, 971,
163, doi: 10.3847/1538-4357/ad5554
Rudolph, A., Petropoulou, M., Winter, W., & Boˇsnjak, ˇZ.
2023, ApJL, 944, L34, doi: 10.3847/2041-8213/acb6d7
Selvaraju, R. R., Cogswell, M., Das, A., et al. 2017, in 2017
IEEE International Conference on Computer Vision
(ICCV), Venice, Italy, 618–626,
doi: 10.1109/ICCV.2017.74
Shorten, C., & Khoshgoftaar, T. M. 2019, JOURNAL OF
BIG DATA, 6, doi: 10.1186/s40537-019-0197-0
Sun, H., Wang, C. W., Yang, J., et al. 2023, arXiv e-prints,
arXiv:2307.05689, doi: 10.48550/arXiv.2307.05689
Tarnopolski, M., & Marchenko, V. 2021, ApJ, 911, 20,
doi: 10.3847/1538-4357/abe5b1
Taye, M. M. 2023, computation, 11, 52,
doi: 10.3390/computation11030052
von Kienlin, A., Meegan, C. A., Paciesas, W. S., et al. 2020,
ApJ, 893, 46, doi: 10.3847/1538-4357/ab7a18
Wang, C.-W., Tan, W.-J., Xiong, S.-L., et al. 2024, arXiv
e-prints, arXiv:2407.02376,
doi: 10.48550/arXiv.2407.02376

Advancing Identification of GRBs
17
—. 2025, ApJ, 979, 73, doi: 10.3847/1538-4357/ad98ec
Wang, J.-S., Peng, Z.-K., Zou, J.-H., Zhang, B.-B., &
Zhang, B. 2020, ApJL, 902, L42,
doi: 10.3847/2041-8213/abbfb8
Wang, Q., Wu, B., Zhu, P., et al. 2020, in 2020 IEEE/CVF
CONFERENCE ON COMPUTER VISION AND
PATTERN RECOGNITION (CVPR 2020), IEEE
Conference on Computer Vision and Pattern
Recognition, IEEE; CVF; IEEE Comp Soc, 11531–11539,
doi: 10.1109/CVPR42600.2020.01155
Woo, S., Park, J., Lee, J.-Y., & Kweon, I. S. 2018, in
Computer Vision – ECCV 2018, ed. V. Ferrari,
M. Hebert, C. Sminchisescu, & Y. Weiss (Cham: Springer
International Publishing), 3–19,
doi: 10.1007/978-3-030-01234-2 1
Wu, H., Hu, T., Liu, Y., et al. 2022, arXiv e-prints,
arXiv:2210.02186, doi: 10.48550/arXiv.2210.02186
Zhang, B. 2011, Comptes Rendus Physique, 12, 206,
doi: 10.1016/j.crhy.2011.03.004
Zhang, P., Li, B., Gui, R., et al. 2024, ApJS, 272, 4,
doi: 10.3847/1538-4365/ad2de5
Zhang, S., Shao, L., Zhang, B.-B., et al. 2022, ApJ, 926,
170, doi: 10.3847/1538-4357/ac4753
Zhong, S.-Q., Dai, Z.-G., Cheng, J.-G., Lan, L., & Zhang,
H.-M. 2019, ApJ, 884, 25, doi: 10.3847/1538-4357/ab3e48
Zhou, T., Ma, Z., Wen, Q., et al. 2022, arXiv e-prints,
arXiv:2201.12740, doi: 10.48550/arXiv.2201.12740
Zhou, Z.-M., Wang, X.-G., Liang, E.-W., et al. 2024, ApJ,
972, 190, doi: 10.3847/1538-4357/ad5f90
Zhu, J.-P., Wang, X. I., Sun, H., et al. 2022, ApJL, 936,
L10, doi: 10.3847/2041-8213/ac85ad
Zhu, S.-Y., Sun, W.-P., Ma, D.-L., & Zhang, F.-W. 2024,
MNRAS, 532, 1434, doi: 10.1093/mnras/stae1594
