Chain-of-Visual-Thought:
Teaching VLMs to See and Think Better with Continuous Visual Tokens
Yiming Qin1
Bomin Wei2
Jiaxin Ge1
Konstantinos Kallidromitis3
Stephanie Fu1
Trevor Darrell1
Xudong Wang1†
1UC Berkeley
2UCLA
3Panasonic AI Research
†Corresponding authors
Vision-Language Models (VLMs)
text tokens 
(discrete)
visual tokens 
(continuous)
Chain-of-Visual-Thought (CoVT)
Answers
How many clouds 
within the image?
<think>
</think>
….
….
text responses 
(discrete)
Figure 1.
Rather than restricting VLM reasoning to the discrete language space with limited representational capacity, COVT
forms a visual thought chain that enables VLMs to reason in continuous visual space. By introducing continuous visual tokens that
encode perceptual cues (e.g., segmentation, depth, instance, and edge structure), COVT composes chains of textual and visual thoughts
that link semantic reasoning with perceptual grounding. These visual “thought chains” bridge language and vision, enabling fine-grained
understanding, spatial precision, and geometric awareness beyond the reach of text-based reasoning.
Abstract
Vision–Language Models (VLMs) excel at reasoning in
linguistic space but struggle with perceptual understand-
ing that requires dense visual perception, e.g., spatial rea-
soning and geometric awareness.
This limitation stems
from the fact that current VLMs have limited mechanisms
to capture dense visual information across spatial dimen-
sions. We introduce Chain-of-Visual-Thought (COVT), a
framework that enables VLMs to reason not only in words
but also through continuous visual tokens—compact latent
representations that encode rich perceptual cues. Within a
small budget of roughly 20 tokens, COVT distills knowl-
edge from lightweight vision experts capturing comple-
mentary properties such as 2D appearance, 3D geome-
try, spatial layout, and edge structure.
During training,
the VLM with COVT autoregressively predicts these vi-
sual tokens to reconstruct dense supervision signals (e.g.,
depth, segmentation, edges, and DINO features). At infer-
ence, the model reasons directly in the continuous visual
token space, preserving efficiency while optionally decod-
ing dense predictions for interpretability. Evaluated across
more than ten diverse perception benchmarks, including
CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA,
and HRBench, integrating COVT into strong VLMs such
as Qwen2.5-VL and LLaVA consistently improves perfor-
mance by 3% to 16% and demonstrates that compact con-
tinuous visual thinking enables more precise, grounded, and
interpretable multimodal intelligence. Our website is avail-
able at https://wakalsprojectpage.github.io/comt-website.
1. Introduction
Vision–Language Models (VLMs) [2, 3, 13, 16, 31, 44, 45,
60, 67, 73] have become the cornerstone of modern multi-
modal intelligence, achieving remarkable progress in under-
standing and reasoning across text and vision. By project-
ing visual input into a language-centric token space, VLMs
inherit the strong compositional and logical reasoning capa-
bilities of large language models (LLMs), enabling unified
multimodal interaction through natural language. Recent
advances in text-based Chain-of-Thought (CoT) reason-
ing [54] further extend this paradigm, showing that struc-
tured intermediate reasoning steps can significantly enhance
performance on tasks involving logic, mathematics, and
knowledge grounding. However, despite these successes,
1
arXiv:2511.19418v1  [cs.CV]  24 Nov 2025

(a) Counting
Answer:  
<think> Because ……, the 
segmentation of the image is  
                   …. </think> 
···
Segment 
tokens
(b) Relative Depth
Question: Two 
points are circled 
on the image. 
Which one is closer 
to the camera? 
A
B
segment mask
Question: How 
many uncut fruits are 
in the image?
Decoder 
(optional)
Answer:  
<think> Because ……, the 
depth map of the image is 
                          …… </think> 
depth map
Decoder 
(optional)
A
B
···
Depth  
tokens
(c) Scene  
Understanding
Question: Is the 
wall behind the bed 
empty or is there a 
painting hanging 
on the wall?
Answer:  
<think> Because the segmentation of the image 
is                     . And, because the depth map of  
the image is                           </think> 
···
Segment 
tokens
···
Depth tokens
depth map
segment mask
Decoder 
(optional)
<answer> There are three 
uncut fruits in the image.  
</answer>
<answer> Point B is closer 
to the camera. </answer>
<answer>The wall behind the bed has a 
painting hanging on it.</answer>
Figure 2. Continuous visual thinking with COVT. COVT introduces compact, continuous visual tokens that encode fine-grained per-
ceptual cues, such as object localization, spatial structure, and scene semantics, directly into VLM reasoning. These tokens ground
multimodal reasoning in visual space, enabling the model to capture fine-grained relationships across vision-centric tasks (e.g., counting,
depth ordering, and scene understanding) without relying on external tools. They can also be decoded into dense predictions, offering
human-interpretable visualizations of the model’s reasoning process.
such reasoning remains fundamentally language-bound.
When continuous visual information is projected into
discrete text space, rich perceptual cues, e.g., boundaries,
layout, depth, and geometry, are lost or poorly represented.
Yet these are precisely the fine-grained signals that hu-
mans rely on when reasoning about the visual world. Con-
sequently, current VLMs often struggle with perception-
intensive tasks such as counting, spatial correspondence, or
relative depth estimation, even when equipped with pow-
erful vision encoders [18, 39, 53], as shown in Fig. 2.
Moreover, by forcing vision reasoning through a discrete
text bottleneck, the model must verbalize continuous spa-
tial and geometric relations.
As a result, text-only CoT
can misdirect and even degrade visual reasoning perfor-
mance, as shown by Qwen3-VL-Thinking [3, 61], which
performs over 5% worse than Qwen3-VL-Instruct with lan-
guage CoT on spatial understanding benchmarks such as
V [58], HRBench8k [50], and VSI-Bench [62]. This ex-
poses a fundamental limitation: visual information is in-
herently continuous and high-dimensional, yet existing
models reason over it using symbolic language tokens that
lack the fidelity of complex perceptual reasoning.
A natural solution is to augment VLMs with external vi-
sion tools [22, 43], leveraging pre-built specialized models
to recover fine-grained perception. While this approach can
partially restore spatial and geometric information, it also
introduces significant drawbacks: perception is delegated to
external tools, and outcomes are bounded by them. It also
introduces higher GPU cost. Another solution is generating
or cropping images in the thinking process. However, these
solutions still project the images into the text space, losing
the dense visual information. These limitations motivate a
central question: Can VLMs learn to reason the way hu-
mans do, by thinking visually rather than translating ev-
erything into words? More concretely, can we inject fine-
grained visual signals directly into a VLM’s reasoning pro-
cess, allowing it to “see” and “think” simultaneously while
remaining efficient and self-contained? Yes! We propose
Chain-of-Visual-Thoughts (COVT).
COVT enables reasoning over rich perceptual cues by
grounding VLMs in continuous visual token space. Each
group of visual tokens corresponds to a lightweight percep-
tual expert (e.g., segmentation, depth, edge detection, or
self-supervised representation learning) that encodes spe-
cific visual features. During training, the VLM is asked
to predict these continuous visual tokens within its rea-
soning chain, compressing rich perceptual information into
a compact latent space. These latent tokens are then de-
coded by task-specific lightweight decoders to reconstruct
the corresponding expert targets (e.g., segmentation masks,
dense depth maps, edge maps, or DINO features). We back-
propagate the reconstruction and distillation losses through
the continuous tokens, aligning the model’s internal latent
representations with expert guidance. This process allows
COVT to internalize fine-grained perceptual knowledge di-
rectly into its token space, enabling grounded reasoning
without explicit visual maps or external tool calls.
More specifically, we highlight different aspects of
fine-grained visual reasoning.
We integrate both task-
oriented experts (e.g., SAM [27], DepthAnything v2 [63],
PIDINet [42]) and representation-based experts (e.g.,
DINO [7], contrastive encoders), with alignment strategies
tailored to each: task-oriented signals are aligned at the
prompt level, while representation-based signals are aligned
in feature space. Training proceeds through four stages,
including comprehension, generation, reasoning, and effi-
cient reasoning, gradually teaching the model to reason ef-
fectively with visual thoughts.
At inference, the model forms chains of visual thoughts,
reasoning across modalities to produce answers that are
2

both semantically coherent and perceptually grounded. This
self-contained, differentiable process enables VLMs to
“think” directly in continuous visual space, thereby provid-
ing a more faithful bridge between internal reasoning and
perceptual understanding. Moreover, this design supports
interpretable multimodal intelligence, allowing users to vi-
sualize the model’s visual thinking process when desired.
If visualization is not required, COVT can operate solely
on the continuous visual tokens without decoding them into
dense predictions, thus maintaining efficiency.
Evaluated across diverse perception benchmark, COVT
consistently improves fine-grained visual reasoning, outper-
forming strong VLM baselines on vision-centric tasks while
maintaining competitive performance on general (non-
vision-centric) benchmarks. For example, COVT achieves
a 5.5% overall gain on CV-Bench [46], delivering a substan-
tial 14.0% improvement on its depth sub-task, and 4.5%
overall gain on HRBench [49]. In addition, COVT offers
flexible interpretability: the continuous visual tokens can be
decoded into human-readable dense predictions, providing
a window into the model’s underlying visual reasoning pro-
cess when desired. Together, these results demonstrate that
compact continuous visual thinking enables more precise,
grounded, and interpretable multimodal intelligence.
The main elements of our contribution are as follows:
• We propose Chain-of-Visual-Thought, a framework that
equips VLMs with the ability to reason through continu-
ous visual tokens, compact perceptual representations that
serve as the building blocks for multimodal thinking.
• We develop tailored alignment strategies and a training
pipeline (comprehension, generation, reasoning, and effi-
cient reasoning) that enable VLMs to learn, interpret, and
reason effectively within continuous visual space.
• We demonstrate consistent performance gains across di-
verse benchmarks, showing that continuous visual tokens
enhance both perceptual grounding and interpretability.
2. Related Work
Tool-Augmented Reasoning Equipping VLMs with exter-
nal tools enables them to use specialized vision models for
targeted visual tasks [22, 34, 43, 56, 65]. While this im-
proves performance, it also introduces computational over-
head. Moreover, tool usage is inherently constrained, as the
final performance is bounded by the ability of each tool in-
stead of the reasoning process itself. In this work, we con-
sider self-contained visual reasoning, which conduct rea-
soning flexibly and does not rely on external vision tools.
Text Space Reasoning Text space reasoning methods, such
as Chain-of-Thought [28], has achieved big success in lan-
guage reasoning [35, 51, 52, 55], solving problems like
math, science, and logical reasoning. The strong perfor-
mance of LLMs with CoT capabilities has led to its broad
adoption and success in models such as DeepSeek-R1 [12].
Desired Properties
VCoT
MCoT
VChain
Aurora
Ours
Operates without relying
on external tools
✓
✗
✓
✓
✓
Reasons in the continuous
visual space
✗
✓
✓
✗
✓
Leverages dense visual
information for reasoning
✗
✓
✗
✓
✓
Has 3D-aware perception
✗
✗
✗
✓
✓
Table 1. Comparison of key properties with prior multimodal
reasoning methods. Unlike prior methods such as VCoT [41],
MCoT [70], VChain [25], and Aurora[5], COVT uniquely satis-
fies all desired properties: it reasons in continuous visual space,
leverages dense visual cues, maintains 3D awareness, and oper-
ates fully without external tools. Desired and undesired properties
are shown in green and magenta, respectively.
With the success of text CoT, many works have extended
reasoning to the visual modality.
A straightforward ap-
proach is to generate dense captions and then reason in lan-
guage space [33], but this process is inherently lossy.
We compare COVT with recent multimodal reasoning
paradigms in Tab. 1. Visual CoT [41] relies on textual inter-
pretations of images, limiting reasoning to the discrete text
space. MCoT [11] enables continuous visual reasoning by
editing or generating supplementary images, but requires
substantial compute and lacks flexibility. VChain [26] in-
terleaves images and text in the reasoning chain, yet still
loses visual information by projecting images into text
space.
COVT uniquely combines continuous visual rea-
soning, dense perceptual cues, and 3D-aware understanding
within a single self-contained framework.
Latent Space Reasoning Concurrent work shows that rea-
soning in latent space can strengthen LLMs in complex,
multi-step tasks [6, 8]. Coconut [23] finds that continu-
ous latent embeddings are more efficient than explicit CoT,
while CCoT [10] compresses CoT into continuous tokens
for denser reasoning. Other studies explore specialized rea-
soning tokens [20] or use hidden states as implicit reason-
ing paths [14]. Latent reasoning has also been extended to
VLMs. Aurora [4] employs VQ-VAE latents of depth and
detection signals to enhance depth estimation and counting,
whereas Mirage [64] uses latent imagination for visual rea-
soning tasks. Our work, COVT, builds upon these previous
contributions: we introduce a form of tool-use directly em-
bedded in continuous latent space, where the implicit ‘tools’
are visual thinking tokens tied to specific perceptual experts.
3. Chain-of-Visual-Thought (COVT)
We first introduce the preamble in Sec. 3.1. We then show
the overall pipeline of COVT in Sec. 3.2. We also discuss
how we select the visual token categories and explain how
different visual tokens are aligned (Sec. 3.3). Finally, we
present the model training pipeline, e.g., the training loss
formulation and the data framework design in (Sec. 3.4).
3

Vision-Language Models (VLMs)
Answering 
<answer> The image depicts a charming, 
handcrafted scene featuring a small doll 
dressed in an orange hooded coat with 
buttons down the front.  
The doll has a cheerful expression and is 
holding a yellow balloon shaped like a 
smiling sun.  
The background consists of a soft, 
pastel blue sky adorned with four fluffy 
white clouds, each cloud having a 
brown button-like detail.  
Below the sky, there are two green, 
conical trees made from felt or similar 
material, adding a touch of nature to the 
scene.  
The doll stands on a circular patch of 
green felt, which resembles grass, 
enhancing the overall whimsical and 
cozy atmosphere of the image.  
</answer>
Visual Thinking 
Question: Please 
summarize what the 
picture describes.
projection
SAM 
Decoder
<think>
projection
BMM 
Layer
projection
Conv 
Layer
projection
DINO  
features
</think>
GT mask 
MSE Loss
Focal Loss +  
Dice Loss + CE Loss
SAM 
Encoder
GT depth map 
L1 Loss 
+ CE Loss
GT  edge map 
CE Loss
CE Loss
CE Loss
CE Loss
PIDINet 
Encoder
Because the  
segmentation 
masks of the 
image is 
Because the 
depth map of 
the image is 
Because the 
edge map of 
the image is 
Because the 
patch 
features of 
the image is 
DepthAny 
Encoder
CE Loss
L1 Loss 
+ CE Loss
Figure 3. The training pipeline of COVT. COVT first generates the thinking process, containing visual thinking tokens, and then
leverages these visual thoughts to condition next token prediction and reason the final answer. To endow these tokens with perceptual
meaning, we align them with lightweight vision experts (e.g., SAM, DepthAnything, PIDINet, DINO) on their respective tasks during
training. Specifically: SAM uses 8 visual tokens as mask prompts; DepthAnything uses 4 tokens to reconstruct depth; PIDINet uses
4 tokens to reconstruct edges; and DINO uses 4 tokens to match patch-level features. The VLM is finetuned with LoRA and all the
projection layers are trainable. Note: During inference, dense predictions are decoded only when interpretability is desired; otherwise,
reasoning occurs entirely in the latent visual space.
3.1. Preamble
Existing VLMs face two key limitations in fine-grained
visual reasoning. 1) Text-only CoT accumulates errors.
Text-only CoT executes a long chain of thought, which may
generate errors at the early stage. These mistakes will ac-
cumulate and ultimately lead to an incorrect final result.
Therefore, we need a reasoning that is short and effective.
2) Supervision is dominated by text responses, which pro-
vides little incentive for the model to capture low-level per-
ceptual cues such as edges, depth, or regions. We need to
equip VLMs themselves with the capability of extracting
fine-grained visual information from the image, which can
be further decoded by vision decoders.
COVT intends to provides a foundation for the next
generation of multimodal reasoning systems, capable of
thinking fluidly across both language and vision in a self-
contained, interpretable manner.
3.2. COVT Overall Pipeline
We propose COVT, a framework that augments VLMs with
chains of visual thoughts. Fig. 3 illustrates the overview of
COVT pipeline. Essentially, this framework equips VLMs
with the capability of outputting fine-grained visual repre-
sentations within a continuous visual token space, enabling
them to reason directly over rich perceptual information and
maintain spatial and geometric coherence throughout the
reasoning process.
At its core, COVT retains the standard next-token pre-
diction paradigm. For standard VLMs, given visual features
V extracted from a frozen vision encoder and text features
T from a language encoder, the VLM estimates the proba-
bility of generating a sequence Y =(y1, y2, . . . , yn) as:
P(Y | V, T ; θ) =
n
Y
i=1
P (yi | y<i, V, T ) .
(1)
As shown in Fig. 3, COVT extends this formulation by in-
troducing Chain-of-Visual-Thought tokens, where each to-
ken yi can represent either a visual token or a text token.
To effectively incorporate COVT tokens into the VLM,
we train the model to function as a dense visual encoder ca-
pable of generating multiple visual tokens that capture di-
verse fine-grained perceptual cues. The VLM is trained to
generate CoVT tokens that, through task-specific decoders,
reconstruct visual outputs under reconstruction supervision.
Through this process, COVT evolves to generate rich, fine-
grained visual information across multiple perceptual di-
mensions within a thinking chain.
3.3. COVT Tokens
Token selection based on core perception ability. As pro-
posed in [72], the vision-centric perceptual ability of VLMs
can be summarized as (i) instance recognition, (ii) 2D and
3D spatial relationships, (iii) structure detection, and (iv)
deep mining of semantic information. Based on this cate-
gorization, we use four vision models to supervise COVT
tokens to learn each ability: 1) Segmentation tokens pro-
4

Original Question
<image> \n How many 
people are jumping in the air?
Original Answer
There are three persons 
jumping in the air.
Question: <image> the segmentation of the image is <segmentation>, the depth map is <depth>, 
the edge map is <edge>, and the patch feature is <dino>\n How many people are jumping in the air?
Stage 1:
Answer: There are three persons jumping in the air.
Stage 2:
Question: <image>\n What’s the segmentation, depth map, edge map, and the patch feature 
of the image?
Answer: <segmentation>, <depth>, <edge>, and <dino>.
Stage 3:
Question: <image>\n How many people are jumping in the air?
Answer: <think>The segmentation of the image is <segmentation>, the depth map of the 
image is <depth>, the edge map of the image is <edge>, and the patch feature of the image 
is <dino>.</think> <answer>There are three persons jumping in the air.</answer>
Question: <image>\n How many people are jumping in the air?
Answer: <think>The segmentation of the image is <segmentation>, and the patch feature of the 
image is <dino>.</think> <answer>There are three persons jumping in the air.</answer>
Stage 4:
Randomly drop 
visual anchors
Figure 4. Four-stage data formatting for COVT. The first stage helps the model comprehend the visual tokens, and the second stage
guides it to generate them. The third stage enables the VLM to integrate visual tokens into its reasoning process, while the final stage
allows the model to efficiently select and utilize visual thinking tokens within visual thought chains.
vide instance-level position and shape information, which
endow VLMs with the instance recognition signals and 2D
spatial perception. 2) Depth tokens provide pixel-level depth
information, equipping VLMs with the capability of figur-
ing out 3D spatial relationships. 3) Edge tokens provide
geometry-level details, which assist models to detect struc-
tural cues and partially provide 2D spatial information. 4)
DINO tokens provide the patch-level representation of the
images, delivering rich semantic information.
Tokens alignment based on granularity of visual models.
Task-oriented models and representative models produce
outputs at different levels of granularity. In general, task-
oriented models tend to be more fine-grained, whereas rep-
resentative models are usually less fine-grained. We adopt
different strategies to align each type of token with the vi-
sual models based on different granularities. Essentially, we
adopt two main alignment methods: For fine-grained task-
oriented models, visual tokens are projected to the prompt
space and then aligned at the prompt level with the de-
coders, while for representative models, alignment with the
encoders is applied at the feature level after the projection.
The projection layer consists of one multi-head attention
layer and two full connected layers.
1) Segmentation tokens are supervised by SAM [27],
which is a task-oriented model that contains dense visual
features. Therefore, following LISA [29], we align Seg-
mentation tokens with the SAM decoder. The 8 Segmenta-
tion tokens are aligned at the prompt level, and each token
prompts one mask, formulated as:
ˆ
Mi = Decoder(T sam
i
, f),
ˆ
Mi ∈[0, 1]H×W ,
(2)
where ˆ
Mi is the ith decoded mask, T sam
i
denotes the ith seg-
mentation token predicted in COVT, serving as the prompt
fed into the SAM decoder, and f means the dense embed-
ding from the SAM encoder. During the training process,
the Hungarian matching algorithm is employed to match the
predicted masks with the ground truths, while dice loss and
focal loss are applied.
2) Depth tokens are supervised by DepthAnything
v2 [63]–a task-oriented model.
Since these tokens con-
tain dense information, they are also aligned with the de-
coder at the prompt level. We use 4 Depth tokens to serve
as 4 prompts to interact with the dense features extracted
by DepthAnythingv2 through batch matrix multiplication
(BMM) to reconstruct the depth map, formulated as:
ˆDi = softmax

T depth
i
· F depth⊤
i

,
(3)
where ˆDi denotes the ith reconstructed depth map, T depth
i
represents the ith depth visual token, and F depth
i
is the ith
middle layer feature from DepthAnything encoder. The fi-
nal depth map is ˆD =
P3
i=0 ˆ
Di
4
. The L1 reconstruction loss
is employed for aligning Depth tokens.
3) Edge tokens are aligned with PIDINet [42].
Each
of 4 Edge tokens functions as an 1 × 1 convolutional ker-
nel applied to the dense features from PIDINet encoder to
reconstruct the ith edge map ˆEi. The final edge map is
ˆE =
P3
i=0 ˆ
Ei
4
, and aligned via L1 loss function.
4) DINO tokens are supervised by DINOv2 [37], which
is trained as the representative model, extracting patch-level
features. Therefore, the 4 DINO tokens are mapped into the
same shape with DINO feature using the projection layer,
and aligned under an MSE objective.
3.4. COVT Training
Training Loss. During training, the joint loss function is
defined as:
Ltotal = Lce + γ
 λseg · Lseg
visual + λdepth · Ldepth
visual
+ λedge · Ledge
visual + λdino · Ldino
visual

,
(4)
5

Visual tokens
CV-Bench
Other vision-centric benchmarks
Seg
Depth
DINO
Edge
CVBench
Count
Depth
Dist.
BLINK
RW-QA
MMT
MMStar-P
MMVP
MME-RW
V*
HR4K
HR8K
Closed-source Models
Claude-4-Sonnet
76.3
62.2
77.7
80.5
39.6
63.7
-
58.8
48.7
-
15.2
32.3
22.7
GPT-4o
79.2
65.6
86.7
81.0
63.0
69.7
-
65.2
72.0
-
42.9
50.6
46.7
Qwen2.5-VL-7B
74.5
65.0
72.8
75.5
55.7
68.6
61.7
67.1
56.0
60.0
76.4
68.6
64.9
COVT (1 Visual Token)
✓
77.9
66.0
80.8
80.5
57.4
71.1
62.1
68.5
58.7
62.1
79.1
71.9
69.0
✓
78.7
65.4
83.2
78.2
56.4
71.5
62.7
69.9
58.7
62.0
79.1
71.9
69.4
✓
71.3
64.7
72.3
66.7
55.8
71.5
62.5
67.9
57.3
61.1
77.5
71.0
68.6
COVT (3 Visual Tokens)
✓
✓
✓
80.0
66.2
86.8
82.5
56.0
71.6
62.1
69.2
58.7
63.7
78.0
72.9
69.4
∆(vs Baseline)
+5.5
+1.2
+14.0
+7.0
+0.3
+3.0
+0.4
+2.1
+2.7
+3.7
+1.6
+4.3
+4.5
COVT (4 Visual Tokens)
✓
✓
✓
✓
79.8
66.1
89.2
80.5
56.2
71.8
61.9
68.4
56.7
63.3
78.5
72.5
69.9
∆(vs Baseline)
+5.3
+1.1
+16.4
+5.0
+0.5
+3.2
+0.2
+1.3
+0.7
+3.3
+2.1
+3.9
+5.0
Table 2. Comparison of COVT with the baseline and closed-source models. COVT delivers consistent improvements across all vision-
centric benchmarks and further reveals that each visual token type contributes most effectively to the tasks related to its rich information.
where Lce is the typical cross-entropy loss for VLMs, γ is
the coefficient of visual loss, and all of the λ coefficients are
the weighting factors for the losses of the corresponding vi-
sual tasks. During the inference process, the visual thinking
tokens are not decoded.
Additionally, our framework supports the flexible inte-
gration of new visual token types. Because the pipeline
follows a clean next-token prediction paradigm, additional
tokens can be incorporated with minimal modification.
Training Data.
To enable VLMs to progressively learn
the visual tokens while not losing ability in the text space,
COVT introduces four data formatting stages as shown
in Fig. 4.
This guides the VLMs to learn progressively
through the sequence from understanding visual tokens
(comprehension stage), to generating visual tokens (gen-
eration stage), to reasoning with chain of visual thoughts
(reasoning stage), and finally to dynamically using visual
tokens in the thinking chain (efficient reasoning stage).
In 1) comprehension stage, we insert visual tokens af-
ter <image> to teach the VLMs to learn the basic seman-
tics of the visual tokens. In 2) generation stage, we modify
the question and answer, as shown in Fig. 4, to guide the
VLMs to generate the visual tokens precisely. 3) reasoning
stage introduces the chain-of-visual-thought format, where
the visual tokens are used within the thinking process. This
teaches the model to leverage the visual tokens to derive the
final answers. 4) efficient reasoning stage randomly drops
out some sets (ranging from 0 to k, where k is the number
of token types) of visual tokens. With a portion of visual
token types, COVT learns to utilize all features effectively
rather than being constrained by a fixed output pattern.
The dataset used for training include: (1) a vision-centric
(and also real-world) subset of the LLaVA-OneVision
dataset [30].
(2) spatial perception data, including Tal-
lyQA [1] and ADE20K-Depth [4, 71].
CV-Bench
BLINK
Count Depth Dist. Count
Obj.
Loc.
Rel.
Depth
Vis.
Corr.
Vis.
Sim.
LLaVA
59.3
61.8
50.2
56.7
54.9
52.4
29.7
51.1
Aurora† (depth)
54.9
67.7
52.3
53.3
55.7
62.9
26.2
47.4
COVT (w/ Depth)
60.7
71.0
52.3
56.7
59.8
75.8
31.4
53.3
∆(vs Aurora)
+5.8
+3.3
+0.0
+3.4
+4.1
+12.9
+5.2
+5.9
Aurora† (count)
56.0
62.2
47.8
31.7
26.2
24.2
26.7
21.5
COVT (w/ Seg)
61.9
60.7
51.3
58.3
56.6
69.4
29.7
52.6
∆(vs Aurora)
+5.9
-1.5
+3.5 +26.6 +30.4 +45.2
+3.0 +31.1
Table 3. Comparison between COVT and Aurora based on
LLaVA-v1.5-13B. † indicates our reproduced results based on the
provided checkpoints.
4. Experiments
In the experiment section, we first describe the exper-
imental settings of Chain-of-Visual-Thought (COVT) in
Sec. 4.1.
Second, we introduce the benchmarks results
on both vision-centric and non-vision-centric datasets in
Sec. 4.2. Third, we present the quantitative results demon-
strating the advantages of COVT in Sec. 4.3. Finally, we
“visualize” the continuous visual tokens in Sec. 4.4 and con-
duct ablation studies in Sec. 4.5.
4.1. Experiment Details
In our experiments, Qwen2.5-VL-7B [3] is selected as
the main baseline, COVT uses LoRA [24] tuning method,
while the rank of LoRA is 16 and the LoRA alpha is 32.
The learning rate of LoRA is set as 5 × 10−5 and the pro-
jection layer learning rate is set to 1×10−5. The first phase
steps are 4000, second and third phase steps are 3000, and
the fourth phase steps are 5000. Batch size is set to 4. The
experiments are carried out on 1×A100 or 4×A6000 GPUs.
γ and all of the λ in Eq. 4 are set as 1.
6

Qwen2.5-VL: 
The chair is closer to the 
foreground and appears 
larger in the image, 
indicating it is closer to 
the camera.
Our CoVT: 
<think>
Because the depth map of the 
image is <depth>, and the edge 
map of the image is <edge>.
</think>
<answer>
The books (highlighted by a red 
box) are closer to the camera 
than the chair (highlighted by a 
blue box).
</answer>
Q: Which object is closer 
to the camera taking 
this photo, the books 
(highlighted by a red box) 
or the chair (highlighted 
by a blue box)? 
Decoded <depth>
Decoded <edge>
Qwen2.5-VL: 
Point A appears to be 
closer to the camera than 
point B. Point A is 
positioned more towards 
the foreground 
compared to point B.
Our CoVT: 
<think>
Because the segmentation of 
the image is <segmentation>, 
and the depth map of the image 
is <depth>.
</think>
<answer>
The answer is the point B.
</answer>
Q: Two points are circled 
on the image, labeled by 
A and B beside each 
circle. Which point is 
closer to the camera?
Decoded <seg>
Decoded <depth>
Qwen2.5-VL: 
There are two white 
vertical lines drawn on 
the ground in the image. 
These lines are part of 
the tennis court's 
boundary, marking the 
service boxes and 
sidelines.
Our CoVT: 
<think>
Because the segmentation of 
the image is <segmentation>, 
and the edge map of the image 
is <edge>.
</think>
<answer>
There are five vertical lines in 
the image.
</answer>
Q: How many white 
vertical lines are drawn 
on the ground?
Decoded <seg>
Decoded <edge>
Figure 5. Visualization of COVT tokens. Different visual tokens contribute complementary cues that enable the model to solve complex
perceptual reasoning tasks. Left: Segmentation tokens localize point B on the face, while the depth tokens capture the relative depth
relationships. Mid: Depth visual tokens provide depth map information, and the edge tokens help highlight the positions of two boxes.
Right: The Segmentation tokens identify the attended region, and the edge tokens delineate the fine-grained line structures.
4.2. Model Evaluation
All evaluations are performed using VLMEvalKit [15].
Vision-centric benchmarks.
Our main focus is on
CV-Bench.
In particular, from CV-Bench we highlight
the sub-tasks Count, Depth, and Distance.
These sub-
tasks act as precise indicators to validate the effective-
ness of our method. We further evaluate on other vision-
centric benchmarks, including BLINK [19], RealWorldQA
(RW-QA) [59], MMT-Bench (MMT) [66], MMStar [9],
MMVP [47], MME-RealWorld (MME-RW) [69], V*
Bench (V*) [57], and HRBench (HR4K and HR8K) [49].
Among them, for MMStar we specifically choose the
Coarse Perception, Fine-grained Perception, and Instance
Reasoning subsets (MMStar-P), as they are more aligned
with real-world reasoning.
Non-vision-centric benchmarks. Besides, we also evalu-
ate COVT on some non-vision-centric visual benchmarks
such as OCRBench [32], MME [17], MUIRBench [48],
HallusionBench
[21],
A-OKVQA
[40],
TaskMeAny-
thing [68], WeMATH [38], and WorldMedQA-V [36]. For
MME, we select the text-centric sub-task text translation as
the evaluation of the text-centric performance.
4.3. Quantitative Results
COVT outperforms the baseline across the vision-
centric benchmarks. As shown in Tab. 2, COVT is capable
of incorporating various kinds of visual tokens. We employ
three visual tokens, Segmentation, Depth, and DINO, as our
main results. Compared to the baseline, COVT consistently
achieve large gains across the main vision-centric bench-
marks. COVT improves by 5.5% on CV-Bench, 14.0% on
the subtask depth in CV-Bench, 3.7% on MME-RealWorld,
and 4.5% on HRBench8K. These results indicate COVT
with visual thinking chain improves across visual-centric
and fine-grained perceptual tasks.
COVT generalizes to the other baseline. In addition to
the experiment based on Qwen, COVT is also implemented
based on LLaVA-v1.5-13B, in order to compare COVT
with Aurora. As shown in Fig. 3, for COVT with depth to-
kens, it excels Aurora-depth by 12.9% on relative-depth in
BLINK. For COVT using segmentation COVT tokens, out-
performs Aurora-count by 26.6% on BLINK-count bench-
mark. These results indicate that COVT generalizes on var-
ious baselines across vision-centric tasks.
4.4. Qualitative Results
To better understand why COVT is effective, we select sev-
eral examples and decode the COVT tokens from the model
outputs to visualize whether these tokens provide useful in-
formation for reasoning.
Fig. 5 illustrates that different COVT tokens carry dif-
ferent rich fine-grained information, and the cues they pro-
vide are highly complementary. To be interpretable, COVT
tokens are decoded into fine-grained output (e.g. masks,
depth maps, edge maps). For the left example, the Seg-
mentation token provides 2D perceptual cues by localizing
“point B” on the face, while the Depth token supplies 3D
information, indicating that the face region is closer to the
camera than the surrounding areas. For the middle exam-
ple, rhe Depth token encodes depth perception, whereas the
Edge token supplies fine-grained boundary cues for the two
target objects. This example is from Depth sub-task in CV-
Bench, thus the synergy explains the 2.4% improvement ob-
served when COVT uses four visual tokens instead of three
on the CVBench-Depth task, as shown in Tab. 2. For the
right example, the Segmentation token localizes the target
region and the Edge token emphasize fine-grained bound-
aries, which is difficult for the Segmentation token, deriving
the correct answer through chains of visual thoughts.
4.5. Ablation Studies
Text-only
Chain-of-Thought
vs.
Chain-of-Visual-
Thought. To isolate the contribution of continuous visual
tokens, we conduct an ablation comparing text-only CoT
with our full COVT framework. For the text-only setting,
7

75.0
80.0
Score
74.574.474.7
CVBench
50.0
55.0
55.7
52.752.6
BLINK
65.0
70.0
68.6
65.5
68.4
RealWorldQA
57.5
60.0
62.5
65.0
61.762.1
61.0
MMT-Bench
60.0
65.0
70.0
67.1
63.764.4
MMStar-P
50.0
55.0
60.0
Score
56.0
51.3
53.3
MMVP
55.0
60.0
65.0
60.0
57.9
55.0
MME-RealWorld
70.0
75.0
80.0
76.4
70.7
78.0
VStarBench
65.0
70.0
75.0
68.6
71.1
73.172.9
HRBench4K
65.0
70.0
64.9
65.865.4
HRBench8K
Baseline
CoT
CoVT
LLaVA-CoT data
Our dataset
Figure 6. Text-only CoT vs COVT. COVT substantially enhances
VLMs’ capabilities on vision-centric tasks, whereas text-only CoT
can even degrade performance.
85.0
90.0
88.5
87.1
OCRBench
90.0
95.0
100.0
92.5
100.0
MME-translate
55.0
57.5
60.0
58.3 58.2
MUIRBench
50.0
52.5
52.9 52.7
HallBench
85.0
90.0
88.3
89.2
A-OKVQA
65.0
67.5
70.0
70.2 69.9
TaskMeAnything
50.0
55.0
53.1
51.1
WeMATHloose
25.0
30.0
25.7
31.3
WorldMedQA
62.0
63.0
64.0
65.0
66.0
67.0
68.0
69.0
Score
66.2
67.4
Average
Qwen2.5-VL-7B
Ours
Figure 7. Beyond the gains on vision-centric benchmarks, COVT
also achieves slight improvements on non–vision-centric tasks
we follow the CoT formatting paradigm used in the
LLaVA-CoT 100k dataset and apply the same formatting
to our own training data, ensuring full consistency with
COVT except for the absence of visual tokens. Fig. 6 shows
that text-only CoT not only fails to improve performance
on vision-centric reasoning tasks, but often degrades it. In
contrast, COVT consistently enhances performance across
vision-centric benchmarks, highlighting the necessity of
continuous visual tokens for effective visual reasoning.
Token Numbers. We ablate various numbers of segmenta-
tion visual tokens, as shown in Tab. 4. The “0 token” set-
ting corresponds to directly fine-tuning the base model on
our dataset. The “16 empty” setting replaces our 16 visual
thinking tokens with 16 ordinary tokens without any visual
alignment, serving as a pure latent-reasoning baseline. Set-
tings with 1, 8, and 32 Segmentation tokens vary the token
count while keeping the Depth and DINO tokens fixed at 4
each; the 8-token setting corresponds to our full model.
We observe that using too few Segmentation tokens leads
to performance degradation, though still better than the “0
token” baseline. However, increasing the token count to 32
harms performance, maybe due to the difficulty of align-
ing a large number of segmentation tokens. The poor per-
formance of the “16 empty” variant further highlights the
importance of visually aligned tokens. Overall, the results
demonstrate that 8 Segmentation tokens, together with 4
Depth and 4 DINO tokens, form a balanced and effective
configuration, and that visual alignment is essential for en-
hancing vision-centric perception in VLMs.
Quantity
CVBench BLINK RW-QA MM*-P MMVP V* HR4K
0 token
76.6
55.5
70.7
68.0
55.3
77.5
68.6
16 empty
75.7
56.0
70.3
67.9
56.7
77.5
68.1
1 token
78.9
55.6
70.8
68.8
56.7
78.5
73.0
8 tokens
80.0
56.0
71.6
69.2
58.7
78.0
72.9
32 tokens
73.9
54.4
68.4
62.1
55.3
77.2
70.8
Table 4. COVT Ablation on segmentation token numbers. The
appropriate token number is essential for COVT. 0 token is the
control group (direct fine-tuning), while 16 empty tokens serve to
isolate the role of the token embeddings themselves. 8 segmenta-
tion tokens perform the best.
Type
Align
CVBench BLINK RW-QA MM*-P MMVP V* HR4K
Seg
Feature
76.8
55.2
70.6
67.7
56.0
78.0
69.8
Ours
77.9
57.4
71.1
68.5
58.7
79.1
71.9
Depth Feature
77.0
54.2
70.5
67.6
55.3
78.0
71.3
Ours
78.7
56.4
71.5
69.9
58.7
77.5
71.9
Table 5. Our tailored alignment strategy plays a crucial role in
further enhancing the performance of COVT.
Segmentation and Depth Alignment Strategies. We ab-
late two alignment approaches. Our primary method aligns
COVT tokens through task decoders, enabling the tokens
to capture richer and more fine-grained perceptual cues. In
contrast, direct feature alignment applies an MSE loss be-
tween the visual tokens and the encoder features of the cor-
responding visual model, which inevitably loses important
perceptual details from the image.
As shown in Tab. 5, direct feature alignment consistently
underperforms COVT. These results highlight the impor-
tance of our tailored alignment strategies and demonstrate
that aligning visual tokens with decoders yields more effec-
tive and perceptually grounded representations.
COVT remains competitive across various non-vision-
centric benchmarks. Fig. 7 shows our method remains
comparable performance, with 1.2% improvement over
eight non-vision-centric benchmarks, demonstrating that
COVT does not lead to a notable degradation in the gen-
eralization, and even yields an improvement for overall.
5. Conclusions
In this paper, we introduced COVT, the chain of continu-
ous visual thoughts that enables vision–language models to
reason beyond discrete linguistic space by leveraging com-
pact, dense visual representations. COVT consistently im-
proves visual-centric reasoning across diverse perception
benchmarks and reveals that different types of visual tokens
contribute to complementary aspects of multimodal under-
standing. These findings suggest that COVT can serve as
a general framework for integrating fine-grained perceptual
reasoning into broader multimodal systems.
8

6. Acknowledgment
We sincerely thank Himanshu Dubey and Sowmay Jain for
API credit support from Anannas AI. We greatly thank
Baifeng Shi, Ji Xie, Shaofeng Yin for their insightful dis-
cussions and valuable feedback on our paper. We especially
thank Mahtab Bigverdi for her assistance in reproducing the
baseline results.
References
[1] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tal-
lyqa: Answering complex counting questions. In AAAI Con-
ference on Artificial Intelligence, 2018. 6
[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
mad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4 technical report.
arXiv preprint arXiv:2303.08774,
2023. 1
[3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun
Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhao-
hai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren
Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen
Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Jun-
yang Lin.
Qwen2.5-vl technical report.
arXiv preprint
arXiv:2502.13923, 2025. 1, 2, 6
[4] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen,
Dongping Chen, Linda Shapiro, and Ranjay Krishna. Per-
ception tokens enhance visual reasoning in multimodal lan-
guage models. In Computer Vision and Pattern Recognition,
2024. 3, 6
[5] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen,
Dongping Chen, Linda G Shapiro, and Ranjay Krishna. Per-
ception tokens enhance visual reasoning in multimodal lan-
guage models. In Proceedings of the Computer Vision and
Pattern Recognition Conference, pages 3836–3845, 2025. 3
[6] Eden Biran, Daniela Gottesman, Sohee Yang, Mor Geva, and
Amir Globerson. Hopping too late: Exploring the limitations
of large language models on multi-hop queries. In Confer-
ence on Empirical Methods in Natural Language Processing,
2024. 3
[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv’e J’egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. 2021
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 9630–9640, 2021. 2
[8] Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara
Prabhakar, Shelby Heinecke, Ricky Ho, Ph´ı ThiM`ui, Silvio
Savarese, Caiming Xiong, and Huan Wang. Language mod-
els are hidden reasoners: Unlocking latent reasoning capa-
bilities via self-rewarding. ArXiv, abs/2411.04282, 2024. 3
[9] Lin Chen, Jinsong Li, Xiao wen Dong, Pan Zhang, Yuhang
Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,
Dahua Lin, and Feng Zhao.
Are we on the right way
for evaluating large vision-language models?
ArXiv,
abs/2403.20330, 2024. 7
[10] Jeffrey Cheng and Benjamin Van Durme. Compressed chain
of thought: Efficient reasoning through dense representa-
tions. ArXiv, abs/2412.13171, 2024. 3
[11] Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun
Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi
Chen, Wanxiang Che, and Libo Qin.
Visual thoughts: A
unified perspective of understanding multimodal chain-of-
thought. ArXiv, abs/2505.15510, 2025. 3
[12] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,
Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-
rong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai
Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu
Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao
Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi
Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,
Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo,
Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han
Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian
Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li,
Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu,
Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai
Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai
Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang,
Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua
Zhang, Minghui Tang, Meng Li, Miaojun Wang, Ming-
ming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng
Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang,
Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen,
Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng
Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan,
S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao
Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wan-
jia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,
Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan
Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu,
Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng
Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xi-
aosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song,
Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q.
Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao
Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yi-
fan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang,
Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan
Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yun-
fan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang
Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li,
Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun
Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe
Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,
Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu,
Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan,
Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.
Deepseek-r1: Incentivizing reasoning capability in llms via
reinforcement learning, 2025. 3
[13] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou,
Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie,
Ziang Song, et al. Emerging properties in unified multimodal
pretraining. arXiv preprint arXiv:2505.14683, 2025. 1
9

[14] Yuntian Deng, Kiran Prasad, Roland Fernandez, Paul
Smolensky, Vishrav Chaudhary, and Stuart Shieber. Implicit
chain of thought reasoning via knowledge distillation. ArXiv,
abs/2311.01460, 2023. 3
[15] Haodong Duan, Junming Yang, Yu Qiao, Xinyu Fang, Lin
Chen, Yuan Liu, Xiao wen Dong, Yuhang Zang, Pan Zhang,
Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-
source toolkit for evaluating large multi-modality models.
Proceedings of the 32nd ACM International Conference on
Multimedia, 2024. 7
[16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab-
hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil
Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The
llama 3 herd of models. arXiv e-prints, pages arXiv–2407,
2024. 1
[17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,
Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A
comprehensive evaluation benchmark for multimodal large
language models. ArXiv, abs/2306.13394, 2023. 7
[18] Stephanie Fu, Tyler Bonnen, Devin Guillory, and Trevor
Darrell. Hidden in plain sight: Vlms overlook their visual
representations, 2025. 2
[19] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang,
Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and
Ranjay Krishna. Blink: Multimodal large language models
can see but not perceive. ArXiv, abs/2404.12390, 2024. 7
[20] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna
Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think be-
fore you speak: Training language models with pause tokens.
ArXiv, abs/2310.02226, 2023. 3
[21] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia
Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang,
Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.
Hal-
lusionbench: An advanced diagnostic suite for entangled
language hallucination and visual illusion in large vision-
language models.
2024 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR), pages 14375–
14385, 2023. 7
[22] Tanmay Gupta and Aniruddha Kembhavi. Visual program-
ming: Compositional visual reasoning without training. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 14953–14962, 2023. 2,
3
[23] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting
Hu, Jason E. Weston, and Yuandong Tian. Training large lan-
guage models to reason in a continuous latent space. ArXiv,
abs/2412.06769, 2024. 3
[24] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
Lora:
Low-rank adaptation of large language models.
ArXiv,
abs/2106.09685, 2021. 6
[25] Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul
Debevec,
and Ziwei Liu.
Vchain:
Chain-of-visual-
thought for reasoning in video generation. arXiv preprint
arXiv:2510.05094, 2025. 3
[26] Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul E.
Debevec, and Ziwei Liu. Vchain: Chain-of-visual-thought
for reasoning in video generation. ArXiv, abs/2510.05094,
2025. 3
[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chlo´e Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll´ar, and
Ross B. Girshick. Segment anything. 2023 IEEE/CVF In-
ternational Conference on Computer Vision (ICCV), pages
3992–4003, 2023. 2, 5
[28] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
Matsuo, and Yusuke Iwasawa. Large language models are
zero-shot reasoners. ArXiv, abs/2205.11916, 2022. 3
[29] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmenta-
tion via large language model. 2024 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages
9579–9589, 2023. 5
[30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng
Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and
Chunyuan Li.
Llava-onevision: Easy visual task transfer.
ArXiv, abs/2408.03326, 2024. 6
[31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning.
2024
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 26286–26296, 2023. 1
[32] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen
Yu, Chunyuan Li, Xucheng Yin, Cheng lin Liu, Lianwen Jin,
and Xiang Bai. Ocrbench: on the hidden mystery of ocr in
large multimodal models. Sci. China Inf. Sci., 67, 2023. 7
[33] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei
Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and
Ashwin Kalyan. Learn to explain: Multimodal reasoning via
thought chains for science question answering, 2022. 3
[34] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei
Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao.
Chameleon:
Plug-and-play compositional reasoning with
large language models. ArXiv, abs/2304.09842, 2023. 3
[35] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hal-
linan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha
Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta,
Bodhisattwa Prasad Majumder, Katherine Hermann, Sean
Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:
Iterative refinement with self-feedback, 2023. 3
[36] Jo˜ao Matos, Shan Chen, Siena Placino, Yingya Li, Juan Car-
los Climent Pardo, Daphna Idan, Takeshi Tohyama, David
Restrepo, Luis Filipe Nakayama, Jose M. M. Pascual-Leone,
Guergana K Savova, Hugo Aerts, Leo Anthony Celi, An-
Kwok Ian Wong, Danielle S. Bitterman, and Jack Gallifant.
Worldmedqa-v: a multilingual, multimodal medical exam-
ination dataset for multimodal language models evaluation.
ArXiv, abs/2410.12722, 2024. 7
[37] Maxime Oquab,
Timoth´ee Darcet,
Th´eo Moutakanni,
Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-
dez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ
Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra,
Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao
Xu, Herv´e J´egou, Julien Mairal, Patrick Labatut, Armand
10

Joulin, and Piotr Bojanowski. Dinov2: Learning robust vi-
sual features without supervision. ArXiv, abs/2304.07193,
2023. 5
[38] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong
Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe
Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao
Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and
Honggang Zhang. We-math: Does your large multimodal
model achieve human-like mathematical reasoning? ArXiv,
abs/2407.01284, 2024. 7
[39] Pooyan
Rahmanzadehgervi,
Logan
Bolton,
Moham-
mad Reza Taesiri, and Anh Totti Nguyen. Vision language
models are blind: Failing to translate detailed visual features
into words, 2025. 2
[40] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
Kenneth Marino, and Roozbeh Mottaghi.
A-okvqa:
A
benchmark for visual question answering using world knowl-
edge. In European Conference on Computer Vision, 2022. 7
[41] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuo-
fan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual
cot: Advancing multi-modal language models with a com-
prehensive dataset and benchmark for chain-of-thought rea-
soning. Advances in Neural Information Processing Systems
37, 2024. 3
[42] Z. Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi
Tian, Matti Pietik¨ainen, and Li Liu. Pixel difference net-
works for efficient edge detection. 2021 IEEE/CVF Interna-
tional Conference on Computer Vision (ICCV), pages 5097–
5107, 2021. 2, 5
[43] D´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt:
Visual inference via python execution for reasoning. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision, pages 11888–11898, 2023. 2, 3
[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,
Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a
family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023. 1
[45] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya
Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Ta-
tiana Matejovicova, Alexandre Ram´e, Morgane Rivi`ere,
et al.
Gemma 3 technical report.
arXiv preprint
arXiv:2503.19786, 2025. 1
[46] Shengbang Tong, Ellis L Brown, Penghao Wu, Sanghyun
Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,
Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang,
Rob Fergus, Yann LeCun, and Saining Xie.
Cambrian-1:
A fully open, vision-centric exploration of multimodal llms.
ArXiv, abs/2406.16860, 2024. 3
[47] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann
LeCun, and Saining Xie.
Eyes wide shut? exploring the
visual shortcomings of multimodal llms. 2024 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 9568–9578, 2024. 7
[48] Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu,
Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou,
Kai Zhang, Tianyi Yan, Wenjie Jacky Mo, Hsiang-Hui Liu,
Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan
Roth, Sheng Zhang, Hoifung Poon, and Muhao Chen. Muir-
bench: A comprehensive benchmark for robust multi-image
understanding. ArXiv, abs/2406.09411, 2024. 7
[49] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li
Shen, Yong Luo, and Dacheng Tao. Divide, conquer and
combine: A training-free framework for high-resolution im-
age perception in multimodal large language models. ArXiv,
abs/2408.15556, 2024. 3, 7
[50] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li
Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer
and combine: A training-free framework for high-resolution
image perception in multimodal large language models. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence, pages 7907–7915, 2025. 2
[51] Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning
without prompting, 2024. 3
[52] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed
Chi, Sharan Narang, Aakanksha Chowdhery, and Denny
Zhou. Self-consistency improves chain of thought reason-
ing in language models, 2023. 3
[53] XuDong Wang, Xingyi Zhou, Alireza Fathi, Trevor Darrell,
and Cordelia Schmid. Visual lexicon: Rich image features in
language space. In Proceedings of the Computer Vision and
Pattern Recognition Conference, pages 19736–19747, 2025.
2
[54] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large lan-
guage models. Advances in neural information processing
systems, 35:24824–24837, 2022. 1
[55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny
Zhou. Chain-of-thought prompting elicits reasoning in large
language models, 2023. 3
[56] Chenfei Wu, Sheng-Kai Yin, Weizhen Qi, Xiaodong Wang,
Zecheng Tang, and Nan Duan.
Visual chatgpt: Talking,
drawing and editing with visual foundation models. ArXiv,
abs/2303.04671, 2023. 3
[57] Penghao Wu and Saining Xie. V*: Guided visual search
as a core mechanism in multimodal llms. 2024 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 13084–13094, 2023. 7
[58] Penghao Wu and Saining Xie. V?: Guided visual search as
a core mechanism in multimodal llms. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13084–13094, 2024. 2
[59] XAI. Grok-1.5 vision preview, 2024. 7
[60] Ji Xie, Trevor Darrell, Luke Zettlemoyer, and XuDong
Wang.
Reconstruction alignment improves unified multi-
modal models. arXiv preprint arXiv:2509.07295, 2025. 1
[61] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chen-
gen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan
Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan
Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang,
Jianxin Yang, Jiaxin Yang, Jingren Zhou, Jingren Zhou,
11

Junyan Lin, Kai Dang, Keqin Bao, Ke-Pei Yang, Le Yu,
Li-Chun Deng, Mei Li, Min Xue, Mingze Li, Pei Zhang,
Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shi-Qiang
Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,
Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng
Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yinger Zhang, Yu
Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang,
Zhipeng Zhou, and Zihan Qiu.
Qwen3 technical report.
ArXiv, abs/2505.09388, 2025. 2
[62] Jihan Yang, Shusheng Yang, Anjali W Gupta, Rilyn Han,
Li Fei-Fei, and Saining Xie. Thinking in space: How mul-
timodal large language models see, remember, and recall
spaces. In Proceedings of the Computer Vision and Pattern
Recognition Conference, pages 10632–10643, 2025. 2
[63] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiao-
gang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-
thing v2. ArXiv, abs/2406.09414, 2024. 2, 5
[64] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and
Chuang Gan.
Machine mental imagery: Empower multi-
modal reasoning with latent visual tokens, 2025. 3
[65] Shaofeng Yin, Ting Lei, and Yang Liu. Toolvqa: A dataset
for multi-step reasoning vqa with external tools. 2025. 3
[66] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqiang Li, Han
Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo
Liu, Jiayi Lei, Quanfeng Lu, Runjian Chen, Peng Xu, Ren-
rui Zhang, Haozhe Zhang, Peng Gao, Yali Wang, Yuning
Qiao, Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmt-
bench: A comprehensive multimodal benchmark for eval-
uating large vision-language models towards multitask agi.
ArXiv, abs/2404.16006, 2024. 7
[67] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
Lucas Beyer. Sigmoid loss for language image pre-training.
2023 IEEE/CVF International Conference on Computer Vi-
sion (ICCV), pages 11941–11952, 2023. 1
[68] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong
He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha
Kembhavi, and Ranjay Krishna. Task me anything. ArXiv,
abs/2406.11775, 2024. 7
[69] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu,
Shuangqing Zhang, Jun Wu, Feng Li, Kun Wang, Qingsong
Wen, Zhang Zhang, Liang Wang, Rong Jin, and Tien-Ping
Tan. Mme-realworld: Could your multimodal llm challenge
high-resolution real-world scenarios that are difficult for hu-
mans? ArXiv, abs/2408.13257, 2024. 7
[70] Zhuosheng Zhang, Aston Zhang, Mu Li, George Karypis,
Alex Smola, et al. Multimodal chain-of-thought reasoning
in language models. Transactions on Machine Learning Re-
search. 3
[71] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Semantic understanding of
scenes through the ade20k dataset. International Journal of
Computer Vision, 127:302 – 321, 2016. 6
[72] Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu,
Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao
Wang, Hengbo Xu, Fei Luo, Xiaohua Chen, Xiaoshuai Hao,
Hehan Li, Andi Zhang, Wenxuan Wang, Lingling Li, Zhiwu
Lu, Yang Lu, and Yi wang Guo. From perception to cog-
nition: A survey of vision-language interactive reasoning in
multimodal large language models. ArXiv, abs/2509.25373,
2025. 4
[73] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shen-
glong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su,
Jie Shao, et al. Internvl3: Exploring advanced training and
test-time recipes for open-source multimodal models. arXiv
preprint arXiv:2504.10479, 2025. 1
12

Chain-of-Visual-Thought:
Teaching VLMs to See and Think Better with Continuous Visual Tokens
Supplementary Material
Table of Contents
A. Additional Details of COVT
13
A.1. Projection Layer . . . . . . . . . . . . . . .
13
A.2. Segmentation COVT Token Alignment . . .
13
A.3. Depth COVT Token Alignment . . . . . . .
14
A.4. Edge COVT Token Alignment . . . . . . . .
15
A.5. COVT Dataset Composition . . . . . . . . .
15
B. Additional Experiments
15
B.1. More settings . . . . . . . . . . . . . . . . .
15
B.2. Training Stage Impact Ablation . . . . . . .
16
B.3. Token Numbers Ablation . . . . . . . . . . .
16
B.4. More Results . . . . . . . . . . . . . . . . .
16
C. Limitations and Future Work
16
A. Additional Details of COVT
In this section, we provide a comprehensive description of
COVT architecture. We first detail the design and function-
ality of the projection layer in (Sec. A.1). Then we pro-
vide more specific alignment architecture about segmenta-
tion token, depth token, and edge token from (Sec.A.2) to
(Sec.A.4). Finally, we provide the detailed composition of
the dataset used in COVT in (Sec. A.5).
SAM
Decoder
Projection 
Layer
SAM
Encoder
Hungarian
Matching
Ground truth masks derived 
from all masks produced by 
SAM after quality filtering.
Dice Loss
+ Focal Loss
+ CE Loss
Predicted masks
Linear 
Layer
Cross 
Attention
Learnable 
query
Mapped tokens
Segmentation 
Token 
Alignment
Segmentation tokens
Figure 8. Detailed frameworks for the projection layer and seg-
mentation token alignment.
Batch Matrix 
Multiplication
Projection 
Layer
Depth tokens
Depth Anything
Encoder
L1 Loss
CE Loss
Predicted 
depth map
Ground truth 
depth map from 
Depth Anything
Depth Token 
Alignment
Mean
Figure 9. Detailed framework for the depth token alignment.
A.1. Projection Layer
As shown in Fig. 8, the details of the projection layer are
illustrated in the yellow block. It contains a single linear
layer that projects the VLM latent space into the decoder’s
prompt space (or the encoder’s feature space while aligning
DINO tokens), formulated as:
zm = Wz + b,
(5)
where z denotes the VLM latent feature and zm is the
mapped prompt-space feature after the linear layer. We then
introduce a learnable query q, while the mapped feature
serves as both the key k and value v in the cross-attention
layer, defined as:
zp = Attn(q, k, v) = softmax
 qk⊤
√dk

v,
(6)
where zp is the projected tokens, functioning as the prompts
for the subsequent visual model decoding.
A.2. Segmentation COVT Token Alignment
Our
model
first
predicts
eight
Segmentation
tokens
{T seg
i
}7
i=0. Shown in Fig. 8, each token is then projected
into the SAM decoder’s prompt space through the projec-
tion layer,
T sam
i
= proj(T sam
i
),
(7)
and the projected token serves as an individual prompt for
mask decoding. Given the projected prompt T sam
i
and the
13

Convolutional 
Layer
Projection 
Layer
Edge tokens
PIDINet
Encoder
L1 Loss
CE Loss
Predicted 
edge map
Ground truth 
edge map from 
PIDINet
Edge Token 
Alignment
Mean
Figure 10. Detailed framework for edge token alignment.
dense embedding f from the SAM encoder, the SAM de-
coder produces one mask per token:
ˆ
Mi = Decoder(T sam
i
, f) ,
ˆ
Mi ∈[0, 1]H×W .
(8)
To construct reliable supervision, we generate all masks
from SAM on the input image and apply a quality filter
based on mask area and stability score. From these, we
retain eight high-quality masks,
G = {Mj}7
j=0,
(9)
which serve as ground truths. We employ the Hungarian al-
gorithm to match each predicted mask with one SAM mask.
Unlike using similarity-based costs, we define the matching
cost directly using the segmentation losses. For each pair
( ˆ
Mi, Mj), the Dice loss and Focal loss are
Ldice( ˆ
Mi, Mj) = 1 −
2 P ˆ
MiMj
P ˆ
Mi + P Mj
,
(10)
Lfocal( ˆ
Mi, Mj) = −(1 −ˆ
Mi)γF Mj log ˆ
Mi,
(11)
where γF is set to 2. Therefore, the matching cost becomes
Ci,j = Ldice( ˆ
Mi, Mj) + α · Lfocal( ˆ
Mi, Mj),
(12)
where the α is set to 1 in our experiments. The optimal
assignment is then obtained via
σ∗= arg min
σ
7
X
i=0
Ci, σ(i).
(13)
Qwen2.5-VL-7B
1 type
3 types
4 types
Optimization
Optimizer
AdamW
Learning rate
5e-5
Projection layer lr
1e-5
lr schedule
cosine
β
(0.9, 0.999)
Weight decay
0.1
Warmup ratio
0.05
First stage steps
4K
6K
8K
Second stage steps
3K
Third stage steps
3K
Forth stage steps
4K
5K
6K
Per-GPU batch size
4
γ
1.0
LoRA settings
LoRA rank
16
LoRA alpha
32
Visual models
SAM Encoder
ViT-H
Depth Anything v2 Encoder
ViT-L
PIDINet Encoder
Table5-Baseline
DINO v2
ViT-L
Table 6. Fine-tuning hyperparameter setup.
After obtaining the matching, the final mask loss is com-
puted using the same Dice and Focal losses:
Lmask =
7
X
i=0
h
Ldice

ˆ
Mi, Mσ∗(i)

+ α · Lfocal

ˆ
Mi, Mσ∗(i)
i
.
(14)
A.3. Depth COVT Token Alignment
As shown in Fig. 9, our model predicts four Depth to-
kens {T depth
i
}3
i=0, each of which is first projected into the
DepthAnything decoder’s prompt space through a linear
projection layer:
T depth-s
i
= WdT depth
i
+ bd.
(15)
DepthAnything v2 provides four dense intermediate-layer
features
{F depth
i
}3
i=0,
where F depth
3
is the final-layer feature. Each projected depth
token interacts with its corresponding feature map through
batch matrix multiplication (BMM) to produce one depth
map. This process is formulated as:
ˆDi = softmax

T depth-s
i
· F depth
i
⊤
,
i = 0, . . . , 3,
(16)
where ˆDi denotes the ith reconstructed depth map. Then
the four reconstructed depth maps are averaged to produce
14

Total Rows
Is:
774.6k!
 Data Distribution of CoVT Dataset 
Subsets
tallyqa(filtered)
 150.0k (19.4%)
image_textualization(filtered)
 99.6k (12.9%)
tallyqa(cauldron_llava_format)
 98.7k (12.7%)
clevr(cauldron_llava_format)
 70.0k (9.0%)
allava_instruct_
 70.0k (9.0%)
sharegpt4o
 57.3k (7.4%)
sharegpt4v(coco)
 50.0k (6.5%)
sharegpt4v(llava)
 30.0k (3.9%)
iconqa(cauldron_llava_format)
 27.3k (3.5%)
iconqa(MathV360K)
 22.6k (2.9%)
llavar_gpt4_20k
 19.8k (2.6%)
st_vqa(cauldron_llava_format)
 17.2k (2.2%)
aokvqa(cauldron_llava_format)
 16.5k (2.1%)
visual7w(cauldron_llava_format)
 14.4k (1.9%)
sharegpt4v(sam)
 9.0k (1.2%)
hateful_memes(cauldron_llava_format)
 8.5k (1.1%)
vizwiz(MathV360K)
 6.6k (0.9%)
ADE20k(Aurora)
 5.0k (0.6%)
vsr(cauldron_llava_format)
 2.1k (0.3%)
Figure 11.
COVT dataset utilizes some subsets of LLaVA-
OneVision, and merges the filtered TallyQA dataset and ADE20K-
Depth from Aurora.
the final prediction:
ˆD = 1
4
3
X
i=0
ˆDi.
(17)
For supervision, we use the depth map predicted by
DepthAnything v2 as the ground truth, denoted as Dgt.
Depth tokens are aligned through an L1 reconstruction loss
between the final depth prediction and the ground truth:
Ldepth =
 ˆD −Dgt
1 .
(18)
A.4. Edge COVT Token Alignment
Similarly, as shown in Fig. 10, we first project the four
predicted Edge tokens into the PIDINet prompt space, ob-
taining {T edge
i
}3
i=0. Each projected token is then used as a
1 × 1 convolutional kernel and applied to the dense inter-
mediate features extracted from the PIDINet encoder. Let
{F edge
i
}3
i=0 denote the four intermediate feature maps. For
each level, the reconstructed edge map is obtained by
ˆEi = T edge
i
∗F edge
i
,
ˆEi ∈RH×W ,
(19)
where “∗” denotes a 1 × 1 convolution operation. The four
reconstructed edge maps are then aggregated by averaging,
followed by a sigmoid normalization:
ˆE = σ
 
1
4
3
X
i=0
ˆEi
!
.
(20)
For supervision, we use the edge map predicted directly by
PIDINet, denoted as Egt. The alignment between the pre-
dicted and ground-truth edges is enforced using the L1 loss:
Ledge =
 ˆE −Egt
1 .
(21)
A.5. COVT Dataset Composition
In order to fully leverage the value of COVT, we select the
vision-centric subsets from the LLaVA-OneVision dataset,
including:
• IconQA(MathV360K)
• VizWiz(MathV360K)
• allava instruct
• aokvqa(cauldron llava format)
• clevr(cauldron llava format)
• hateful memes(cauldron llava format)
• iconqa(cauldron llava format)
• image textualization(filtered)
• llavar gpt4 20k
• sharegpt4o
• sharegpt4v(coco)
• sharegpt4v(llava)
• sharegpt4v(sam)
• st vqa(cauldron llava format)
• tallyqa(cauldron llava format)
• visual7w(cauldron llava format)
• vsr(cauldron llava format)
In addition, we re-filtered the TallyQA dataset. Since
TallyQA is a counting dataset but contains many samples
with the answer 0, we reduce the proportion of zero-count
samples and construct a 150k-sample subset. Moreover, fol-
lowing the methodology used in the Aurora paper, we gen-
erate 5k samples related to relative depth from the ADE20K
dataset using the same procedure. We integrate these three
components to form the complete COVT dataset.
B. Additional Experiments
In this section, we first describe the experimental settings
used throughout our study in (Sec. B.1). We then present
ablation studies on the first two training stages in (Sec. B.2).
Subsequently, we investigate the impact of varying the num-
ber of COVT tokens in (Sec. B.3). Finally, we provide ad-
ditional output examples in (Sec. B.4).
B.1. More settings
In Tab. 6, we present the complete hyperparameter config-
urations to ensure full reproducibility of our experiments.
In this table, 1 type denotes that the model is aligned with a
single supervision signal, chosen from segmentation, depth,
or DINO tokens. 3 types corresponds to jointly aligning
the model with segmentation, depth, and DINO tokens. 4
types further incorporates edge tokens, thereby enabling si-
multaneous alignment across segmentation, depth, DINO,
and edge tokens. For hyperparameters that remain consis-
tent across all three experimental settings, we consolidate
the corresponding columns and report them using a single
centered entry for clarity and conciseness.
15

Quantity
CVBench BLINK RW-QA MM*-P MMVP V* HR4K
Stage 3 & 4
78.2
53.8
70.0
68.3
60.7
78.0
71.2
Ours
80.0
56.0
71.6
69.2
58.7
78.0
72.9
Table 7. The first two stages in COVT 4-stage training strategy
enhance the stability of the improvement.
B.2. Training Stage Impact Ablation
To elucidate the pivotal contribution of the first two stages
in the four-stage training strategy of COVT, we perform
an ablation study comparing the full model with a variant
trained solely on Stages 3 and 4, as reported in Tab. 7.
The model trained across all four stages exhibits consis-
tent and robust improvements over all evaluated bench-
marks. In contrast, when restricting training to only the
last two stages, COVT experiences notable degradation on
the BLINK benchmark and yields only marginal gains on
RealWorld-QA and MMStar-Perception. These results un-
derscore the critical role of the early-stage training sig-
nals and highlight their importance in shaping the model’s
downstream performance.
B.3. Token Numbers Ablation
To determine the optimal number of segmentation tokens,
we conduct a detailed ablation while fixing both depth and
DINO tokens at 4. As shown in Fig. 12, increasing the num-
ber of segmentation tokens in COVT from 1 to 32 yields
an initial performance gain followed by a gradual decline,
whereas the computational overhead rises steadily. Notably,
the overall time cost of COVT is considerably higher than
that of the baseline. Only a minor fraction of this over-
head originates from the additional COVT tokens; a more
substantial portion stems from COVT producing richer and
more fine-grained responses for tasks such as image cap-
tioning, which naturally leads to longer output sequences
(examples provided in Sec. B.4). Under our experimen-
tal conditions, eight segmentation tokens—combined with
four depth tokens and four DINO tokens—offer the most
favorable balance between performance and efficiency.
B.4. More Results
We provide additional VQA examples in Fig. 13 through
Fig. 17, including detailed image captioning, counting,
instance identification, depth-aware questions, real-world
OCR, and so on. In Fig. 13, we compare COVT with the
baseline and show that COVT offers more fine-grained cap-
tioning ability. In Fig. 14, COVT improves instance identi-
fication (e.g., describing an object as the white car hood)
and better counting performance.
In the subsequent fig-
ures from Fig. 15 to Fig. 17, we demonstrate that our model
maintains stable performance across various vision-centric
tasks and is also capable of tackling text-centric tasks such
as real-world OCR. Specifically, in Fig. 15, COVT demon-
strates its ability to correctly identify the NBA teams and
N=1
N=2
N=4
N=8
N=16
N=32
Number of Samples (N)
1
0
1
2
3
4
5
Relative Improvement over Baseline (%)
Baseline (0%)
Avg Score Improvement
Time Cost
+3.30%
+3.41%
+3.58%
+4.18%
+2.78%
-1.03%
50
0
50
100
150
200
250
Relative Time Cost Increase (%)
+112%
+118%
+124%
+136%
+167%
+220%
Figure 12. Equipped with 4 depth tokens and 4 DINO tokens, the
model achieves its best performance when using 8 segmentation
tokens in COVT. Allocating more COVT tokens leads to a slight
diminishing performance and increased computational cost.
their scores in the first example, and to recognize visually
ambiguous backgrounds in the second example. In Fig. 16,
the first example shows that COVT can identify the far-
thest object and classify it correctly, while the second exam-
ple illustrates that COVT can handle common-sense VQA
tasks (e.g., identifying that the tall buildings are from Times
Square). The last figure, Fig. 17, shows that COVT main-
tains stable performance on OCR tasks. In the first example,
the model accurately detects the text “Sales Tax 4.24”, and
in the second example, it identifies the partially visible text
“Mer” in the background, along with several car logos on
the side.
C. Limitations and Future Work
Despite the strong performance of COVT, several limita-
tions remain. First, we have not exhaustively explored the
design space of visual models or token combinations. Our
current setup focuses on representative perception axes such
as segmentation, depth, edges, and DINO features, but al-
ternative or hybrid visual experts may yield more expressive
or complementary tokens. Systematic exploration of this
space could further enhance reasoning performance. Sec-
ond, our current framework does not yet realize fully in-
terleaved multimodal reasoning. COVT generates contin-
uous visual thoughts but does not integrate them with free-
form textual reasoning in an interleaved sequence. Develop-
ing such a unified chain that seamlessly blends textual and
visual thoughts represents a promising direction for future
work.
We believe that COVT provides a foundation for the next
generation of multimodal reasoning systems—those capa-
ble of thinking fluidly across both language and vision in a
self-contained, interpretable manner.
16

The image depicts a charming, handcrafted scene featuring a small, knitted 
figure dressed in an orange hooded outfit with matching boots. The figure is 
holding a cheerful, smiling sun-shaped balloon on a stick. The background 
includes three fluffy, cloud-like shapes in various shades of pink and white, 
floating above a small, green, grassy area with two coniferous trees. The sky is a 
soft, light blue color.
There are four clouds visible in the sky: one large pink cloud on the left, one 
smaller white cloud near the top right, and two more white clouds further to the 
right.
Describe the scene in the picture in detail, and find 
out how many clouds are there in the sky.
<think> Because the segmentation of the image is
<|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|
><|sam_pad|><|sam_pad|>, the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>, and the 
perception feature of the image is
<|dino_pad|><|dino_pad|><|dino_pad|><|dino_pad|>. </think>
<answer> The image depicts a charming, handcrafted scene featuring a small 
doll-like figure standing on a grassy patch. The figure is dressed in an orange 
outfit with matching boots, and it has a cheerful expression. The doll is holding 
a stick with a smiling sun at the top, which appears to be made from felt or 
similar material.
In the background, there are two green felt trees, each with a simple triangular 
shape and a brown trunk. Above the trees, there are four fluffy white clouds, 
each adorned with a brown button-like detail. The sky is painted in a soft blue 
color, providing a serene backdrop for the scene.
The overall composition is whimsical and playful, likely intended to evoke a 
sense of joy and innocence. The use of bright colors and simple shapes 
contributes to the overall aesthetic appeal of the image.
To answer the question about the number of clouds: There are four clouds 
visible in the sky above the trees. </answer>
Question
Qwen2.5-VL
CoVT
Figure 13. Example of COVT compared to the baseline Qwen2.5-VL-7B.
17

The woman in the picture is sitting on the hood of a car.
What is the beautiful woman sitting on in 
the picture?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>, 
and the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>. </think>
<answer> The beautiful woman is sitting on the hood of a white car. </answer>
Question
Qwen2.5-VL
CoVT
In the picture, there are three men facing the camera.
How many men are facing the camera in the 
picture?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>, 
and the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>. </think>
<answer> 2 </answer>
Question
Qwen2.5-VL
CoVT
Figure 14. Examples of COVT compared to the baseline Qwen2.5-VL-7B.
In the NBA basketball game shown in the 
picture, which team is leading?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>. 
</think>
<answer> The Golden State Warriors are leading with a score of 107 to the 
Houston Rockets' 106. </answer>
Question
CoVT
What's the background of the horses in the 
image?
<think> Because the segmentation of the image is
<|seg_pad|><|seg_pad|><|seg_pad|><|seg_pad|><|seg_pad|><|seg_pad|><|
seg_pad|><|seg_pad|>, the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>, and the 
perception feature of the image is
<|dino_pad|><|dino_pad|><|dino_pad|><|dino_pad|>. </think>
<answer> The background of the horses in the image consists of a beautiful 
mountain range with snow-capped peaks, creating a picturesque scene. 
</answer>
Question
CoVT
Figure 15. More examples of COVT.
18

This is a picture of a cozy Christmas interior 
decoration. Please answer the question: What is the 
instance furthest from the camera lens in this 
picture?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>, 
and the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>. </think>
<answer> The instance furthest from the camera lens in this picture is the 
Christmas tree. </answer>
Question
CoVT
What is the man in the picture holding? Where in 
New York City is he standing?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>, 
and the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>. </think>
<answer> The man in the picture is holding a banana. He is standing on a street 
in New York City, likely Times Square or another busy area with tall buildings 
and advertisements in the background. </answer>
Question
CoVT
Figure 16. More examples of COVT.
Based on the information in the image, how 
much sales tax does the customer need to pay?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>, 
and the perception feature of the image is <|dino_pad|><|dino_pad|><|
dino_pad|><|dino_pad|>. </think>
<answer> The customer needs to pay $4.24 in sales tax. </answer>
Question
CoVT
The picture shows many cars of the same 
brand, so what brand is this?
<think> Because the segmentation of the image is <|sam_pad|><|sam_pad|><|
sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|><|sam_pad|>, 
and the depth map of the image is
<|depth_pad|><|depth_pad|><|depth_pad|><|depth_pad|>. </think>
<answer> Mercedes-Benz. </answer>
Question
CoVT
Figure 17. More examples of COVT.
19
