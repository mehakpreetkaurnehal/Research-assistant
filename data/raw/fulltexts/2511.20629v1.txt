MapReduce LoRA: Advancing the Pareto Front in
Multi-Preference Optimization for Generative Models
Chieh-Yun Chen1, Zhonghao Wang2, Qi Chen2, Zhifan Ye1, Min Shi1, Yue Zhao1,
Yinan Zhao2, Hui Qu2, Wei-An Lin2, Yiru Shen2, Ajinkya Kale2, Irfan Essa1, Humphrey Shi1
1Georgia Tech 2Adobe
Text-to-Image
SD 3.5 M
FLUX.1-dev
Text-to-Video
HunyuanVideo
Llama-2 7B
Language Task
Figure 1. MapReduce LoRA advances the Pareto fronts on Text-to-Image, Text-to-Video and language tasks. Left: On Stable
Diffusion 3.5 Medium [2] (top) and FLUX.1-dev [17] (bottom), we plot the 3D Pareto front over GenEval [11], PickScore [15], and OCR [6]
(Column 1), and the 2D Pareto fronts where one reward weight is set to zero (Columns 2‚Äì4). Right (top): On HunyuanVideo [16], we plot
the 2D Pareto front over Visual Quality (VQ) and Motion Quality (MQ). Right (bottom): On Llama-2 7B [38] (following Bone Soup [40]‚Äôs
setup), we plot the 2D Pareto front on Helpful Assistant task over helpful and harmless rewards. Please refer to the Supplement D.1 and A
for full results.
Abstract
Reinforcement learning from human feedback (RLHF) with
reward models has advanced alignment of generative mod-
els to human aesthetic and perceptual preferences. How-
ever, jointly optimizing multiple rewards often incurs an
alignment tax‚Äîimproving one dimension while degrading
others.
To address this, we introduce two complemen-
tary methods: MapReduce LoRA and Reward-aware Token
Embedding (RaTE). MapReduce LoRA trains preference-
specific LoRA experts in parallel and iteratively merges
them to refine a shared base model; RaTE learns reward-
specific token embeddings that compose at inference for
flexible preference control. Experiments on Text-to-Image
generation (Stable Diffusion 3.5 Medium and FLUX.1-
dev) show improvements of 36.1%, 4.6%, and 55.7%, and
32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR,
respectively. On Text-to-Video generation (HunyuanVideo),
visual and motion quality improve by 48.1% and 90.0%, re-
spectively. On the language task, Helpful Assistant, with
Llama-2 7B, helpful and harmless improve by 43.4% and
136.7%, respectively. Our framework sets a new state-of-
the-art multi-preference alignment recipe across modalities.
1. Introduction
Recent progress in Text-to-Image [2, 7, 8, 17] and Text-
to-Video [16, 31, 39] has been driven by flow-based dif-
fusion models [22, 25], achieving unprecedented visual fi-
delity. To better align outputs with human judgment, post-
training methods, particularly reinforcement learning from
human feedback (RLHF) [23, 32, 35, 41], have become es-
sential. Reward-based RLHF trains reward models from
human annotations to capture specific evaluation criteria,
and optimizes the generator by sampling generations, scor-
1
arXiv:2511.20629v1  [cs.CV]  25 Nov 2025

SD 3.5 M
Merge 1
Merge 2
Merge 3
Merge 4
+ RaTE
+ MapReduce LoRA
A photo of a suitcase right of a boat
A geometric abstract impasto oil painting representing the conflicting emotions of humanity
A vintage suitcase adorned with a colorful sticker collection, prominently featuring a worn, antique sticker that reads "Lost in Atlantis 1923", 
surrounded by other nostalgic travel stickers from the early 20th century.
<GE>
<PS>
<OCR>
Figure 2. MapReduce LoRA progressively advances performance across iterations; Reward-aware Token Embedding (RaTE) enables
flexible preference control.
ing them with the reward models, and updating parameters
with reward-weighted objectives. However, human percep-
tion of quality is inherently multi-dimensional.
In prac-
tice, multiple reward models are used to reflect criteria such
as text‚Äìimage alignment [11, 21], aesthetic quality [14],
text rendering [6], and overall preference [15, 27]. This
raises a central challenge: how to optimize generative mod-
els that can improve across multiple preferences simultane-
ously without sacrificing any single dimension.
While post-training methods [23, 41] perform well un-
der a single reward, they either degrade on unoptimized
metrics at evaluation time or suffer competing gradients
when jointly optimizing multiple rewards‚Äîtwo manifesta-
tions of the alignment tax. Multi-objective reinforcement
learning (MORL) [1, 18, 20, 33] attempts to optimize mul-
tiple rewards simultaneously by combining rewards through
weighted mixtures or approximating a Pareto set with test-
time weight control. However, weighted mixtures are dom-
inated by easily optimized objectives, leaving harder pref-
erences under-trained or even regressed. Calibrated Prefer-
ence Optimization (CaPO) [18] partially addresses this im-
balance but remains limited by fixed weighting and mod-
est performance gains. Rewarded Soup [33] offers a pos-
teriori weight selection but still lags behind models fine-
tuned individually per reward. Consequently, existing ap-
proaches struggle to deliver models that generalize well
across diverse reward signals. Achieving a unified frame-
work that improves multiple human-aligned preferences ef-
ficiently and robustly remains a fundamental challenge in
multi-objective post-training.
To address these limitations, we propose two com-
plementary approaches‚ÄîMapReduce LoRA and Reward-
aware Token Embedding (RaTE)‚Äîthat jointly advance
multi-preference alignment.
MapReduce LoRA decom-
poses multi-objective optimization into i) a Map phase that
trains reward-specific LoRA experts in parallel, and ii) a
Reduce phase that merges experts with user-controlled in-
terpolation, folds the merged adapter into the base, and it-
erates to advance the Pareto front. Complementarily, RaTE
introduces a lightweight inference-time control by assign-
ing each reward a learned token embedding, enabling com-
posable conditioning by appending multiple tokens to the
input prompt. Together, MapReduce LoRA and RaTE en-
able efficient a posteriori customization without retraining,
yielding unified generative models that excel across multi-
ple reward dimensions.
2

Extensive experiments demonstrate that our proposed
methods substantially advance the Pareto front of multi-
preference alignment across both Text-to-Image and Text-
to-Video generation (Fig. 1).
On Stable Diffusion 3.5
Medium [2] and FLUX.1-dev [17], our method improves
GenEval [11], PickScore [15], and OCR [6] by 36.1%,
4.6%, 55.7% and 32.7%, 4.3%, 67.1%, respectively. Be-
yond in-domain metrics, untargeted rewards‚Äîi.e., VQAS-
core [21], MPS [43], and VILA [14]‚Äîalso improve by
1.85%, 6.49%, and 19.96%, validating the robustness and
scalability of our method.
On HunyuanVideo [16], vi-
sual and motion quality improve by 48.1% and 90.0%,
achieving state-of-the-art results among post-trained diffu-
sion systems. On language task, Helpful Assistant, with
Llama-2 7B [38], helpful and harmless improve by 43.4%
and 136.7%, respectively. Collectively, these results high-
light the effectiveness of combining MapReduce LoRA and
RaTE for efficient, controllable, and human-aligned gener-
ation. Our contributions are fourfold:
‚Ä¢ We introduce MapReduce LoRA, a scalable multi-reward
training framework that iteratively advances the Pareto
front across preferences.
‚Ä¢ We propose Reward-aware Token Embedding for flexible,
composable inference-time control of reward trade-offs.
‚Ä¢ MapReduce LoRA achieves state-of-the-art performance
on Text-to-Image, Text-to-Video and language tasks, with
substantial multi-rewards gains.
‚Ä¢ MapReduce LoRA exhibits strong generalization to un-
targeted rewards, demonstrating robust cross-preference
alignment.
2. Related Work
2.1. Flow-based Generative Models
Flow Matching (FM) [22] trains continuous normalizing
flows by regressing the conditional velocity field along a
path between the data distribution and a standard normal,
yielding a score-free, simulation-free objective with more
stable and efficient optimization than diffusion losses. Rec-
tified Flow (RF) [25] specializes FM to a straight-line,
approximately constant-speed path, rectifying trajectories
and simplifying supervision, which improves stability and
sampling efficiency.
Recent Text-to-Image [2, 7, 8, 17]
and Text-to-Video [16, 31, 39] methods adopt FM/RF to
learn the velocity field directly, improving efficiency over
diffusion-style denoising and often yielding stronger gradi-
ents for conditional generation.
2.2. Reinforcement Learning from Human Feed-
back (RLHF)
RLHF [5, 13] was first used to train agents in simulators
and Atari, and later to fine-tune language models for sum-
marization [37]. InstructGPT [29] scaled RLHF to align
broad language tasks with a three-stage pipeline: supervised
fine-tuning (SFT), reward-model training, and policy op-
timization with Proximal Policy Optimization (PPO) [35].
Direct Preference Optimization (DPO) [32] removes the re-
ward model and online RL, directly optimizing the policy
from human preference pairs. Group Relative Policy Op-
timization (GRPO) [36] is a PPO-style preference method
that drops the critique and uses relative scores across mul-
tiple samples per prompt, improving stability and sample
efficiency.
To apply RL to diffusion models, Denoising
Diffusion Policy Optimization (DDPO) [4] formulates the
denoising process as a Markov Decision Process (MDP).
Building on this MDP formulation, Flow-GRPO [23] and
DanceGRPO [41] extend GRPO [36] to flow-based gen-
erative models by using an ODE-to-SDE strategy to in-
ject stochasticity and overcome the determinism of standard
flow models.
However, single-reward RLHF optimizes one preference
dimension in isolation, while generative quality spans mul-
tiple and often conflicting objectives. This creates a natu-
ral bridge to Multi-Objective RL (MORL), where the ob-
jective is to balance conflicting rewards and approximate
a Pareto-optimal solution‚Äîprecisely the challenge in post-
training generative models. MORL methods can be cate-
gorized into two classes: a priori [1, 18], which scalarizes
objectives into a single signal (e.g., weighted sums), and a
posteriori [20, 33], which approximate a Pareto set and en-
able test-time control. CaPO [18] calibrates and balances
multiple T2I preferences, and MOPO [1] frames alignment
as constrained, KL-regularized optimization; both are a pri-
ori and lack test-time control. Rewarded Soups [33] extends
Linear Mode Connectivity [9, 28] to RL, showing linear
weight soups can approximate Pareto fronts and reduce re-
ward misspecification, yet multi-reward methods often un-
derperform specialized single-reward experts. We therefore
propose two a posteriori solutions, RaTE and MapReduce
LoRA, to address reward conflict and achieve comparable
performances to single-reward experts‚Äô performances.
3. Method
3.1. Preliminaries
Low-Rank
Adaptation
(LoRA)
[12]
is
a
leading
parameter-efficient fine-tuning method that adapts large
models by learning a low-rank update to frozen pre-trained
weights. For a linear layer with weight W ‚ààRd√ók and
input x, the adapted layer is
 W + ‚àÜW

x = Wx + BAx,
with ‚àÜW = BA,
where A ‚ààRr√ók, B ‚ààRd√ór, and r ‚â™min(d, k). Dur-
ing fine-tuning, W remains fixed while only A and B are
trained, optionally scaled by a factor Œ±/r; at inference, ‚àÜW
can be merged into W. Recent work [34] finds that, in com-
3

(a) Individual Experts
ùëÖ!
ùëÄ!
ùëÄ
e.g., Flow-GRPO
Train
(b) Multi-objective Reinforcement Learning (MORL)
! ùúá!
"ùëÖ!
#
!$%
ùëÄ"
ùëÄ
(i) Priori (e.g., CaPO)
Train
(ii) Posteriori (e.g., Rewarded Soup)
ùëÖ#
ùëÖ$
ùëÖ!
ùëÄ!
ùëÄ#
ùëÄ$
ùëÄ"
! ùúá!
"ùëÄ!
#
!$%
ùëÄ
Train
Model 
Merge
(c) MapReduce LoRA (Ours)
ùëÖ#
ùëÖ$
ùëÖ!
ùëÄ!
(&)
ùëÄ#
(&)
ùëÄ$
(&)
ùëÄ(&(#)
! ùúá!
(')ùëÄ!
(')
#
!$%
ùëÄ(&)
Train
Model 
Merge
Figure 3. Overview of MapReduce LoRA and comparison with (a) individual experts, e.g., Flow-GRPO [23], and (b) Multi-
Objective Reinforcement Learning, e.g., CaPO [18] and Rewarded soup [33]. All methods begin from a base model M and optimize
with respect to reward R. Our proposed MapReduce LoRA iteratively trains per-reward LoRA experts and initializes iteration k + 1 using
the merged model from iteration k. Notably, the black dashed curve for MapReduce LoRA indicates fewer training steps compared with
the black solid curves representing other methods.
mon post-training settings (small/medium-scale SFT, rea-
soning, or RL), LoRA matches full fine-tuning performance
when rank and layer coverage suffice. LoRA has been used
for style [44] and skill control [20], but multi-preference
control remains underexplored; we therefore study LoRA
merging for multi-preference optimization.
Textual Inversion [10] learns a new embedding vector for a
pseudo-token that represents a specific concept. Given only
three to five reference images, it optimizes a single token
embedding to encode both the high-level semantics and the
fine-grained visual details of the concept, enabling person-
alized and reference-guided generation without modifying
the base model.
Pareto
Fronts
(PF)
[30]
represents
the
collec-
tion
of
non-dominated
solutions,
model
param-
eters
for
which
no
other
candidate
can
improve
one objective without degrading another.
Formally,
PF = {Œ∏ | ‚àÑŒ∏‚Ä≤ ‚ààŒò, {Ri(Œ∏‚Ä≤)}i ‚âªRN {Ri(Œ∏)}i} , where
‚âªRN denotes the dominance relation in the N-dimensional
reward space indexed by i, and N denotes the total number
of reward objectives.
Group Relative Policy Optimization (GRPO) [23, 36, 41]
is a PPO-style preference optimization algorithm but based
on group-normalized rewards. For each prompt p, GRPO
samples G results {yg}G
g=1 and computes group-normalized
advantages ÀÜAg by z-scoring rewards within that group as
shown in Eqn. (2). The policy is then updated with a clipped
likelihood-ratio objective and regularized by a KL penalty
w.r.t a frozen reference policy as shown in Eqn. (1).
JGRPO = Ep
Ô£Æ
Ô£∞1
G
G
X
g=1
1
T
T
X
t=1
min
 rg
t ÀÜ
Ag, clip(rg
t , 1 ‚àíœµ, 1 + œµ) ÀÜ
Ag
Ô£π
Ô£ª
‚àíŒ≤ DKL
 œÄŒ∏(¬∑ | p)
 œÄref(¬∑ | p)

,
(1)
ÀÜ
Ag =
R(yg, p) ‚àímean

R(yg, p)
G
g=1
std

R(yg, p)
G
g=1
.
(2)
3.2. MapReduce LoRA
In this section, we first demonstrate that MapReduce is a
progressive souping process via averaged proximal consen-
sus optimization. Then, we prove that MapReduce progres-
sively converges toward the joint optimum of the conditions
set by multiple reward models. Finally, we explain why
MapReduce outperforms the one-shot soup method [33].
Fig. 3 shows the overview of MapReduce LoRA and a com-
parison with individual experts and multi-objective rein-
forcement learning [18, 33].
Let {fi(Œ∏)}n
i=1 denote differentiable reward objec-
tives, each derived from a distinct reward model used in
GRPO [36] fine-tuning. The joint optimization goal is to
maximize their average:
F(Œ∏) = 1
n
n
X
i=1
fi(Œ∏),
(3)
where Œ∏ ‚ààRd are model parameters of the diffusion model.
MapReduce is implemented by iteratively optimizing
multiple reward functions for a few steps and averaging
the resulting weights, a process we formalize as progressive
souping. At iteration k:
Œ∏k
i = proxŒ∑fi(Œ∏k) = arg max
Œ∏ (fi(Œ∏) ‚àí1
2Œ∑ ‚à•Œ∏ ‚àíŒ∏k‚à•2),
Œ∏k+1 = 1
n
n
X
i=1
Œ∏k
i ,
(4)
where the proximal term 1
2Œ∑‚à•Œ∏‚àíŒ∏k‚à•2 corresponds to a trust
region in GRPO. The overall operator can be written as:
T(Œ∏) = 1
n
n
X
i=1
proxŒ∑fi(Œ∏).
(5)
Thus, progressive souping performs repeated applications
of the averaged proximal map Œ∏k+1 = T(Œ∏k), while a one-
shot ‚Äúfinal soup‚Äù corresponds to a single application T(Œ∏0).
4

Now we prove that MapReduce progressively reaches
the optimum of Eqn. (3). Because the sampling distribu-
tion pŒ∏ of a pretrained diffusion model changes smoothly
with Œ∏ (the score function ‚àáŒ∏ log(pŒ∏) is continuous), we can
safely assume that each reward objective fi(Œ∏) is locally L-
smooth and the aggregated objective Eqn. (3) satisfies the
Polyak‚Äì≈Åojasiewicz (PL) condition in a neighborhood con-
taining the optimization trajectory:
1
2‚à•‚àáF(Œ∏)‚à•2 ‚â•¬µ‚à•F(Œ∏) ‚àíF ‚àó‚à•,
(6)
for some constant ¬µ > 0. Then, for step size 0 < Œ∑ ‚â§1/L,
each proximal operator proxŒ∑fi(Œ∏) is nonexpansive in a lo-
cal neighborhood, and the overall operator T(Œ∏) is thus
an Œ±-averaged operator.
The fixed points of T(Œ∏) coin-
cide with the stationary points of the aggregated objective
‚àáF(Œ∏‚àó) = 0. The progressive-soup iteration in Eqn. (4)
converges to a stationary point of Eqn. (3) and satisfies the
geometric contraction bound:
‚à•F(Œ∏k+1) ‚àíF ‚àó‚à•‚â§(1 ‚àícŒ∑¬µ)‚à•F(Œ∏k) ‚àíF ‚àó‚à•,
(7)
for some constant c ‚àà(0, 1).
A one-shot final soup performs a single application of
T; progressive souping applies T repeatedly, resulting in a
smaller sub-optimality gap:
‚à•F(Œ∏m) ‚àíF ‚àó‚à•‚â§(1 ‚àícŒ∑¬µ)m‚à•F(Œ∏0) ‚àíF ‚àó‚à•,
(8)
where m denotes the iteration. Hence, each progressive-
soup iteration further contracts toward the joint optimum,
while the one-shot soup remains suboptimal unless initial-
ized near a stationary point. The comparison between pro-
gressive souping and one-shot soup is shown in Figs. 1, 2
and 4. Please refer to the Supplement B for the pseudocode.
3.3. Reward-aware Token Embedding (RaTE)
We propose RaTE to make the token embedding sensitive
to user preferences and dynamically control inference-time
reward influences. Inspired by Textual Inversion [10], we
distill each preference into a single, trainable special token
embedding via supervised fine-tuning. Both teacher and
student share the same frozen Transformer backbone M.
We detail the setup below.
‚Ä¢ Teacher: For each preference i, we attach its expert
LoRA adapter Œ∏i to M. This teacher produces the target
latent zteacher
0,i
for the i-th preference.
‚Ä¢ Student: The student is the frozen M without adapters.
Its prompt includes the i-th special token; the token em-
bedding Œ∏tokeni is the only trainable parameter.
We distill Œ∏i into Œ∏tokeni using a Flow Matching objec-
tive [22]. Sample t via a flow-matching timestep schedule
(e.g., logit-normal or uniform (depending on the scheduler/-
model)), sample œµ ‚àºN(0, I), and set zt = (1‚àíœÉt) zteacher
0,i
+
œÉt œµ, where œÉt is given by the scheduler. The student M
predicts the velocity from zt and the conditioned prompt
c(p, Œ∏tokeni). The target velocity is vtarget = œµ ‚àízteacher
0,i
. The
loss minimizes the Mean Squared Error (MSE) between
prediction and target, updating only Œ∏tokeni:
L(Œ∏tokeni) = Ep, zteacher
0,i
, œµ, t
hM(zt, t, c(p, Œ∏tokeni)) ‚àívtarget
2
2
i
.
(9)
After training, we obtain the expert-infused token em-
bedding Œ∏tokeni. At inference, appending its special token to
a prompt injects the desired preference. RaTE is lightweight
and modular, enabling composable control with other cus-
tomizations such as LoRA adapters.
4. Experiments
4.1. Experimental Setup
Models and datasets.
For Text-to-Image generation,
we use Stable Diffusion 3.5 Medium (SD 3.5 M) [2]
and FLUX.1-dev [17] as base models.
We adopt
the same datasets as Flow-GRPO [23]:
GenEval [11],
PickScore [15], and OCR [6].
We compare against
CaPO [18], Flow-GRPO individual experts [23], multi-
objective RL with mixed data (MORL-D) and with mixed
data plus mixed rewards (MORL-DR), and Rewarded
Soup [33]. The merging iteration is 4 for all reported re-
sults if not specified.
For Text-to-Video generation, we
use the HunyuanVideo backbone [16].
We follow the
Dance-GRPO [41] GitHub recipe (optimizer, GRPO group-
ing, scheduler, data sampling), but apply parameter-efficient
LoRA instead of full fine-tuning. We compare to Dance-
GRPO individual experts [41]. Because the GitHub config-
uration differs from the paper, we report reproduced base-
lines trained on the released data for fairness. The merging
iteration is 3 for all reported results if not specified. For
Language tasks, we use llama2-7B [38] as base model, fol-
lowing Bone Soup [40] setup. We first train the Llama2-
7B [38] backbone via supervised fine-tuning (SFT) using
the same datasets and hyperparameters as Bone Soup [40],
then train LoRA experts with PPO [35] under the same set-
tings of datasets and training hyperparameters. The merg-
ing iteration is 3 for all reported results if not specified. For
implementation details, please refer to the Supplement C.
Reward models.
For Text-to-Image, we train with
three rewards:
detection-based text-image alignment
(GenEval [11]), human preference (PickScore [15]), and
text rendering quality (OCR [6]). For evaluation, we ad-
ditionally report clip-based text-image alignment (VQAS-
core [21]), human preference (MPS [43]), and aesthetic
quality (VILA [14]).
For Text-to-Video, we use two re-
wards: Visual Quality (VQ) and Motion Quality (MQ) from
VideoAlign [24]. For the Language task, we employ four
5

A photo of a vase right of a horse
A photo of a truck left of a baseball bat
A photo of a hot dog left of a suitcase
A detailed science fair poster titled "Robot Rebellion Study", featuring charts, 
graphs, and images of futuristic robots in various stages of revolt, with a 
backdrop of a cityscape under siege, all presented in a sleek, modern design.
SD 3.5 M
+ Rewarded Soup
+ MapReduce LoRA
FLUX.1-dev
+ Rewarded Soup
+ MapReduce LoRA
An angry cat hitting a nervous dog who accidentally stepped on its tail.
A man teaching a young boy how to ride a bike in a sunny park.
A photo of a dog right of a teddy bear.
A photo of an elephant below a horse.
Astronaut in a sleek spacesuit standing on the rugged Martian surface, 
with a clear "Mars Colony One" patch emblazoned on the chest, 
overlooking a vast, dusty red landscape under a pale sky.
A cat sleeps peacefully in a dog's bed, 
while the dog has no choice but to nap on the floor.
Figure 4. MapReduce LoRA enhances the generative qualities, i.e., image aesthetics, positional relationship and text rendering quality,
by optimizing the model with multiple rewards simultaneously.
rewards across two tasks: Reddit Summary (Faithful and
Preference1) and Helpful Assistant (Helpful and Harmless).
4.2. Qualitative Results
Fig. 2 shows iterative improvements from MapReduce
LoRA and the controllability of RaTE (see also Fig. 5).
Compared to the base model and Rewarded Soup [33]
(Fig. 4), MapReduce LoRA better handles challenging
cases, including uncommon spatial relations (e.g., an ele-
phant below a horse) and high-quality rendering of small
text. For more qualitative results on Text-to-Image and Text-
to-Video, please refer to the Supplement E.1 and E.2.
6

Table 1. Text-to-Image performance comparison on in-domain rewards (GenEval [11], PickScore [15], and OCR [6]) within corresponding
datasets and out-of-domain rewards (VQAScore [21], MPS [43], and VILA [14]) within PartiPrompts [42] and GenAI-Bench [19]. The
performance is evaluated with fp32 precision. Red color means the performance is degraded compared to the baseline.
Method
Reward
GenEval [11]
PickScore
OCR
PartiPrompts
GenAIBench
Single Obj. Two Obj. Counting
Colors
Position
Color Attr. Overall
VQAScore
MPS
VILA
VQAScore
SD 3 M‚àó[3]
x
0.99
0.84
0.56
0.84
0.32
0.52
0.68
-
-
0.908
13.39
5.793
-
SD 3 M + CaPO‚àó[18]
VQAScore [21]
+MPS [43]
0.99
0.87
0.63
0.86
0.31
0.59
0.71
-
-
0.914
13.58
5.943
-
+VILA [14]
(+0.00%)
(+3.57%)
(+12.50%)
(+2.38%)
(-3.13%)
(+13.46%)
(+4.41%)
-
-
(+0.66%)
(+1.42%)
(+2.59%)
-
SD 3.5 M [2]
x
1.00
0.86
0.55
0.82
0.23
0.62
0.68
21.784
0.601
0.861
11.68
6.034
0.744
Individual Experts
(Flow-GRPO [23])
GenEval [11]
1.00
0.96
0.93
0.91
0.93
0.84
0.93
21.852
0.654
0.879
11.81
6.101
0.782
PickScore [15]
1.00
0.94
0.76
0.88
0.36
0.59
0.76
23.359
0.716
0.880
12.66
6.755
0.774
OCR [6]
1.00
0.87
0.56
0.82
0.25
0.59
0.68
21.800
0.934
0.863
11.75
6.014
0.750
Rewarded Soup [33]
[11] + [15] + [6]
1.00
0.94
0.76
0.86
0.43
0.72
0.79
22.276
0.805
0.875
12.13
6.194
0.772
MORL-D
[11] + [15] + [6]
1.00
0.99
0.94
0.89
0.88
0.78
0.91
22.067
0.942
0.881
12.00
6.113
0.777
MORL-DR
[11] + [15] + [6]
1.00
0.82
0.61
0.81
0.21
0.59
0.67
21.834
0.641
0.860
11.71
6.014
0.746
MapReduce LoRA
[11] + [15] + [6]
1.00
0.98
0.93
0.88
0.81
0.78
0.90
22.552
0.923
0.885
12.25
6.266
0.782
(+0.00%)
(+14.12%)
(+68.18%)
(+7.80%)
(+252.17%)
(+25.81%)
(+31.88%)
(+3.53%)
(+53.55%)
(+2.78%)
(+4.89%)
(+3.85%)
(+5.13%)
MapReduce LoRA + RaTE [11] + [15] + [6]
1.00
0.98
0.93
0.90
0.95
0.79
0.92
22.777
0.936
0.877
12.44
7.238
0.777
(+0.00%)
(+14.12%)
(+68.18%)
(+10.40%) (+313.04%)
(+27.42%)
(+36.08%)
(+4.56%)
(+55.72%)
(+1.85%)
(+6.49%) (+19.96%)
(+4.37%)
FLUX.1-dev [17]
x
0.99
0.87
0.71
0.82
0.18
0.44
0.67
22.006
0.573
0.830
12.37
6.629
0.737
Individual Experts
(Flow-GRPO [23])
GenEval [11]
1.00
1.00
0.90
0.89
0.90
0.73
0.90
22.057
0.692
0.854
12.45
6.661
0.764
PickScore [15]
1.00
0.95
0.61
0.85
0.24
0.57
0.70
23.566
0.723
0.875
12.76
5.514
0.772
OCR [6]
1.00
0.84
0.76
0.83
0.20
0.46
0.68
21.997
0.971
0.841
12.32
6.591
0.749
Rewarded Soup [33]
[11] + [15] + [6]
0.99
0.96
0.81
0.88
0.31
0.66
0.77
22.503
0.868
0.862
12.73
6.650
0.767
MORL-D
[11] + [15] + [6]
1.00
0.99
0.89
0.90
0.78
0.73
0.88
22.026
0.932
0.834
12.36
6.513
0.733
MORL-DR
[11] + [15] + [6]
1.00
0.95
0.90
0.94
0.73
0.79
0.88
22.112
0.952
0.840
12.41
6.534
0.745
MapReduce LoRA
[11] + [15] + [6]
1.00
0.96
0.88
0.88
0.81
0.79
0.89
22.951
0.957
0.875
12.95
6.572
0.775
(+1.27%)
(+10.46%)
(+22.81%)
(+7.80%)
(+350.00%)
(+79.55%)
(+32.68%)
(+4.29%)
(+67.06%)
(+5.49%)
(+4.70%)
(-0.87%)
(+5.09%)
‚àóFrom CaPO Table 2 and 3 [18]; as the code is not released, we report their numbers and cannot compare CaPO on SD 3.5 M [2] and FLUX.1-dev [17].
Enigmatic black square building on top of a purple hill, smoke stacks
SD 3.5 M
+ MapReduce LoRA
+ RaTE
A photo of a zebra right of a bed
A cozy bakery with a large glass window, featuring a stylish decal that
reads "Gluten-Free Options Available" in elegant font, surrounded by
displayed pastries and bread, with soft morning light filtering through,
highlighting the inviting atmosphere.
<GE>
<PS>
<OCR>
Figure 5. RaTE enables per-reward control at inference.
4.3. Quantitative Results
MapReduce LoRA. Table 1 reports Text-to-Image perfor-
mance on GenEval [11], PickScore [15], OCR [6], VQAS-
core [21], MPS [43], and VILA [14] using SD 3.5 M [2] and
FLUX.1-dev [17]. MapReduce LoRA improves both in-
domain and out-of-domain metrics. Relative to CaPO [18],
it delivers larger gains across the four CaPO metrics. On
CaPO‚Äôs in-domain metrics (VQAScore [21], MPS [43],
VILA [14]), which are out-of-domain for us, our improve-
ments are 2.78%, 4.89%, and 3.85%, exceeding CaPO‚Äôs
0.66%, 1.42%, and 2.59%.
On CaPO‚Äôs out-of-domain
metric (GenEval [11]), our improvement is 31.88% ver-
sus CaPO‚Äôs 4.41%. Flow-GRPO individual experts [23],
each tuned to a single reward, substantially boost the tar-
geted metric but transfer poorly to others. MORL-D and
MORL-DR can be unstable under conflicting objectives;
for instance, PickScore often collapses during joint train-
ing and is dominated by other rewards (e.g., OCR). In
contrast, MapReduce LoRA maintains competitive perfor-
mance across all targeted rewards.
Table 2 reports Text-to-Video results. We evaluate 1,024
held-out prompts (fixed once). For each prompt, we gen-
erate four samples using a fixed set of seeds shared across
models and report mean Visual Quality (VQ) and Motion
Quality (MQ) from VideoAlign [24]. MapReduce LoRA
improves both VQ and MQ (+48.09% and +89.96%) over
individually trained experts (+34.55% and +74.10%).
7

GenEval
PickScore
OCR
k = 4
0
1000
2000
3000
4000
5000
6000
7000
7400
steps
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Score
0.65
0.93
0.77
0.92
0.82
0.93
0.85
0.94
0.87
k=0
k=1
k=2
k=3
k=4
0
1000
2000
3000
4000
5000
6000
7000
7900
steps
21.6
22.0
22.4
22.8
23.2
Score
21.72
23.36
22.26
23.23
22.42
23.26
22.54
23.27
22.542
k=0
k=1
k=2
k=3
k=4
0
1000
2000
3000
4000
4900
steps
0.6
0.7
0.8
0.9
Score
0.571
0.928
0.773
0.928
0.863
0.935
0.886
0.946
0.9158
k=0
k=1
k=2
k=3
k=4
k = 10
0
1000
2000
3000
4000
5000
6000
7000
7400
steps
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Score
0.65
0.78
0.71
0.82
0.75
0.84
0.78
0.85
0.78
0.88
0.81
0.89
0.82
0.91
0.85
0.91
0.86
0.91
0.88
0.93
0.88
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
0
1000
2000
3000
4000
5000
6000
7000
7900
steps
21.6
22.0
22.4
22.8
23.2
Score
21.72
22.74
22.06
22.90
22.24
22.97
22.31
23.07
22.40
23.08
22.49
23.10
22.51
23.14
22.58
23.19
22.64
23.22
22.63
23.21
22.671
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
0
1000
2000
3000
4000
4900
steps
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
Score
0.571
0.836
0.703
0.898
0.777
0.902
0.833
0.899
0.861
0.914
0.883
0.916
0.896
0.909
0.907
0.929
0.906
0.934
0.913
0.920
0.9161
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=8
k=9
k=10
Figure 6. Ablation study on merging iterations (k = 4 vs. 10). The performance is evaluated during training with mixed precision,
where the text encoder is set to fp16 precision and the rest of the model to fp32.
In Figs. 7 and 8, we compare MapReduce LoRA
against Llama2 (base) [38], Llama2 after SFT, Rewarded
Soup [33], and Bone Soup [40].
MapReduce LoRA
achieves state-of-the-art performance on both tasks, under-
scoring its cross-modal generality.
Table 2. Text-to-Video comparison on Visual and Motion Quality.
Method
Reward
VQ
MQ
HunyuanVideo [16]
x
3.25
0.95
Individual Experts
(DanceGRPO [41])
VQ
4.37 (+34.55%) 1.20 (+26.41%)
MQ
3.80 (+16.95%) 1.66 (+74.10%)
Rewarded Soup [33] VQ+MQ 4.13 (+27.37%) 1.43 (+50.42%)
MapReduce LoRA
VQ+MQ 4.81 (+48.09%) 1.81 (+89.96%)
Reward-aware token embedding (RaTE). Tables 1 and 3
report RaTE performance on GenEval [11], PickScore [15],
and OCR [6] using SD 3.5 M [2]. At inference, RaTE ap-
pends a trained control token to the end of the prompt based
on the specified preference. RaTE enables per-reward con-
trol and jointly improves multiple rewards. In Table 1, to-
ken gains are orthogonal to transformer tuning: RaTE adds
4.20%, 1.03%, and 2.17% on GenEval [11], PickScore [15],
and OCR [6], respectively. In Table 3, when using all three
tokens, the first appended token accounts for most of the
gain. Token control is effective on Stable Diffusion [2, 10],
which uses explicit cross-attention for text conditioning. In
contrast, FLUX.1-dev [17] performs joint text‚Äìimage se-
quence modeling, causing a modified token embedding to
perturb both text and image tokens across layers, which
makes reward-specific token control substantially less sta-
ble. We leave this for future exploration.
4.4. Ablation Study
[MapReduce LoRA] Effect of merging iterations under
fixed steps. In Fig. 6, with total training steps fixed, we
compare merging iterations (k) between 4 and 10. The per-
formance is similar but k = 10 slightly outperforms k = 4:
Table 3.
Reward-aware Token Embedding results.
<GE>,
<PS>, and <OCR> denote tokens trained on GenEval [11],
PickScore [15], and OCR [6], respectively. Gray indicates not
trained on that reward; bold indicates the best among variants.
Method
GenEval [11] PickScore [15] OCR [6]
SD 3.5 M [2]
0.68
21.784
0.601
+ <GE>
0.76
21.732
0.601
+ <PS>
0.69
22.052
0.591
+ <OCR>
0.68
21.724
0.623
+ <GE> + <PS>
0.77
21.990
0.610
+ <GE> + <OCR>
0.77
21.690
0.633
+ <PS> + <GE>
0.74
22.014
0.599
+ <PS> + <OCR>
0.69
22.064
0.592
+ <OCR> + <GE>
0.75
21.736
0.619
+ <OCR> + <PS>
0.69
22.040
0.607
+ <GE> + <PS> + <OCR>
0.76
21.992
0.613
+ <GE> + <OCR> + <PS>
0.75
21.981
0.615
+ <PS> + <GE> + <OCR>
0.75
22.009
0.606
+ <PS> + <OCR> + <GE>
0.72
22.027
0.606
+ <OCR> + <GE> + <PS>
0.73
22.013
0.617
+ <OCR> + <PS> + <GE>
0.74
22.014
0.614
+1.12% on GenEval [11], +0.57% on PickScore [15], and
+0.03% on OCR [6].
[RaTE] Effect of appended token count. Table 4 reports
RaTE performance as we vary the number of appended to-
kens. In the table, <RaTE> denotes the metric-specific token:
<GE> for GenEval [11], <PS> for PickScore [15], and <OCR>
for OCR [6]. Performance peaks differ by reward: GenEval satu-
rates at 2‚Äì3 tokens (0.78), PickScore peaks at 1 (22.052), and OCR
peaks at 3 (0.635).
Table 4. Ablation on the number of appended RaTE tokens.
Method
GenEval [11] PickScore [15] OCR [6]
SD 3.5 M [2]
0.68
21.784
0.601
+ <RaTE> x1
0.76
22.052
0.623
+ <RaTE> x2
0.78
22.037
0.621
+ <RaTE> x3
0.78
22.021
0.635
+ <RaTE> x10
0.78
21.993
0.630
8

5. Conclusion
We tackled the core challenge of multi-preference post-training
for generative models, where improving one objective (e.g., text‚Äì
image alignment) often harms others (e.g., aesthetic quality),
creating an alignment tax.
We introduced two complemen-
tary components‚ÄîReward-aware Token Embedding (RaTE) and
MapReduce LoRA‚Äîthat convert this trade-off into a favorable,
controllable multi-objective tuning strategy. MapReduce LoRA
trains per-reward LoRA experts in parallel, merges them with user-
controlled weights, folds the merged adapter into the base model,
and iterates to steadily advance the Pareto front. RaTE distills each
expert into a lightweight learned token embedding that composes
at inference, enabling flexible, a posteriori preference control
without retraining. Across Text-to-Image (Stable Diffusion 3.5
Medium, FLUX.1-dev), Text-to-Video (HunyuanVideo) and lan-
guage task (Helpful Assistant, with Llama-2 7B), our approach de-
livers substantial gains: on T2I, we improve GenEval, PickScore,
and OCR by 36.1%, 4.6%, and 55.7% (SD 3.5 M) and 32.7%,
4.3%, and 67.1% (FLUX.1-dev); on T2V, visual and motion qual-
ity improve by 48.1% and 90.0%, respectively. On language task,
helpful and harmless improve by 43.4% and 136.7%, respectively.
Beyond the targeted rewards, untargeted metrics (i.e., VQAScore,
MPS, VILA) also increase, indicating robust cross-preference gen-
eralization. Overall, MapReduce LoRA offers a simple, scalable
recipe for systematically pushing the multi-preference Pareto front
and enabling practical post-training customization.
6. Acknowledgment
We appreciate the valuable suggestions provided by Tsung-Han
Wu, Yu-Heng Hung, Fengzhe Zhou, and Jitesh Jain for this
paper.
This research was supported in part by the National
Science Foundation under Award #2427478 - CAREER Program,
and by the National Science Foundation and the Institute of
Education Sciences, U.S. Department of Education under Award
#2229873 - National AI Institute for Exceptional Education.
This project was also partially supported by cyberinfrastructure
resources and services provided by College of Computing
at the Georgia Institute of Technology, Atlanta, Georgia, USA.
References
[1] Akhil Agnihotri, Rahul Jain, Deepak Ramachandran, and
Zheng Wen. Multi-objective preference optimization: Im-
proving human alignment of generative models.
arXiv
preprint arXiv:2505.10892, 2025. 2, 3
[2] Stability AI.
Stable diffusion 3.5 medium.
https:
/ / huggingface . co / stabilityai / stable -
diffusion-3.5-medium, 2024. 1, 3, 5, 7, 8, 14, 15,
16, 17
[3] Stability AI.
Stable diffusion 3 medium.
https:
/ / huggingface . co / stabilityai / stable -
diffusion-3-medium, 2024. 7
[4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and
Sergey Levine.
Training diffusion models with reinforce-
ment learning. In Proceedings of the International Confer-
ence on Learning Representations (ICLR), 2024. 3
[5] Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei.
Deep reinforcement
learning from human preferences. In Advances in Neural
Information Processing Systems (NeurIPS), 2017. 3
[6] Cheng Cui et al. Paddleocr 3.0 technical report, 2025. 1, 2,
3, 5, 7, 8, 14, 15, 17
[7] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou,
Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan
Nie, Ziang Song, Shi Guang, and Haoqi Fan.
Emerging
properties in unified multimodal pretraining. arXiv preprint
arXiv:2505.14683, 2025. 1, 3
[8] Patrick Esser et al. Scaling rectified flow transformers for
high-resolution image synthesis. In Proceedings of the In-
ternational Conference on Machine Learning (ICML), 2024.
1, 3
[9] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M.
Roy, and Michael Carbin. Linear mode connectivity and the
lottery ticket hypothesis. In Proceedings of the International
Conference on Machine Learning (ICML), 2020. 3
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or.
An image is worth one word: Personalizing text-to-image
generation using textual inversion. In Proceedings of the In-
ternational Conference on Learning Representations (ICLR),
2023. 4, 5, 8
[11] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.
Geneval: An object-focused framework for evaluating text-
to-image alignment. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), 2023. 1, 2, 3, 5, 7, 8, 14, 15, 17
[12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. In Pro-
ceedings of the International Conference on Learning Rep-
resentations (ICLR), 2022. 3
[13] Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving,
Shane Legg, and Dario Amodei. Reward learning from hu-
man preferences and demonstrations in atari. In Advances in
Neural Information Processing Systems (NeurIPS), 2018. 3
[14] Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milan-
far, and Feng Yang. VILA: learning image aesthetics from
user comments with vision-language pretraining.
In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2023. 2, 3, 5, 7, 15
[15] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland
Matiana, Joe Penna, and Omer Levy.
Pick-a-pic:
An
open dataset of user preferences for text-to-image genera-
tion. In Advances in Neural Information Processing Systems
(NeurIPS), 2023. 1, 2, 3, 5, 7, 8, 14, 15, 17
[16] Weijie Kong et al.
HunyuanVideo: A systematic frame-
work for large video generative models.
arXiv preprint
arXiv:2412.03603, 2024. 1, 3, 5, 8, 14, 15, 17, 23
[17] Black Forest Labs.
FLUX.
https://github.com/
black-forest-labs/flux, 2024. 1, 3, 5, 7, 8, 14, 15,
16, 17
[18] Kyungmin Lee, Xiahong Li, Qifei Wang, Junfeng He, Jun-
jie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng
Yang, and Yinxiao Li.
Calibrated multi-preference opti-
mization for aligning diffusion models. In Proceedings of
9

the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2025. 2, 3, 4, 5, 7
[19] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Emily Li,
Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ra-
manan. GenAI-bench: A holistic benchmark for composi-
tional text-to-visual generation. In Synthetic Data for Com-
puter Vision Workshop @ CVPR, 2024. 7
[20] Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, and Mohit
Bansal. SELMA: learning and merging skill-specific text-
to-image experts with auto-generated data. In Advances in
Neural Information Processing Systems (NeurIPS), 2024. 2,
3, 4
[21] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia,
Graham Neubig, Pengchuan Zhang, and Deva Ramanan.
Evaluating text-to-visual generation with image-to-text gen-
eration. In Proceedings of the European Conference on Com-
puter Vision (ECCV), 2024. 2, 3, 5, 7, 15
[22] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxim-
ilian Nickel, and Matthew Le. Flow matching for generative
modeling. In Proceedings of the International Conference
on Learning Representations (ICLR), 2023. 1, 3, 5
[23] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng
Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli
Ouyang. Flow-grpo: Training flow matching models via on-
line RL. arXiv preprint arXiv:2505.05470, 2025. 1, 2, 3, 4,
5, 7, 14, 17
[24] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun
Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin,
Menghan Xia, Xintao Wang, Xiaohong Liu, Fei Yang,
Pengfei Wan, Di Zhang, Kun Gai, Yujiu Yang, and Wanli
Ouyang. Improving video generation with human feedback.
arXiv preprint arXiv:2501.13918, 2025. 5, 7, 15
[25] Xingchao Liu, Chengyue Gong, and Qiang Liu.
Flow
straight and fast: Learning to generate and transfer data with
rectified flow. In Proceedings of the International Confer-
ence on Learning Representations (ICLR), 2023. 1, 3
[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In Proceedings of the International Confer-
ence on Learning Representations (ICLR), 2019. 14
[27] Yuhang Ma, Yunhao Shui, Xiaoshi Wu, Keqiang Sun, and
Hongsheng Li. Hpsv3: Towards wide-spectrum human pref-
erence score. arXiv preprint arXiv:2508.03789, 2025. 2
[28] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
What is being transferred in transfer learning? In Advances
in Neural Information Processing Systems (NeurIPS), 2020.
3
[29] Long Ouyang et al. Training language models to follow in-
structions with human feedback. In Advances in Neural In-
formation Processing Systems (NeurIPS), 2022. 3
[30] Vilfredo Pareto.
Cours d‚Äô¬¥economie politique.
F. Rouge,
1896. 4
[31] Adam Polyak et al. Movie Gen: A cast of media foundation
models. arXiv preprint arXiv:2410.13720, 2024. 1, 3
[32] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D. Manning, Stefano Ermon, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a
reward model. In Advances in Neural Information Process-
ing Systems (NeurIPS), 2023. 1, 3
[33] Alexandre Ram¬¥e, Guillaume Couairon, Corentin Dancette,
Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and
Matthieu Cord.
Rewarded soups: towards pareto-optimal
alignment by interpolating weights fine-tuned on diverse re-
wards. In Advances in Neural Information Processing Sys-
tems (NeurIPS), 2023. 2, 3, 4, 5, 6, 7, 8, 12, 16, 17, 23
[34] John Schulman and Thinking Machines Lab.
Lora with-
out regret. Thinking Machines Lab: Connectionism, 2025.
https://thinkingmachines.ai/blog/lora/. 3
[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347, 2017. 1, 3, 5, 12
[36] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junx-
iao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya
Guo.
Deepseekmath: Pushing the limits of mathemati-
cal reasoning in open language models.
arXiv preprint
arXiv:2402.03300, 2024. 3, 4, 14
[37] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and
Paul F. Christiano. Learning to summarize from human feed-
back. arXiv preprint arXiv:2009.01325, 2020. 3
[38] Hugo Touvron et al. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
1, 3, 5, 8, 12
[39] Ang Wang et al. Wan: Open and advanced large-scale video
generative models. arXiv preprint arXiv:2503.20314, 2025.
1, 3
[40] Guofu Xie, Xiao Zhang, Ting Yao, and Yunsheng Shi. Bone
soups: A seek-and-soup model merging approach for con-
trollable multi-objective generation. In Proceedings of the
Association for Computational Linguistics (ACL), 2025. 1,
5, 8, 12
[41] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting
Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan
Guo, Weilin Huang, and Ping Luo.
DanceGRPO: Un-
leashing GRPO on visual generation.
arXiv preprint
arXiv:2505.07818, 2025. 1, 2, 3, 4, 5, 8, 14
[42] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, et al.
Scaling autoregressive models for content-rich text-to-image
generation.
Transactions on Machine Learning Research
(TMLR), 2022. 7
[43] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingt-
ing Gao, Di Zhang, and Zhongyuan Wang. Learning multi-
dimensional human preference for text-to-image generation.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), 2024. 3, 5, 7, 15
[44] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu,
Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and
Weizhu Chen. Multi-lora composition for image generation.
Transactions on Machine Learning Research (TMLR), 2024.
4
10

Supplementary Materials
A. MapReduce LoRA on Language Tasks
12
B. Pseudocode
13
B.1. Train: MapReduce LoRA
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
B.2. Train: Reward-aware Token Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
B.3. Inference with MapReduce LoRA and RaTE
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
C. Implementation Details
14
C.1. Training Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
C.2. Base Models and Reward Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
D. More Quantitative Results
15
D.1. Full Results of Different Merging Ratios (Corresponding to Fig. 1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
D.2. Pareto Front Comparison of MORL vs. MapReduce LoRA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
E. More Qualitative Results
17
E.1. Text-to-Image qualitative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
E.2. Text-to-Video qualitative results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
F. Limitations and Future Works
26
11

A. MapReduce LoRA on Language Tasks
Beyond Text-to-Image and Text-to-Video, we evaluate MapReduce LoRA on language tasks to demonstrate cross-modal generality. Fol-
lowing the Bone Soup [40] setup, we consider two tasks, each with two rewards: (i) Reddit Summary (Faithful, Preference1) and (ii)
Helpful Assistant (Helpful, Harmless). We first train the Llama2-7B [38] backbone via supervised fine-tuning (SFT) using the same
datasets and hyperparameters as Bone Soup [40], then train LoRA experts with PPO [35] under the same settings of datasets and training
hyperparameters. In Figs. 7 and 8, we compare MapReduce LoRA against Llama2 (base) [38], Llama2 after SFT, Rewarded Soup [33],
and Bone Soup [40]. MapReduce LoRA achieves state-of-the-art performance on both tasks, underscoring its cross-modal generality.
-0.70
-0.65
-0.60
-0.55
-0.50
-0.45
-0.30
-1.40
-1.20
0.20
0.40
0.60
0.80
1.00
1.20
1.40
1.60
1.80
MapReduce LoRA
Bone Soup
Rewarded Soup
Llama2 (sft)
Llama2 (base)
Faithful
Preference1
//
//
-0.70
-0.65
-0.60
-0.55
-0.50
-0.45-0.30
-1.40
-1.20
0.20
0.40
0.60
0.80
1.00
1.20
1.40
1.60
1.80
Merge 3
Merge 2
Merge 1
Llama2 (sft)
Llama2 (base)
Faithful
Preference1
//
//
Figure 7. Language Task: Reddit Summary. Left: MapReduce LoRA outperforms Rewarded Soup [33] and Bone Soup [40] (reproduced
from Fig. 5(c) in [40]) on both rewards. Right: MapReduce LoRA improves progressively across merge iterations.
-1.00
-0.50
0.00
0.50
1.00
1.50
2.00
-1.00
-0.50
0.00
0.50
1.00
1.50
2.00
MapReduce LoRA
Bone Soup
Rewarded Soup
Llama2 (sft)
Llama2 (base)
Helpful
Harmless
-0.50
0.00
0.50
1.00
1.50
2.00
-1.00
-0.50
0.00
0.50
1.00
1.50
Merge 3
Merge 2
Merge 1
Llama2 (sft)
Llama2 (base)
Helpful
Harmless
Figure 8. Language Task: Helful Assistant. Left: MapReduce LoRA outperforms Rewarded Soup [33] and Bone Soup [40] (reproduced
from Fig. 5(a) in [40]) on both rewards. Right: MapReduce LoRA improves progressively across merging iterations.
12

B. Pseudocode
B.1. Train: MapReduce LoRA
Algorithm 1 MapReduce LoRA: Multi-Preference Training
Require: Base model M with parameters Œ∏(0); Reward models {Ri}n
i=1; LoRA configuration (target layers, rank r, scale Œ±);
Number of iterations K; GRPO steps TGRPO; Merge weights {¬µi}n
i=1 (default ¬µi = 1/n, s.t. ¬µi ‚â•0 and Pn
i=1 ¬µi = 1);
GRPO hyperparameters: Group size G, Clip œµ, KL weight Œ≤
Ensure: Multi-preference aligned model M (K)
1: Initialize reference policy œÄref ‚ÜêM(Œ∏(0))
2: for k = 0 to K ‚àí1 do
3:
for all i ‚àà{1, . . . , n} do
‚ñ∑Map phase: train per-reward LoRA experts in parallel
4:
Initialize LoRA adapter œï(k)
i
5:
Freeze base weights Œ∏(k); only œï(k)
i
is trainable
6:
for t = 1 to TGRPO do
7:
œï(k)
i
‚ÜêGRPO
 M, Œ∏(k), œï(k)
i
, Ri, œÄref, G, œµ, Œ≤

8:
end for
9:
end for
10:
¬Øœï(k) ‚ÜêPn
i=1 ¬µi œï(k)
i
‚ñ∑Reduce phase: average experts
11:
Œ∏(k+1) ‚ÜêMergeLoRAIntoBase
 Œ∏(k), ¬Øœï(k)
‚ñ∑update base model
12:
M ‚ÜêM(Œ∏(k+1))
13:
Reset all adapters œï(k)
i
to zero
14:
Set œÄref ‚ÜêM(Œ∏(k+1))
15: end for
16: return M (K)
B.2. Train: Reward-aware Token Embedding
Algorithm 2 Reward-aware Token Embedding (RaTE) Training
Require: Frozen base model M with parameters Œ∏; per-reward LoRA expert œïi (teacher) for reward Ri; training prompts
D; special token index RaTEi for reward i; number of RaTE steps TRaTE; Flow Matching noise schedule {œÉt}
Ensure: Trained token embedding Œ∏token
i
for reward i
1: Attach LoRA expert œïi to M to form teacher model Mteacher
2: Freeze all Transformer weights and all token embeddings except Œ∏token
i
3: for t = 1 to TRaTE do
4:
Sample a minibatch of prompts {pb}B
b=1 from D
5:
for each prompt p in {pb} do
6:
// Teacher: obtain preference-specific latent
7:
Sample initial noise œµ0 ‚àºN(0, I)
8:
Generate teacher latent zteacher
0,i
‚ÜêSAMPLE(Mteacher, p, œµ0)
9:
// Flow Matching distillation setup
10:
Sample t via a flow-matching schedule (e.g., logit-normal over timesteps); sample œµ ‚àºN(0, I)
11:
Compute œÉt from the scheduler
12:
zt ‚Üê(1 ‚àíœÉt) zteacher
0,i
+ œÉt œµ
13:
Target velocity: vtarget ‚Üêœµ ‚àízteacher
0,i
14:
// Student: base model + special token
15:
Construct prompt: p‚Ä≤ = Concat(p, <RaTEi>)
16:
Student velocity prediction: vpred ‚ÜêM(zt, t, c(p‚Ä≤, Œ∏token
i
))
17:
Loss: LRaTE ‚Üê
vpred ‚àívtarget
2
2
18:
Backpropagate and update only Œ∏token
i
: Œ∏token
i
‚ÜêŒ∏token
i
‚àíŒ∑RaTE ‚àáŒ∏token
i
LRaTE
19:
end for
20: end for
21: return Œ∏token
i
13

B.3. Inference with MapReduce LoRA and RaTE
Algorithm 3 Inference with MapReduce LoRA and RaTE
Require: Final merged model M (K) with parameters Œ∏(K); reward-aware tokens {<RaTEi>} and embeddings {Œ∏token
i
};
user prompt p; user-specified preference set S ‚äÜ{1, . . . , n}; sampling hyperparameters (number of steps, seeds, guid-
ance scale, etc.)
Ensure: Generated image or video sample x
1: Build preference-aware prompt:
2:
p‚àó‚Üêp
3: for each i ‚ààS in user-defined order do
4:
Append control token: p‚àó‚ÜêConcat(p‚àó, <RaTEi>)
5: end for
6: Sample initial noise (image or video latent) œµ0 ‚àºN(0, I)
7: Run the sampler with M (K) conditioned on p‚àó:
8:
x ‚ÜêSAMPLE(M (K), p‚àó, œµ0)
9: return x
C. Implementation Details
C.1. Training Configuration
Reward-aware Token Embedding.
For each reward-aware token embedding, it contains 3 embeddings corresponding to 3 text
encoders within SD 3.5 M [2]. The embedding is trained on the GenEval, PickScore, and OCR datasets, individually. The teacher model
is trained with LoRA via GRPO [36] with 4,200 steps on GenEval [11], 4,500 steps on PickScore [15] and 1,600 steps on OCR [6]. The
training batchsize for GenEval is 512, PickScore is 768 and OCR is 768. We use the AdamW optimizer [26] with learning rate 3e-4, Œ≤1 =
0.9, Œ≤2 = 0.999, weight decay = 1e-4 and no warmup schedule; no KL penalty is applied.
MapReduce LoRA.
We adopt uniform averaging for all the default experiments if not specified.
‚Ä¢ Text-to-Image: We train all models using AdamW optimizer [26] with learning rate 3e-4, Œ≤1 = 0.9, Œ≤2 = 0.999, weight decay = 1e-4,
and no warmup schedule. All experiments use 32 GPUs with global batch size 576.
‚ó¶SD 3.5 M [2]: We follow the training configuration in Flow-GRPO [23], where only the KL ratio Œ≤ is different. Œ≤GenEval = 0.04,
Œ≤PickScore = 0.01, and Œ≤OCR = 0.04. The sampling timestep T is 10 and the evaluation timestep T = 40. The GRPO group size G is
24, the resolution is 512, and the LoRA settings are Œ± = 64 and r = 32. LoRA is applied to all attention layers (attn.add q proj,
attn.add k proj, attn.add v proj, attn.to add out, attn.to q, attn.to k, attn.to v, attn.to out.0).
The per-GPU batch sizes are 6, 9, and 9 with gradient accumulation steps of 3, 2, and 2 for GenEval, PickScore, and OCR, respectively.
We train GenEval, PickScore and OCR for {4,100, 1,200, 1,000, 1,100}, {4,500, 1,400, 1,000, 1,000}, and {1,600, 1,300, 900, 1,100}
steps, respectively; the bracketed numbers denote merge iterations 0-3.
‚ó¶FLUX.1-dev [17]: The only task-dependent change is the KL ratio Œ≤ (Œ≤GenEval=0.04, Œ≤PickScore=0, Œ≤OCR=0.04). The sampling
timestep T is 6 and the evaluation timestep T = 28. The GRPO group size G is 24, the resolution is 512, and the LoRA settings are
Œ± = 128 and r = 64. LoRA is applied to attention layers and feed-forward network layers (attn.to q, attn.to k, attn.to v,
attn.to out.0, attn.add q proj, attn.add k proj, attn.add v proj, attn.to add out, ff.net.0.proj,
ff.net.2, ff context.net.0.proj,
ff context.net.2). The per-GPU batch size is 3 with gradient accumulation steps of 6 for all three tasks. We train GenEval,
PickScore and OCR for {2,700, 1,900, 1,800, 1,600}, {1,250, 600, 1,550, 2,200}, and {1,250, 650, 850, 1,100} steps, respectively;
the bracketed numbers denote merge iterations 0-3.
‚Ä¢ Text-to-Video:
We train the HunyuanVideo [16] with the same configuration as the DanceGRPO [41] but apply LoRA in-
stead of full finetuning.
The LoRA settings are Œ± = 64 and r = 32.
LoRA is applied to attention layers ‚Äúattn.to q,
attn.to k, attn.to v‚Äù in single transformer blocks (40 blocks), ‚Äúattn.to q, attn.to k, attn.to v, attn.to out.0,
attn.to add out, attn.add q proj, attn.add k proj, attn.add v proj‚Äù in transformer blocks (20 blocks). We train
VQ and MQ for {200, 140, 100}, {195, 145, 115} steps, respectively; the bracketed numbers denote merge iterations 0-2.
Multi-objective Reinforcement Learning (MORL).
The hyperparameters for MORL are identical to the MapReduce LoRA
mentioned above. During each epoch, batches are sampled from GenEval [11], PickScore [15], and OCR [6] datasets according to
specified ratios (default 1:1:1), where each individual batch contains samples from a single source. During evaluation, each data source is
independently evaluated using its corresponding reward model to provide per-task performance metrics. The training steps are comparable
14

to the overall training steps of MapReduce LoRA for fairness. There are two variations of MORL in our experiments, and the key difference
lies in the data mixture and reward mixture strategy:
‚Ä¢ Data mixture (MORL-D): Each sample is only evaluated by its corresponding reward model based on its source dataset.
‚Ä¢ Data mixture and reward mixture (MORL-DR): Samples from GenEval [11] and OCR [6] are scored by both their task-specific
reward and PickScore [15]; we take the average as the final reward (e.g., for GenEval, (rgeneval + rpickscore)/2). In contrast, samples from
the PickScore dataset are evaluated only with PickScore. This choice reflects dataset constraints: GenEval requires a structured prompt
format, whereas OCR requires prompts that contain text for a valid evaluation. PickScore serves as a shared aesthetic quality anchor
across tasks. Overall, this protocol better aligns with the multi-objective setting by applying multiple rewards in a dataset-agnostic
manner (where applicable), rather than restricting evaluation to dataset-specific rewards.
Compute Resources.
All the experiments are conducted on 32 NVIDIA A100 GPUs (80GB).
C.2. Base Models and Reward Models
The following table presents the base model and reward models along with their corresponding links.
Models
Links
SD 3.5 M [2]
https://huggingface.co/stabilityai/stable-diffusion-3.5-medium
FLUX.1-dev [17]
https://huggingface.co/black-forest-labs/FLUX.1-dev
HunyuanVideo [16]
https://huggingface.co/hunyuanvideo-community/HunyuanVideo
Reward Models
Links
GenEval [11]
https://github.com/djghosh13/geneval
PickScore [15]
https://huggingface.co/yuvalkirstain/PickScore_v1
OCR [6]
https://github.com/PaddlePaddle/PaddleOCR
VQAScore [21]
https://github.com/linzhiqiu/t2v_metrics
MPS [43]
https://github.com/Kwai-Kolors/MPS
VILA [14]
https://github.com/google-research/google-research/tree/master/vila
VQ and MQ [24]
https://github.com/KlingTeam/VideoAlign/tree/main
D. More Quantitative Results
D.1. Full Results of Different Merging Ratios (Corresponding to Fig. 1)
Text-to-Image
Text-to-Video
SD 3.5 M
FLUX.1-dev
HunyuanVideo
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure 9. MapReduce LoRA advances the Pareto fronts on Text-to-Image and Text-to-Video tasks. We include the non-Pareto sets in
this figure for sharing the full results.
15

Pareto fronts denote the set of non-dominated solutions (parameter settings for which no objective can be improved without degrading
another). In Fig. 9, we include dominated (non-Pareto-optimal) points to present the full result set, together with the scores reported in
Tables 5 and 6. Dominated points can arise for four reasons:
‚Ä¢ Estimation noise and stochasticity: Each reward is estimated from finite samples and noisy reward evaluators. For GenEval, PickScore
and OCR, we evaluate 553, 2,048 and 1,018 cases, respectively.
‚Ä¢ Non-convex objective landscapes: The optimization surfaces can be highly non-convex. The merged model checkpoints may lie in
different basins or apexes for different reward functions.
‚Ä¢ Partial or misaligned objectives: When the two rewards are not orthogonal (i.e., they share underlying structure), optimizing one can
initially improve the other; once the shared structure is exhausted, trade-offs dominate again, yielding dominated points.
‚Ä¢ Training unsaturation: In Fig. 9 (g) and (h), the edge points are mixed together on the top-left corner because iteration 1 for OCR uses
much fewer training steps (650) than iteration 0 (1,250); additional training may further improve performance.
Table 5. 3D merging results on Text-to-Image tasks: SD 3.5 M [2] and FLUX.1-dev [17].
Group
Merging Ratios
SD 3.5 M
FLUX.1-dev
GenEval PickScore OCR GenEval PickScore OCR GenEval PickScore OCR
base model
0
0
0
0.68
21.784
0.601
0.67
22.006
0.573
Merge 1
1
0
0
0.93
21.852
0.658
0.90
22.057
0.692
0
1
0
0.76
23.359
0.716
0.70
23.566
0.723
0
0
1
0.68
21.800
0.934
0.68
21.997
0.971
0.8
0.1
0.1
0.90
21.993
0.704
0.88
22.175
0.756
0.1
0.8
0.1
0.77
23.090
0.746
0.75
23.408
0.794
0.1
0.1
0.8
0.70
21.948
0.911
0.71
22.136
0.959
0.6
0.2
0.2
0.86
22.106
0.743
0.83
22.299
0.802
0.2
0.6
0.2
0.78
22.731
0.781
0.76
23.046
0.827
0.2
0.2
0.6
0.76
22.087
0.882
0.73
22.288
0.928
0.4
0.4
0.2
0.81
22.382
0.758
0.81
22.633
0.813
0.4
0.2
0.4
0.80
22.098
0.826
0.79
22.294
0.883
0.2
0.4
0.4
0.77
22.374
0.836
0.76
22.636
0.886
0.3
0.3
0.3
0.79
22.276
0.805
0.77
22.503
0.868
Merge 2
1
0
0
0.92
22.101
0.802
0.91
22.156
0.844
0
1
0
0.79
23.205
0.793
0.80
23.515
0.833
0
0
1
0.74
21.986
0.945
0.71
22.116
0.953
0.8
0.1
0.1
0.90
22.208
0.831
0.89
22.303
0.883
0.1
0.8
0.1
0.82
23.004
0.820
0.81
23.337
0.888
0.1
0.1
0.8
0.76
22.109
0.933
0.75
22.249
0.956
0.6
0.2
0.2
0.89
22.316
0.839
0.88
22.437
0.913
0.2
0.6
0.2
0.84
22.778
0.843
0.83
23.037
0.916
0.2
0.2
0.6
0.81
22.232
0.917
0.80
22.405
0.941
0.4
0.4
0.2
0.85
22.532
0.850
0.85
22.718
0.918
0.4
0.2
0.4
0.84
22.290
0.895
0.85
22.429
0.927
0.2
0.4
0.4
0.83
22.486
0.899
0.81
22.709
0.929
0.3
0.3
0.3
0.84
22.436
0.871
0.84
22.619
0.926
Merge 3
1
0
0
0.92
22.138
0.854
0.93
22.115
0.894
0
1
0
0.82
23.243
0.850
0.79
23.902
0.904
0
0
1
0.78
22.128
0.954
0.77
22.167
0.963
0.8
0.1
0.1
0.92
22.281
0.890
0.91
22.324
0.928
0.1
0.8
0.1
0.83
23.080
0.875
0.85
23.683
0.930
0.1
0.1
0.8
0.81
22.248
0.944
0.80
22.331
0.960
0.6
0.2
0.2
0.91
22.408
0.900
0.90
22.508
0.936
0.2
0.6
0.2
0.85
22.869
0.887
0.87
23.285
0.938
0.2
0.2
0.6
0.84
22.369
0.929
0.85
22.496
0.955
0.4
0.4
0.2
0.88
22.637
0.904
0.90
22.878
0.934
0.4
0.2
0.4
0.87
22.393
0.920
0.88
22.515
0.943
0.2
0.4
0.4
0.85
22.615
0.913
0.86
22.864
0.946
0.3
0.3
0.3
0.88
22.554
0.908
0.88
22.740
0.941
Evaluation details:
‚Ä¢ 3D Pareto fronts: For each merge, we
evaluate thirteen model weight configu-
rations with different coefficient ratios:
three single-reward configurations (the
other two coefficients set to zero) and
ten mixed configurations. The specific
{GenEval: PickScore: OCR} ratios are
{1 : 0 : 0}, {0 : 1 : 0}, {0 : 0 : 1},
{0.8 : 0.1 : 0.1}, {0.1 : 0.8 : 0.1},
{0.1 : 0.1 : 0.8}, {0.6 : 0.2 : 0.2},
{0.2 : 0.6 : 0.2}, {0.2 : 0.2 : 0.6},
{0.4 : 0.4 : 0.2}, {0.4 : 0.2 : 0.4},
{0.2 : 0.4 : 0.4}, and {0.3 : 0.3 : 0.3}.
Each plotted point is obtained by eval-
uating one merged model on all three
metrics.
‚Ä¢ 2D Pareto fronts: For each merge, we
evaluate eleven model weight configu-
rations with different ratios: two single-
reward configurations (the other coeffi-
cient set to zero) and nine mixed config-
urations. The specific ratios are {1 : 0},
{0 : 1}, {0.1 : 0.9}, {0.2 : 0.8},
{0.3 : 0.7}, {0.4 : 0.6}, {0.5 : 0.5},
{0.6 : 0.4}, {0.7 : 0.3}, {0.8 : 0.2},
and {0.9 : 0.1}. Each point is obtained
by evaluating one merged model on the
two metrics.
D.2. Pareto Front Comparison of
MORL vs. MapReduce LoRA
To compare naive multi-objective RL with
data mixing (MORL-D) against MapRe-
duce LoRA, we train MORL-D under ten {GenEval:PickScore:OCR} sampling ratios‚Äî{1:1:1}, {1:2:3}, {1:3:2}, {2:1:3}, {2:3:1},
{3:1:2}, {3:2:1}, {1:1:4}, {1:4:1}, {4:1:1}‚Äîusing the same number of training steps as MapReduce LoRA on SD 3.5 M [2]. Fig. 10
compares Pareto fronts for Rewarded Soup [33], MORL-D, and MapReduce LoRA (left: 3D; right: 2D projections). In 3D, MORL-D is
confined to a small region across the three rewards. For readability, we also show 2D projections of the 3D front; because MORL-D is an
a priori method, we cannot fix one reward to 0 as in Fig. 9, so the right panels are projections rather than true 2D Pareto fronts. Although
MORL-D is competitive on GenEval and OCR, conflicting objectives reduce visual quality (PickScore; see Fig. 15), yielding only limited
PickScore gains.
16

Table 6. 2D merging results on Text-to-Image tasks, SD 3.5 M [2] and FLUX.1-dev [17], and Text-to-Video task, HunyuanVideo [16].
Group
Merging Ratios
SD 3.5 M
FLUX.1-dev
HunyuanVideo
reward A reward B
AGenEval
BPickScore
AGenEval
BOCR
APickScore
BOCR
AGenEval
BPickScore
AGenEval
BOCR
APickScore
BOCR
AVQ
BMQ
base model
0
0
0.68
21.784
0.68
0.601
21.784
0.601
0.67
22.006
0.67
0.573
22.006
0.573
3.25
0.95
0
1
0.76
23.359
0.68
0.934
21.800
0.934
0.70
23.566
0.68
0.971
21.997
0.971
3.80
1.66
0.1
0.9
0.77
23.233
0.71
0.923
21.926
0.925
0.74
23.525
0.68
0.966
22.122
0.972
3.89
1.62
0.2
0.8
0.79
23.084
0.73
0.904
22.062
0.909
0.77
23.403
0.71
0.962
22.260
0.968
3.95
1.57
0.3
0.7
0.83
22.923
0.75
0.891
22.207
0.903
0.79
23.220
0.72
0.946
22.436
0.946
4.01
1.52
0.4
0.6
0.83
22.736
0.78
0.864
22.358
0.878
0.80
23.024
0.75
0.929
22.627
0.930
4.06
1.48
Merge 1
0.5
0.5
0.85
22.549
0.82
0.838
22.534
0.861
0.83
22.825
0.79
0.899
22.828
0.916
4.13
1.43
0.6
0.4
0.88
22.379
0.86
0.811
22.715
0.848
0.85
22.629
0.81
0.862
23.032
0.891
4.18
1.38
0.7
0.3
0.90
22.232
0.86
0.766
22.902
0.803
0.86
22.446
0.83
0.831
23.242
0.857
4.23
1.34
0.8
0.2
0.90
22.093
0.89
0.734
23.085
0.782
0.87
22.297
0.87
0.787
23.410
0.823
4.29
1.30
0.9
0.1
0.92
21.966
0.91
0.706
23.237
0.754
0.88
22.169
0.89
0.742
23.530
0.785
4.33
1.25
1
0
0.93
21.852
0.93
0.658
23.359
0.716
0.90
22.057
0.90
0.692
23.566
0.723
4.37
1.20
0
1
0.79
23.205
0.74
0.945
21.986
0.945
0.80
23.515
0.71
0.953
22.116
0.953
4.18
1.84
0.1
0.9
0.81
23.125
0.76
0.945
22.090
0.943
0.82
23.447
0.74
0.945
22.241
0.945
4.25
1.80
0.2
0.8
0.82
23.039
0.77
0.935
22.178
0.931
0.83
23.332
0.75
0.952
22.377
0.948
4.32
1.77
0.3
0.7
0.84
22.930
0.80
0.925
22.307
0.924
0.85
23.195
0.80
0.950
22.524
0.953
4.40
1.73
0.4
0.6
0.86
22.817
0.81
0.915
22.421
0.917
0.86
23.040
0.82
0.939
22.696
0.947
4.46
1.70
Merge 2
0.5
0.5
0.87
22.699
0.84
0.897
22.557
0.902
0.88
22.867
0.83
0.931
22.856
0.934
4.52
1.67
0.6
0.4
0.90
22.569
0.86
0.897
22.706
0.884
0.88
22.713
0.87
0.933
23.017
0.927
4.56
1.63
0.7
0.3
0.90
22.438
0.88
0.871
22.849
0.872
0.90
22.557
0.89
0.918
23.189
0.914
4.60
1.60
0.8
0.2
0.90
22.318
0.90
0.856
22.977
0.847
0.89
22.418
0.89
0.907
23.339
0.907
4.63
1.56
0.9
0.1
0.91
22.208
0.91
0.826
23.100
0.823
0.90
22.282
0.90
0.872
23.450
0.879
4.66
1.53
1
0
0.92
22.101
0.92
0.802
23.205
0.793
0.91
22.156
0.91
0.844
23.515
0.833
4.70
1.50
0
1
0.82
23.243
0.78
0.954
22.128
0.954
0.79
23.902
0.77
0.963
22.167
0.963
4.59
1.84
0.1
0.9
0.83
23.179
0.79
0.949
22.238
0.951
0.83
23.844
0.78
0.962
22.316
0.951
4.64
1.82
0.2
0.8
0.84
23.097
0.82
0.947
22.338
0.940
0.87
23.693
0.82
0.960
22.467
0.952
4.67
1.82
0.3
0.7
0.87
23.003
0.83
0.936
22.454
0.931
0.90
23.504
0.84
0.962
22.642
0.956
4.71
1.82
0.4
0.6
0.89
22.892
0.86
0.931
22.578
0.939
0.90
23.299
0.86
0.958
22.834
0.955
4.77
1.81
Merge 3
0.5
0.5
0.91
22.772
0.88
0.920
22.699
0.924
0.93
23.091
0.87
0.950
23.046
0.949
4.81
1.81
0.6
0.4
0.90
22.640
0.90
0.914
22.842
0.915
0.92
22.872
0.90
0.946
23.263
0.948
4.85
1.79
0.7
0.3
0.90
22.504
0.90
0.902
22.955
0.908
0.93
22.680
0.90
0.943
23.467
0.939
4.91
1.79
0.8
0.2
0.92
22.375
0.91
0.896
23.055
0.891
0.93
22.485
0.91
0.936
23.666
0.937
4.95
1.77
0.9
0.1
0.91
22.261
0.92
0.890
23.164
0.868
0.92
22.301
0.91
0.922
23.838
0.919
4.99
1.77
1
0
0.92
22.138
0.92
0.854
23.243
0.850
0.93
22.115
0.93
0.894
23.902
0.904
5.03
1.75
Figure 10. Merging performance comparison across Rewarded Soup [33], MORL-D, and MapReduce LoRA. Left: 3D Pareto-front
comparison. Right: 2D projections of the 3D Pareto front (for readability), which are not 2D Pareto fronts. Unlike Fig. 9, these are
projections rather than a 2D merge with the third reward fixed to 0. MORL-D performance is confined to a small region across the three
rewards and yields only limited improvement on PickScore.
E. More Qualitative Results
E.1. Text-to-Image qualitative results
Figs 11, 12 and 13 demonstrate the visual comparison of two rewards with different merging ratios. Figs 14 and 15 demonstrate the visual
comparison of all methods, including Flow-GRPO [23] tuned on GenEval [11], PickScore [15], OCR [6], Rewarded Soup [33], MORL-D,
MORL-DR and MapReduce LoRA. PickScore and OCR guidance cause strong reward overfitting: the former drives the model toward a
narrow, high-scoring aesthetic style, while the latter encourages increasingly large and prominent text regardless of overall visual harmony
or contextual appropriateness. Our proposed MapReduce LoRA mitigates these issues by iteratively merging per-reward experts, enabling
the model to discover a more balanced (and often near-optimal) preference that harmonizes aesthetic fidelity with text readability.
17

A photo of a surfboard
A photo of a car
A photo of a toothbrush and a snowboard
A photo of a dining table and a bear
A photo of a frisbee and a couch
A photo of a cat below a baseball glove
GenEval
PickScore
{1 : 0}
{0.9 : 0.1}
{0.8 : 0.2}
{0.7 : 0.3}
{0.6 : 0.4}
{0.5 : 0.5}
{0.4 : 0.6}
{0.3 : 0.7}
{0.2 : 0.8}
{0.1 : 0.9}
{0 : 1}
A photo of a hot dog right of a skateboard
A photo of a dining table
A photo of a cow
Figure 11. Visual comparison across different merging ratios between PickScore and GenEval.
18

OCR
PickScore
{1 : 0}
{0.9 : 0.1}
{0.8 : 0.2}
{0.7 : 0.3}
{0.6 : 0.4}
{0.5 : 0.5}
{0.4 : 0.6}
{0.3 : 0.7}
{0.2 : 0.8}
{0.1 : 0.9}
{0 : 1}
A high-fashion runway with a sleek, modern backdrop displaying "Spring Collection 2024". Models walk confidently on the catwalk,
showcasing vibrant, floral prints and pastel tones, under soft, ambient lighting that enhances the fresh, spring vibe.
A realistic photograph of a fast food drive-thru menu board at dusk, featuring a bold and colorful advertisement that reads "Try Our New Burger" 
with an appetizing image of the burger below, set against the backdrop of a busy suburban street.
A high-altitude mountain summit with a wooden signpost clearly marked "Elevation 8000 Feet", 
surrounded by rocky terrain and a backdrop of distant, snow-capped peaks under a clear blue sky.
A weathered pirate ship flag, tattered and fluttering in the sea breeze, is intricately embroidered with the bold words "Sea Wolf Crew" 
in deep navy thread, against a backdrop of stormy skies and turbulent waves.
An astronaut in space, the helmet visor reflecting a floating "Low Oxygen Warning" sign, 
surrounded by the vast, star-filled darkness, with Earth faintly visible in the distance.
A bustling deli with vintage decor, the counter lined with gleaming metal and glass. A retro ticket machine in the corner, lights blinking, 
prints out a paper slip that reads "Now Serving 42", amid the hum of conversation and the scent of fresh bread.
A vibrant skatepark with dynamic graffiti tagging that reads "Skate Or Die Trying" 
across a worn, concrete wall, surrounded by skaters performing tricks and an urban backdrop.
A vintage suitcase adorned with a "World Traveler Adventures" sticker, placed on a rustic wooden table. 
Sunlight filters through a nearby window, casting warm, golden hues on the suitcase and highlighting the sticker's faded, nostalgic colors.
A UFO hovers in the night sky, its underside glowing with a soft, otherworldly light. Beneath it, a beam of light illuminates a peaceful landscape. 
The words "We Come in Peace" are clearly visible on its surface, radiating a sense of calm and hope.
Figure 12. Visual comparison across different merging ratios between PickScore and OCR.
19

OCR
GenEval
{1 : 0}
{0.9 : 0.1}
{0.8 : 0.2}
{0.7 : 0.3}
{0.6 : 0.4}
{0.5 : 0.5}
{0.4 : 0.6}
{0.3 : 0.7}
{0.2 : 0.8}
{0.1 : 0.9}
{0 : 1}
A photo of an elephant below a surfboard
A photo of a frisbee below a horse
A photo of an orange giraffe and a white baseball glove
A photo of a purple computer keyboard and a red chair
A realistic photograph of a modern highway scene with a large billboard prominently displaying the text "Gas Next Exit 2 Miles" 
against a backdrop of rolling hills and clear blue sky.
A close-up of a concert wristband, prominently stamped with "VIP Access Backstage Pass", glowing under the stage lights, 
with a blurred crowd and vibrant stage colors in the background.
A vibrant TV show poster titled "Stories Floating on the Wind", featuring ethereal illustrations of characters and symbols drifting 
through a serene, misty landscape, with soft, warm lighting and a nostalgic, whimsical atmosphere.
A serene coastal scene with a fisherman's boat named "Sea Legend II" gently bobbing on the calm waters at sunrise, 
the golden light casting a warm glow over the wooden hull and the rippling waves.
A photo of a frisbee right of a motorcycle
Figure 13. Visual comparison across different merging ratios between GenEval and OCR.
20

SD 3.5 M
GenEval
PickScore
OCR
Rewarded 
Soup
MORL-D
MORL-DR
MapReduce 
LoRA
Flow-GRPO
A photo of a microwave and a truck
A photo of a fire hydrant and a tennis racket
A photo of a sports ball and a cow
A photo of a backpack right of a sandwich
A photo of a suitcase left of a banana
A photo of a cup left of an umbrella
A photo of a horse right of a broccoli
A photo of a donut right of a bench
Figure 14. Visual comparison across all methods.
21

SD 3.5 M
GenEval
PickScore
OCR
Rewarded 
Soup
MORL-D
MORL-DR
MapReduce 
LoRA
Flow-GRPO
A realistic cockpit interior with a digital screen displaying "Prepare for Landing" in bold text, 
illuminated by soft blue backlighting, with the pilot's hand resting on the control yoke.
A bustling city street at night with a movie theater marquee prominently displaying "Now Showing Action 5". 
Crowds of people in casual attire are entering the theater, while neon lights and billboards light up the background, 
creating a vibrant urban atmosphere.
A bustling stadium at dusk, the scoreboard prominently displays "Home Team Wins" in vibrant, illuminated letters, 
surrounded by cheering fans in team colors, with the victorious team's players celebrating on the field.
A beekeeper stands beside a wooden hive box, carefully labeled "Buzz Central Handle With Care", surrounded by a bustling field 
of wildflowers, with bees fluttering around the hive and the keeper wearing traditional protective gear.
A realistic photograph of a gas station with a large, red price sign prominently displaying "Fuel Prices Rising" 
against a cloudy sky, cars filling up in the background.
A detailed close-up of a magic carpet with intricate embroidery, prominently featuring the text "No Shoes Fly Casual" 
woven in shimmering threads, set against a backdrop of a serene, cloudy sky.
A movie poster titled "Invasion of the Robots" featuring towering robots destroying cityscapes, with skyscrapers collapsing 
and fiery explosions lighting up the night sky, emphasizing the chaos and scale of the robotic invasion.
Figure 15. Visual comparison across all methods on text rendering quality.
22

E.2. Text-to-Video qualitative results
Fig. 16 demonstrates the visual comparison of HunyuanVideo [16], Rewarded Soup [33] and our proposed MapReduce LoRA. Also,
Figs. 17 and 18 demonstrate the visual performance of merging progress that advances the visual quality, motion quality, and prompt
alignment.
A men from the back part of the room with a skimask on, sitting in a chair writting a 
letter in a dark lighted room, shot on film, cinematic scene, realistic, vertical size
HunyuanVideo
Rewarded Soup
MapReduce LoRA
Drone movement of taj mahal 8k video
HunyuanVideo
Rewarded Soup
MapReduce LoRA
Figure 16. Visual comparison of HunyuanVideo [16], Rewarded Soup [33], and MapReduce LoRA (Ours). In the upper case,
MapReduce LoRA more faithfully renders the intended motion and writing action described in the prompt. In the lower case, MapReduce
LoRA better adheres to the specified drone-movement control, producing a trajectory that aligns with the prompt.
23

An interesting visual for an industrial techno party
HunyuanVideo
MapReduce LoRA
(merge 2)
MapReduce LoRA
(merge 3)
MapReduce LoRA
(merge 1)
Deadpool walking around in an cyber city
HunyuanVideo
MapReduce LoRA
(merge 2)
MapReduce LoRA
(merge 3)
MapReduce LoRA
(merge 1)
Figure 17. Visual performance across two cases under different MapReduce LoRA merging iterations. In the upper case, the
HunyuanVideo result fails to depict the walking motion, while increasing the merging iterations progressively restores natural walking
dynamics and improves the background building quality. In the lower case, the initial generation shows limited visual fidelity, whereas
successive merging iterations consistently enhance the overall clarity and scene quality.
24

Korean pregnant woman holding baby while selfie with iphone
HunyuanVideo
MapReduce LoRA
(merge 2)
MapReduce LoRA
(merge 3)
MapReduce LoRA
(merge 1)
Two people sitting on the beach drinking, cozy images, the sound of waves, seagulls
HunyuanVideo
MapReduce LoRA
(merge 2)
MapReduce LoRA
(merge 3)
MapReduce LoRA
(merge 1)
Figure 18. Visual performance across two cases under different MapReduce LoRA merging iterations. In the upper case, the Hun-
yuanVideo result shows limited visual clarity‚Äîpartly due to suboptimal motion quality‚Äîwhile increasing merging iterations progressively
refine facial and background details, improve pose naturalness, and enhance overall realism. In the lower case, the initial generation offers
limited scene clarity‚Äîthe drinking containers are barely visible‚Äîwhereas additional merging iterations clearly render these objects and
deliver sharper, more coherent visuals.
25

F. Limitations and Future Works
This paper presents MapReduce LoRA, a simple, scalable recipe for systematically pushing the multi-preference Pareto front and enabling
practical post-training customization. We note several scope considerations and opportunities for further study:
‚Ä¢ Scaling to more preferences: In Text-to-Image, we validate on three targeted preferences and three additional untargeted ones; extend-
ing to larger numbers of preferences is a promising scaling direction.
‚Ä¢ Merging policies and schedules: We default to uniform averaging and compare a few merge frequencies under fixed training steps;
exploring adaptive/learned policies and schedules may yield further gains.
‚Ä¢ Architecture-agnostic Reward-aware Token Embedding (RaTE): RaTE is lightweight and effective on Stable Diffusion series models,
which contain explicit cross-attention between text and image information, but is less reliable for joint sequence models, i.e., FLUX.
Exploring model-agnostic designs is a practical direction.
We leave these promising directions for future work.
26
