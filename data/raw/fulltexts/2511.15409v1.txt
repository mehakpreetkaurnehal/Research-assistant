Proximal Approximate Inference in State-Space Models
Hany Abdulsamad
h.abdulsamad@uva.nl
Amsterdam Machine Learning Lab
University of Amsterdam, Netherlands
√Ångel F. Garc√≠a-Fern√°ndez
angel.garcia.fernandez@upm.es
IPTC, ETSI Telecomunicaci√≥n
Universidad Polit√©cnica de Madrid, Spain
Simo S√§rkk√§
simo.sarkka@aalto.fi
Department of Electrical Engineering and Automation
Aalto University, Finland
Abstract
We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space
models. Our approach is based on a variational Lagrangian formulation that casts Bayesian
inference as a sequence of entropic trust-region updates subject to dynamic constraints. This
framework gives rise to a family of forward-backward algorithms, whose structure is deter-
mined by the chosen factorization of the variational posterior. By focusing on Gauss‚ÄìMarkov
approximations, we derive recursive schemes with favorable computational complexity. For
general nonlinear, non-Gaussian models we close the recursions using generalized statistical
linear regression and Fourier‚ÄìHermite moment matching.
1
Introduction
Accurate state estimation in partially observable dynamical phenomena is a fundamental problem across sci-
entific and engineering disciplines, including robotics (Barfoot, 2024), economics (Jacquier et al., 2002), and
biology (Murray, 2002). Bayesian inference in state-space models (SSMs) provides a principled framework for
this task, encompassing techniques such as Kalman filtering, sequential Monte Carlo, and message-passing
algorithms. Additionally, an important link between inference and optimization has been established early in
the field (K√°lm√°n, 1960; Cox, 1964; Bell, 1994), giving rise to a suite of inference-as-optimization algorithms.
Recently, this relationship has been further deepened by the advent of approximate methods, such as vari-
ational inference (VI, Wainwright & Jordan, 2008) and expectation propagation (EP, Minka, 2001), which
generalize the optimization perspective beyond point estimates to arbitrary full posterior approximations.
For state-space models with conjugate prior-likelihood pairs,
the corresponding Bayesian posterior
distribution admit closed-form solutions by a variety of efficient recursive algorithms, such as the
Rauch‚ÄìTung‚ÄìStriebel (RTS) smoother for linear-Gaussian SSMs (Rauch et al., 1965). However, extensions
to general nonlinear, non-Gaussian settings are not trivial. While sampling-based methods such as sequen-
tial Monte Carlo (Chopin & Papaspiliopoulos, 2020) and Markov Chain Monte Carlo (Brooks et al., 2011)
provide asymptotically exact solutions, they do so at a price of high computational complexity and limited
scalability, particularly for sequences in high-dimensional spaces and long time horizons (Beskos et al., 2014).
Approximate Bayesian techniques, though often suboptimal, can offer a trade-off that sacrifices exactness
for a significant computational advantage. This aspect motivates our focus on this class of methods.
Variational inference and expectation propagation methods have been adapted for Bayesian inference in
state-space models (Deisenroth & Mohamed, 2012; Chang et al., 2020; Wilkinson et al., 2020). However,
despite the strong connection of both frameworks to numerical optimization (Wilkinson et al., 2023), many
existing approaches extend VI and EP to state-space representations in an ad-hoc manner. Such methods
often integrate variational principles into the structure of a Rauch-Tung-Striebel smoother (Rauch et al.,
1
arXiv:2511.15409v1  [cs.LG]  19 Nov 2025

1965) template rather than deriving algorithms from a foundational optimization-based perspective. As a
result, they may not fully leverage the potential benefits of a systematic approach to state estimation in the
most general settings.
In this work, we cast approximate Bayesian inference in nonlinear, non-Gaussian state-space models as an
iterative dynamic optimization problem over the space of candidate posteriors. Our key contributions are:
‚Ä¢ Dynamic optimization formulation: We introduce a principled variational framework for Bayesian
inference that formulates inference as a sequence of entropic trust-region updates in probability-
density space, using structured entropic proximal regularization.
‚Ä¢ Unified recursive algorithms: We derive a family of efficient forward‚Äìbackward recursion schemes
whose structure is determined entirely by the factorization of the variational posterior ‚Äî forward-,
reverse-, or hybrid-Markov factorizations.
‚Ä¢ Adaptive step size: We show that the step size between iterative updates corresponds to a Lagrangian
multiplier whose optimal value is determined by a dual R√©nyi divergence objective.
‚Ä¢ Generalization of classical smoothers: Our method generalizes algorithms in the linear-Gaussian
setting, thereby providing a bridge between classical and modern approximate inference techniques.
‚Ä¢ Flexible posterior approximations: We provide practical realizations using Gauss‚ÄìMarkov approxima-
tions, instantiated via generalized statistical linear regression and Fourier‚ÄìHermite moment match-
ing, allowing for efficient inference in nonlinear and non-Gaussian models.
The paper is organized as follows. Section 2 introduces the approximate inference problem in state-space
models. Section 3 provides a brief review of proximal variational optimization, which serves as the foundation
for our method. In Section 4, we present our dynamic Lagrangian framework for approximate Bayesian infer-
ence. Section 5 specializes this framework to the Gaussian setting and derives practical recursive algorithms.
We then connect our approach to related work in Section 6.
2
Problem Statement
We consider the problem of Bayesian state estimation in nonlinear, non-Gaussian state-space models. Let
{xk ‚ààRd}T
k=0 be a latent discrete-time Markov process and {yk ‚ààRm}T
k=1 a corresponding sequence of noisy
observations. The system is governed by the state-space dynamics:
x0 ‚àºp0(¬∑),
xk+1 ‚àºfk(¬∑ | xk),
yk+1 ‚àºhk+1(¬∑ | xk+1),
(1)
where p0(x0) is the initial prior probability density, fk(xk+1 | xk) is the Markovian prior probability of the
latent stochastic dynamics, and hk(yk | xk) is the conditional probability density of the stochastic mea-
surements. State estimation refers to the problems of online filtering and offline smoothing, which aim to
reconstruct the posterior distributions pk(xk | y1:k) and pk(xk | y1:T), for all k > 0, respectively. For arbi-
trary forms of the latent dynamics fk(xk+1 | xk) and the measurement model hk(yk | xk), these filtering and
smoothing posterior densities are generally intractable.
In this work, we focus on finding the best approximation of the smoothing posterior density within a certain
variational family of joint distributions
q(x0:T | y1:T) ‚âàp(x0:T | y1:T) ‚àùp0(x0)
T ‚àí1
Y
k=0
fk(xk+1 | xk) hk+1(yk+1 | xk+1).
More specifically, to leverage the structure of state-space models, we focus on Gauss‚ÄìMarkov approximate
posterior densities, leading to the following possibilities:
Assumption 1 (Forward-Markov factorization) For state-space models of the form (1) the Bayesian
posterior p(x0:T | y1:T ) can be approximated by a forward-Markov decomposition
‚àí‚áÄq (x0:T | y1:T) = ‚àí‚áÄq0(x0 | y1:T)
T ‚àí1
Y
k=0
‚àí‚áÄqk(xk+1 | xk, yk+1:T).
2

Assumption 2 (Reverse-Markov factorization) For state-space models of the form (1), the Bayesian
posterior p(x0:T | y1:T ) can be approximated by a reverse-Markov decomposition
‚Üº‚àíq (x0:T | y1:T) = ‚Üº‚àíqT(xT | y1:T)
T
Y
k=1
‚Üº‚àíqk(xk‚àí1 | xk, y1:k‚àí1).
We use ‚àí‚áÄq and ‚Üº‚àíq to distinguish forward- and reverse-Markov approximations, respectively. Moreover, while
Assumption 1 and 2 explicitly state the dependency of the (conditional) posteriors on the measurements, we
omit it in subsequent sections for a simpler notation.
3
Entropic Proximal Variational Optimization
Before introducing our approach to variational smoothing in state-space models, we briefly review the prin-
ciples of entropic proximal variational optimization in a simpler, static setting. Consider a hidden variable
x ‚ààRd, observed data y ‚ààRm, a likelihood p(y | x), and a prior p(x). The goal is to approximate the
posterior p(x | y) by a distribution q(x) that minimizes the Kullback‚ÄìLeibler (KL) divergence:
q‚àó(x) = arg min
q(x)
DKL

q(x) || p(x | y)

.
Since the exact posterior p(x | y) is generally intractable, variational inference optimizes the evidence lower
bound (ELBO) instead (Blei et al., 2017):
q‚àó(x) = arg max
q(x)
L(q) = Eq
h
log p(x, y)
i
‚àíEq
h
log q(x)
i
‚â§log p(y),
(2)
where p(y) is the marginal likelihood or evidence. In conjugate models, the ELBO can be optimized ex-
actly (Bishop, 2006), but for more general settings, approximate solutions are required. Black-box variational
inference (Ranganath et al., 2014) provides a flexible framework for such cases. However, alternative meth-
ods such as natural and Riemannian gradient approaches (Honkela et al., 2010) and proximal variational
inference (Chr√©tien & Hero, 2002; Khan et al., 2015; Theis & Hoffman, 2015) explicitly account for the
geometry of the variational family and often lead to more stable and efficient optimization (Amari, 1998).
In this work, we focus on the entropic proximal variational optimization framework to derive recursive
algorithms for structured approximate inference in nonlinear non-Gaussian state-space models. However,
before directing our attention to the case of state-space models, we use the static case in (2) to illustrate the
mechanics of entropic proximal variational optimization. We start by formulating the approximate inference
problem within the iterative entropic proximal optimization framework proposed by Teboulle (1992); Iusem
et al. (1994). Starting from the ELBO in (2), we introduce an entropic-proximal constraint on subsequent
posterior iterate in the form of a Kullback‚ÄìLeibler divergence, leading to the following nonlinear program:
maximize
q(x)
Eq
h
log p(x, y)
i
‚àíEq
h
log q(x)
i
,
subject to
DKL
h
q(x) || q[i](x)
i
‚â§Œµ
and
Z
q(x) dx = 1,
(3)
where q[i](x) is approximate posterior at iteration i and Œµ ‚â•0 is a hyperparameter that controls the infor-
mation bottleneck between two iterations. Additionally, problem (3) includes a distributional normalization
constraint for q(x), whereas the positivity of q(x) is implied by the logarithmic function embedded within
the KL constraint, which enforces a trust region around the current iterate q[i](x). The following lemma
derives the solution to (3), the variational distribution iterate q[i+1](x), by constructing the Lagrangian.
Proposition 1 (Damped Gibbs posterior) The Bayesian posterior characterized by a likelihood p(y | x)
and a prior p(x) and constrained to lie within a Kullback‚ÄìLeibler Œµ-ball centered at q[i](x) is the maximizer
of the constrained nonlinear program (3) and takes the form of the following Gibbs posterior
q[i+1](x) =
h
Z[i+1](Œ≤)
i‚àí1 h
p(y | x) p(x)
i1‚àíŒ≤h
q[i](x)
iŒ≤
,
3

with a damping parameter Œ≤ ‚àà[0, 1) and a normalizing constant
Z[i+1](Œ≤) =
Z h
p(y | x) p(x)
i1‚àíŒ≤h
q[i](x)
iŒ≤
dx.
The damping Œ≤ is a proxy of the Lagrangian multiplier Œ± ‚â•0 associated with the Kullback‚ÄìLeibler divergence
constraint in (3), so that Œ≤ = Œ±/(1 + Œ±). Furthermore, the optimal Œ≤ is a minimizer of the dual problem
minimize
Œ≤
G(Œ≤) =
Œ≤Œµ
1 ‚àíŒ≤ +
1
1 ‚àíŒ≤ log Z[i+1](Œ≤),
subject to 0 ‚â§Œ≤ < 1.
Proof. See Appendix A.
Remark 1 The posterior iterate q[i+1](x) is as an interpolation in the space of densities between the true
posterior p(x | y) ‚àùp(y | x) p(x) and the previous iterate q[i](x) in the space of probability densities.
Remark 2 The dual objective over Œ≤ in Proposition 1 can be interpreted as an optimization over a family
of statistical divergences, as revealed by the variational R√©nyi bound (Li & Turner, 2016):
1
1 ‚àíŒ≤ log Z[i+1](Œ≤) = log p(y) ‚àíDŒ≤
h
q[i](x) || p(x | y)
i
,
where DŒ≤

¬∑ || ¬∑

denotes the R√©nyi divergence with order Œ≤. As the damping parameter Œ≤ varies, the corre-
sponding R√©nyi divergence traces a continuum of Œ±-divergence geometries, each emphasizing different regions
of the posterior distribution. Thus, varying Œ≤ smoothly alters the global geometric structure of the optimiza-
tion problem but preserves the local notion of distance defined by the Fisher information metric induced by
primal Kullback‚ÄìLeibler divergence objective (Amari, 2016).
Having reviewed the principles of entropic proximal optimization, we now turn to state-space models. In
the next section, we extend this framework into the dynamic setting and derive recursive algorithms for
approximate Bayesian inference that exploit the structure of the assumed posterior.
4
Entropic Proximal Bayesian Smoothing
For a state-space model of the form (1), we can adapt the ELBO from (2) as follows
L(q) = Eq
h
log p(x0:T, y1:T)
i
‚àíEq
h
log q(x0:T)
i
‚â§log p(y1:T),
where p(x0:T, y1:T) is the joint state-measurement distribution given by
p(x0:T, y1:T) = p0(x0)
T ‚àí1
Y
k=0
fk(xk+1 | xk) hk+1(yk+1 | xk+1).
We now formulate the entropic proximal optimization problem over the joint posterior q(x0:T):
maximize
q(x0:T )
Eq
h
log p(x0:T, y1:T)
i
‚àíEq
h
log q(x0:T)
i
,
subject to
DKL
h
q(x0:T) || q[i](x0:T)
i
‚â§Œµ
and
Z
q(x0:T) dx0:T = 1.
(4)
This formulation does not yet impose any assumptions on the structure of the approximate posterior q(x0:T).
While (4) can be solved in a manner similar to (3), such an approach can lead to significant computational
complexity due to the high dimensionality of q(x0:T) as it extends over the state and time dimensions. To
address this challenge, we introduce variations to (4) that take advantage of sparsity induced by the forward-
and reverse Markov structure from Assumption 1 and Assumption 2, enabling recursive inference algorithms
with linear time complexity in the horizon T.
4

4.1
Damped Forward-Markov Posterior
Here, we assume a forward-Markov decomposition of q(x0:T) as stated in Assumption 1. This decomposition
of the posterior leads to a generic backward-forward recursive algorithm for computing the approximate
Bayesian smoothing posterior.
Proposition 2 (Optimal forward-Markov posterior) For state-space models of the form (1), and un-
der Assumption 1, the approximate forward-Markov smoothing posterior that solves problem (4) is charac-
terized by the following set of tilted (conditional) distributions:
‚àí‚áÄq [i+1]
0
(x0) =
h‚àí‚áÄ
Z [i+1]
0
i‚àí1 h‚àí‚áÄq [i]
0 (x0)
iŒ≤h
exp
n‚àí‚áÄ
V [i+1]
0
(x0)
o i1‚àíŒ≤
,
(5)
‚àí‚áÄq [i+1]
k
(xk+1 | xk) =
h‚àí‚áÄ
œà [i+1]
k
(xk)
i‚àí1h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
,
(6)
where Œ≤ ‚àà[0, 1) is the damping associated with the Lagrangian multiplier Œ± ‚â•0 so that Œ≤ = Œ±/(1 + Œ±), while
‚àí‚áÄ
Z [i+1]
0
and ‚àí‚áÄ
œà [i+1]
k
(xk) are the corresponding normalizing factors
‚àí‚áÄ
Z [i+1]
0
=
Z h‚àí‚áÄq [i]
0 (x0)
iŒ≤h
exp
n‚àí‚áÄ
V [i+1]
0
(x0)
o i1‚àíŒ≤
dx0,
‚àí‚áÄ
œà [i+1]
k
(xk) =
Z h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1.
(7)
The potential functions ‚àí‚áÄ
V [i+1]
k
(xk), for all 0 ‚â§k ‚â§T, are computed recursively backwards via
‚àí‚áÄ
V [i+1]
k
(xk) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
log hT(yT | xT)
if k = T,
log hk(yk | xk) + 1/(1 ‚àíŒ≤) log‚àí‚áÄ
œà [i+1]
k
(xk)
if 0 < k < T,
log p0(x0) + 1/(1 ‚àíŒ≤) log‚àí‚áÄ
œà [i+1]
0
(x0)
if k = 0,
(8)
bearing in mind that ‚àí‚áÄ
œà [i+1]
k
(xk) are functions of ‚àí‚áÄ
V [i+1]
k+1 (xk+1). Finally, the optimal damping Œ≤ is the mini-
mizer of the dual objective
minimize
Œ≤
‚àí‚áÄ
G(Œ≤) =
Œ≤Œµ
1 ‚àíŒ≤ +
1
1 ‚àíŒ≤ log‚àí‚áÄ
Z [i+1]
0
(Œ≤),
subject to 0 ‚â§Œ≤ < 1.
(9)
Proof. See Appendix B.
Remark 3 The potential functions ‚àí‚áÄ
Vk(xk) can be interpreted as the log-space accumulation of the
backward filter message log q(yk:T | xk), where the log-normalizers log‚àí‚áÄ
œàk(xk) and log‚àí‚áÄ
Z0 correspond to
log q(yk+1:T | xk) and log q(y1:T), respectively.
Remark 4 Under Assumption 1, the marginal smoothing distributions ‚àí‚áÄq [i+1]
k+1 (xk+1), for all 0 < k ‚â§T, are
computed via forward propagation starting from ‚àí‚áÄq [i+1]
0
(x0)
‚àí‚áÄq [i+1]
k+1 (xk+1) =
Z
‚àí‚áÄq [i+1]
k
(xk)‚àí‚áÄq [i+1]
k
(xk+1 | xk) dxk,
where ‚àí‚áÄq [i+1]
k
(xk+1 | xk) and ‚àí‚áÄq [i+1]
0
(x0) are given by Proposition 2.
5

4.2
Damped Reverse-Markov Posterior
Next, we assume a reverse-Markov decomposition of q(x0:T) as described in Assumption 2. This decompo-
sition, in contrast to the forward-Markov assumption, leads to a forward-backward recursive algorithm.
Proposition 3 (Optimal reverse-Markov posterior) For state-space models of the form (1), and under
Assumption 2, the approximate reverse-Markov smoothing posterior that solves problem (4) is characterized
by the following set of tilted (conditional) distributions:
‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) =
h‚Üº‚àí
œà [i+1]
k
(xk)
i‚àí1h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ≤
√ó
h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1)
o i1‚àíŒ≤
,
(10)
‚Üº‚àíq [i+1]
T
(xT) =
h‚Üº‚àí
Z [i+1]
T
i‚àí1h
q[i]
T (xT)
iŒ≤h
exp
n‚Üº‚àí
V [i+1]
T
(xT)
o i1‚àíŒ≤
,
(11)
where Œ≤ ‚àà[0, 1) is the damping associated with the Lagrangian multiplier Œ± ‚â•0 so that Œ≤ = Œ±/(1 + Œ±), while
‚Üº‚àí
œà [i+1]
k
(xk) and ‚Üº‚àí
Z [i+1]
T
are the corresponding normalizing factors
‚Üº‚àí
œà [i+1]
k
(xk) =
Z h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ≤h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1)
o i1‚àíŒ≤
dxk‚àí1,
‚Üº‚àí
Z [i+1]
T
=
Z h‚Üº‚àíq [i]
T (xT)
iŒ≤h
exp
n‚Üº‚àí
V [i+1]
T
(xT)
o i1‚àíŒ≤
dxT.
(12)
The potential functions ‚Üº‚àí
V [i+1]
k
(xk), for all 0 ‚â§k < T, are computed recursively forward via
‚Üº‚àí
V [i+1]
k
(xk) =
Ô£±
Ô£≤
Ô£≥
log p0(x0)
if k = 0,
log hk(yk | xk) + 1/(1 ‚àíŒ≤) log‚Üº‚àí
œà [i+1]
k
(xk)
if 0 < k ‚â§T,
(13)
bearing in mind that ‚Üº‚àí
œà [i+1]
k
(xk) are functions of ‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1). Finally, the optimal damping Œ≤ is the mini-
mizer of the dual objective
minimize
Œ≤
‚Üº‚àí
G(Œ≤) =
Œ≤Œµ
1 ‚àíŒ≤ +
1
1 ‚àíŒ≤ log‚Üº‚àí
Z [i+1]
T
(Œ≤),
subject to 0 ‚â§Œ≤ < 1.
(14)
Proof. See Appendix C.
Remark 5 The potential functions ‚Üº‚àí
Vk(xk) can be interpreted as the log-space accumulation of the forward fil-
ter message log q(xk | y1:k), where the log-normalizers log‚Üº‚àí
œàk(xk) and log‚Üº‚àí
ZT correspond to log q(xk | y1:k‚àí1)
and log q(y1:T), respectively.
Remark 6 Under Assumption 2, the marginal smoothing distributions ‚Üº‚àíq [i+1]
k
(xk), for all 0 ‚â§k < T, are
computed via backward propagation starting from ‚Üº‚àíq [i+1]
T
(xT)
‚Üº‚àíq [i+1]
k‚àí1 (xk‚àí1) =
Z
‚Üº‚àíq [i+1]
k
(xk)‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) dxk,
where ‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) and ‚Üº‚àíq [i+1]
T
(xT) are given by Proposition 3.
4.3
Hybrid Posterior Factorization
The recursions in Proposition 2 and Proposition 3 are a direct result of the forward- and reverse-Markov
decompositions from Assumption 1 and Assumption 2. While these schemes lead to two distinct smoothing
algorithms, it is possible to combine elements of both to construct a hybrid smoothing solution that leverages
both decompositions in one algorithm.
6

Corollary 1 (Optimal hybrid marginals) For state-space models of the form 1, given the forward-
Markov conditionals ‚àí‚áÄq [i]
k (xk+1 | xk) and reverse-Markov conditionals ‚Üº‚àíq [i]
k (xk‚àí1 | xk) associated with the
same joint smoothing distribution q[i](x0:T | y1:T), we compute the marginals q[i+1]
k
(xk) associated with prob-
lem 4 by leveraging the interpretation from Remark 3 and Remark 5, we combine the recursions from Propo-
sition 2 and Proposition 3
q[i+1]
k
(xk) ‚àù
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
h
q[i]
0 (x0)
iŒ≤h
exp
n‚àí‚áÄ
V [i+1]
0
(x0)
o i1‚àíŒ≤
if k = 0,
h
q[i]
k (xk)
iŒ≤h
exp
n‚Üº‚àí
V [i+1]
k
(xk)
o ‚àí‚áÄ
œà [i+1]
k
(xk)
i1‚àíŒ≤
if 0 < k < T,
h
q[i]
T (xT)
iŒ≤h
exp
n‚Üº‚àí
V [i+1]
T
(xT)
o i1‚àíŒ≤
if k = T.
The forward‚Äìbackward recursion schemes derived in Proposition 2, Proposition 3, and Corollary 1 share a
general algorithmic structure that does not rely on specific assumptions about the dynamics fk(xk+1 | xk),
the measurement model hk(yk | xk), or the forms of the approximate posteriors ‚àí‚áÄq (x0:T) and ‚Üº‚àíq (x0:T). In
the sections that follow, we present concrete implementations of these schemes by adopting conditionally
Gaussian approximations for the forward- and reverse-Markov posterior distributions.
5
Practical Recursive Inference Algorithms
To transform the recursive schemes introduced in Section 4.1 and Section 4.2 into concrete and tractable
algorithms, we impose additional structure on the forward and backward updates. This is accomplished
by specializing the conditional posteriors defined in (6) and (10), along with the corresponding potential
functions from (8) and (13). In particular, we focus on a class of approximate smoothing algorithms where
the joint posterior is restricted to be Gaussian, and the Markov conditionals are constrained to the Gauss‚Äì
Markov family. This restriction enables recursive formulations that admit closed-form updates.
Assumption 3 (Forward Gauss‚ÄìMarkov approximation) Given
a
state-space
model
(1)
and
a
forward-Markov factorization in Assumption 1, we restrict the approximate posterior ‚àí‚áÄq (x0:T | y1:T) to the
family of forward Gauss‚ÄìMarkov densities, leading to the following parameterization
‚àí‚áÄq (x0:T | y1:T) = N(x0 | ‚àí ‚áÄ
m0,‚àí‚áÄ
P0)
T ‚àí1
Y
k=0
N(xk+1 | ‚àí‚áÄ
Fk xk +‚àí‚áÄdk,‚àí‚áÄ
Œ£k).
Assumption 4 (Reverse Gauss‚ÄìMarkov approximation) Given a state-space model (1) and a reverse-
Markov decomposition in Assumption 2, we restrict the approximate posterior ‚Üº‚àíq (x0:T | y1:T) to the family
of reverse Gauss‚ÄìMarkov densities, leading to the following parameterization
‚Üº‚àíq (x0:T | y1:T) = N(xT | ‚Üº ‚àí
mT,‚Üº‚àí
PT)
T
Y
k=1
N(xk‚àí1 | ‚Üº‚àí
Fk xk +‚Üº‚àídk,‚Üº‚àí
Œ£k).
In the following sections, we develop tractable recursive inference schemes by constructing local quadratic ap-
proximations to the potential functions and deriving update rules that preserve the Gauss‚ÄìMarkov structure
of the approximate posterior across iteration.
5.1
Statistical Function Approximations
A key step in constructing efficient smoothing algorithms is to approximate the potential functions in (8)
and (13) with tractable forms that preserve the recursive structure and admit closed-form updates. We
achieve this by introducing second-order statistical expansions of the log-density functions associated with
the latent dynamics log fk(xk+1 | xk) and the measurement model log hk(yk | xk).
7

Given the iterative nature of the proposed optimization procedure (4), it is natural to construct these
expansions locally, around the current iterate q[i](x0:T). This ensures that the approximation is tailored to
the current posterior belief and captures relevant structure in the vicinity of the current iterate. Crucially,
the KL-based trust-region constraint used in the entropic proximal updates plays an important role in
controlling the step size of each iteration. It ensures that the distribution q[i+1] does not deviate too far from
q[i], thereby keeping the updates within the region where the local expansions remain valid. This interaction
between local approximations and bounded updates stabilizes the optimization and maintains the fidelity of
the recursive inference scheme (Teboulle, 1992; Iusem et al., 1994; Chr√©tien & Hero, 2002).
Definition 1 (Statistical second-order expansion) Let z
‚àº
N(m, P) and let g(z) be a twice-
differentiable scalar function. A second-order statistical expansion of g(z) with respect to the random variable
z takes a quadratic form g(z) ‚âà‚àí1
2 z‚ä§U z + z‚ä§u + Œ∑.
Definition 1 specifies the structure but not the computation of the expansion parameters. We return to this
in later sections, where we describe two approximation strategies for computing (U, u, Œ∑).
Assumption 5 (Quadratic expansion of log-densities) Let (xk+1, xk) ‚àºq[i]
k (xk+1, xk), we assume the
statistical expansion of ‚Ñì[i]
f (xk+1, xk) ‚âàlog fk(xk+1 | xk), for all 0 ‚â§k < T, is parameterized by
‚Ñì[i]
f (xk+1, xk) = ‚àí1
2
h
x‚ä§
k+1
x‚ä§
k
i
Ô£Æ
Ô£∞C[i]
¬Øx¬Øx,k
‚àíC[i]
¬Øxx,k
‚àíC[i]
x¬Øx,k
C[i]
xx,k
Ô£π
Ô£ª
Ô£Æ
Ô£∞xk+1
xk
Ô£π
Ô£ª+
h
x‚ä§
k+1
x‚ä§
k
i
Ô£Æ
Ô£∞c[i]
¬Øx,k
c[i]
x,k
Ô£π
Ô£ª+ Œ∫[i]
k ,
and the statistical expansion of ‚Ñì[i]
h (xk) ‚âàlog hk(yk | xk), for all 0 < k ‚â§T, is given as
‚Ñì[i]
h (xk) = ‚àí1
2x‚ä§
k L[i]
k xk + x‚ä§
k l[i]
k + ŒΩ[i]
k .
Finally, for x0 ‚àºq[i]
0 (x0), we define the statistical expansion of ‚Ñì[i]
p (x0) ‚âàlog p0(x0) as
‚Ñì[i]
p (x0) = ‚àí1
2x‚ä§
0 L[i]
0 x0 + x‚ä§
0 l[i]
0 + ŒΩ[i]
0 .
5.2
Recursive Quadratic Potentials
In this section, we derive tractable schemes for computing the potential functions defined in (8) and (13),
leveraging the statistical approximations introduced in Assumption 5. We show that these approximations
lead to tractable recursions over quadratic forms of ‚àí‚áÄ
Vk(xk) and ‚Üº‚àí
Vk(xk) from Proposition 2 and Proposition 3.
The use of quadratic potentials is consistent with the interpretation in Remark 3 and Remark 5, where these
functions are interpreted as log-space forward and backward filtering messages. Under the Gaussian approx-
imation, such messages are naturally parameterized by quadratic functions, which justifies and supports the
form adopted in our smoothing framework.
Before introducing these recursive itself, let us first start by defining the following parametric forms for the
potential functions and associated log-normalizing functions
V [i+1]
k
(xk) = ‚àí1
2x‚ä§
k R[i+1]
k
xk + x‚ä§
k r[i+1]
k
+ œÅ[i+1]
k
,
(15)
log œà[i+1]
k
(xk) = ‚àí1
2x‚ä§
k S[i+1]
k
xk + x‚ä§
k s[i+1]
k
+ Œæ[i+1]
k
.
(16)
These quadratic forms enable efficient message-passing updates while preserving the Gaussian structure of
the approximate posterior.
Proposition 4 (Recursive forward Gauss‚ÄìMarkov potentials) Let ‚àí‚áÄq [i]
k (xk+1 | xk)
be
a
forward
Gauss‚ÄìMarkov conditional as defined in Assumption 3, and let ‚Ñì[i]
f (xk+1, xk), ‚Ñì[i]
h (xk), and ‚Ñì[i]
p (x0) be the
8

second-order approximations of the log-probabilities from Assumption 5, then the potentials ‚àí‚áÄ
V [i+1]
k
(xk) in (8)
are quadratic functions of the form (15), computed recursively backwards starting from k = T as:
‚àí‚áÄ
R [i+1]
T
= L[i]
T ,
‚àí‚áÄr [i+1]
T
= l[i]
T .
For 0 ‚â§k < T, the updates follow:
‚àí‚áÄ
R [i+1]
k
= L[i]
k +
1
1 ‚àíŒ≤
‚àí‚áÄ
S [i+1]
k
,
‚àí‚áÄr [i+1]
k
= l[i]
k +
1
1 ‚àíŒ≤
‚àí‚áÄs [i+1]
k
,
where the log-normalizing function log‚àí‚áÄ
œà [i+1]
k
(xk) has quadratic form (16) with parameters:
‚àí‚áÄ
S [i+1]
k
= ‚àí‚áÄ
G [i+1]
xx,k ‚àí
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄ
G [i+1]
¬Øxx,k ,
‚àí‚áÄs [i+1]
k
= ‚àí‚áÄg [i+1]
x,k
+
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
¬Øx,k
,
with intermediate quantities defined as:
‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k := (1 ‚àíŒ≤)
h
C[i]
¬Øx¬Øx,k +‚àí‚áÄ
R [i+1]
k+1
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1
,
‚àí‚áÄ
G [i+1]
xx,k := (1 ‚àíŒ≤) C[i]
xx,k + Œ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k ,
‚àí‚áÄ
G [i+1]
¬Øxx,k := (1 ‚àíŒ≤) C[i]
¬Øxx,k + Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k ,
‚àí‚áÄg [i+1]
¬Øx,k
:= (1 ‚àíŒ≤)
h
c[i]
¬Øx,k +‚àí‚áÄr [i+1]
k
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k ,
‚àí‚áÄg [i+1]
x,k
:= (1 ‚àíŒ≤) c[i]
x,k ‚àíŒ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k .
Finally, given a quadratic potential function ‚àí‚áÄ
V [i+1]
0
(x0) of the form (15) and Gaussian posterior density
‚àí‚áÄq [i]
0 (x0) = N(‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ), the log-normalizing constant log‚àí‚áÄ
Z [i+1]
0
is computed according to (7)
log‚àí‚áÄ
Z [i+1]
0
= ‚àí1
2
h‚àí ‚áÄ
m [i]
0
i‚ä§‚àí‚áÄ
U [i+1]‚àí ‚áÄ
m [i]
0
+
h‚àí ‚áÄ
m [i]
0
i‚ä§‚àí‚áÄu [i+1] +‚àí‚áÄŒ∑ [i+1],
(17)
where
‚àí‚áÄ
U [i+1] = ‚àí‚áÄ
J [i+1]
mm
‚àí
h‚àí‚áÄ
J [i+1]
xm
i‚ä§h‚àí‚áÄ
J [i+1]
xx
i‚àí1‚àí‚áÄ
J [i+1]
xm
,
‚àí‚áÄu [i+1] = ‚àí
h‚àí‚áÄ
J [i+1]
xm
i‚ä§h‚àí‚áÄ
J [i+1]
xx
i‚àí1‚àí‚áÄj [i+1]
x
,
and we have defined
‚àí‚áÄ
J [i+1]
xx
:= (1 ‚àíŒ≤)‚àí‚áÄ
R [i+1]
0
+ Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄ
J [i+1]
xm
:= Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄ
J [i+1]
mm
:= Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄj [i+1]
x
:= (1 ‚àíŒ≤)‚àí‚áÄr [i+1]
0
.
Proof. See Appendix D.
Proposition 5 (Recursive reverse Gauss‚ÄìMarkov potentials) Let
‚Üº‚àíq [i]
k (xk‚àí1 | xk)
be
a
reverse
Gauss‚ÄìMarkov conditional as defined in Assumption 4) and let ‚Ñì[i]
f (xk, xk‚àí1), ‚Ñì[i]
h (xk), and ‚Ñì[i]
p (x0) be
the second-order approximations of the log-probabilities in Assumption 5, then the potentials ‚Üº‚àí
V [i+1]
k
(xk)
from (13) are quadratic functions of the form (15), computed recursively forwards starting from k = 0 with
‚Üº‚àí
R [i+1]
0
= L[i]
0 ,
‚Üº‚àír [i+1]
0
= l[i]
0 ,
9

For 0 < k ‚â§T, the updates follow:
‚Üº‚àí
R [i+1]
k
= L[i]
k +
1
1 ‚àíŒ≤
‚Üº‚àí
S [i+1]
k
,
‚Üº‚àír [i+1]
k
= l[i]
k +
1
1 ‚àíŒ≤
‚Üº‚àís [i+1]
k
,
where the log-normalizing function log‚Üº‚àí
œà [i+1]
k
(xk) has quadratic form (16) with parameters:
‚Üº‚àí
S [i+1]
k
= ‚Üº‚àí
G [i+1]
¬Øx¬Øx,k ‚àí
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àí
G [i+1]
x¬Øx,k ,
‚Üº‚àís [i+1]
k
= ‚Üº‚àíg [i+1]
¬Øx,k
+
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àíg [i+1]
x,k
,
with intermediate quantities defined as:
‚Üº‚àí
G [i+1]
¬Øx¬Øx,k := (1 ‚àíŒ≤) C[i]
¬Øx¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k ,
‚Üº‚àí
G [i+1]
xx,k := (1 ‚àíŒ≤)
h
C[i]
xx,k‚àí1 +‚Üº‚àí
R [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1
,
‚Üº‚àí
G [i+1]
x¬Øx,k := (1 ‚àíŒ≤) C[i]
x¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k ,
‚Üº‚àíg [i+1]
¬Øx,k
:= (1 ‚àíŒ≤) c[i]
¬Øx,k‚àí1 ‚àíŒ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k ,
‚Üº‚àíg [i+1]
x,k
:= (1 ‚àíŒ≤)
h
c[i]
x,k‚àí1 +‚Üº‚àír [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k .
Finally, given a quadratic potential function ‚Üº‚àí
V [i+1]
T
(xT) of the form (15)and Gaussian posterior density
‚Üº‚àíq [i]
T (xT) = N(‚Üº ‚àí
m [i]
T ,‚Üº‚àí
P [i]
T ), the log-normalizing constant log‚Üº‚àí
Z [i+1]
T
is computed according to (12)
log‚Üº‚àí
Z [i+1]
T
= ‚àí1
2
h‚Üº ‚àí
m [i]
T
i‚ä§‚Üº‚àí
U [i+1]‚Üº ‚àí
m [i]
T
+
h‚Üº ‚àí
m [i]
T
i‚ä§‚Üº‚àíu [i+1] +‚Üº‚àíŒ∑ [i+1],
(18)
where
‚Üº‚àí
U [i+1] = ‚Üº‚àí
J [i+1]
xx
‚àí
h‚Üº‚àí
J [i+1]
mx
i‚ä§h‚Üº‚àí
J [i+1]
mm
i‚àí1‚Üº‚àí
J [i+1]
mx
,
‚Üº‚àíu [i+1] = ‚àí
h‚Üº‚àí
J [i+1]
mx
i‚ä§h‚Üº‚àí
J [i+1]
mm
i‚àí1‚Üº‚àíj [i+1]
m
,
and we have defined
‚Üº‚àí
J [i+1]
xx
:= Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
‚Üº‚àí
J [i+1]
mx
:= Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
‚Üº‚àí
J [i+1]
mm
:= (1 ‚àíŒ≤)‚Üº‚àí
R [i+1]
T
+ Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
‚Üº‚àíj [i+1]
m
:= (1 ‚àíŒ≤)‚Üº‚àír [i+1]
T
.
Proof. See Appendix E.
The results in Proposition 4 and Proposition 5 are derived using general second-order statistical expansions
of the log-transition and log-likelihood functions. We now turn to the practical question of how to construct
these approximations and present two distinct strategies for realizing them.
5.2.1
Generalized Statistical Linear Regression
Our first approach to realizing the second-order statistical expansions is based on generalized statistical lin-
ear regression (GSLR, Garc√≠a-Fern√°ndez et al., 2015; Tronarp et al., 2018). These enabling approximations
(S√§rkk√§ & Svensson, 2023) produce affine-Gaussian surrogates of the dynamics fk(xk+1 | xk) and the mea-
surement model hk(yk | xk), yielding functional forms that directly satisfy the required quadratic structure
for potentials and log-normalizing functions from Assumption 5.
10

Definition 2 (Generalized statistical linear regression) Let x and y be two random variables and sup-
posed that the first and second moments E [x], V [x], E [y], V [y], and C [y, x] are known. Then, statistical
linear regression approximates p(y | x) by an affine-Gaussian model y ‚âàA x + b + œâ, œâ ‚àºN(0, ‚Ñ¶), with
A = C [y, x] V [x]‚àí1 ,
b = E [y] ‚àíA E [x] ,
‚Ñ¶= V [y] ‚àíA V [x] A‚ä§,
where the parameters {A, b} minimize the mean squared error objective MSE(A, b) = E

(y ‚àíA x ‚àíb)‚ä§(y ‚àí
A x ‚àíb)

and the covariance matrix satisfies ‚Ñ¶= V [y ‚àíA x ‚àíb].
When only the conditional moments
E

y | x

and V

y | x

are available, generalized statistical linear regression estimates marginal moments via
the law of total expectation, so that:
E [y] = E
h
E

y | x
i
,
V [y] = E
h
V

y | x
i
+ V
h
E

y | x
i
,
C [y, x] = C
h
E

y | x

, x
i
,
where the outer expectations are evaluated with respect to a Gaussian marginal distribution p(x) using nu-
merical quadrature integration rules (S√§rkk√§ & Svensson, 2023).
Based on the recipe from Definition 2, and given the conditional moments E

xk+1 | xk

, V

xk+1 | xk

,
E

yk | xk

, and V

yk | xk

, we can replace the conditional densities fk(xk+1 | xk) and hk(yk | xk), at each
iteration [i + 1], by their affine-Gaussian approximations
xk+1 ‚âàA[i]
k xk + b[i]
k + œâ[i]
k ,
œâ[i]
k ‚àºN(0, ‚Ñ¶[i]
k ),
0 ‚â§k < T,
yk ‚âàH[i]
k xk + e[i]
k + Œ¥[i]
k ,
Œ¥[i]
k ‚àºN(0, ‚àÜ[i]
k ),
0 < k ‚â§T,
(19)
which corresponds to generalized statistical linear regression with respect to the Gaussian marginal distri-
butions q[i]
k (xk) from iteration [i]
A[i]
k
=
C[i] [xk+1, xk] V[i] [xk]‚àí1 ,
b[i]
k
=
E[i] [xk+1] ‚àíA[i]
k E[i] [xk] ,
‚Ñ¶[i]
k
=
V[i] [xk+1] ‚àíA[i]
k V[i] [xk]
h
A[i]
k
i‚ä§
,
H[i]
k
=
C[i] [yk, xk] V[i] [xk]‚àí1 ,
e[i]
k
=
E[i] [yk] ‚àíH[i]
k E[i] [xk] ,
‚àÜ[i]
k
=
V[i] [yk] ‚àíH[i]
k V[i] [xk]
h
H[i]
k
i‚ä§
.
Finally, the prior density p0(x0) is likewise approximated by a Gaussian density by matching the first and
second moments
p0(x0) ‚âàN(¬µ0, Œõ0),
with
¬µ0 = E [x0] ,
Œõ0 = V [x0] .
(20)
Given the GSLR approximations (19) and (20), we obtain the quadratic log-density expansions required
by Assumption 5, enabling closed-form computation in Proposition 4 and Proposition 5. Specifically, the
resulting coefficients for ‚Ñì[i]
f (xk+1, xk) are:
C[i]
¬Øx¬Øx,k =
h
‚Ñ¶[i]
k
i‚àí1
,
C[i]
¬Øxx,k
=
h
‚Ñ¶[i]
k
i‚àí1
A[i]
k ,
c[i]
¬Øx,k
=
h
‚Ñ¶[i]
k
i‚àí1
b[i]
k ,
C[i]
xx,k =
h
A[i]
k
i‚ä§h
‚Ñ¶[i]
k
i‚àí1
A[i]
k ,
C[i]
x¬Øx,k
=
h
A[i]
k
i‚ä§h
‚Ñ¶[i]
k
i‚àí1
,
c[i]
x,k
= ‚àí
h
A[i]
k
i‚ä§h
‚Ñ¶[i]
k
i‚àí1
bk.
for all 0 ‚â§k < T, while the coefficients of ‚Ñì[i]
h (xk), for all 0 < k ‚â§T, are:
L[i]
k =
h
H[i]
k
i‚ä§h
‚àÜ[i]
k
i‚àí1
H[i]
k ,
l[i]
k =
h
H[i]
k
i‚ä§h
‚àÜ[i]
k
i‚àí1 h
yk ‚àíe[i]
k
i
,
and, finally, the coefficients associated with ‚Ñì[i]
p (x0) are:
L[i]
0 = Œõ‚àí1
0 ,
l[i]
0 = Œõ‚àí1
0 ¬µ0.
11

5.2.2
Fourier‚ÄìHermite Series Expansion
The statistical approximation provided by statistical linear regression comes with two main limitations:
it imposes an explicit additive Gaussian noise model on the approximated dynamics and measurements,
and it inherently neglects second-order information. Alternatively, we can retrieve second-order statistical
approximations by using Fourier‚ÄìHermite series (Sarmavuori & S√§rkk√§, 2011; Hassan & S√§rkk√§, 2023), which
leverage Hermite polynomial bases in a Hilbert space H (Malliavin, 2015) to capture higher-order effects.
Definition 3 Let g ‚ààH be a scalar-valued function and let s ‚àºN(0, I) be a standard Gaussian random
variable. A second-order Fourier‚ÄìHermite expansion of g(s) is given by
g(s) ‚âàE

g(s)

+ E

g(s) H1(s)
‚ä§H1(s) + 1
2 tr
n
E

g(s) H2(s)

H2(s)
o
,
where H1(s) and H2(s) are first- and second-order Hermite polynomials defined as
H1(s) = s,
H2(s) = ss‚ä§‚àíI.
This expansion generalizes to any Gaussian N(m, P) by letting z = Rs + m and P = RR‚ä§, so that:
g(z) ‚âàE

g(z)

+ E
h
g(z) H1(R‚àí1(z ‚àím))
i‚ä§
H1(R‚àí1(z ‚àím))
+ 1
2 tr

E
h
g(z) H2(R‚àí1(z ‚àím))
i h
R‚àí1(z ‚àím)(z ‚àím)‚ä§R‚àí‚ä§‚àíI
i
= ‚àí1
2z‚ä§Uz + z‚ä§u + Œ∑.
The coefficients of this quadratic form are:
U := ‚àíE

Gzz(z)

,
u := E

Gz(z)

‚àíE

Gzz(z)

m,
Œ∑ := E

g(z)

‚àíE

Gz(z)

m + 1
2m‚ä§E

Gzz(z)

m ‚àí1
2 tr
n
R‚ä§E

Gzz(z)

R
o
.
where Gz(z) and Gzz(z) are the Jacobian and Hessian of g(z), respectively, which, for z ‚àºN(m, P), satisfy
the following identities via integration by parts (Hassan & S√§rkk√§, 2023)
E
h
g(z) H1(R‚àí1(z ‚àím))
i
= R‚ä§E

Gz(z)

,
E
h
g(z) H2(R‚àí1(z ‚àím))
i
= R‚ä§E

Gzz(z)

R.
The expectations involved in this approximation can be efficiently evaluated using numerical quadrature in-
tegration rules (S√§rkk√§ & Svensson, 2023).
Given a set of marginal distributions q[i]
k (xk), we can apply the expansion in Definition 3 to the log-transition
and log-measurement functions to obtain second-order approximations consistent with Assumption 5. This
approach provides an alternative approximation to GLSR, which does not impose an explicit additive Gaus-
sian noise assumption and inherently incorporates second-order information of the state-space model, offering
potentially higher fidelity in the approximation.
5.3
Gauss‚ÄìMarkov Posterior Updates
Having established the quadratic approximations that enable recursive computation of the potential func-
tions, we now turn our attention to the computation of the tilted distributions introduced in Proposition 2
and Proposition 3. As specified in Assumption 3 and Assumption 4, our goal is to maintain the Gauss‚ÄìMarkov
structure of the forward and reverse variational posteriors throughout the iterative optimization process. To
ensure that the updated variational distributions at iteration [i+1] remain within the Gauss‚ÄìMarkov family,
we must project the corresponding tilted distributions onto a (conditional) Gaussian form.
We begin by deriving a general moment-matching rule for tilted marginal distributions of the form in equa-
tions (5) and (11). These results then directly yield closed-form updates for the parameters ‚àí ‚áÄ
m [i+1]
0
, ‚àí‚áÄ
P [i+1]
0
,
‚Üº ‚àí
m [i+1]
T
, and ‚Üº‚àí
P [i+1]
T
, preserving the tractability and structure of the overall smoothing algorithm.
12

Lemma 1 (Tilted Gaussian moment matching) Let q[i+1](x) be a tilted distribution of the form
q[i+1](x) =
h
Z[i+1]i‚àí1 h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
,
with q[i](x) = N(x | m[i], P [i]) and a normalizing constant
Z[i+1] =
Z h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx,
then the first and second moments of q[i+1](x) are given by
E[i+1] [x] = m[i] + 1
Œ≤ P [i] ‚àÇlog Z[i+1]
‚àÇm[i]
,
V[i+1] [x] = 1
Œ≤ P [i] + 1
Œ≤2 P [i] ‚àÇ2 log Z[i+1]
‚àÇm[i] ‚àÇ

m[i]‚ä§P [i].
Proof. See Appendix F.
Corollary 2 (Forward Gauss‚ÄìMarkov boundary) Let ‚àí‚áÄq [i]
0 (x0)
be
a
Gaussian
marginal
and
let
log‚àí‚áÄ
Z [i+1]
0
be the log-normalizing constant of the tilted distribution in (17). Then, applying Lemma 1, the
optimal Gaussian approximation to the updated tilted marginal ‚àí‚áÄq [i+1]
0
(x0) from (5) is given by:
‚àí‚áÄ
P [i+1]
0
=
h‚àí‚áÄ
J [i+1]
xx
i‚àí1
,
‚àí ‚áÄ
m [i+1]
0
=
h‚àí‚áÄ
J [i+1]
xx
i‚àí1 h‚àí‚áÄj [i+1]
x
+‚àí‚áÄ
J [i+1]
xm
‚àí ‚áÄ
m [i]
0
i
.
By substituting in the explicit expressions, the update simplifies to:
‚àí‚áÄ
P [i+1]
0
=
h
(1 ‚àíŒ≤)‚àí‚áÄ
R [i+1]
0
+ Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1 i‚àí1
,
‚àí ‚áÄ
m [i+1]
0
= ‚àí‚áÄ
P [i+1]
0
h
(1 ‚àíŒ≤)‚àí‚áÄr [i+1]
0
+ Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1‚àí ‚áÄ
m [i]
0
i
.
Corollary 3 (Reverse Gauss‚ÄìMarkov boundary) Let ‚Üº‚àíq [i]
T (xT)
be
a
Gaussian
marginal
and
let
log‚Üº‚àí
Z [i+1]
T
be the log-normalizing constant of the tilted distribution in (18). Then, applying Lemma 1, the
optimal Gaussian approximation to the updated tilted marginal ‚Üº‚àíq [i+1]
T
(xT) from (11) is given by:
‚Üº‚àí
P [i+1]
T
=
h‚Üº‚àí
J [i+1]
mm
i‚àí1
,
‚Üº ‚àí
m [i+1]
T
=
h‚Üº‚àí
J [i+1]
mm
i‚àí1 h‚Üº‚àíj [i+1]
m
+‚Üº‚àí
J [i+1]
mx
‚Üº ‚àí
m [i]
T
i
.
By substituting in the explicit expressions, the update simplifies to:
‚Üº‚àí
P [i+1]
T
=
h
(1 ‚àíŒ≤)‚Üº‚àí
R [i+1]
T
+ Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1 i‚àí1
,
‚Üº ‚àí
m [i+1]
T
= ‚Üº‚àí
P [i+1]
T
h
(1 ‚àíŒ≤)‚Üº‚àír [i+1]
T
+ Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1‚Üº ‚àí
m [i]
T
i
.
Projecting the tilted conditionals in (6) and (10) onto the affine-Gaussian parametric family defined in
Assumption 3 and Assumption 4 is more involved, as it requires deriving the conditional moments of the
forward and reverse distributions E

xk+1 | xk

, V

xk+1 | xk

, E

xk‚àí1 | xk

, and V

xk‚àí1 | xk

. In what
follows, we derive closed-form expressions for these moments and use them to update the parameters of the
forward and reverse Gauss‚ÄìMarkov posteriors.
Lemma 2 (Forward Gauss‚ÄìMarkov conditionals) Let ‚àí‚áÄq [i]
k (xk+1 | xk) be a forward Gauss‚ÄìMarkov
conditional as defined in Assumption 3.
Given a second-order expansion of ‚Ñì[i]
f (xk+1, xk) following As-
sumption 5, then, the conditional mean and covariance of the updated tilted conditional ‚àí‚áÄq [i+1]
k
(xk+1 | xk)
13

are given by:
E[i+1] 
xk+1 | xk

=
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àí‚áÄ
G [i+1]
xx,k xk ‚àí‚àí‚áÄg [i+1]
x,k
+ ‚àÇlog‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk

,
V[i+1] 
xk+1 | xk

=
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àí‚áÄ
G [i+1]
xx,k + ‚àÇ2 log‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
 h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
.
When log‚àí‚áÄ
œà [i+1]
k
(xk) is approximated quadratically as in Proposition 4, then parameters of the affine-
Gaussian conditional ‚àí‚áÄq [i+1]
k
(xk+1 | xk) become:
‚àí‚áÄ
Œ£ [i+1]
k
=
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1
,
‚àí‚áÄ
F [i+1]
k
=
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄ
G [i+1]
¬Øxx,k ,
‚àí‚áÄd [i+1]
k
=
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
¬Øx,k
,
and can be further simplified to:
‚àí‚áÄ
Œ£ [i+1]
k
=
h
(1 ‚àíŒ≤)
h
C[i]
¬Øx¬Øx,k +‚àí‚áÄ
R [i+1]
k+1
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1 i‚àí1
,
‚àí‚áÄ
F [i+1]
k
= ‚àí‚áÄ
Œ£ [i+1]
k
h
(1 ‚àíŒ≤) C[i]
¬Øxx,k + Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k
i
,
‚àí‚áÄd [i+1]
k
= ‚àí‚áÄ
Œ£ [i+1]
k
h
(1 ‚àíŒ≤)

c[i]
¬Øx,k +‚àí‚áÄr [i+1]
k

+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k
i
.
Proof. See Appendix G.
Lemma 3 (Reverse Gauss‚ÄìMarkov conditionals) Let‚Üº‚àíq [i]
k (xk‚àí1 | xk) be a forward Gauss‚ÄìMarkov con-
ditional as defined in Assumption 4. Given a second-order expansion of ‚Ñì[i]
f (xk, xk‚àí1) following Assumption 5,
then, the conditional mean and covariance of the updated tilted conditional ‚Üº‚àíq [i]
k (xk‚àí1 | xk) are given by:
E[i+1] 
xk‚àí1 | xk

=
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚Üº‚àí
G [i+1]
¬Øx¬Øx,k xk ‚àí‚Üº‚àíg [i+1]
¬Øx,k
+ ‚àÇlog‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk

,
V[i+1] 
xk‚àí1 | xk

=
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚Üº‚àí
G [i+1]
¬Øx¬Øx,k + ‚àÇ2 log‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
 h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
.
When log‚Üº‚àí
œà [i+1]
k
(xk) is approximated quadratically as in Proposition 5, then the parameters of the affine-
Gaussian conditional ‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) become:
‚Üº‚àí
Œ£ [i+1]
k
=
h‚Üº‚àí
G [i+1]
xx,k
i‚àí1
,
‚Üº‚àí
F [i+1]
k
=
h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àí
G [i+1]
x¬Øx,k ,
‚Üº‚àíd [i+1]
k
=
h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àíg [i+1]
x,k
,
and can be further simplified to:
‚Üº‚àí
Œ£ [i+1]
k
=
h
(1 ‚àíŒ≤)
h
C[i]
xx,k‚àí1 +‚Üº‚àí
R [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1 i‚àí1
,
‚Üº‚àí
F [i+1]
k
= ‚Üº‚àí
Œ£ [i+1]
k
h
(1 ‚àíŒ≤) C[i]
x¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k
i
,
‚Üº‚àíd [i+1]
k
= ‚Üº‚àí
Œ£ [i+1]
k
h
(1 ‚àíŒ≤)

c[i]
x,k‚àí1 +‚Üº‚àír [i+1]
k‚àí1

+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k
i
.
Proof. See Appendix H.
5.4
Recursive Gaussian Marginals
Given the Gauss‚ÄìMarkov parameterization of the posterior in Assumption 3 and Assumption 4 and the up-
date rules introduced in Section 5.3, the smoothing posterior marginals at each iteration [i] can be computed
14

efficiently. Specifically, we use a forward recursion to evaluate the marginals of the forward Gauss‚ÄìMarkov
posterior as described in Remark 4 and a backward recursion for the reverse Gauss‚ÄìMarkov posterior from
Remark 6. These marginals play a central role in our proposed iterative smoothing framework, as they are
used to perform second-order statistical expansions of the log-densities associated with the dynamics and
measurement model, as discussed in Section 5.2.
Corollary 4 (Forward Gauss‚ÄìMarkov marginals) Given the updated forward Gauss‚ÄìMarkov posterior
from Corollary 2 and Lemma 2, the forward marginal smoothing distributions ‚àí‚áÄq [i+1]
k
(xk), for all 0 < k ‚â§T,
are computed in closed form via the forward recursion:
‚àí ‚áÄ
m [i+1]
k+1
= ‚àí‚áÄ
F [i+1]
k
‚àí ‚áÄ
m [i+1]
k
+‚àí‚áÄd [i+1]
k
,
‚àí‚áÄ
P [i+1]
k+1
= ‚àí‚áÄ
F [i+1]
k
‚àí‚áÄ
P [i+1]
k
h‚àí‚áÄ
F [i+1]
k
i‚ä§
+‚àí‚áÄ
Œ£ [i+1]
k
,
with the initial condition ‚àí‚áÄq [i+1]
0
= N(‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
).
Corollary 5 (Reverse Gauss‚ÄìMarkov marginals) Given the updated reverse Gauss‚ÄìMarkov posterior
from Corollary 3 and Lemma 3, the reverse marginal smoothing distributions ‚Üº‚àíq [i+1]
k
(xk), for all 0 ‚â§k < T,
are computed in closed form via the backward recursion:
‚Üº ‚àí
m [i+1]
k‚àí1
= ‚Üº‚àí
F [i+1]
k
‚Üº ‚àí
m [i+1]
k
+‚Üº‚àíd [i+1]
k
,
‚Üº‚àí
P [i+1]
k‚àí1
= ‚Üº‚àí
F [i+1]
k
‚Üº‚àí
P [i+1]
k
h‚Üº‚àí
F [i+1]
k
i‚ä§
+‚Üº‚àí
Œ£ [i+1]
k
,
with the terminal condition ‚Üº‚àíq [i+1]
T
= N(‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
).
Corollary 6 (Hybrid marginals) Given the recursion from Proposition 4 and Proposition 5, the marginal
smoothing distributions q[i+1]
k
(xk), for all 0 < k < T, are computed according to Corollary 1 in closed form
by completing the squares:
P [i+1]
k
=
h
(1 ‚àíŒ≤)
h‚Üº‚àí
R [i+1]
k
+‚àí‚áÄ
S [i+1]
k
i
+ Œ≤
h
P [i]
k
i‚àí1i‚àí1
,
m[i+1]
k
= P [i+1]
k
h
(1 ‚àíŒ≤)
‚Üº‚àír [i+1]
k
+‚àí‚áÄs [i+1]
k

+ Œ≤
h
P [i]
k
i‚àí1
m[i]
k
i
,
where q[i+1]
0
(x0) = N(‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
) and q[i+1]
T
(xT) = N(‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
).
5.5
Optimal Damping Parameter
We now address the selection of the damping parameter Œ≤, which plays a critical role in the entropic proximal
update.
As described in (4), Œ≤ arises from the Kullback‚ÄìLeibler constraint and varies across iterations.
Propositions 2 and 3 show that the optimal value of Œ≤ at each iteration is obtained by minimizing the
corresponding dual objective associated with the forward or reverse factorization.
The dual problems defined in (9) and (14) are nonlinear in Œ≤ and can be approached using standard numerical
optimization techniques (Nocedal & Wright, 2006). However, in practice, we found that off-the-shelf solvers
may struggle with feasibility issues in highly nonlinear regimes. These difficulties typically stem from viola-
tions of the convexity assumptions implicitly required for the quadratic potentials defined in Propositions 4
and 5, and can lead to instability unless positive definiteness is carefully handled.
To circumvent these challenges, we adopt a simple yet robust alternative. We apply a bisection method to the
Lagrange multiplier Œ±, which implicitly determines the damping parameter via the relation Œ≤ = Œ±/(1 + Œ±).
This approach searches for the root of the gradient of the Lagrangian R(Œ±), ensuring that the KL constraint
is satisfied at each iteration. The root-finding condition is given by:
‚àÇR(Œ±‚àó)
‚àÇŒ±‚àó
= Œµ ‚àíDKL
h
q[i+1](x0:T; Œ±‚àó) || q[i](x0:T)
i
= 0,
(21)
15

where R(¬∑) represents the Lagrangian associated with Problem (4)
R(Œ±) = Eq[i+1]
Œ±
h
log p(x0:T, y1:T) ‚àílog q[i+1](x0:T; Œ±)
i
+ Œ±
h
Œµ ‚àíDKL
h
q[i+1](x0:T; Œ±) || q[i](x0:T)
i i
+ const.
Here, q[i+1](x0:T; Œ±) denotes the updated variational posterior that depends implicitly on Œ≤, and thus on Œ±.
Detailed derivations of the forward and reverse Lagrangian formulations are provided in Appendix B and
Appendix C, respectively.
An outline of the bisection procedure for determining the optimal damping parameter is presented in Algo-
rithm 3 and Algorithm 6, corresponding to the forward and reverse Gauss‚ÄìMarkov smoothers. Importantly,
the root condition (21) guarantees that the optimal solution q[i+1](x0:T) lies on the boundary of the KL-
ball centered at q[i](x0:T) with radius Œµ. This induces an automatic damping adaptation mechanism that
dynamically enforces the trust-region constraint across iterations.
5.6
Proximal Variational Bayesian Smoothing Algorithms
The preceding sections provide all the necessary components to instantiate the general results of Proposi-
tion 2, Proposition 3, and Corollary 1 for the case of Gaussian variational posterior approximations. In this
section, we present an overview of the resulting three iterative Bayesian smoothing algorithms.
The Forward Proximal Variational Smoother (FPVS), described in Algorithm 7, proceeds by iteratively
refining the posterior estimate in key steps. First, a backward recursion is used to compute the potential
functions and the forward conditional smoothing distributions, as detailed in Algorithm 1. This is followed
by a forward recursion that infers the smoothed marginal distributions, as outlined in Algorithm 2. At each
iteration, the damping parameter is updated via the procedure described in Algorithm 3, which ensures
satisfaction of the proximal constraint imposed by the entropic regularization.
The Reverse Proximal Variational Smoother (RPVS), presented in Algorithm 8, follows a similar structure
but inverts the order of recursion. Specifically, a forward recursion computes the potential functions and the
reverse conditional smoothing distributions, as given in Algorithm 4. This is followed by a backward recursion
to compute the smoothed marginal distributions, according to Algorithm 5. As in the forward smoother,
the damping parameter is adaptively chosen at each iteration using the method outlined in Algorithm 6.
The Hybrid Proximal Variational Smoother (HPVS), shown in Algorithm 10, combines elements of both
the forward and reverse approaches to jointly leverage their structural advantages.
In this scheme, the
potential functions and log-normalizing terms are computed using both the backward recursion from the
forward smoother in Algorithm 1 and the forward recursion from the reverse smoother in Algorithm 4. The
resulting forward and reverse representations are then fused to compute the smoothed marginal distributions
in parallel, as described in Corollary 1. The damping parameter can be adapted following either the forward
or reverse update strategy, depending on implementation preferences.
All of the proposed methods share several favorable properties. First, the recursions are formulated in log-
space, which improves numerical stability and mitigates issues related to underflow in long time horizons.
Second, each recursion is inherently damped through a trust-region constraint, ensuring that updates remain
well-behaved on the statistical manifold and avoiding erratic jumps in the posterior estimates. Finally, all
algorithms exhibit linear time complexity with respect to the temporal dimension, making them scalable
and practical for long sequences.
6
Connection to Existing Bayesian Inference Algorithms
This section provides an overview of recent research that helps situate our contribution. We highlight related
research in signal processing, Markovian Gaussian processes, and approximate Bayesian inference.
6.1
Forward-Backward Smoothing Algorithms
For the forward Gauss‚ÄìMarkov decomposition introduced in Proposition 2 and Proposition 4, a direct concep-
tual connection can be made to the classical smoother proposed by Cox (1964). In his work, Cox formulates
16

Bayesian smoothing in state-space models with additive Gaussian noise as a maximum a posteriori (MAP)
optimization problem. Subject to certain non-singularity conditions, he derives a dynamic programming
solution that propagates adjoint state functions backward in time, which aligns closely with the backward
recursion over potential functions used in our framework. This method was later extended to nonlinear sys-
tems via iterative linearization techniques (Mortensen, 1968). Our approach generalizes Cox‚Äôs formulation
by moving beyond MAP estimation to target a full Gaussian approximation of the smoothing posterior. In
doing so, we accommodate a broader class of nonlinear, non-Gaussian state-space models. Moreover, by in-
corporating entropic regularization, our method introduces a principled mechanism for trust-region control,
ensuring stable and well-posed updates over the space of variational densities.
In contrast, the reverse Gauss‚ÄìMarkov decomposition described in Proposition 3 and Proposition 5 yields
a recursion that closely parallels the structure of the Rauch‚ÄìTung‚ÄìStriebel (RTS) smoother (Rauch et al.,
1965). The RTS smoother operates by first performing a forward filtering pass to accumulate measurement
information, followed by a backward recursion that propagates smoothed marginals through the implicit
computation of reverse posterior conditionals. Our iterated reverse Gauss‚ÄìMarkov smoother generalizes this
two-pass structure to a significantly broader class of models. It lifts the standard assumptions of linear
dynamics and Gaussian noise, enabling application to nonlinear, non-Gaussian systems. Additionally, by
operating directly in the log-domain of the filtered densities and incorporating entropic proximal regular-
ization, our method enhances numerical stability and provides a principled means of controlling the update
step size throughout the recursion.
Finally, the hybrid posterior decomposition introduced in Corollary 1 and Corollary 6 bears a strong re-
semblance to log-space two-filter smoothers (Mayne, 1966; Fraser & Potter, 1969). Originally developed for
MAP-based smoothing in linear-Gaussian state-space models, these methods were later extended to nonlin-
ear settings through Gaussian-sum filters (Kitagawa, 1987). The canonical structure involves independent
forward and backward filtering recursions that are subsequently combined to form the smoothing solution.
Our hybrid smoother adopts this two-filter architecture by jointly leveraging both the forward and reverse
Gauss‚ÄìMarkov decompositions of the variational posterior. In doing so, it generalizes the classical two-filter
approach to accommodate nonlinear and non-Gaussian models within a variational inference framework. To
the best of our knowledge, this is the first attempt to formulate an iterated two-filter smoother that integrates
proximal updates and operates in the log-domain of the densities, akin to the forward and reverse variants.
A notable advantage of this hybrid scheme is the structural independence of the forward and backward
recursions, which allows for parallel execution. Although this design increases the overall computational
cost relative to the single-pass algorithms, it offers practical benefits in terms of speed and modularity when
implemented on modern parallel hardware.
6.2
Posterior-Linearization Bayesian Smoothing
Our work is primarily inspired by posterior-linearization algorithms (Garc√≠a-Fern√°ndez et al., 2016; Tronarp
et al., 2018), which perform approximate Gaussian Bayesian smoothing in nonlinear state-space models by
alternating between posterior linearization and a Rauch-Tung-Striebel smoothing pass. These algorithms can
be viewed as generalizations of the maximum-a-posteriori iterated smoother proposed by Bell (1994). While
these methods offer advantages over traditional extended and unscented smoothers (S√§rkk√§ & Svensson,
2023), they have two significant limitations.
First, they impose linearity and additive noise assumptions on the dynamics and measurement models,
which can be problematic in state-space models with multiplicative noise, as highlighted by Corenflos &
Abdulsamad (2023). Second, these algorithms, in their original form, fall within the class of undamped
Gauss‚ÄìNewton optimization methods, as they rely on linear approximations of the dynamics and measure-
ment models. Gauss‚ÄìNewton methods, however, require a full-rank Jacobian to ensure convergence (Nocedal
& Wright, 2006), a condition that is not generally met in nonlinear settings.
To address the latter issue, a partial remedy was proposed by Raitoharju et al. (2018), who introduced an
ad-hoc damping mechanism for the mean updates in the iterated posterior-linearization filter, albeit at the
cost of a computationally expensive nested optimization loop. In contrast, Lindqvist et al. (2021) proposed
a posterior-linearization smoother that implements damping through Levenberg‚ÄìMarquardt regularization
17

and line-search procedures with convergence guarantees. Both approaches of Raitoharju et al. (2018) and
Lindqvist et al. (2021), however, fail to exploit the information-geometric structure of the statistical manifold
on which the approximate smoothing distribution is assumed to reside.
Our approach generalizes these algorithms and resolves both weaknesses in a theoretically grounded manner.
By using Fourier‚ÄìHermite expansions, we overcome the limitations of linear-Gaussian approximations, while
the introduction of entropic proximal constraints provides a principled way to damp optimization over the
space of densities, leading to techniques akin to trust-region approaches (Nocedal & Wright, 2006; Teboulle,
1992). This improvement, however, comes at the cost of increased algorithmic complexity.
6.3
Approximate Bayesian Inference in State-Space Models
The field of approximate Bayesian inference has seen significant progress in adapting standard techniques
to exploit the temporal structure of state-space models, resulting in specialized algorithms for Bayesian
smoothing. One influential approach builds on expectation propagation (EP), introduced by Minka (2001).
In particular, Deisenroth & Mohamed (2012) applied EP to nonlinear state-space models with additive
Gaussian noise.
Their method iteratively linearizes the dynamics and measurement models around the
current posterior estimate, yielding tractable forward‚Äìbackward message-passing recursions. However, the
construction is largely heuristic and lacks a clearly defined global objective, which complicates convergence
analysis, especially in nonlinear or non-Gaussian regimes. In contrast, our method is grounded in a well-posed
variational optimization problem, enabling us to draw on a body of theoretical analysis from convex and
information-theoretic optimization (Teboulle, 1992; Chr√©tien & Hero, 2002), and ensuring a more principled
treatment of smoothing in complex models.
A parallel line of work has explored variational inference (VI)(Wainwright & Jordan, 2008) as a foundation
for approximate smoothing. For instance, Courts et al. (2021) proposed a variational Gaussian smoother
that optimizes the evidence lower bound using off-the-shelf constrained nonlinear solvers. Their formulation
relies on a particular parameterization of twin-marginal distributions, subject to feasibility constraints. A
key limitation, however, is that the resulting inference procedure is non-recursive, and optimization is carried
out directly in the parameter space of the posterior, rather than over the statistical manifold where natural
gradient methods can be more efficient(Amari, 1998). In contrast, Barfoot et al. (2020), building on the work
of Opper & Archambeau (2009), derived a natural gradient-based variational Gaussian smoother that, while
also non-recursive, leverages structured parameterizations to exploit the sparsity of the state-space model,
significantly improving computational efficiency. Recently, Tronarp (2025) introduced a recursive variational
framework that derives forward-backward algorithms similar to ours, albeit without proximal regularization.
Our framework is closely related to this line of work. Like the structured natural gradient approach of Barfoot
et al. (2020), our method falls within the class of information-theoretic variational optimizers identified by
Khan et al. (2015). However, it differs in two important respects. First, it is inherently recursive, producing
a family of forward‚Äìbackward smoothing algorithms that scale linearly in the time horizon. Second, it is
agnostic to the parameterization of the variational posterior, allowing for flexible approximation families.
By casting inference as a dynamic optimization problem constrained by KL-based trust regions, our method
naturally exploits the temporal structure of state-space models and inherits convergence properties from
proximal optimization theory.
6.4
Approximate Bayesian Inference in Temporal Gaussian Processes
There is a well-established connection between temporal Gaussian processes (GP) and linear state-space
models (O‚ÄôHagan, 1978).
Hartikainen & S√§rkk√§ (2010) showed that temporal GP models with Mat√©rn
kernels can be reformulated exactly as linear-Gaussian SSMs.
This enables using Kalman filtering and
smoothing with linear time complexity to perform inference in temporal GPs, a significant improvement
over the cubic complexity of standard GP inference.
Building on this foundation, Chang et al. (2020) proposed a variational method that combines conjugate-
computation variational inference (Khan & Lin, 2017) with Rauch‚ÄìTung‚ÄìStriebel recursions, enabling princi-
pled inference in models with linear dynamics and nonlinear measurement models. Wilkinson et al. (2020) ex-
18

tended this framework by unifying variational inference and expectation propagation within Kalman smooth-
ing. Wilkinson et al. (2023) further generalized these approaches via the Bayes‚ÄìNewton framework, which
interprets VI, EP, and posterior linearization as optimization algorithms akin to Newton‚Äôs method. Despite
these advances, a common critique of this class of algorithms is their ad-hoc treatment of the temporal dy-
namics. While inference is formulated from an optimization perspective, the temporal structure is handled
implicitly using linear RTS smoothing, rather than being explicitly integrated into the inference objective.
In the continuous-time setting, several variational methods have been developed. Archambeau et al. (2007)
proposed one of the earliest approaches for inference in partially-observed diffusion processes with linear
dynamics by approximating the posterior with a linear stochastic differential equation. This approximation
can be viewed as the continuous-time limit of a Gauss‚ÄìMarkov factorization. Ala-Luhtala et al. (2015) ex-
tended this line of work to nonlinear dynamics through sigma-point approximations. More recently, Wildner
& Koeppl (2021) and Verma et al. (2024) proposed natural-gradient-based generalizations of the method
introduced by Archambeau et al. (2007). Wildner & Koeppl (2021) developed a moment-based recursive
inference algorithm that reformulates the smoothing problem as an optimal control problem, thereby explic-
itly incorporating the dynamic structure of the setting. In contrast, Verma et al. (2024) adopts a site-based
approach inspired by Minka (2001), introducing a non-recursive algorithm that leverages iterative posterior
linearization to handle nonlinear dynamics. Finally, Bartosh et al. (2025) recently proposed a continuous-
time simulation-free approach based on modern flow and score matching techniques that scales to high
dimensional problem.
References
Juha Ala-Luhtala, Simo S√§rkk√§, and Robert Pich√©. Gaussian filtering and variational approximations for
Bayesian smoothing in continuous-discrete stochastic dynamic systems. Signal Processing, 111:124‚Äì136,
2015.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251‚Äì276, 1998.
Shun-ichi Amari. Information Geometry and Its Applications, volume 194. Springer, 2016.
C√©dric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John Shawe-Taylor. Variational infer-
ence for diffusion processes. Advances in Neural Information Processing Systems, 20, 2007.
Timothy D Barfoot. State Estimation for Robotics. Cambridge University Press, 2024.
Timothy D Barfoot, James R Forbes, and David J Yoon. Exactly sparse Gaussian variational inference
with application to derivative-free batch nonlinear state estimation. The International Journal of Robotics
Research, 39(13):1473‚Äì1502, 2020.
Grigory Bartosh, Dmitry Vetrov, and Christian A Naesseth. SDE Matching: Scalable and simulation-free
training of latent stochastic differential equations.
In International Conference on Machine Learning.
PMLR, 2025.
Bradley M Bell. The iterated Kalman smoother as a Gauss‚ÄìNewton method. SIAM Journal on Optimization,
4(3):626‚Äì636, 1994.
Alexandros Beskos, Dan Crisan, and Ajay Jasra. On the stability of sequential Monte Carlo methods in high
dimensions. The Annals of Applied Probability, 24(4):1396 ‚Äì 1445, 2014.
Christopher M Bishop. Pattern Recognition and Machine Learning, volume 4. Springer, 2006.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe.
Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112(518):859‚Äì877, 2017.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte Carlo.
CRC press, 2011.
19

Paul E Chang, William J Wilkinson, Mohammad Emtiyaz Khan, and Arno Solin. Fast variational learning
in state-space Gaussian process models. In IEEE International Workshop on Machine Learning for Signal
Processing, pp. 1‚Äì6. IEEE, 2020.
Nicolas Chopin and Omiros Papaspiliopoulos. An Introduction to Sequential Monte Carlo. Springer Cham,
2020.
St√©phane Chr√©tien and Alfred OIII Hero. Kullback proximal algorithms for maximum-likelihood estimation.
IEEE Transactions on Information Theory, 46(5):1800‚Äì1810, 2002.
Adrien Corenflos and Hany Abdulsamad. Variational Gaussian filtering via Wasserstein gradient flows. In
European Signal Processing Conference, pp. 1838‚Äì1842. IEEE, 2023.
Jarrad Courts, Adrian G Wills, and Thomas B Sch√∂n. Gaussian variational state estimation for nonlinear
state-space models. IEEE Transactions on Signal Processing, 69:5979‚Äì5993, 2021.
Henry Cox. On the estimation of state variables and parameters for noisy dynamic systems. IEEE Trans-
actions on Automatic Control, 9(1):5‚Äì12, 1964.
Marc Deisenroth and Shakir Mohamed. Expectation propagation in Gaussian process dynamical systems.
Advances in Neural Information Processing Systems, 25, 2012.
D Fraser and J Potter. The optimum linear smoother as a combination of two optimum linear filters. IEEE
Transactions on Automatic Control, 14(4):387‚Äì390, 1969.
√Ångel F Garc√≠a-Fern√°ndez, Lennart Svensson, Mark R Morelande, and Simo S√§rkk√§. Posterior linearization
filter: Principles and implementation using sigma points. IEEE Transactions on Signal Processing, 63
(20):5561‚Äì5573, 2015.
√Ångel F Garc√≠a-Fern√°ndez, Lennart Svensson, and Simo S√§rkk√§. Iterated posterior linearization smoother.
IEEE Transactions on Automatic Control, 62(4):2056‚Äì2063, 2016.
Jouni Hartikainen and Simo S√§rkk√§. Kalman filtering and smoothing solutions to temporal Gaussian process
regression models. In IEEE International Workshop on Machine Learning for Signal Processing, pp. 379‚Äì
384. IEEE, 2010.
Syeda Sakira Hassan and Simo S√§rkk√§. Fourier‚ÄìHermite dynamic programming for optimal control. IEEE
Transactions on Automatic Control, 68(10):6377‚Äì6384, 2023.
Antti Honkela, Tapani Raiko, Mikael Kuusela, Matti Tornio, and Juha Karhunen. Approximate Riemannian
conjugate gradient learning for fixed-form variational Bayes. The Journal of Machine Learning Research,
11:3235‚Äì3268, 2010.
Alfredo N Iusem, Benar Fux Svaiter, and Marc Teboulle. Entropy-like proximal methods in convex program-
ming. Mathematics of Operations Research, 19(4):790‚Äì814, 1994.
Eric Jacquier, Nicholas G Polson, and Peter E Rossi.
Bayesian analysis of stochastic volatility models.
Journal of Business & Economic Statistics, 20(1):69‚Äì87, 2002.
Mohammad Khan and Wu Lin. Conjugate-computation variational inference: Converting variational infer-
ence in non-conjugate models to inferences in conjugate models. In Artificial Intelligence and Statistics,
pp. 878‚Äì887. PMLR, 2017.
Mohammad Emtiyaz E Khan, Pierre Baque, Fran√ßois Fleuret, and Pascal Fua. Kullback-Leibler proximal
variational inference. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances
in Neural Information Processing Systems, volume 28, 2015.
Genshiro Kitagawa. Non-Gaussian state-space modeling of nonstationary time series. Journal of the Amer-
ican Statistical Association, 82(400):1032‚Äì1041, 1987.
20

Rudolf E. K√°lm√°n. A new approach to linear filtering and prediction problems. Transactions of the ASME
Journal of Basic Engineering, 82:35‚Äì45, 1960.
Yingzhen Li and Richard E Turner. R√©nyi divergence variational inference. Advances in Neural Information
Processing Systems, 29, 2016.
Jakob Lindqvist, Simo S√§rkk√§, Angel F. Garc√≠a-Fern√°ndez, Matti Raitoharju, and Lennart Svensson. Pos-
terior linearisation smoothing with robust iterations. 2021.
Paul Malliavin. Stochastic Analysis, volume 313. Springer, 2015.
David Q Mayne. A solution of the smoothing problem for linear dynamic systems. Automatica, 4(2):73‚Äì92,
1966.
Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In Conference on Uncer-
tainty in Artificial Intelligence, pp. 362‚Äì369. Morgan Kaufmann Publishers Inc., 2001.
Richard E Mortensen. Maximum-likelihood recursive nonlinear filtering. Journal of Optimization Theory
and Applications, 2(6):386‚Äì394, 1968.
James D. Murray. Mathematical Biology: An Introduction. Interdisciplinary Applied Mathematics. Springer,
2002.
Jorge Nocedal and Stephen J Wright. Numerical Optimization. Springer, 2006.
Anthony O‚ÄôHagan. Curve fitting and optimal design for prediction. Journal of the Royal Statistical Society:
Series B (Methodological), 40(1):1‚Äì24, 1978.
Manfred Opper and C√©dric Archambeau. The variational Gaussian approximation revisited. Neural Com-
putation, 21(3):786‚Äì792, 2009.
Matti Raitoharju, Lennart Svensson, Angel Froilan Garc√≠a-Fern√°ndez, and Robert Pich√©. Damped posterior
linearization filter. IEEE Signal Processing Letters, 25(4):536‚Äì540, 2018.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black-box variational inference. In Artificial Intelligence
and Statistics, pp. 814‚Äì822. PMLR, 2014.
Herbert E Rauch, F Tung, and Charlotte T Striebel.
Maximum likelihood estimates of linear dynamic
systems. AIAA Journal, 3(8):1445‚Äì1450, 1965.
Simo S√§rkk√§ and Lennart Svensson. Bayesian Filtering and Smoothing. Cambridge University Press, 2nd
edition, 2023.
Juha Sarmavuori and Simo S√§rkk√§.
Fourier‚ÄìHermite Kalman filter.
IEEE Transactions on Automatic
Control, 57(6):1511‚Äì1515, 2011.
Marc Teboulle. Entropic proximal mappings with applications to nonlinear programming. Mathematics of
Operations Research, 17(3):670‚Äì690, 1992.
Lucas Theis and Matt Hoffman. A trust-region method for stochastic variational inference with applications
to streaming data. In International Conference on Machine Learning, pp. 2503‚Äì2511. PMLR, 2015.
Filip Tronarp. A recursive theory of variational state estimation: The dynamic programming approach,
2025. URL https://arxiv.org/abs/2511.11497.
Filip Tronarp, Angel F Garcia-Fernandez, and Simo S√§rkk√§. Iterative filtering and smoothing in nonlinear
and non-Gaussian systems using conditional moments. IEEE Signal Processing Letters, 25(3):408‚Äì412,
2018.
Prakhar Verma, Vincent Adam, and Arno Solin.
Variational Gaussian process diffusion processes.
In
International Conference on Artificial Intelligence and Statistics, pp. 1909‚Äì1917. PMLR, 2024.
21

Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational inference.
Foundations and Trends¬Æ in Machine Learning, 1(1‚Äì2):1‚Äì305, 2008.
Christian Wildner and Heinz Koeppl. Moment-based variational inference for stochastic differential equa-
tions. In International Conference on Artificial Intelligence and Statistics, pp. 1918‚Äì1926. PMLR, 2021.
William Wilkinson, Paul Chang, Michael Andersen, and Arno Solin. State space expectation propagation:
Efficient inference schemes for temporal Gaussian processes.
In International Conference on Machine
Learning, pp. 10270‚Äì10281. PMLR, 2020.
William Wilkinson, Simo S√§rkk√§, and Arno Solin. Bayes-Newton methods for approximate Bayesian inference
with PSD guarantees. Journal of Machine Learning Research, 24(83):1‚Äì50, 2023.
22

A
Proof of Proposition 1
To find the critical point of (3) with respect to distribution q(x) given the constraints, we first construct the
corresponding Lagrangian functional
R(q, Œª, Œ±) = Eq
h
log p(y, x)
i
‚àíEq
h
log q(x)
i
+ Œª

1 ‚àí
Z
q(x) dx

+ Œ±

Œµ ‚àíDKL
h
q(x) || q[i](x)
i
,
(22)
where Œª and Œ± ‚â•0 are the Lagrangian multipliers associated with the constraints. Next, we set the functional
derivative of R(¬∑) with respect to q(x) to zero
‚àÇR(q, Œª, Œ±)
‚àÇq(x)
= log p(y, x) ‚àílog q(x) ‚àí1 ‚àíŒª ‚àíŒ± log q(x) + Œ± log q[i](x) ‚àíŒ± := 0,
which in turn leads to the optimal iterate q[i+1](x)
q[i+1](x) =
h
exp

Œª + (1 + Œ±)
	 i‚àí1/(1+Œ±)h
p(y | x) p(x)
i1/(1+Œ±)h
q[i](x)
iŒ±/(1+Œ±)
‚àù
h
p(y | x) p(x)
i1/(1+Œ±)h
q[i](x)
iŒ±/(1+Œ±)
.
(23)
Plugging the result from (23) back into the Lagrangian (22), we get the dual problem
minimize
Œª,Œ±
G(Œª, Œ±) = Œ± Œµ + Œª + (1 + Œ±)
Z
q[i+1](x) dx,
subject to Œ± ‚â•0.
(24)
If we compute and set the gradient of G(¬∑) with respect to Œª to zero, we get
Œª[i+1] = (1 + Œ±)

‚àí1 + log
Z h
p(y | x) p(x)
i1/(1+Œ±)h
q[i](x)
iŒ±/(1+Œ±)
dx

(25)
= (1 + Œ±)
h
‚àí1 + log Z[i+1](Œ±)
i
which when plugged back into (23) leads to the normalized approximate Gibbs posterior
q[i+1](x) =
h
Z[i+1](Œ≤)
i‚àí1 h
p(y | x) p(x)
i1‚àíŒ≤h
q[i](x)
iŒ≤
.
(26)
where have defined Œ≤ := Œ±/(1 + Œ±), so that Œ≤ = 0 when Œ± = 0 and Œ≤ ‚Üí1 as Œ± ‚Üí‚àû. By using the results
from (25) and (26), we can rewrite the dual problem (24) as follows
minimize
Œ≤
G(Œ≤) =
Œ≤Œµ
1 ‚àíŒ≤ +
1
1 ‚àíŒ≤ log Z[i+1](Œ≤),
subject to 0 ‚â§Œ≤ < 1.
B
Proof of Proposition 2
Given the forward-Markov decomposition from Assumption 1, we can factorize problem (4) over time. Start-
ing with the ELBO objective, we can write
L(‚àí‚áÄq ) = E‚àí
‚áÄ
q
h
log p(x0:T, y1:T)
i
‚àíE‚àí
‚áÄ
q
h
log‚àí‚áÄq (x0:T)
i
=
Z
‚àí‚áÄq0(x0)
h
log p(x0) ‚àílog‚àí‚áÄq0(x0)
i
dx0 +
T
X
k=1
Z
‚àí‚áÄqk(xk) log hk(yk | xk) dxk
+
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)‚àí‚áÄqk(xk+1 | xk)
h
log fk(xk+1 | xk) ‚àílog‚àí‚áÄqk(xk+1 | xk)
i
dxk dxk+1.
23

Additionally, the Kullback‚ÄìLeibler divergences factorizes forward as follows
Œµ ‚â•DKL
h‚àí‚áÄq (x0:T) || ‚àí‚áÄq [i](x0:T)
i
Œµ ‚â•
Z
‚àí‚áÄq0(x0)
T ‚àí1
Y
k=0
‚àí‚áÄqk(xk+1 | xk) log
‚àí‚áÄq0(x0) QT ‚àí1
k=0 ‚àí‚áÄqk(xk+1 | xk)
‚àí‚áÄq [i]
0 (x0) QT ‚àí1
k=0 ‚àí‚áÄq [i]
k (xk+1 | xk)
dx0:T
Œµ ‚â•
Z
‚àí‚áÄq0(x0) log
‚àí‚áÄq0(x0)
‚àí‚áÄq [i]
0 (x0)
dx0
+
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)
Z
‚àí‚áÄqk(xk+1 | xk) log
‚àí‚áÄqk(xk+1 | xk)
‚àí‚áÄq [i]
k (xk+1 | xk)
dxk+1 dxk.
Finally, the normalization constraint also factorizes to
1 =
Z
‚àí‚áÄq0(x0) dx0,
1 =
Z
‚àí‚áÄqk(xk+1 | xk) dxk+1,
‚àÄxk, 0 ‚â§k < T.
The information-theoretic proximal optimization problem (4) now has the form
maximize
‚àí
‚áÄ
qk(xk+1 | xk),
‚àí
‚áÄ
q0(x0)
Z
‚àí‚áÄq0(x0)
h
log p(x0) ‚àílog‚àí‚áÄq0(x0)
i
dx0 +
T
X
k=1
Z
‚àí‚áÄqk(xk) log hk(yk | xk) dxk
+
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)‚àí‚áÄqk(xk+1 | xk)
h
log fk(xk+1 | xk) ‚àílog‚àí‚áÄqk(xk+1 | xk)
i
dxk dxk+1,
subject to
‚àí‚áÄqk+1(xk+1) =
Z
‚àí‚áÄqk(xk)‚àí‚áÄqk(xk+1 | xk) dxk,
‚àÄxk+1, 0 ‚â§k < T,
1 =
Z
‚àí‚áÄq0(x0) dx0,
1 =
Z
‚àí‚áÄqk(xk+1 | xk) dxk+1,
‚àÄxk, 0 ‚â§k < T,
Œµ ‚â•
Z
‚àí‚áÄq0(x0) log
‚àí‚áÄq0(x0)
‚àí‚áÄq [i]
0 (x0)
dx0
+
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)
Z
‚àí‚áÄqk(xk+1 | xk) log
‚àí‚áÄqk(xk+1 | xk)
‚àí‚áÄq [i]
k (xk+1 | xk)
dxk+1 dxk.
Next, we construct the Lagrangian functional ‚àí ‚áÄ
R(‚àí‚áÄq0,‚àí‚áÄqk,‚àí‚áÄŒ≥0,‚àí‚áÄŒªk,‚àí‚áÄ
Vk, Œ±)
‚àí ‚áÄ
R(¬∑) =
Z
‚àí‚áÄq0(x0)
h
log p(x0) ‚àílog‚àí‚áÄq0(x0)
i
dx0 +
T
X
k=1
Z
‚àí‚áÄqk(xk) log hk(yk | xk) dxk
+
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)‚àí‚áÄqk(xk+1 | xk)
h
log fk(xk+1 | xk) ‚àílog‚àí‚áÄqk(xk+1 | xk)
i
dxk dxk+1
+
T ‚àí1
X
k=0
Z ‚àí‚áÄ
Vk+1(xk+1)
Z
‚àí‚áÄqk(xk)‚àí‚áÄqk(xk+1 | xk) dxk ‚àí‚àí‚áÄqk+1(xk+1)

dxk+1
+
T ‚àí1
X
k=0
Z ‚àí‚áÄŒªk(xk)

1 ‚àí
Z
‚àí‚áÄqk(xk+1 | xk) dxk+1

dxk +‚àí‚áÄŒ≥0

1 ‚àí
Z
‚àí‚áÄq0(x0) dx0

+ Œ±
"
Œµ ‚àí
Z
‚àí‚áÄq0(x0) log
‚àí‚áÄq0(x0)
‚àí‚áÄq [i]
0 (x0)
dx0
‚àí
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)
Z
‚àí‚áÄqk(xk+1 | xk) log
‚àí‚áÄqk(xk+1 | xk)
‚àí‚áÄq [i]
k (xk+1 | xk)
dxk+1 dxk
#
,
(27)
24

where ‚àí‚áÄ
V k(xk), ‚àí‚áÄŒªk(xk), ‚àí‚áÄŒ≥0, and Œ± ‚â•0 are Lagrangian multipliers. To find the critical point, we set the
functional derivative with respect to ‚àí‚áÄqk(xk+1 | xk) to zero
‚àí‚áÄq [i+1]
k
(xk+1 | xk) =
h
exp
n‚àí‚áÄŒªk(xk)/‚àí‚áÄqk(xk) + (1 + Œ±)
o i‚àí1/(1+Œ±)
√ó
h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ±/(1+Œ±)h
fk(xk+1 | xk) exp
n‚àí‚áÄ
Vk+1(xk+1)
o i1/(1+Œ±)
.
(28)
Additionally, we set the functional derivative with respect to ‚àí‚áÄq0(x0) to zero and get
‚àí‚áÄq [i+1]
0
(x0) =
h
exp
n‚àí‚áÄŒ≥0 + (1 + Œ±)
oi‚àí1/(1+Œ±)h‚àí‚áÄq [i]
0 (x0)
iŒ±/(1+Œ±)h
exp
n‚àí‚áÄ
V0(x0)
o i1/(1+Œ±)
,
(29)
where, for convenience of notation, we define
‚àí‚áÄ
V0(x0) = log p0(x0) + (1 + Œ±) log
Z h‚àí‚áÄq [i]
0 (x1 | x0)
iŒ±/(1+Œ±) h
f0(x1 | x0) exp
n‚àí‚áÄ
V1(x1)
o i1/(1+Œ±)
dx1.
(30)
Notice that, it is not clear at this point that the solutions (28) and (29) are normalized densities. Estab-
lishing that requires solving for the (functional) multipliers ‚àí‚áÄŒªk(xk) and ‚àí‚áÄŒ≥0, which are associated with the
normalization constraints. Plugging the solution (28) into the Lagrangian (27) results in the dual functional
‚àí‚áÄ
G(¬∑) = Œ± Œµ +
Z
‚àí‚áÄq0(x0)
h
log p(x0) ‚àílog‚àí‚áÄq0(x0)
i
dx0 +
T
X
k=1
Z
‚àí‚áÄqk(xk) log hk(yk | xk) dxk
+ Œ±
Z
‚àí‚áÄq0(x0)
h
log‚àí‚áÄq [i]
0 (x0) ‚àílog‚àí‚áÄq0(x0)
i
dx0 +‚àí‚áÄŒ≥0

1 ‚àí
Z
‚àí‚áÄq0(x0) dx0

+
T ‚àí1
X
k=0
Z ‚àí‚áÄŒªk(xk) dxk ‚àí
T ‚àí1
X
k=0
Z ‚àí‚áÄ
Vk+1(xk+1)‚àí‚áÄqk+1(xk+1) dxk+1
+ (1 + Œ±)
T ‚àí1
X
k=0
Z
‚àí‚áÄqk(xk)
Z
‚àí‚áÄq [i+1]
k
(xk+1 | xk) dxk+1 dxk.
Now, we can solve for the multipliers ‚àí‚áÄŒªk(xk) by setting the associated derivatives to zero
‚àí‚áÄŒª [i+1]
k
(xk) = (1 + Œ±)‚àí‚áÄqk(xk)
h
‚àí1 + log
Z h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ±/(1+Œ±)
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
Vk+1(xk+1)
o i1/(1+Œ±)
dxk+1
i
(31)
= (1 + Œ±)‚àí‚áÄqk(xk)
h
‚àí1 + log‚àí‚áÄ
œà [i+1]
k
(xk)
i
.
Plugging (31) into (28) returns the normalized tilted conditional
‚àí‚áÄq [i+1]
k
(xk+1 | xk) =
h‚àí‚áÄ
œà [i+1]
k
(xk)
i‚àí1h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
Vk+1(xk+1)
o i1‚àíŒ≤
,
(32)
where have defined Œ≤ := Œ±/(1 + Œ±), so that Œ≤ = 0 when Œ± = 0, and Œ≤ ‚Üí1 as Œ± ‚Üí‚àû. Next, we use the
results from (29), (31), and (32) to simplify the functional ‚àí‚áÄ
G(¬∑) further
‚àí‚áÄ
G(¬∑) = Œ± Œµ +
T
X
k=1
Z
‚àí‚áÄqk(xk) log hk(yk | xk) dxk ‚àí
T
X
k=1
Z ‚àí‚áÄ
Vk(xk)‚àí‚áÄqk(xk) dxk
+ (1 + Œ±)
T ‚àí1
X
k=1
Z
‚àí‚áÄqk(xk) log‚àí‚áÄ
œà [i+1]
k
(xk) dxk +‚àí‚áÄŒ≥0 + (1 + Œ±)
Z
‚àí‚áÄq [i+1]
0
(x0) dx0,
25

and solve for the optimal multiplier ‚àí‚áÄŒ≥0 by zeroing the associated derivative, leading to
‚àí‚áÄŒ≥ [i+1]
0
= (1 + Œ±)

‚àí1 + log
Z h‚àí‚áÄq [i]
0 (x0)
iŒ±/(1+Œ±)h
exp
n‚àí‚áÄ
V0(x0)
o i1/(1+Œ±)
dx0

(33)
= (1 + Œ±)
h
‚àí1 + log‚àí‚áÄ
Z [i+1]
0
i
,
which after plugging into (29) delivers
‚àí‚áÄq [i+1]
0
(x0) =
h‚àí‚áÄ
Z [i+1]
0
i‚àí1h‚àí‚áÄq [i]
0 (x0)
iŒ≤h
exp
n‚àí‚áÄ
V0(x0)
o i1‚àíŒ≤
.
(34)
Plugging (34) and (33) back into ‚àí‚áÄ
G(¬∑) leads to further simplification of the dual
‚àí‚áÄ
G(¬∑) = Œ± Œµ +
T
X
k=1
Z
‚àí‚áÄqk(xk) log hk(yk | xk) dxk ‚àí
T
X
k=1
Z ‚àí‚áÄ
Vk(xk)‚àí‚áÄqk(xk) dxk
+ (1 + Œ±)
T ‚àí1
X
k=1
Z
‚àí‚áÄqk(xk) log‚àí‚áÄ
œà [i+1]
k
(xk) dxk + (1 + Œ±) log‚àí‚áÄ
Z [i+1]
0
.
Finally, to find the optimal potentials, we zero the derivative of ‚àí‚áÄ
G(¬∑) with respect to ‚àí‚áÄqk(xk), ‚àÄ0 < k ‚â§T.
Combined with the results from (30) and (31), we can write
‚àí‚áÄ
V [i+1]
k
(xk) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
log hT(yT | xT)
if k = T,
log hk(yk | xk) + 1/(1 ‚àíŒ≤) log‚àí‚áÄ
œà [i+1]
k
(xk)
if 0 < k < T,
log p0(x0) + 1/(1 ‚àíŒ≤) log‚àí‚áÄ
œà [i+1]
0
(x0)
if k = 0,
which, when plugged back into the dual ‚àí‚áÄ
G(¬∑), leads to the final simplification
‚àí‚áÄ
G(Œ≤) =
Œ≤Œµ
1 ‚àíŒ≤ +
1
1 ‚àíŒ≤ log‚àí‚áÄ
Z [i+1]
0
(Œ≤).
C
Proof of Proposition 3
This proof follows a similar scheme to that in Section B. However, the reverse-Markov decomposition results in
novel recursions. Starting from the reverse-Markov decomposition in Assumption 2, we factorize problem (4)
over time. Thus, we can write the ELBO as
L(‚Üº‚àíq ) = E‚Üº
‚àí
q
h
log p(x0:T, y1:T)
i
‚àíE‚Üº
‚àí
q
h
log‚Üº‚àíq (x0:T)
i
=
Z
‚Üº‚àíq0(x0) log p(x0) dx0 ‚àí
Z
‚Üº‚àíqT(xT) log‚Üº‚àíqT(xT) dxT +
T
X
k=1
Z
‚Üº‚àíqk(xk) log hk(yk | xk) dxk
+
T
X
k=1
Z
‚Üº‚àíqk(xk)‚Üº‚àíqk(xk‚àí1 | xk)
h
log fk‚àí1(xk | xk‚àí1) ‚àílog‚Üº‚àíqk(xk‚àí1 | xk)
i
dxk‚àí1 dxk.
Furthermore, the Kullback‚ÄìLeibler divergence factorizes reversely to
Œµ ‚â•DKL
h‚Üº‚àíq (x0:T) || ‚Üº‚àíq [i](x0:T)
i
Œµ ‚â•
Z
‚Üº‚àíqT(xT)
T
Y
k=1
‚Üº‚àíqk(xk‚àí1 | xk) log
‚Üº‚àíqT(xT) QT
k=1‚Üº‚àíqk(xk‚àí1 | xk)
‚Üº‚àíq [i]
T (xT) QT
k=1‚Üº‚àíq [i]
k (xk‚àí1 | xk)
dx0:T
26

Œµ ‚â•
Z
‚Üº‚àíqT(xT) log
‚Üº‚àíqT(xT)
‚Üº‚àíq [i]
T (xT)
dxT
+
T
X
k=1
Z
‚Üº‚àíqk(xk)
Z
‚Üº‚àíqk(xk‚àí1 | xk) log
‚Üº‚àíqk(xk‚àí1 | xk)
‚Üº‚àíq [i]
k (xk‚àí1 | xk)
dxk dxk‚àí1
Finally, the normalization constraint also factorizes to
1 =
Z
‚Üº‚àíqT(xT) dxT,
1 =
Z
‚Üº‚àíqk(xk‚àí1 | xk) dxk‚àí1,
‚àÄxk, 0 < k ‚â§T.
Now we can rewrite the optimization problem (4) as follows
maximize
‚Üº
‚àí
qk(xk‚àí1 | xk),
‚Üº
‚àí
qT (xT )
Z
‚Üº‚àíq0(x0) log p(x0) dx0 +
T
X
k=1
Z
‚Üº‚àíqk(xk) log hk(yk | xk) dxk
+
T
X
k=1
Z
‚Üº‚àíqk(xk)‚Üº‚àíqk(xk‚àí1 | xk)
h
log fk‚àí1(xk | xk‚àí1) ‚àílog‚Üº‚àíqk(xk‚àí1 | xk)
i
dxk‚àí1 dxk
‚àí
Z
‚àí‚áÄqT(xT) log‚Üº‚àíqT(xT) dxT
subject to
‚Üº‚àíqk‚àí1(xk‚àí1) =
Z
‚Üº‚àíqk(xk)‚Üº‚àíqk(xk‚àí1 | xk) dxk,
‚àÄxk‚àí1, 0 < k ‚â§T,
1 =
Z
‚Üº‚àíqT(xT) dxT,
1 =
Z
‚Üº‚àíqk(xk‚àí1 | xk) dxk‚àí1,
‚àÄxk, 0 < k ‚â§T,
Œµ ‚â•
Z
‚Üº‚àíqT(xT) log
‚Üº‚àíqT(xT)
‚Üº‚àíq [i]
T (xT)
dxT
+
T
X
k=1
Z
‚Üº‚àíqk(xk)
Z
‚Üº‚àíqk(xk‚àí1 | xk) log
‚Üº‚àíqk(xk‚àí1 | xk)
‚Üº‚àíq [i]
k (xk‚àí1 | xk)
dxk dxk‚àí1,
and the corresponding Lagrangian functional ‚Üº ‚àí
R(‚Üº‚àíqT,‚Üº‚àíqk,‚Üº‚àíŒ≥T,‚Üº‚àíŒªk,‚Üº‚àí
Vk, Œ±) is
‚Üº ‚àí
R(¬∑) =
Z
‚Üº‚àíq0(x0) log p(x0) dx0 +
T
X
k=1
Z
‚Üº‚àíqk(xk) log hk(yk | xk) dxk ‚àí
Z
‚àí‚áÄqT(xT) log‚Üº‚àíqT(xT) dxT
+
T
X
k=1
Z
‚Üº‚àíqk(xk)‚Üº‚àíqk(xk‚àí1 | xk)
h
log fk‚àí1(xk | xk‚àí1) ‚àílog‚Üº‚àíqk(xk‚àí1 | xk)
i
dxk‚àí1 dxk
+
T
X
k=1
Z ‚Üº‚àí
Vk‚àí1(xk‚àí1)
Z
‚Üº‚àíqk(xk)‚Üº‚àíqk(xk‚àí1 | xk) dxk ‚àí‚Üº‚àíqk‚àí1(xk‚àí1)

dxk‚àí1
+
T
X
k=1
Z ‚Üº‚àíŒªk(xk)

1 ‚àí
Z
‚Üº‚àíqk(xk‚àí1 | xk) dxk‚àí1

dxk +‚Üº‚àíŒ≥T

1 ‚àí
Z
‚Üº‚àíqT(xT) dxT

+ Œ±
"
Œµ ‚àí
Z
‚Üº‚àíqT(xT) log
‚Üº‚àíqT(xT)
‚Üº‚àíq [i]
T (xT)
dxT
‚àí
T
X
k=1
Z
‚Üº‚àíqk(xk)
Z
‚Üº‚àíqk(xk‚àí1 | xk) log
‚Üº‚àíqk(xk‚àí1 | xk)
‚Üº‚àíq [i]
k (xk‚àí1 | xk)
dxk dxk‚àí1
#
,
(35)
where ‚Üº‚àí
V k(xk), ‚Üº‚àíŒªk(xk), ‚Üº‚àíŒ≥T, and Œ± ‚â•0 are Lagrangian multipliers. To find the solution with respect to
‚Üº‚àíqk(xk‚àí1 | xk), we zero the associated derivatives of ‚Üº ‚àí
R(¬∑)
‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) =
h
exp
n‚Üº‚àíŒªk(xk)/‚Üº‚àíqk(xk) + (1 + Œ±)
o i‚àí1/(1+Œ±)
√ó
h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ±/(1+Œ±)h
fk‚àí1(xk+1 | xk) exp
n‚Üº‚àí
Vk‚àí1(xk‚àí1)
o i1/(1+Œ±)
(36)
27

Moreover, by setting the functional derivative with respect to ‚Üº‚àíqT(xT) to zero and get
‚Üº‚àíq [i+1]
T
(xT) =
h
exp
n‚Üº‚àíŒ≥T + (1 + Œ±)
oi‚àí1/(1+Œ±)h‚Üº‚àíq [i]
T (xT)
iŒ±/(1+Œ±)h
exp
n‚Üº‚àí
VT(xT)
o i1/(1+Œ±)
(37)
where, for convenience of notation, we define
‚Üº‚àí
VT(xT) = log hT(yT | xT) + (1 + Œ±) log
Z h‚Üº‚àíq [i]
T ‚àí1(xT ‚àí1 | xT)
iŒ±/(1+Œ±)
√ó
h
fT ‚àí1(xT | xT ‚àí1) exp
n‚Üº‚àí
VT ‚àí1(x1)
o i1/(1+Œ±)
dxT ‚àí1.
(38)
Again, the solutions (36) and (37) are not yet normalized densities. We need to solve for the (functional)
multipliers associated with the normalization constraints, ‚Üº‚àíŒªk(xk) and ‚Üº‚àíŒ≥T. Plugging the solution (36) into
the Lagrangian (35) results in the dual functional
‚Üº‚àí
G(¬∑) = Œ± Œµ +
Z
‚Üº‚àíq0(x0) log p(x0) dx0 +
T
X
k=1
Z
‚Üº‚àíqk(xk) log hk(yk | xk) dxk
+
Z
‚Üº‚àíqT(xT)
h
Œ± log‚Üº‚àíq [i]
T (xT) ‚àí(1 + Œ±) log‚Üº‚àíqT(xT)
i
dxT +‚Üº‚àíŒ≥T

1 ‚àí
Z
‚Üº‚àíqT(xT) dxT

+
T
X
k=1
Z ‚Üº‚àíŒªk(xk) dxk ‚àí
T
X
k=1
Z ‚Üº‚àí
Vk‚àí1(xk‚àí1)‚Üº‚àíqk‚àí1(xk‚àí1) dxk‚àí1
+ (1 + Œ±)
T
X
k=1
Z
‚Üº‚àíqk(xk)
Z
‚Üº‚àíq [i]
k (xk‚àí1 | xk) dxk dxk‚àí1,
which we use to solve for the multipliers ‚Üº‚àíŒªk(xk) by zeroing the associated derivatives
‚Üº‚àíŒª [i+1]
k
(xk) = (1 + Œ±)‚Üº‚àíqk(xk)
h
‚àí1 + log
Z h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ±/(1+Œ±)
√ó
h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
Vk‚àí1(xk‚àí1)
o i1/(1+Œ±)
dxk‚àí1
i
(39)
= (1 + Œ±)‚Üº‚àíqk(xk)
h
‚àí1 + log‚Üº‚àí
œà [i+1]
k
(xk)
i
.
We retrieve the normalized tilted conditionals by plugging (39) into (36)
‚Üº‚àíq [i+1]
k
(xk+1 | xk) =
h‚Üº‚àí
œà [i+1]
k
(xk)
i‚àí1h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ≤
√ó
h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
Vk‚àí1(xk‚àí1)
o i1‚àíŒ≤
,
(40)
where have defined Œ≤ := Œ±/(1 + Œ±), so that Œ≤ = 0 when Œ± = 0, and Œ≤ ‚Üí1 as Œ± ‚Üí‚àû. Given the results (39)
and (40), we simplify the functional ‚Üº‚àí
G(¬∑) further
‚Üº‚àí
G(¬∑) = Œ± Œµ +
Z
‚Üº‚àíq0(x0) log p(x0) dx0 +
T
X
k=1
Z
‚Üº‚àíqk(xk) log hk(yk | xk) dxk
‚àí
T
X
k=0
Z ‚Üº‚àí
Vk(xk)‚Üº‚àíqk(xk) dxk + (1 + Œ±)
T
X
k=1
Z
‚Üº‚àíqk(xk) log‚Üº‚àí
œà [i+1]
k
(xk) dxk
+‚Üº‚àíŒ≥T + (1 + Œ±)
Z
‚Üº‚àíq [i+1]
T
(xT) dxT.
Next, we solve for the optimal multiplier ‚Üº‚àíŒ≥T by zeroing the associated derivative
‚Üº‚àíŒ≥ [i+1]
T
= (1 + Œ±)

‚àí1 + log
Z h‚Üº‚àíq [i]
T (xT)
iŒ±/(1+Œ±)h
exp
n‚Üº‚àí
VT(xT)
o i1/(1+Œ±)
dxT

(41)
28

= (1 + Œ±)
h
‚àí1 + log‚Üº‚àí
Z [i+1]
T
i
,
leading to the normalization of the tilted distribution (37)
‚Üº‚àíq [i+1]
T
(xT) =
h‚Üº‚àí
Z [i+1]
T
i‚àí1h‚Üº‚àíq [i]
T (xT)
iŒ≤h
exp
n‚Üº‚àí
VT(xT)
o i1‚àíŒ≤
.
(42)
Using (42) and (41) leads to further simplification of the dual ‚Üº‚àí
G(¬∑)
‚Üº‚àí
G(¬∑) = Œ± Œµ +
Z
‚Üº‚àíq0(x0) log p(x0) dx0 +
T
X
k=1
Z
‚Üº‚àíqk(xk) log hk(yk | xk) dxk
‚àí
T
X
k=0
Z ‚Üº‚àí
Vk(xk)‚Üº‚àíqk(xk) dxk + (1 + Œ±)
T
X
k=1
Z
‚Üº‚àíqk(xk) log‚Üº‚àí
œà [i+1]
k
(xk) dxk + (1 + Œ±) log‚Üº‚àí
Z [i+1]
T
.
Finally, by taking (38) into consideration and zeroing the derivative of ‚Üº‚àí
G(¬∑) with respect to ‚Üº‚àíqk(xk), ‚àÄ0 ‚â§
k < T, we find the optimal potential functions
‚Üº‚àí
V [i+1]
k
(xk) =
Ô£±
Ô£≤
Ô£≥
log p0(x0)
if k = 0,
log hk(yk | xk) + 1/(1 ‚àíŒ≤) log‚Üº‚àí
œà [i+1]
k
(xk)
if 0 < k ‚â§T.
If we plug this result back into the dual ‚Üº‚àí
G(¬∑), we retrieve the simplest form of the dual
‚Üº‚àí
G(Œ≤) =
Œ≤Œµ
1 ‚àíŒ≤ +
1
1 ‚àíŒ≤ log‚Üº‚àí
Z [i+1]
T
(Œ≤).
D
Proof of Proposition 4
At k = T, Assumption 5 readily delivers a quadratic form for the potential function
‚àí‚áÄ
V [i+1]
T
(xT) = log hT(yT | xT) = ‚àí1
2x‚ä§
T
‚àí‚áÄ
R [i+1]
T
xT + x‚ä§
T ‚àí‚áÄr [i+1]
T
+‚àí‚áÄœÅ [i+1]
T
= ‚àí1
2x‚ä§
T L[i]
T xT + x‚ä§
T l[i]
T + ŒΩ [i]
T .
For ‚àí‚áÄ
V [i+1]
k
(xk), for all 0 ‚â§k < T, we have the backward recursion
‚àí‚áÄ
V [i+1]
k
(xk) = log hk(yk | xk) + 1/(1 ‚àíŒ≤) log‚àí‚áÄ
œà [i+1]
k
(xk)
= log hk(yk | xk) + 1/(1 ‚àíŒ≤) log
Z h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1.
Our aim is to show that this recursion is a tractable reverse propagation of quadratic forms. We start by
examining ‚àí‚áÄ
œà [i+1]
k
(xk), the integral over xk+1. Let us first drag all terms into the exponential, then we can
treat the exponent as a quadratic function over xk and xk+1
‚àí1
2
h
x‚ä§
k+1
x‚ä§
k
i
Ô£Æ
Ô£∞
‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
‚àí‚àí‚áÄ
G [i+1]
¬Øxx,k
‚àí‚àí‚áÄ
G [i+1]
x¬Øx,k
‚àí‚áÄ
G [i+1]
xx,k
Ô£π
Ô£ª
Ô£Æ
Ô£∞xk+1
xk
Ô£π
Ô£ª+
h
x‚ä§
k+1
x‚ä§
k
i
Ô£Æ
Ô£∞
‚àí‚áÄg [i+1]
¬Øx,k
‚àí‚áÄg [i+1]
x,k
Ô£π
Ô£ª+‚àí‚áÄŒ∏ [i+1]
k
= Œ≤ log‚àí‚áÄq [i]
k (xk+1 | xk) + (1 ‚àíŒ≤) log fk(xk+1 | xk) + (1 ‚àíŒ≤)‚àí‚áÄ
V [i+1]
k+1 (xk+1).
Now, given an affine-Gaussian‚àí‚áÄq [i]
k (xk+1 | xk) (Assumption 3), a quadratic log fk(xk+1 | xk) (Assumption 5),
and a quadratic potential function ‚àí‚áÄ
V [i+1]
k+1 (xk) of the form (15), we can match the quadratic factors between
the left- and right-hand sides, leading to
‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k = (1 ‚àíŒ≤)
h
C[i]
¬Øx¬Øx,k +‚àí‚áÄ
R [i+1]
k+1
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1
,
29

‚àí‚áÄ
G [i+1]
xx,k = (1 ‚àíŒ≤) C[i]
xx,k + Œ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k ,
‚àí‚áÄ
G [i+1]
¬Øxx,k = (1 ‚àíŒ≤) C[i]
¬Øxx,k + Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k ,
‚àí‚áÄ
G [i+1]
x¬Øx,k = (1 ‚àíŒ≤) C[i]
x¬Øx,k + Œ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1
,
‚àí‚áÄg [i+1]
¬Øx,k
= (1 ‚àíŒ≤)
h
c[i]
¬Øx,k +‚àí‚áÄr [i+1]
k
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k ,
‚àí‚áÄg [i+1]
x,k
= (1 ‚àíŒ≤) c[i]
x,k ‚àíŒ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k ,
‚àí‚áÄŒ∏ [i+1]
k
= (1 ‚àíŒ≤)
h
Œ∫[i]
k +‚àí‚áÄœÅ [i+1]
k+1
i
‚àíŒ≤
2 log
2œÄ‚àí‚áÄ
Œ£ [i]
k
 ‚àíŒ≤
2
h‚àí‚áÄd [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k .
Next, we reformulate the quadratic function in the exponent explicitly as a function of xk+1
‚àí1
2x‚ä§
k+1
‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k xk+1 + x‚ä§
k+1
h‚àí‚áÄ
G [i+1]
¬Øxx,k xk +‚àí‚áÄg [i+1]
¬Øx,k
i
+

‚àí1
2x‚ä§
k
‚àí‚áÄ
G [i+1]
xx,k xk + x‚ä§
k ‚àí‚áÄg [i+1]
x,k
+‚àí‚áÄŒ∏ [i+1]
k

.
Consequently, the exponential now resembles an unnormalized Gaussian distribution in information form
with a precision matrix ‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k . In this case, we can use the identity
log
Z
exp

‚àí1
2x‚ä§Ux + x‚ä§u + œÉ

dx = 1
2 log |2œÄ U ‚àí1| + 1
2u‚ä§U ‚àí1u + œÉ,
(43)
to express the log-normalizing function log‚àí‚áÄ
œà [i+1]
k
(xk) as a quadratic function
log‚àí‚áÄ
œà [i+1]
k
(xk) = ‚àí1
2x‚ä§
k
‚àí‚áÄ
S [i+1]
k
xk + x‚ä§
k‚àí‚áÄs [i+1]
k
+‚àí‚áÄŒæ [i+1]
k
= 1
2 log
2œÄ
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1  +

‚àí1
2x‚ä§
k
‚àí‚áÄ
G [i+1]
xx,k xk + x‚ä§
k ‚àí‚áÄg [i+1]
x,k
+‚àí‚áÄŒ∏ [i+1]
k

+ 1
2
h‚àí‚áÄ
G [i+1]
¬Øxx,k xk +‚àí‚áÄg [i+1]
¬Øx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
¬Øxx,k xk +‚àí‚áÄg [i+1]
¬Øx,k
i
which, after matching terms, leads to
‚àí‚áÄ
S [i+1]
k
= ‚àí‚áÄ
G [i+1]
xx,k ‚àí
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄ
G [i+1]
¬Øxx,k ,
‚àí‚áÄs [i+1]
k
= ‚àí‚áÄg [i+1]
x,k
+
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
¬Øx,k
,
‚àí‚áÄŒæ [i+1]
k
= ‚àí‚áÄŒ∏ [i+1]
k
+ 1
2 log
2œÄ
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1  + 1
2
h‚àí‚áÄg [i+1]
¬Øx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
¬Øx,k
.
To get the final form of the quadratic potential ‚àí‚áÄ
V [i+1]
k
(xk), we add the contribution of the quadratic log-
measurement and log-prior determined by Assumption 5
‚àí‚áÄ
R [i+1]
k
= L[i]
k +
1
1 ‚àíŒ≤
‚àí‚áÄ
S [i+1]
k
,
‚àí‚áÄr [i+1]
k
= l[i]
k +
1
1 ‚àíŒ≤
‚àí‚áÄs [i+1]
k
,
‚àí‚áÄœÅ [i+1]
k
= ŒΩ[i]
k +
1
1 ‚àíŒ≤
‚àí‚áÄŒæ [i+1]
k
.
Finally, for a Gaussian ‚àí‚áÄq [i]
0 (x0) and a quadratic ‚àí‚áÄ
V [i+1]
0
(x0), we derive a quadratic log‚àí‚áÄ
Z [i+1]
0
log‚àí‚áÄ
Z [i+1]
0
= log
Z h‚àí‚áÄq [i]
0 (x0)
iŒ≤h
exp
n‚àí‚áÄ
V [i+1]
0
(x0)
o i1‚àíŒ≤
dx0.
Again, we formulate a quadratic function over x0 and ‚àí ‚áÄ
m [i]
0
‚àí1
2

x‚ä§
0
h‚àí ‚áÄ
m [i]
0
i‚ä§ Ô£Æ
Ô£∞
‚àí‚áÄ
J [i+1]
xx
‚àí‚àí‚áÄ
J [i+1]
xm
‚àí‚àí‚áÄ
J [i+1]
mx
‚àí‚áÄ
J [i+1]
mm
Ô£π
Ô£ª
Ô£Æ
Ô£∞
x0
‚àí ‚áÄ
m [i]
0
Ô£π
Ô£ª+

x‚ä§
0
h‚àí ‚áÄ
m [i]
0
i‚ä§ Ô£Æ
Ô£∞
‚àí‚áÄj [i+1]
x
‚àí‚áÄj [i+1]
m
Ô£π
Ô£ª+‚àí‚áÄœÑ [i+1]
30

= Œ≤ log‚àí‚áÄq [i]
0 (x0) + (1 ‚àíŒ≤)‚àí‚áÄ
V [i+1]
0
(x0),
where by matching the quadratic factors, we get
‚àí‚áÄ
J [i+1]
xx
= (1 ‚àíŒ≤)‚àí‚áÄ
R [i+1]
0
+ Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄ
J [i+1]
xm
= Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄ
J [i+1]
mm
= Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄ
J [i+1]
mx
= Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
,
‚àí‚áÄj [i+1]
x
= (1 ‚àíŒ≤)‚àí‚áÄr [i+1]
0
,
‚àí‚áÄj [i+1]
m
= 0,
‚àí‚áÄœÑ [i+1] = (1 ‚àíŒ≤)‚àí‚áÄœÅ [i+1]
0
‚àíŒ≤
2 log
2œÄ‚àí‚áÄ
P [i]
0
.
Using the identity (43), we get a log-normalizer as a quadratic function over ‚àí ‚áÄ
m [i]
0
log‚àí‚áÄ
Z [i+1]
0
= ‚àí1
2
h‚àí ‚áÄ
m [i]
0
i‚ä§‚àí‚áÄ
U [i+1]‚àí ‚áÄ
m [i]
0
+
h‚àí ‚áÄ
m [i]
0
i‚ä§‚àí‚áÄu [i+1] +‚àí‚áÄŒ∑ [i+1],
where
‚àí‚áÄ
U [i+1] = ‚àí‚áÄ
J [i+1]
mm
‚àí
h‚àí‚áÄ
J [i+1]
xm
i‚ä§h‚àí‚áÄ
J [i+1]
xx
i‚àí1‚àí‚áÄ
J [i+1]
xm
,
‚àí‚áÄu [i+1] = ‚àí‚áÄj [i+1]
m
‚àí
h‚àí‚áÄ
J [i+1]
xm
i‚ä§h‚àí‚áÄ
J [i+1]
xx
i‚àí1‚àí‚áÄj [i+1]
x
,
‚àí‚áÄŒ∑ [i+1] = ‚àí‚áÄœÑ [i+1] ‚àí1
2
2œÄ‚àí‚áÄ
J [i+1]
xx
i‚àí1 + 1
2
h‚àí‚áÄj [i+1]
x
i‚ä§h‚àí‚áÄ
J [i+1]
xx
i‚àí1h‚àí‚áÄj [i+1]
x
i
.
E
Proof of Proposition 5
For k = 0, the log-prior is a quadratic function per Assumption 5, leading to the following parameterization
of the potential
‚Üº‚àí
V [i+1]
0
(x0) = log p0(x0) = ‚àí1
2x‚ä§
0
‚Üº‚àí
R [i+1]
0
x0 + x‚ä§
0 ‚Üº‚àír [i+1]
0
+‚Üº‚àíœÅ [i+1]
0
= ‚àí1
2x‚ä§
0 L[i]
0 x0 + x‚ä§
0 l[i]
0 + ŒΩ [i]
0 .
For ‚Üº‚àí
V [i+1]
k
(xk), for all 0 < k ‚â§T, we have derived a forward recursion
‚Üº‚àí
V [i+1]
k
(xk) = log hk(yk | xk) + 1/(1 ‚àíŒ≤) log‚Üº‚àí
œà [i+1]
k
(xk)
= log hk(yk | xk) + 1/(1 ‚àíŒ≤) log
Z h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ≤
√ó
h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1)
o i1‚àíŒ≤
dxk‚àí1.
We show that this recursion is a tractable forward propagation of quadratic forms. We start by moving all
terms within the integral into the exponential and treat the exponent as a quadratic function over xk and
xk‚àí1
‚àí1
2
h
x‚ä§
k
x‚ä§
k‚àí1
i
Ô£Æ
Ô£∞
‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
‚àí‚Üº‚àí
G [i+1]
¬Øxx,k
‚àí‚Üº‚àí
G [i+1]
x¬Øx,k
‚Üº‚àí
G [i+1]
xx,k
Ô£π
Ô£ª
Ô£Æ
Ô£∞xk
xk‚àí1
Ô£π
Ô£ª+
h
x‚ä§
k
x‚ä§
k‚àí1
i
Ô£Æ
Ô£∞
‚Üº‚àíg [i+1]
¬Øx,k
‚Üº‚àíg [i+1]
x,k
Ô£π
Ô£ª+‚Üº‚àíŒ∏ [i+1]
k
= Œ≤ log‚Üº‚àíq [i]
k (xk | xk‚àí1) + (1 ‚àíŒ≤) log fk‚àí1(xk | xk‚àí1) + (1 ‚àíŒ≤)‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1).
For an affine-Gaussian ‚Üº‚àíq [i]
k (xk‚àí1 | xk) (Assumption 4), a quadratic log fk‚àí1(xk | xk‚àí1) (Assumption 5), and
a quadratic potential function ‚Üº‚àí
V [i+1]
k+1 (xk) of the form (15), we get
‚Üº‚àí
G [i+1]
¬Øx¬Øx,k = (1 ‚àíŒ≤) C[i]
¬Øx¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k ,
31

‚Üº‚àí
G [i+1]
xx,k = (1 ‚àíŒ≤)
h
C[i]
xx,k‚àí1 +‚Üº‚àí
R [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1
,
‚Üº‚àí
G [i+1]
¬Øxx,k = (1 ‚àíŒ≤) C[i]
¬Øxx,k‚àí1 + Œ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1
,
‚Üº‚àí
G [i+1]
x¬Øx,k = (1 ‚àíŒ≤) C[i]
x¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k ,
‚Üº‚àíg [i+1]
¬Øx,k
= (1 ‚àíŒ≤) c[i]
¬Øx,k‚àí1 ‚àíŒ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k ,
‚Üº‚àíg [i+1]
x,k
= (1 ‚àíŒ≤)
h
c[i]
x,k‚àí1 +‚Üº‚àír [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k ,
‚Üº‚àíŒ∏ [i+1]
k
= (1 ‚àíŒ≤)
h
Œ∫[i]
k +‚Üº‚àíœÅ [i+1]
k‚àí1
i
‚àíŒ≤
2 log
2œÄ‚Üº‚àí
Œ£ [i]
k
 ‚àíŒ≤
2
h‚Üº‚àíd [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k .
By writing this quadratic function explicitly in terms of xk‚àí1
‚àí1
2x‚ä§
k‚àí1
‚Üº‚àí
G [i+1]
xx,k xk‚àí1 + x‚ä§
k‚àí1
h‚Üº‚àí
G [i+1]
x¬Øx,k xk +‚Üº‚àíg [i+1]
x,k
i
+

‚àí1
2x‚ä§
k
‚Üº‚àí
G [i+1]
¬Øx¬Øx,k xk + x‚ä§
k ‚Üº‚àíg [i+1]
¬Øx,k
+‚Üº‚àíŒ∏ [i+1]
k

.
we can make use of identity (43) to express the log-normalizing function log‚Üº‚àí
œà [i+1]
k
(xk) itself as a quadratic
function in xk
log‚Üº‚àí
œà [i+1]
k
(xk) = ‚àí1
2x‚ä§
k
‚Üº‚àí
S [i+1]
k
xk + x‚ä§
k‚Üº‚àís [i+1]
k
+‚Üº‚àíŒæ [i+1]
k
= 1
2 log
2œÄ
h‚àí‚áÄ
G [i+1]
xx,k
i‚àí1  +

‚àí1
2x‚ä§
k
‚Üº‚àí
G [i+1]
¬Øx¬Øx,k xk + x‚ä§
k ‚Üº‚àíg [i+1]
¬Øx,k
+‚Üº‚àíŒ∏ [i+1]
k

+ 1
2
h‚àí‚áÄ
G [i+1]
x¬Øx,k xk +‚àí‚áÄg [i+1]
x,k
i‚ä§h‚àí‚áÄ
G [i+1]
xx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
x¬Øx,k xk +‚àí‚áÄg [i+1]
x,k
i
which, after matching terms, leads to
‚Üº‚àí
S [i+1]
k
= ‚Üº‚àí
G [i+1]
¬Øx¬Øx,k ‚àí
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àí
G [i+1]
x¬Øx,k ,
‚Üº‚àís [i+1]
k
= ‚Üº‚àíg [i+1]
¬Øx,k
+
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àíg [i+1]
x,k
,
‚Üº‚àíŒæ [i+1]
k
= ‚Üº‚àíŒ∏ [i+1]
k
+ 1
2 log
2œÄ
h‚Üº‚àí
G [i+1]
xx,k
i‚àí1  + 1
2
h‚Üº‚àíg [i+1]
x,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àíg [i+1]
x,k
.
Given log‚Üº‚àí
œà [i+1]
k
(xk), we can now construct the quadratic potential function ‚Üº‚àí
V [i+1]
k
(xk) that accounts for
the log-measurement contribution according to Assumption 5
‚Üº‚àí
R [i+1]
k
= L[i]
k +
1
1 ‚àíŒ≤
‚Üº‚àí
S [i+1]
k
,
‚Üº‚àír [i+1]
k
= l[i]
k +
1
1 ‚àíŒ≤
‚Üº‚àís [i+1]
k
,
‚Üº‚àíœÅ [i+1]
k
= ŒΩ[i]
k +
1
1 ‚àíŒ≤
‚Üº‚àíŒæ [i+1]
k
.
Finally, for a Gaussian ‚Üº‚àíq [i]
T (xT) and a quadratic ‚Üº‚àí
V [i+1]
T
(xT), we derive a quadratic log‚Üº‚àí
Z [i+1]
T
log‚Üº‚àí
Z [i+1]
T
= log
Z h‚Üº‚àíq [i]
T (xT)
iŒ≤h
exp
n‚Üº‚àí
V [i+1]
T
(xT)
o i1‚àíŒ≤
dxT.
Similar to the proof in Appendix D, we formulate a quadratic over xT and ‚Üº ‚àí
m [i]
T
‚àí1
2
h‚Üº ‚àí
m [i]
T
i‚ä§
x‚ä§
T
 Ô£Æ
Ô£∞
‚Üº‚àí
J [i+1]
xx
‚àí‚Üº‚àí
J [i+1]
xm
‚àí‚Üº‚àí
J [i+1]
mx
‚Üº‚àí
J [i+1]
mm
Ô£π
Ô£ª
Ô£Æ
Ô£∞
‚àí ‚áÄ
m [i]
T
xT
Ô£π
Ô£ª+
h‚Üº ‚àí
m [i]
T
i‚ä§
x‚ä§
T
 Ô£Æ
Ô£∞
‚Üº‚àíj [i+1]
x
‚Üº‚àíj [i+1]
m
Ô£π
Ô£ª+‚Üº‚àíœÑ [i+1]
= Œ≤ log‚Üº‚àíq [i]
T (xT) + (1 ‚àíŒ≤)‚Üº‚àí
V [i+1]
T
(xT),
where by matching the quadratic factors, we get
‚Üº‚àí
J [i+1]
xx
= Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
‚Üº‚àí
J [i+1]
xm
= Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
32

‚Üº‚àí
J [i+1]
mm
= (1 ‚àíŒ≤)‚Üº‚àí
R [i+1]
T
+ Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
‚Üº‚àí
J [i+1]
mx
= Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
,
‚Üº‚àíj [i+1]
x
= 0,
‚Üº‚àíj [i+1]
m
= (1 ‚àíŒ≤)‚Üº‚àír [i+1]
T
,
‚Üº‚àíœÑ [i+1] = (1 ‚àíŒ≤)‚Üº‚àíœÅ [i+1]
T
‚àíŒ≤
2 log
2œÄ‚Üº‚àí
P [i]
T
.
Using the identity (43), we get a log-normalizing constant as a quadratic function over ‚Üº ‚àí
m [i]
T
log‚Üº‚àí
Z [i+1]
T
= ‚àí1
2
h‚Üº ‚àí
m [i]
T
i‚ä§‚Üº‚àí
U [i+1]‚Üº ‚àí
m [i]
T
+
h‚Üº ‚àí
m [i]
T
i‚ä§‚Üº‚àíu [i+1] +‚Üº‚àíŒ∑ [i+1],
where
‚Üº‚àí
U [i+1] = ‚Üº‚àí
J [i+1]
xx
‚àí
h‚Üº‚àí
J [i+1]
mx
i‚ä§h‚Üº‚àí
J [i+1]
mm
i‚àí1‚Üº‚àí
J [i+1]
mx
,
‚Üº‚àíu [i+1] = ‚Üº‚àíj [i+1]
x
‚àí
h‚Üº‚àí
J [i+1]
mx
i‚ä§h‚Üº‚àí
J [i+1]
mm
i‚àí1‚Üº‚àíj [i+1]
m
,
‚Üº‚àíŒ∑ [i+1] = ‚Üº‚àíœÑ [i+1] ‚àí1
2
2œÄ‚Üº‚àí
J [i+1]
mm
i‚àí1 + 1
2
h‚Üº‚àíj [i+1]
m
i‚ä§h‚Üº‚àí
J [i+1]
mm
i‚àí1h‚Üº‚àíj [i+1]
m
i
.
F
Proof of Lemma 1
For a tilted distribution of the form
q[i+1](x) =
h
Z[i+1]i‚àí1 h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
,
with a normalizing constant
Z[i+1] =
Z h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx,
where q[i](x) = N(x | m[i], P [i]), we compute the moments of q[i+1](x) by considering the derivatives of
Z[i+1] with respect to m[i]. Starting with the first-order derivative
‚àÇZ[i+1]
‚àÇm[i]
= Œ≤
h
P [i]i‚àí1 Z
(x ‚àím[i])
h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx
= Œ≤
h
P [i]i‚àí1 Z
x
h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx
‚àíŒ≤
h
P [i]i‚àí1
m[i]
Z h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx
= Œ≤ Z[i+1] h
P [i]i‚àí1 h
E[i+1] [x] ‚àím[i]i
.
After rearranging the terms, we get
E[i+1] [x] = m[i] + 1
Œ≤ P [i]h
Z[i+1]i‚àí1 ‚àÇZ[i+1]
‚àÇm[i]
= m[i] + 1
Œ≤ P [i] ‚àÇlog Z[i+1]
‚àÇm[i]
,
which is an expression for the first moment of q[i+1](x).
Next, we consider the second-order derivative Z[i+1] with respect to m[i]
‚àÇ2Z[i+1]
‚àÇm[i] ‚àÇ

m[i]‚ä§= Œ≤2 h
P [i]i‚àí1 Z
(x ‚àím[i])(x ‚àím[i])‚ä§h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx
 h
P [i]i‚àí1
‚àíŒ≤
h
P [i]i‚àí1 Z h
q[i](x)
iŒ≤h
exp
n
V [i+1](x)
o i1‚àíŒ≤
dx
33

= Œ≤2 Z[i+1] h
P [i]i‚àí1
E[i+1] h
x x‚ä§i h
P [i]i‚àí1
+ Œ≤2 Z[i+1] h
P [i]i‚àí1
m[i] h
m[i]i‚ä§h
P [i]i‚àí1
‚àí2 Œ≤2 Z[i+1] h
P [i]i‚àí1
E[i+1] [x]
h
m[i]i‚ä§h
P [i]i‚àí1
‚àíŒ≤ Z[i+1] h
P [i]i‚àí1
,
which leads to the second raw moment of q[i+1](xk)
E[i+1] h
x x‚ä§i
= ‚àím[i] h
m[i]i‚ä§
+ 2 E[i+1] [x]
h
m[i]i‚ä§
+ 1
Œ≤ P [i] + 1
Œ≤2
h
Z[i+1]i‚àí1
P [i]
‚àÇ2Z[i+1]
‚àÇm[i] ‚àÇ

m[i]‚ä§P [i].
We can now derive the second central moment as follows
V[i+1] [x] = E[i+1] h
x x‚ä§i
‚àíE[i+1] [x] E[i+1] [x]‚ä§
= m[i] h
m[i]i‚ä§
+ 2 1
Œ≤
h
Z[i+1]i‚àí1
P [i] ‚àÇZ[i+1]
‚àÇm[i]
h
m[i]i‚ä§
+ 1
Œ≤ P [i]
+ 1
Œ≤2
h
Z[i+1]i‚àí1
P [i]
‚àÇ2Z[i+1]
‚àÇm[i] ‚àÇ

m[i]‚ä§P [i] ‚àím[i] h
m[i]i‚ä§
‚àí2 1
Œ≤
h
Z[i+1]i‚àí1
m[i]
"
P [i] ‚àÇZ[i+1]
‚àÇm[i]
#‚ä§
‚àí1
Œ≤2
h
Z[i+1]i‚àí2
P [i] ‚àÇZ[i+1]
‚àÇm[i]
"
‚àÇZ[i+1]
‚àÇm[i]
#‚ä§
P [i]
= 1
Œ≤ P [i] + 1
Œ≤2
h
Z[i+1]i‚àí1
P [i]
‚àÇ2Z[i+1]
‚àÇm[i]‚àÇ

m[i]‚ä§P [i] ‚àí1
Œ≤2
h
Z[i+1]i‚àí2
P [i] ‚àÇZ[i+1]
‚àÇm[i]
"
‚àÇZ[i+1]
‚àÇm[i]
#‚ä§
P [i]
= 1
Œ≤ P [i] + 1
Œ≤2 P [i] ‚àÇ2 log Z[i+1]
‚àÇm[i] ‚àÇ

m[i]‚ä§P [i].
G
Proof of Lemma 2
For a tilted forward-Markov conditional distribution of the form
‚àí‚áÄq [i+1]
k
(xk+1 | xk) =
h‚àí‚áÄ
œà [i+1]
k
(xk)
i‚àí1h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
,
with a normalizing function
‚àí‚áÄ
œà [i+1]
k
(xk) =
Z h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1,
where ‚àí‚áÄq [i]
k (xk+1 | xk) = N(xk+1 | ‚àí‚áÄ
F [i]
k
xk +‚àí‚áÄd [i]
k ,‚àí‚áÄ
Œ£ [i]
k ) and fk(xk+1 | xk) ‚âàexp

‚Ñì[i]
f (xk+1, xk)
	
‚Ñì[i]
f (xk+1, xk) ‚âà‚àí1
2
h
x‚ä§
k+1
x‚ä§
k
i
Ô£Æ
Ô£∞C[i]
¬Øx¬Øx,k
‚àíC[i]
¬Øxx,k
‚àíC[i]
x¬Øx,k
C[i]
xx,k
Ô£π
Ô£ª
Ô£Æ
Ô£∞xk+1
xk
Ô£π
Ô£ª+
h
x‚ä§
k+1
x‚ä§
k
i
Ô£Æ
Ô£∞c[i]
¬Øx,k
c[i]
x,k
Ô£π
Ô£ª+ Œ∫[i]
k .
We compute the conditional moments of q[i+1](y | x) from the derivatives of œà[i+1](x) with respect to x.
Starting with the first-order derivative
‚àÇ‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk
= Œ≤
Z ‚àÇlog‚àí‚áÄq [i]
k (xk+1 | xk)
‚àÇxk
h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1
+ (1 ‚àíŒ≤)
Z ‚àÇlog fk(xk+1 | xk)
‚àÇxk
h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1,
34

= Œ≤
Z h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1 h
xk+1 ‚àí‚àí‚áÄ
F [i]
k
xk ‚àí‚àí‚áÄd [i]
k
i h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1
+ (1 ‚àíŒ≤)
Z h
‚àíC[i]
xx,k xk + C[i]
x¬Øx,k xk+1 + c[i]
x,k
i h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
dxk+1,
= ‚àí‚áÄ
œà [i+1]
k
(xk)

(1 ‚àíŒ≤) C[i]
x¬Øx,k + Œ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1
E[i+1] 
xk+1 | xk

‚àí‚àí‚áÄ
œà [i+1]
k
(xk)

(1 ‚àíŒ≤) C[i]
xx,k + Œ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k

xk
+‚àí‚áÄ
œà [i+1]
k
(xk)

(1 ‚àíŒ≤) c[i]
x,k ‚àíŒ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1 ‚àí‚áÄd [i]
k

= ‚àí‚áÄ
œà [i+1]
k
(xk)‚àí‚áÄ
G [i+1]
x¬Øx,k E[i+1] 
xk+1 | xk

‚àí‚àí‚áÄ
œà [i+1]
k
(xk)‚àí‚áÄ
G [i+1]
xx,k x +‚àí‚áÄ
œà [i+1]
k
(xk)‚àí‚áÄg [i+1]
x,k
,
where we use the definition of ‚àí‚áÄ
G [i+1]
x¬Øx,k , ‚àí‚áÄ
G [i+1]
xx,k , and ‚àí‚áÄg [i+1]
x,k
from Proposition 4.
Consequently, the first conditional moment of ‚àí‚áÄq [i+1]
k
(xk+1 | xk) is
E[i+1] 
xk+1 | xk

=
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1‚àí‚áÄ
G [i+1]
xx,k xk ‚àí
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
x,k
+
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àÇlog‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk
.
Now we consider the second-order derivatives of ‚àí‚áÄ
œà [i+1]
k
(xk)(x) with respect to xk
‚àÇ2‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
= Œ≤2
Z h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1 h
xk+1 ‚àí‚àí‚áÄ
F [i]
k
xk ‚àí‚àí‚áÄd [i]
k
i h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
√ó
h
xk+1 ‚àí‚àí‚áÄ
F [i]
k
xk ‚àí‚àí‚áÄd [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k
dxk+1
+ Œ≤ (1 ‚àíŒ≤)
Z h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1 h
xk+1 ‚àí‚àí‚áÄ
F [i]
k
xk ‚àí‚àí‚áÄd [i]
k
i h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
√ó
h
‚àíC[i]
xx,k x + C[i]
x¬Øx,k y + c[i]
x,k
i‚ä§
dxk+1
+ (1 ‚àíŒ≤)2
Z h
‚àíC[i]
xx,k x + C[i]
x¬Øx,k y + c[i]
x,k
i h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
√ó
h
‚àíC[i]
xx,k x + C[i]
x¬Øx,k y + c[i]
x,k
i‚ä§
dxk+1
+ Œ≤ (1 ‚àíŒ≤)
Z h
‚àíC[i]
xx,k x + C[i]
x¬Øx,k y + c[i]
x,k
i h‚àí‚áÄq [i]
k (xk+1 | xk)
iŒ≤
√ó
h
fk(xk+1 | xk) exp
n‚àí‚áÄ
V [i+1]
k+1 (xk+1)
o i1‚àíŒ≤
√ó
h
xk+1 ‚àí‚àí‚áÄ
F [i]
k
xk ‚àí‚àí‚áÄd [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k
dxk+1
‚àíŒ≤‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k
‚àí(1 ‚àíŒ≤)‚àí‚áÄ
œà [i+1]
k
(xk) C[i]
xx,k
35

= ‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i
E[i+1] h
xk+1 x‚ä§
k+1 | xk
i h‚àí‚áÄ
G [i+1]
¬Øxx,k
i
+‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
G [i+1]
xx,k
i h
xk x‚ä§
k
i h‚àí‚áÄ
G [i+1]
xx,k
i‚ä§
+‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄg [i+1]
x,k
i h‚àí‚áÄg [i+1]
x,k
i‚ä§
‚àí2‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i
E[i+1] 
xk+1 | xk

x‚ä§
k
h‚àí‚áÄ
G [i+1]
xx,k
i‚ä§
+ 2‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i
E[i+1] 
xk+1 | xk
 h‚àí‚áÄg [i+1]
x,k
i‚ä§
‚àí2‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
G [i+1]
xx,k
i
xk
h‚àí‚áÄg [i+1]
x,k
i‚ä§
‚àí‚àí‚áÄ
œà [i+1]
k
(xk)
h‚àí‚áÄ
G [i+1]
xx,k
i
.
This gives us an expression for the second raw conditional moment
E[i+1] h
xk+1 x‚ä§
k+1 | xk
i
=
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
xx,k
i h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
+
h‚àí‚áÄ
œà [i+1]
k
(xk)
i‚àí1 h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àÇ2‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
‚àí
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
xx,k
i h
xk x‚ä§
k
i h‚àí‚áÄ
G [i+1]
xx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
‚àí
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄg [i+1]
x,k
i h‚àí‚áÄg [i+1]
x,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
+ 2
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
x¬Øx,k
i
E[i+1] 
xk+1 | xk

x‚ä§
k
h‚àí‚áÄ
G [i+1]
xx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
‚àí2
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
x¬Øx,k
i
E[i+1] 
xk+1 | xk
 h‚àí‚áÄg [i+1]
x,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
+ 2
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
xx,k
i
xk
h‚àí‚áÄg [i+1]
x,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
.
Finally, we can derive the second central moment according to
V[i+1] 
xk+1 | xk

= E[i+1] h
xk+1 x‚ä§
k+1 | xk
i
‚àíE[i+1] 
xk+1 | xk

E[i+1] 
xk+1 | xk
‚ä§
=
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
xx,k
i h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
+
h‚àí‚áÄ
œà [i+1]
k
(xk)
i‚àí1 h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àÇ2‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
‚àí
h‚àí‚áÄ
œà [i+1]
k
(xk)
i‚àí2 h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àÇ‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk
"
‚àÇ‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk
#‚ä§h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
=
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 h‚àí‚áÄ
G [i+1]
xx,k
i h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
+
h‚àí‚áÄ
G [i+1]
x¬Øx,k
i‚àí1 ‚àÇ2 log‚àí‚áÄ
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚àí1
.
H
Proof of Lemma 3
For a tilted reverse-Markov conditional distribution of the form
‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) =
h‚Üº‚àí
œà [i+1]
k
(xk)
i‚àí1h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ≤h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1)
o i1‚àíŒ≤
,
36

with a normalizing function
‚Üº‚àí
œà [i+1]
k
(xk) =
Z h‚Üº‚àíq [i]
k (xk‚àí1 | xk)
iŒ≤h
fk‚àí1(xk | xk‚àí1) exp
n‚Üº‚àí
V [i+1]
k‚àí1 (xk‚àí1)
o i1‚àíŒ≤
dxk‚àí1,
where ‚Üº‚àíq [i]
k (xk‚àí1 | xk) = N(xk‚àí1 | ‚Üº‚àí
F [i]
k
xk +‚Üº‚àíd [i]
k ,‚Üº‚àí
Œ£ [i]
k ) and fk‚àí1(xk | xk‚àí1) ‚âàexp

‚Ñì[i]
f (xk, xk‚àí1)
	
‚Ñì[i]
f (xk, xk‚àí1) ‚âà‚àí1
2
h
x‚ä§
k
x‚ä§
k‚àí1
i
Ô£Æ
Ô£∞C[i]
¬Øx¬Øx,k
‚àíC[i]
¬Øxx,k
‚àíC[i]
x¬Øx,k
C[i]
xx,k
Ô£π
Ô£ª
Ô£Æ
Ô£∞xk
xk‚àí1
Ô£π
Ô£ª+
h
x‚ä§
k
x‚ä§
k‚àí1
i
Ô£Æ
Ô£∞c[i]
¬Øx,k
c[i]
x,k
Ô£π
Ô£ª+ Œ∫[i]
k .
The following steps are analogous to those described in Appendix G, thus we will omit redundant steps to
avoid repetition. Starting with the first-order derivative of ‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇ‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk
= ‚Üº‚àí
œà [i+1]
k
(xk)

(1 ‚àíŒ≤) C[i]
¬Øxx,k + Œ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1
E[i+1] 
xk‚àí1 | xk

‚àí‚Üº‚àí
œà [i+1]
k
(xk)

(1 ‚àíŒ≤) C[i]
¬Øx¬Øx,k + Œ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k

xk
+‚Üº‚àí
œà [i+1]
k
(xk)

(1 ‚àíŒ≤) c[i]
¬Øx,k ‚àíŒ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1 ‚Üº‚àíd [i]
k

= ‚Üº‚àí
œà [i+1]
k
(xk)‚Üº‚àí
G [i+1]
¬Øxx,k E[i+1] 
xk‚àí1 | xk

‚àí‚Üº‚àí
œà [i+1]
k
(xk)‚Üº‚àí
G [i+1]
¬Øx¬Øx,k x +‚Üº‚àí
œà [i+1]
k
(xk)‚Üº‚àíg [i+1]
¬Øx,k
,
where we use the definition of ‚Üº‚àí
G [i+1]
¬Øxx,k , ‚Üº‚àí
G [i+1]
¬Øx¬Øx,k , and ‚Üº‚àíg [i+1]
¬Øx,k
from Proposition 5.
Consequently, the first
conditional moment of ‚Üº‚àíq [i+1]
k
(xk‚àí1 | xk) is
E[i+1] 
xk‚àí1 | xk

=
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1‚Üº‚àí
G [i+1]
¬Øx¬Øx,k xk ‚àí
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1‚Üº‚àíg [i+1]
¬Øx,k
+
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚àÇlog‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk
.
Now we consider the second-order derivatives of ‚Üº‚àí
œà [i+1]
k
(xk)(x) with respect to xk
‚àÇ2‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
= ‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àí
G [i+1]
¬Øxx,k
i
E[i+1] h
xk‚àí1 x‚ä§
k‚àí1 | xk
i h‚Üº‚àí
G [i+1]
x¬Øx,k
i
+‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i h
xk x‚ä§
k
i h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i‚ä§
+‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àíg [i+1]
¬Øx,k
i h‚Üº‚àíg [i+1]
¬Øx,k
i‚ä§
‚àí2‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àí
G [i+1]
¬Øxx,k
i
E[i+1] 
xk‚àí1 | xk

x‚ä§
k
h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i‚ä§
+ 2‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àí
G [i+1]
¬Øxx,k
i
E[i+1] 
xk‚àí1 | xk
 h‚Üº‚àíg [i+1]
¬Øx,k
i‚ä§
‚àí2‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i
xk
h‚Üº‚àíg [i+1]
¬Øx,k
i‚ä§
‚àí‚Üº‚àí
œà [i+1]
k
(xk)
h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i
.
37

This gives us an expression for the second raw conditional moment
E[i+1] h
xk‚àí1 x‚ä§
k‚àí1 | xk
i
=
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
+
h‚Üº‚àí
œà [i+1]
k
(xk)
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚àÇ2‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
‚àí
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i h
xk x‚ä§
k
i h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
‚àí
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àíg [i+1]
¬Øx,k
i h‚Üº‚àíg [i+1]
¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
+ 2
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øxx,k
i
E[i+1] 
xk‚àí1 | xk

x‚ä§
k
h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
‚àí2
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øxx,k
i
E[i+1] 
xk‚àí1 | xk
 h‚Üº‚àíg [i+1]
¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
+ 2
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i
xk
h‚Üº‚àíg [i+1]
¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
.
Finally, we can derive the second central moment according to
V[i+1] 
xk‚àí1 | xk

= E[i+1] h
xk‚àí1 x‚ä§
k‚àí1 | xk
i
‚àíE[i+1] 
xk‚àí1 | xk

E[i+1] 
xk‚àí1 | xk
‚ä§
=
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
+
h‚Üº‚àí
œà [i+1]
k
(xk)
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚àÇ2‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
‚àí
h‚Üº‚àí
œà [i+1]
k
(xk)
i‚àí2 h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚àÇ‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk
"
‚àÇ‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk
#‚ä§h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
=
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 h‚Üº‚àí
G [i+1]
¬Øx¬Øx,k
i h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
+
h‚Üº‚àí
G [i+1]
¬Øxx,k
i‚àí1 ‚àÇ2 log‚Üº‚àí
œà [i+1]
k
(xk)
‚àÇxk ‚àÇx‚ä§
k
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚àí1
.
38

I
Recursive Bayesian Inference Algorithms
Algorithm 1 Backward Recursion of the Forward Proximal Variational Smoother
Require: Damping: Œ≤, first Gaussian marginal: ‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ,
forward affine-Gaussian conditionals: ‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1,
log prior ‚Ñìp: L[i]
0 , l[i]
0 , log measurement ‚Ñìh: L[i]
1:T, l[i]
1:T,
log dynamics ‚Ñìf: C[i]
¬Øx¬Øx,0:T ‚àí1, C[i]
¬Øxx,0:T ‚àí1, C[i]
xx,0:T ‚àí1, c[i]
¬Øx,0:T ‚àí1, c[i]
x,0:T ‚àí1.
1: ‚àí‚áÄ
R [i+1]
T
= L[i]
T ,
‚àí‚áÄr [i+1]
T
= l[i]
T
‚ñ∑initialize backward recursion
2: for k ‚ÜêT ‚àí1, . . . , 0 do
3:
‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k = (1 ‚àíŒ≤)
h
C[i]
¬Øx¬Øx,k +‚àí‚áÄ
R [i+1]
k+1
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1
4:
‚àí‚áÄ
G [i+1]
xx,k = (1 ‚àíŒ≤) C[i]
xx,k + Œ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k
5:
‚àí‚áÄ
G [i+1]
¬Øxx,k = (1 ‚àíŒ≤) C[i]
¬Øxx,k + Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄ
F [i]
k
6:
‚àí‚áÄg [i+1]
¬Øx,k
= (1 ‚àíŒ≤)
h
c[i]
¬Øx,k +‚àí‚áÄr [i+1]
k+1
i
+ Œ≤
h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k
7:
‚àí‚áÄg [i+1]
x,k
= (1 ‚àíŒ≤) c[i]
x,k ‚àíŒ≤
h‚àí‚áÄ
F [i]
k
i‚ä§h‚àí‚áÄ
Œ£ [i]
k
i‚àí1‚àí‚áÄd [i]
k
8:
‚àí‚áÄ
S [i+1]
k
= ‚àí‚áÄ
G [i]
xx,k ‚àí
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄ
G [i+1]
¬Øxx,k
‚ñ∑update log-normalizers
9:
‚àí‚áÄs [i+1]
k
= ‚àí‚áÄg [i]
x,k +
h‚àí‚áÄ
G [i+1]
¬Øxx,k
i‚ä§h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
¬Øx,k
10:
‚àí‚áÄ
R [i+1]
k
= L[i]
k + 1/(1 ‚àíŒ≤)‚àí‚áÄ
S [i+1]
k
‚ñ∑update potential functions
11:
‚àí‚áÄr [i+1]
k
= l[i]
k + 1/(1 ‚àíŒ≤)‚àí‚áÄs [i+1]
k
12:
‚àí‚áÄ
F [i+1]
k
=
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄ
G [i+1]
¬Øxx,k
‚ñ∑update conditional posteriors
13:
‚àí‚áÄd [i+1]
k
=
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1‚àí‚áÄg [i+1]
¬Øx,k
14:
‚àí‚áÄ
Œ£ [i+1]
k
=
h‚àí‚áÄ
G [i+1]
¬Øx¬Øx,k
i‚àí1
15: end for
16: ‚àí‚áÄ
J [i+1]
xx
= (1 ‚àíŒ≤)‚àí‚áÄ
R [i+1]
0
+ Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
17: ‚àí‚áÄ
J [i+1]
xm
= Œ≤
h‚àí‚áÄ
P [i]
0
i‚àí1
18: ‚àí‚áÄj [i+1]
x
= (1 ‚àíŒ≤)‚àí‚áÄr [i+1]
0
19: ‚àí ‚áÄ
m [i+1]
0
=
h‚àí‚áÄ
J [i+1]
xx
i‚àí1 h‚àí‚áÄj [i+1]
x
+‚àí‚áÄ
J [i+1]
xm
‚àí ‚áÄ
m [i]
0
i
‚ñ∑update boundary marginal
20: ‚àí‚áÄ
P [i+1]
0
=
h‚àí‚áÄ
J [i+1]
xx
i‚àí1
21: return ‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
, ‚àí‚áÄ
F [i+1]
0:T ‚àí1,‚àí‚áÄd [i+1]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i+1]
0:T ‚àí1,‚àí‚áÄ
R [i+1]
0:T
,‚àí‚áÄr [i+1]
0:T
,‚àí‚áÄ
S [i+1]
0:T ‚àí1,‚àí‚áÄs [i+1]
0:T ‚àí1.
39

Algorithm 2 Forward Recursion of the Forward Proximal Variational Smoother
Require: First Gaussian marginal: ‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
,
forward affine-Gaussian conditionals: ‚àí‚áÄ
F [i+1]
0:T ‚àí1,‚àí‚áÄd [i+1]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i+1]
0:T ‚àí1.
1: for k ‚Üê0, . . . , T ‚àí1 do
2:
‚àí ‚áÄ
m [i+1]
k+1
= ‚àí‚áÄ
F [i+1]
k
‚àí ‚áÄ
m [i+1]
k
+‚àí‚áÄd [i+1]
k
‚ñ∑update marginals
3:
‚àí‚áÄ
P [i+1]
k+1
= ‚àí‚áÄ
F [i+1]
k
‚àí‚áÄ
P [i+1]
k
h‚àí‚áÄ
F [i+1]
k
i‚ä§
+‚àí‚áÄ
Œ£ [i+1]
k
4: end for
5: return ‚àí ‚áÄ
m [i+1]
0:T
,‚àí‚áÄ
P [i+1]
0:T
.
Algorithm 3 Optimal Damping for Forward Proximal Variational Smoother
Require: Maximum multiplier: Œ±max, minimum multiplier: Œ±min,
initial multiplier: Œ±0, first Gaussian marginal: ‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ,
forward affine-Gaussian conditionals: ‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1,
log prior ‚Ñìp, log measurement ‚Ñìh, log dynamics ‚Ñìf.
1: Œ± ‚ÜêŒ±0
2: repeat
3:
Œ≤ ‚ÜêŒ±/(1 + Œ±)
4:
‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
,‚àí‚áÄ
F [i+1]
0:T ‚àí1,‚àí‚áÄd [i+1]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i+1]
0:T ‚àí1 ‚Üê
Forward Conditionals (Œ≤, ‚Ñìp, ‚Ñìh, ‚Ñìf,‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ,‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1)
‚ñ∑Algorithm 1
5:
‚àí ‚áÄ
m [i+1]
0:T
,‚àí‚áÄ
P [i+1]
0:T
‚Üê
Forward Marginals (‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
,‚àí‚áÄ
F [i+1]
0:T ‚àí1,‚àí‚áÄd [i+1]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i+1]
0:T ‚àí1)
‚ñ∑Algorithm 2
6:
if Œµ ‚àíDKL
h‚àí‚áÄq [i+1] || ‚àí‚áÄq [i]i
> 0 then
7:
Œ±max ‚ÜêŒ±
8:
Œ± ‚Üê‚àöŒ± ¬∑ Œ±min
‚ñ∑reduce multiplier
9:
else if Œµ ‚àíDKL
h‚àí‚áÄq [i+1] || ‚àí‚áÄq [i]i
< 0 then
10:
Œ±min ‚ÜêŒ±
11:
Œ± ‚Üê‚àöŒ± ¬∑ Œ±max
‚ñ∑increase multiplier
12:
end if
13: until Œµ ‚àíDKL
h‚àí‚áÄq [i+1] || ‚àí‚áÄq [i]i
‚âà0
‚ñ∑gradient vanishes
14: Œ≤ ‚ÜêŒ±/(1 + Œ±)
15: return Œ≤
40

Algorithm 4 Forward Recursion of the Reverse Proximal Variational Smoother
Require: Damping: Œ≤, last Gaussian marginal: ‚Üº ‚àí
m [i]
T ,‚Üº‚àí
P [i]
T ,
reverse affine-Gaussian conditionals: ‚Üº‚àí
F [i]
0:T ‚àí1,‚Üº‚àíd [i]
0:T ‚àí1,‚Üº‚àí
Œ£ [i]
0:T ‚àí1,
log prior ‚Ñìp(x0): L[i]
0 , l[i]
0 , log measurement ‚Ñìh(xk): L[i]
1:T, l[i]
1:T,
log dynamics ‚Ñìf: C[i]
¬Øx¬Øx,0:T ‚àí1, C[i]
¬Øxx,0:T ‚àí1, C[i]
xx,0:T ‚àí1, c[i]
¬Øx,0:T ‚àí1, c[i]
x,0:T ‚àí1.
1: ‚Üº‚àí
R [i+1]
0
= L[i]
0 ,
‚Üº‚àír [i+1]
0
= l[i]
0
‚ñ∑initialize forward recursion
2: for k ‚Üê1, . . . , T do
3:
‚Üº‚àí
G [i+1]
¬Øx¬Øx,k = (1 ‚àíŒ≤) C[i]
¬Øx¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k
4:
‚Üº‚àí
G [i+1]
xx,k = (1 ‚àíŒ≤)
h
C[i]
xx,k‚àí1 +‚Üº‚àí
R [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1
5:
‚Üº‚àí
G [i+1]
x¬Øx,k = (1 ‚àíŒ≤) C[i]
x¬Øx,k‚àí1 + Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àí
F [i]
k
6:
‚Üº‚àíg [i+1]
¬Øx,k
= (1 ‚àíŒ≤) c[i]
¬Øx,k‚àí1 ‚àíŒ≤
h‚Üº‚àí
F [i]
k
i‚ä§h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k
7:
‚Üº‚àíg [i+1]
x,k
= (1 ‚àíŒ≤)
h
c[i]
x,k‚àí1 +‚Üº‚àír [i+1]
k‚àí1
i
+ Œ≤
h‚Üº‚àí
Œ£ [i]
k
i‚àí1‚Üº‚àíd [i]
k
8:
‚Üº‚àí
S [i+1]
k
= ‚Üº‚àí
G [i+1]
¬Øx¬Øx,k ‚àí
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àí
G [i+1]
x¬Øx,k
‚ñ∑update log-normalizers
9:
‚Üº‚àís [i+1]
k
= ‚Üº‚àíg [i+1]
¬Øx,k
+
h‚Üº‚àí
G [i+1]
x¬Øx,k
i‚ä§h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àíg [i+1]
x,k
10:
‚Üº‚àí
R [i+1]
k
= L[i]
k + 1/(1 ‚àíŒ≤)‚Üº‚àí
S [i+1]
k
‚ñ∑update potential functions
11:
‚Üº‚àír [i+1]
k
= l[i]
k + 1/(1 ‚àíŒ≤)‚Üº‚àís [i+1]
k
12:
‚Üº‚àí
F [i+1]
k
=
h‚Üº‚àí
G [i+1]
x2,k
i‚àí1‚Üº‚àí
G [i+1]
x¬Øx,k
‚ñ∑update conditional posteriors
13:
‚Üº‚àíd [i+1]
k
=
h‚Üº‚àí
G [i+1]
xx,k
i‚àí1‚Üº‚àíg [i+1]
x,k
14:
‚Üº‚àí
Œ£ [i+1]
k
=
h‚Üº‚àí
G [i+1]
xx,k
i‚àí1
15: end for
16: ‚Üº‚àí
J [i+1]
mm
= (1 ‚àíŒ≤)‚Üº‚àí
R [i+1]
T
+ Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
17: ‚Üº‚àí
J [i+1]
xm
= Œ≤
h‚Üº‚àí
P [i]
T
i‚àí1
18: ‚Üº‚àíj [i+1]
m
= (1 ‚àíŒ≤)‚Üº‚àír [i+1]
T
19: ‚Üº ‚àí
m [i+1]
T
=
h‚Üº‚àí
J [i+1]
mm
i‚àí1 h‚Üº‚àíj [i+1]
m
+‚Üº‚àí
J [i+1]
xm
‚Üº ‚àí
m [i]
T
i
‚ñ∑update boundary marginal
20: ‚Üº‚àí
P [i+1]
T
=
h‚Üº‚àí
J [i+1]
mm
i‚àí1
21: return ‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
,‚Üº‚àí
F [i+1]
1:T
,‚Üº‚àíd [i+1]
1:T
,‚Üº‚àí
Œ£ [i+1]
1:T
,‚Üº‚àí
R [i+1]
0:T
,‚Üº‚àír [i+1]
0:T
,‚Üº‚àí
S [i+1]
1:T
,‚Üº‚àís [i+1]
1:T
.
41

Algorithm 5 Backward Recursion of the Reverse Proximal Variational Smoother
Require: Last Gaussian marginal: ‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
,
reverse affine-Gaussian conditionals: ‚Üº‚àí
F [i+1]
1:T
,‚Üº‚àíd [i+1]
1:T
,‚Üº‚àí
Œ£ [i+1]
1:T
.
1: for k ‚ÜêT, . . . , 1 do
2:
‚Üº ‚àí
m [i+1]
k‚àí1
= ‚Üº‚àí
F [i+1]
k
‚Üº ‚àí
m [i+1]
k
+‚Üº‚àíd [i+1]
k
‚ñ∑update marginals
3:
‚Üº‚àí
P [i+1]
k‚àí1
= ‚Üº‚àí
F [i+1]
k
‚Üº‚àí
P [i+1]
k
h‚Üº‚àí
F [i+1]
k
i‚ä§
+‚Üº‚àí
Œ£ [i+1]
k
4: end for
5: return ‚Üº ‚àí
m [i+1]
0:T
,‚Üº‚àí
P [i+1]
0:T
.
Algorithm 6 Optimal Damping for the Reverse Proximal Variational Smoother
Require: Maximum multiplier: Œ±max, minimum multiplier: Œ±min,
initial multiplier: Œ±0, last Gaussian marginal: ‚Üº ‚àí
m [i]
T ,‚Üº‚àí
P [i]
T ,
reverse affine-Gaussian conditionals: ‚Üº‚àí
F [i]
1:T,‚Üº‚àíd [i]
1:T,‚Üº‚àí
Œ£ [i]
1:T,
log prior ‚Ñìp, log measurement ‚Ñìh, log dynamics ‚Ñìf.
1: Œ± ‚ÜêŒ±0
2: repeat
3:
Œ≤ ‚ÜêŒ±/(1 + Œ±)
4:
‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
,‚Üº‚àí
F [i+1]
1:T
,‚Üº‚àíd [i+1]
1:T
,‚Üº‚àí
Œ£ [i+1]
1:T
‚Üê
Reverse Conditionals (Œ≤, ‚Ñìp, ‚Ñìh, ‚Ñìf,‚Üº ‚àí
m [i]
T ,‚Üº‚àí
P [i]
T ,‚Üº‚àí
F [i]
1:T,‚Üº‚àíd [i]
1:T,‚Üº‚àí
Œ£ [i]
1:T)
‚ñ∑Algorithm 4
5:
‚Üº ‚àí
m [i+1]
0:T
,‚Üº‚àí
P [i+1]
0:T
‚Üê
Reverse Marginals (‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
,‚Üº‚àí
F [i+1]
1:T
,‚Üº‚àíd [i+1]
1:T
,‚Üº‚àí
Œ£ [i+1]
1:T
)
‚ñ∑Algorithm 5
6:
if Œµ ‚àíDKL
h‚Üº‚àíq [i+1] || ‚Üº‚àíq [i]i
> 0 then
7:
Œ±max ‚ÜêŒ±
8:
Œ± ‚Üê‚àöŒ± ¬∑ Œ±min
‚ñ∑reduce multiplier
9:
else if Œµ ‚àíDKL
h‚Üº‚àíq [i+1] || ‚Üº‚àíq [i]i
< 0 then
10:
Œ±min ‚ÜêŒ±
11:
Œ± ‚Üê‚àöŒ± ¬∑ Œ±max
‚ñ∑increase multiplier
12:
end if
13: until Œµ ‚àíDKL
h‚Üº‚àíq [i+1] || ‚Üº‚àíq [i]i
‚âà0
‚ñ∑gradient vanishes
14: Œ≤ ‚ÜêŒ±/(1 + Œ±)
15: return Œ≤
42

Algorithm 7 Forward Proximal Variational Smoother
Require: First Gaussian marginal: ‚àí ‚áÄ
m [0]
0
,‚àí‚áÄ
P [0]
0
,
forward affine-Gaussian conditionals: ‚àí‚áÄ
F [0]
0:T ‚àí1,‚àí‚áÄd [0]
0:T ‚àí1,‚àí‚áÄ
Œ£ [0]
0:T ‚àí1.
1: i ‚Üê0
2: while not converged do
3:
‚àí ‚áÄ
m [i]
0:T,‚àí‚áÄ
P [i]
0:T ‚ÜêForward Marginals (‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ,‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1)
‚ñ∑Algorithm 2
4:
‚Ñìp, ‚Ñìh, ‚Ñìf ‚ÜêApproximate Model (p, f, h,‚àí ‚áÄ
m [i]
0:T,‚àí‚áÄ
P [i]
0:T)
‚ñ∑Definition 2/3
5:
Œ≤ ‚ÜêOptimal Damping (‚Ñìp, ‚Ñìh, ‚Ñìf,‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ,‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1)
‚ñ∑Algorithm 3
6:
‚àí ‚áÄ
m [i+1]
0
,‚àí‚áÄ
P [i+1]
0
,‚àí‚áÄ
F [i+1]
0:T ‚àí1,‚àí‚áÄd [i+1]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i+1]
0:T ‚àí1 ‚Üê
Forward Conditionals (Œ≤, ‚Ñìp, ‚Ñìh, ‚Ñìf,‚àí ‚áÄ
m [i]
0 ,‚àí‚áÄ
P [i]
0 ,‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1)
‚ñ∑Algorithm 1
7:
i ‚Üêi + 1
8: end while
9: return ‚àí ‚áÄ
m [‚àû]
0:T ,‚àí‚áÄ
P [‚àû]
0:T .
Algorithm 8 Reverse Proximal Variational Smoother
Require: Last Gaussian marginal: ‚Üº ‚àí
m [0]
T
,‚Üº‚àí
P [0]
T
,
reverse affine-Gaussian conditionals: ‚Üº‚àí
F [0]
1:T ,‚Üº‚àíd [0]
1:T ,‚Üº‚àí
Œ£ [0]
1:T .
1: i ‚Üê0
2: while not converged do
3:
‚Üº ‚àí
m [i]
0:T,‚Üº‚àí
P [i]
0:T ‚ÜêReverse Marginals (‚Üº ‚àí
m [i]
0 ,‚Üº‚àí
P [i]
0 ,‚Üº‚àí
F [i]
1:T,‚Üº‚àíd [i]
1:T,‚Üº‚àí
Œ£ [i]
1:T)
‚ñ∑Algorithm 5
4:
‚Ñìp, ‚Ñìh, ‚Ñìf ‚ÜêApproximate Model (p, f, h,‚Üº ‚àí
m [i]
0:T,‚Üº‚àí
P [i]
0:T)
‚ñ∑Definition 2/3
5:
Œ≤ ‚ÜêOptimal Damping (‚Ñìp, ‚Ñìh, ‚Ñìf,‚Üº ‚àí
m [i]
T ,‚Üº‚àí
P [i]
T ,‚Üº‚àí
F [i]
1:T,‚Üº‚àíd [i]
1:T,‚Üº‚àí
Œ£ [i]
1:T)
‚ñ∑Algorithm 6
6:
‚Üº ‚àí
m [i+1]
T
,‚Üº‚àí
P [i+1]
T
,‚Üº‚àí
F [i+1]
1:T
,‚Üº‚àíd [i+1]
1:T
,‚Üº‚àí
Œ£ [i+1]
1:T
‚Üê
Reverse Conditionals (Œ≤, ‚Ñìp, ‚Ñìh, ‚Ñìf,‚Üº ‚àí
m [i]
T ,‚Üº‚àí
P [i]
T ,‚Üº‚àí
F [i]
1:T,‚Üº‚àíd [i]
1:T,‚Üº‚àí
Œ£ [i]
1:T)
‚ñ∑Algorithm 4
7:
i ‚Üêi + 1
8: end while
9: return ‚Üº ‚àí
m [‚àû]
0:T ,‚Üº‚àí
P [‚àû]
0:T .
43

Algorithm 9 Marginals of the Hybrid Proximal Variational Smoother
Require: Damping: Œ≤, prior Gaussian marginal: m[i]
1:T ‚àí1, P [i]
1:T ‚àí1,
forward log-normalizing functions: ‚àí‚áÄ
S [i+1]
1:T ‚àí1,‚àí‚áÄs [i+1]
1:T ‚àí1,
reverse potential functions: ‚Üº‚àí
R [i+1]
1:T ‚àí1,‚Üº‚àír [i+1]
1:T ‚àí1.
1: for k ‚Üê1, . . . , T ‚àí1 do
2:
P [i+1]
k
=
h
(1 ‚àíŒ≤)
h‚Üº‚àí
R [i+1]
k
+‚àí‚áÄ
S [i+1]
k
i
+ Œ≤
h
P [i]
k
i‚àí1i‚àí1
‚ñ∑update marginals
3:
m[i+1]
k
= P [i+1]
k
h
(1 ‚àíŒ≤)
h‚Üº‚àír [i+1]
k
+‚àí‚áÄs [i+1]
k
i
+ Œ≤
h
P [i]
k
i‚àí1
m[i]
k
i
4: end for
5: return m[i+1]
1:T ‚àí1, P [i+1]
1:T ‚àí1.
Algorithm 10 Hybrid Proximal Variational Smoother
Require: Initial Gaussian marginals: m[0]
0:T , P [0]
0:T ,
forward affine-Gaussian conditionals: ‚àí‚áÄ
F [0]
0:T ‚àí1,‚àí‚áÄd [0]
0:T ‚àí1,‚àí‚áÄ
Œ£ [0]
0:T ‚àí1,
reverse affine-Gaussian conditionals: ‚Üº‚àí
F [0]
1:T ,‚Üº‚àíd [0]
1:T ,‚Üº‚àí
Œ£ [0]
1:T .
1: i ‚Üê0
2: while not converged do
3:
‚Ñìp, ‚Ñìh, ‚Ñìf ‚ÜêApproximate Model (p, f, h, m[i]
0:T, P [i]
0:T)
‚ñ∑Definition 2/3
4:
Œ≤ ‚ÜêOptimal Damping (‚Ñìp, ‚Ñìh, ‚Ñìf, m[i]
0 , P [i]
0 ,‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1)
‚ñ∑Algorithm 3
5:
m[i+1]
0
, P [i+1]
0
,‚àí‚áÄ
F [i+1]
0:T ‚àí1,‚àí‚áÄd [i+1]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i+1]
0:T ‚àí1,‚àí‚áÄ
S [i+1]
1:T ‚àí1,‚àí‚áÄs [i+1]
1:T ‚àí1 ‚Üê
Forward Normalizers (Œ≤, ‚Ñìp, ‚Ñìh, ‚Ñìf, m[i]
0 , P [i]
0 ,‚àí‚áÄ
F [i]
0:T ‚àí1,‚àí‚áÄd [i]
0:T ‚àí1,‚àí‚áÄ
Œ£ [i]
0:T ‚àí1)
‚ñ∑Algorithm 1
6:
m[i+1]
T
, P [i+1]
T
,‚Üº‚àí
F [i+1]
1:T
,‚Üº‚àíd [i+1]
1:T
,‚Üº‚àí
Œ£ [i+1]
1:T
,‚Üº‚àí
R [i+1]
1:T ‚àí1,‚Üº‚àír [i+1]
1:T ‚àí1 ‚Üê
Reverse Potentials (Œ≤, ‚Ñìp, ‚Ñìh, ‚Ñìf, m[i]
T , P [i]
T ,‚Üº‚àí
F [i]
1:T,‚Üº‚àíd [i]
1:T,‚Üº‚àí
Œ£ [i]
1:T)
‚ñ∑Algorithm 4
7:
m[i+1]
1:T ‚àí1, P [i+1]
1:T ‚àí1 ‚Üê
Hybrid Marginals (Œ≤, m[i]
1:T ‚àí1, P [i]
1:T ‚àí1,‚àí‚áÄ
S [i+1]
1:T ‚àí1,‚àí‚áÄs [i+1]
1:T ‚àí1,‚Üº‚àí
R [i+1]
1:T ‚àí1,‚Üº‚àír [i+1]
1:T ‚àí1)
‚ñ∑Algorithm 9
8:
i ‚Üêi + 1
9: end while
10: return m[‚àû]
0:T , P [‚àû]
0:T .
44
