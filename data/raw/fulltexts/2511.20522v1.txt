Classifying seizure generation mechanisms:
A critical transitions framework.
New insights from combining brain voltage recordings, models of critical transitions and machine learning.
Andrew Flynn1,2,*, Cian McCafferty3, Klaus Lehnertz4,5,6, FrancÂ¸ois David7, Vincenzo
Crunelli8, William P. Marnane2,9, and Sebastian Wieczorek1
1School of Mathematical Sciences, University College Cork, T12 XF62 Cork, Ireland.
2INFANT Research Centre, University College Cork, T12 DC4A Cork, Ireland.
3Department of Anatomy & Neuroscience, University College Cork, Cork, Ireland.
4Department of Epileptology, University of Bonn Medical Centre, 53127 Bonn, Germany.
5Helmholtz-Institute for Radiation and Nuclear Physics, University of Bonn, 53115 Bonn, Germany.
6Interdisciplinary Center for Complex Systems, University of Bonn, 53175 Bonn, Germany.
7Center for Interdisciplinary Research in Biology, Coll`ege de France, 75005 Paris, France
8Department of Pharmacology and Neuroscience, Faculty of Medicine, University of Lisbon, Lisbon, Portugal
9Electrical and Electronic Engineering, University College Cork, T12 YF78 Cork, Ireland.
*andrewflynn@ucc.ie
ABSTRACT
Understanding how the brain switches from normal activity to an epileptic seizure is essential for improving seizure therapy,
yet the underlying mechanisms remain largely unknown. In particular, seizure onset can be described as a critical transition
(CT), but there is no consensus on whether (i) bifurcation-induced, (ii) noise-induced, or (iii) bifurcation/noise-induced CTs
are responsible. To clarify this, we develop a versatile CT-classification framework that can be applied to seizures in both
animals and humans. First, we identify a canonical mathematical model which displays CTs that closely resemble voltage
recordings of real seizures and can be of the three types mentioned above. We then identify distinctive properties of each
CT-type in the modelâ€™s output and use them to train a machine learning CT-type classifier. Finally, we apply the model-trained
classifier to voltage recordings from epileptic rodents. We find that the largest proportion of analysed seizures are classified
as noise-induced CTs. This challenges the conventional view that seizures are predominantly bifurcation-induced and could
inform new therapeutic strategies for seizures.
1
arXiv:2511.20522v1  [math.DS]  25 Nov 2025

Introduction
Epileptic seizures are states of high-amplitude, synchronous electrical activity in the brain with defined onset and offset1.
Seizures have a wide range of associated symptoms but all constitute a large change in brain state and have a commensurate
impact on the life of a person with epilepsy2. Why the brain suddenly changes from normal activity to an epileptic seizure is
unknown3. Understanding this process may lead to the more optimal use of existing therapeutic interventions and advance the
development of new ones that either prevent onset of seizure or accelerate its end. A sudden and large change in the state of a
complex system exactly fits Ashwin et al.â€™s4 definition of a critical transition (CT). The mathematical theory of CTs is perhaps
best known for describing and foreseeing sudden and large changes in climate and ecological systems5,6. It has provided a
better understanding of key tipping elements in the Earth system and their impacts on our lives, and has also informed global
policymaking7,8 Inspired by its impact on the environmental sciences, we apply the theory of CTs to analyse seizure activity in
the brain. Specifically, we model the onset of seizure activity as a CT between two different states of the brain, the non-seizure
state (NS state) and the seizure state (S state).
Despite many previous efforts, no consensus has emerged on what types of CT are predominantly responsible for the
onset of seizures in the brain. Scheffer et al.9 reported that, in an example of an electroencephalography (EEG) time series
with seizure activity taken from McSharry et al.10, the variance of the time series increased prior to seizure onset. From the
theory of CTs, this increase in variance is a signature of â€˜critical slowing downâ€™ (CSD), a phenomenon that occurs prior to a
bifurcation-induced CT, thus indicating that the seizure occurred due to a bifurcation taking place in the brain. Meisel and
Kuehn11 later analysed an EEG time series from eight adults with epilepsy and found similar increases in variance before only
two out of eight seizures. They also showed that such increases occur prior to a subcritical Hopf bifurcation in a mathematical
model. Milanowski and Suffczynski12 expanded on this approach by analysing hundreds of EEG time series of various seizure
types in adult humans. They restricted their analysis to CTs due to crossing different Hopf bifurcations and concluded that
seizures start without common signatures of such CTs. Jirsa et al.13 used a different modelling approach, based on noise-free
dynamic bifurcations, to argue that seizures occur due to slow passage through a saddle-node bifurcation. In more recent times,
further opposing reports emerged based on large-scale investigations of EEG time series: Wilkat et al.14 found no evidence of
CSD before seizures, whereas Maturana et al.15 did. From a CT point of view, if a bifurcation does not appear to be responsible
for the onset of the seizure, then another likely candidate is a noise-induced CT, which have long been theorised to play a role in
seizure onset16. However, very little is known about the typical signatures of noise-induced CTs in a time series, nor how these
signatures manifest themselves in voltage recordings of seizure activity17.
In this paper, we approach the question of â€˜how are seizures generated?â€™ from a broader perspective. Specifically, we go
beyond the realm of bifurcation-induced CTs and hypothesise that the mechanism of seizure generation is a CT from the NS to S
state that can be: noise-induced, bifurcation-induced, or a combination of both. In other words, we consider these three CT-types
as potential seizure generation mechanisms. To test this hypothesis, we introduce a new CT-based framework, summarised in
four steps in Table 1, to analyse voltage recordings of real seizure activity. This framework consists of an algorithm to detect
CTs between NS and S states, and a machine learning classifier to distinguish between the different CT-types. This enables us
to test whether, at a given moment in time near a CT to the S state, the brain: (i) operates within a multistable regime whereby
noise alone in the form of exogenous or endogenous disturbances can cause it to cross some threshold and trigger a seizure,
(ii) slowly drifts towards and then past a bifurcation point that generates the seizure, or (iii) experiences a combination of
both (i) and (ii). Seen in this light, our framework can be used not only to address a fundamental question regarding seizure
generation mechanisms, but also to inform future seizure prevention and treatment strategies - once we know the CT types that
are responsible, we can relate them to different types of neural activity, and adapt therapeutic strategies accordingly.
The structure of the Results section reflects our four-step framework from Table 1. In the first step, we identify a set of two
differential equations (an augmented Bautin bifurcation normal form) as a canonical mathematical model capable of generating
the three CT-types listed above. We then show that the parameters of our mathematical model can be tuned so that its output
resembles real seizure activity in voltage recordings taken from inside the brain of epileptic rodents. More specifically, we use
local field potential (LFP) measurements of neural activity in Genetic Absence Epilepsy Rats from Strasbourg (GAERS) that
were obtained and annotated by McCafferty et al.18. We refer to these LFP measurements as the â€˜voltage recordingsâ€™ throughout
the paper. We choose to work with GAERS because their absence seizures are fully generalised and morphologically distinctive
in EEG19 compared to other seizures which may be focal or have variable EEG signatures20. In other words, GAERS seizures
are both stable in amplitude and frequent in occurrence21, making them ideal for investigating seizure onset dynamics and for
constructing our framework. We see this as a proof of concept and an important step towards analysing more complex human
seizures. In the second step, we construct an algorithm based on the concept of a non-ideal relay to detect CTs between the NS
and S states in noisy time series. We test the algorithm on our mathematical model and, crucially, show that it detects CTs
between NS and S states in the actual voltage recordings from different GAERS in excellent agreement with expert annotations.
We then identify several distinctive properties in time series of the mathematical modelâ€™s output that are associated with each
CT-type. This is one of the most important components of our study. It allows us, in the third step, to train a support vector
2/42

Step 1: Build mathematical
model of CTs with real
seizure characteristics
Construct a canonical mathematical model that displays CTs from its NS to S state that
have characteristics of real seizure activity and can be of three types:
(i) bifurcation-induced, (ii) noise-induced, (iii) bifurcation/noise-induced.
Step 2: Detect
model-generated CTs and
identify distinctive
properties of each CT-type
Use the mathematical model from Step 1 to generate many examples of CTs from its NS
to S state, where the CT-type is known. Develop a CT detection algorithm to detect CTs
from the NS to S state in the modelâ€™s output and group them by their type. For detected
CTs of each type, identify their characteristic time series properties.
Step 3: Build CT-type
classifier
Train a machine learning classifier using the characteristic time series properties
identified in the modelâ€™s output for each CT-type in Step 2. Optimise the classifier by
maximising its ability to correctly classify the types of CT from the NS to S state in the
unseen modelâ€™s output.
Step 4: Detect real seizures
and classify their CT-type.
Use the CT detection algorithm from Step 2 to detect CTs from the NS to S state in the
voltage recordings, where the CT-type is not known. For each detected CT, calculate the
characteristic time series properties identified in Step 2 and use these properties as an
input for the model-trained classifier from Step 3 to classify the CT-type.
Table 1. A summary of our proposed four-step framework for classifying CTs that are responsible for the onset of seizures in
the voltage recordings of brain activity as: (i) bifurcation-induced, (ii) noise-induced or (iii) bifurcation/noise-induced.
machine classifier (SVM classifier) to distinguish between the three CT-types from the NS to S state in a time series, without
any prior knowledge of CT-types. In the fourth and final step, we use the model-trained SVM classifier to identify the types of
CT that are responsible for real seizure onsets in the voltage recordings from different GAERS.
Our findings demonstrate that: (i) seizures are generated by different types of CT and (ii) noise-induced CTs appear to be
the dominant mechanism responsible for generating the seizures we analysed. These results align with recent experiments on
seizure termination in GAERS22, challenge the conventional view that seizures occur predominantly due to bifurcation-induced
CTs, and highlight the need to account for noise-induced CTs when developing seizure therapies. Furthermore, our framework
is versatile in the following ways: (i) it can use other, possibly more realistic, mathematical models of seizure activity to train
the SVM, (ii) it can classify CT-types responsible for other types of seizures, including those in humans, and (iii) it can also be
applied to similar instabilities found in complex systems beyond the brain.
Results
The main outcome of this paper is a CT-type classification framework consisting of:
â€¢ A CT-based formulation of how the brain transitions between non-seizure and seizure states.
â€¢ An algorithm that detects seizure onset and offset times in voltage recordings of real seizure activity.
â€¢ A CT-type classifier based on explainable machine learning tools that classifies seizure generation mechanisms in voltage
recordings of real seizure activity.
The main result is as follows: Comparing animal seizure data with a mathematical model within our classification framework,
we found that seizures are predominantly generated by noise-induced CTs. This aligns with recent experiments, challenges
recent trends that refer to seizures as bifurcation-induced CTs, and could inform future seizure therapies.
Step 1: A mathematical model with real seizure characteristics and three CT types
In this section, we describe three important characteristics of real seizure activity in the voltage recordings from GAERS. We
then introduce a canonical mathematical model that mimics these three characteristics and also show the three CT types.
Characteristics of voltage recordings of real seizure activity
Before following the 4 steps in Table 1, we briefly describe three important characteristics of real seizure activity in the voltage
recordings that we tune our mathematical model to mimic. These characteristics allow us to (i) use the same algorithm to detect
3/42

0
5
10
15
20
25
-0.1
0.0
0.1
t [s]
LFP [mV]
(a)
100
101
102
103
residence times [s]
-1
-3
-5
log10(prob. density) [arb.]
(b)
Rat S
S state
NS state
100
101
102
103
residence times [s]
(c)
Rat T
100
101
102
103
residence times [s]
(d)
Rat K
Figure 1. Characteristics of real seizure activity in GAERS. (a) An example of voltage recordings of real seizure activity,
where the vertical lines indicate times of electrical seizure (orange) onset and (purple) offset according expert annotations.
(b)-(d) The probability density of residence times in the S and NS states of rat S, T, and K respectively.
CTs between the NS and S states in the modelâ€™s output and in the voltage recordings, and (ii) train a machine learning classifier
using the modelâ€™s output and then apply it to the voltage recordings.
Figure 1 (a) shows a typical example of real seizure activity in GAERS, taken from recording session â€˜S5Aâ€™ (rat S, day 5 of
recording, Ağ‘¡â„recording session). This voltage recording was obtained for ğ‘¡â‰¥0 with time resolution of 0.001 seconds via the
procedure outlined in M1 of the Methods section. The vertical orange and purple lines in Fig. 1 (a) indicate the electrical seizure
onset and offset times, respectively, according to the expert. For the ğ‘–ğ‘¡â„seizure, we denote the onset time by ğ‘¡(ğ‘–)
1 > 0 and the
offset time by ğ‘¡(ğ‘–)
2 > ğ‘¡(ğ‘–)
1 . The same notation is used to denote times where our algorithm detects CTs between the NS and S states
in our mathematical model. Note that the (orange) electrical seizure onset is characterised by the sudden change in the voltage
recordings from a small-amplitude and weakly-correlated oscillation to a larger-amplitude and highly-correlated oscillation.
The opposite occurs for the (purple) electrical seizure offset. These sudden changes and the timescale of the oscillations are two
important characteristics that we tune our mathematical model to mimic as closely as possible.
The seizure in Fig. 1 (a) is just one of several seizures that have been annotated by the expert in this recording session.
Looking at all seizures in the session, there is significant variation in the length of the different intervals of real seizure activity
(ğ‘¡(ğ‘–)
2 âˆ’ğ‘¡(ğ‘–)
1 ), which we refer to as residence times in the S state, and real non-seizure activity (ğ‘¡(ğ‘–+1)
1
âˆ’ğ‘¡(ğ‘–)
2 ), which we refer to as
residence times in the NS state. Figures 1 (b)-(d) shows the probability density of residence times in the S and NS states from
â€˜rat Sâ€™, â€˜rat Tâ€™, and â€˜rat Kâ€™, using all the annotations of electrical seizure onset and offset times provided by the expert. The
information presented here is based on 621, 1593, and 1143 seizures from each respective rat. Note that the residence times in
the S state are noticeably shorter than those in the NS state, and that residence times for both states have a similar range for
different rats. The range of residence times is the third important characteristic that we tune our mathematical model to mimic
for noise-induced CTs.
A canonical mathematical model
In most mathematical models of seizure dynamics, NS states are represented by noisy oscillations about a stable equilibrium
point near 0 mV, and S states are represented by noisy motion along a non-stationary stable state, such as a stable limit cycle,
with amplitude much larger than the amplitude of the noisy oscillations near 0 mV. If the model is well designed, NS states
mimic the small-amplitude and weakly-correlated oscillations, and S states mimic the larger-amplitude and highly-correlated
oscillations seen in voltage recordings such as Fig. 1. These mathematical models can be separated into the following groups:
process-based models which are derived from a number of physical processes occurring in the brain to produce seizure-like
4/42

0
10
20
30
-1
0
1
x
t
BCT
(a)
NS state
S state
0
10
20
30
t
NCT
(b)
NS state
S state
0
10
20
30
t
BNCT
(c)
NS state
S state
SLC
SLC
0
5
10
-0.1
0
0.1
LFP [mV]
t[s]
BCT
(d)
0
5
10
t[s]
NCT
(e)
0
5
10
t[s]
BNCT
(f)
Figure 2. Examples of the three CT-types from the NS to S state in the modelâ€™s output. (a) Bifurcation-induced CT due to
drifting across a supercritical bifurcation point Hâˆ’. (b) Noise-induced CT in the bistability region between the stable NS and S
states. (c) Bifurcation/noise-induced CT due to a combination of drifting across a subcritical bifurcation point, H+, and
bistability. The solid (dashed) lines represent the stable (unstable) NS and S states of the noise-free system (1), and Sğ¿ğ¶in (c)
denotes the fold of limit cycles bifurcation point. The coloured time series show the outputs of our model (5). Lower row shows
corresponding CTs in voltage recordings classified via our framework (see Fig. S-20 in [30] for further details).
activity16,23â€“25, phenomenological models which are designed to mimic real seizure activity in voltage recordings without
taking into account the underlying processes in the brain11â€“13,26â€“28, and a mixture of both29. CTs from the NS to S state in
these models can be related to processes in the brain and largely categorised according to three CT-types, namely:
â€¢ Bifurcation-induced CTs (BCTs): The brain begins in the NS state, which is the only stable state. However, the NS state
becomes unstable when the brain slowly drifts across a supercritical Hopf bifurcation point due to changing environmental
or structural parameters. This causes the brain to transition to the stable S state, which gradually emerges from the
bifurcation point and is the only stable state beyond it. See Fig. 2 (a).
â€¢ Noise-induced CTs (NCTs): The brain is bistable, meaning that it has two stable states, NS and S, that coexist. A sudden
transition from the NS to S state is triggered by noise alone, in the form of endogenous or exogenous disturbances. See
Fig. 2 (b).
â€¢ Bifurcation/noise-induced CTs (BNCTs): The brain is bistable as it slowly drifts towards a subcritical Hopf bifurcation,
at which point the NS state loses stability and the S state becomes the only stable state. A combination of noise and the
imminent instability of the NS state causes a sudden transition to the S state. See Fig. 2 (c).
We now introduce a phenomenological model in the form of two coupled stochastic differential equations. This model has
equivalent NS and S states and mimics the three important characteristics of the voltage recordings specified in the previous
section. Most importantly, it can also generate the three CT-types listed above. In the first step, we introduce and analyse the
noise-free version of the model to identify candidates for NS and S states, as well as CTs between them. In the second step, we
include random noise for direct comparison with voltage recordings of real seizure activity.
We consider the Bautin (generalised Hopf) bifurcation normal form31 for the time evolution of a complex-valued variable ğ‘§
that we augment with a shear term,
ğ‘‘ğ‘§
ğ‘‘ğ‘¡= ğ›¾(
(ğœ‡+ğ‘–ğœ”) ğ‘§+ğ‘ (1+ğ‘–ğœ) |ğ‘§|2 ğ‘§âˆ’ğ‘|ğ‘§|4 ğ‘§),
(1)
where ğœ‡âˆˆâ„and ğ‘ âˆˆâ„are bifurcation parameters, ğœ”âˆˆâ„is the frequency of small-amplitude oscillation, ğœâˆˆâ„in the
augmented term is the shear parameter, and ğ‘= 1 remains fixed throughout our investigations and is no longer explicitly referred
5/42

âˆ’3
âˆ’2
âˆ’1
0
1
Âµ
âˆ’1
0
1
2
3
s
GH
SLC
H+
Hâˆ’
PB
PBN
PN
non-seizure
seizure or
non-seizure
seizure
Figure 3. Two-parameter stability diagram for System (2) in the (ğœ‡,ğ‘ ) parameter plane showing the regions of (blue) stable NS
state, (red) stable S state, and (green) bistability between the stable NS and S states. Sğ¿ğ¶denotes the curve of saddle-node of
limit cycle bifurcations, Hâˆ’and H+ denote the curves of supercritical and subcritical Hopf bifurcations, respectively, and GH
denotes the Bautin (generalised Hopf) bifurcation point. The parameter paths, ğ‘ƒğµand ğ‘ƒğµğ‘, and the point ğ‘ƒğ‘, denote the
values of ğœ‡and ğ‘ used to generate BCTs, BNCTs, and NCTs, respectively, in the modelâ€™s output. Note that the bifurcation
curves do not depend on any other system parameters.
to. The timescale parameter ğ›¾> 0 allows us to adjust the timescale of the solutions. For a fixed ğ‘ we choose ğœso that these
parameters are always of the same sign. Although the shear term is not present in normal forms, it is essential for analysing a
systemâ€™s response to external disturbances32â€“34. In our mathematical model, it enables us to produce seizure-like activity that
more closely resembles real seizure activity seen in the voltage recordings.
We now discuss the existence of candidates for NS and S states and then conduct linear stability and bifurcation analysis to
uncover different parameter paths in the (ğœ‡,ğ‘ ) parameter plane that give rise to the different types of CT between these states.
To simplify the discussion, we transform Eq. (1) to a polar coordinate system by setting ğ‘§= ğ‘Ÿğ‘’ğ‘–ğœƒwhere ğ‘Ÿâ‰¥0 and ğœƒâˆˆ[0,2ğœ‹),
ğ‘‘ğ‘Ÿ
ğ‘‘ğ‘¡= ğ›¾ğ‘Ÿ(ğœ‡+ğ‘ ğ‘Ÿ2 âˆ’ğ‘Ÿ4),
ğ‘‘ğœƒ
ğ‘‘ğ‘¡= ğ›¾(ğœ”+ğœğ‘ ğ‘Ÿ2).
(2)
Note that the ğ‘‘ğ‘Ÿâˆ•ğ‘‘ğ‘¡equation is decoupled from the ğ‘‘ğœƒâˆ•ğ‘‘ğ‘¡equation in the sense that it does not depend on ğœƒand can thus be
solved on its own. An equilibrium point ğ‘Ÿğ‘’for the ğ‘‘ğ‘Ÿâˆ•ğ‘‘ğ‘¡equation corresponds to an equilibrium point in the full system if
ğ‘Ÿğ‘’= 0, and to a limit cycle in the full system if ğ‘Ÿğ‘’> 0. Thus, System (2) has an equilibrium point, ğ‘’1, that is located at the origin
and exists for all parameter values. Noisy motion around ğ‘’1 corresponds to an NS state. Furthermore, System (2) has two limit
cycles, ğ¿+ and ğ¿âˆ’, whose radii are given by
ğ‘Ÿ+ =
âˆš
1
2
(
ğ‘ +
âˆš
ğ‘ 2 +4ğœ‡
)
and
ğ‘Ÿâˆ’=
âˆš
1
2
(
ğ‘ âˆ’
âˆš
ğ‘ 2 +4ğœ‡
)
,
(3)
respectively. ğ¿+ exists for ğœ‡> âˆ’ğ‘ 2âˆ•4 if ğ‘ > 0 and for ğœ‡> 0 if ğ‘ < 0, whereas ğ¿âˆ’exists for âˆ’ğ‘ 2âˆ•4 < ğœ‡< 0 and ğ‘ > 0. Noisy
motion around ğ¿+ corresponds to an S state.
Linear stability analysis of equilibria for the ğ‘‘ğ‘Ÿâˆ•ğ‘‘ğ‘¡equation reveals the following. The equilibrium ğ‘’1 is stable for ğœ‡< 0 and
unstable for ğœ‡> 0. The limit cycle ğ¿+ is stable while ğ¿âˆ’is unstable. Bifurcations of ğ‘’1, ğ¿âˆ’, and ğ¿+ for changes in ğœ‡and ğ‘ are
shown in a two-parameter bifurcation diagram in Fig. 3. For ğ‘ < 0 and as ğœ‡changes from negative to positive values, ğ‘’1 becomes
unstable and ğ¿+ is born in a supercritical Hopf bifurcation Hâˆ’that occurs when crossing the solid part of the vertical red line at
ğœ‡= 0; see also the black solution branches in Fig. 2 (a). For ğ‘ > 0 and as ğœ‡changes from positive to negative values, ğ‘’1 becomes
6/42

BCT
BNCT
NCT
ğœ‡: bifurcation parameter
ğœ‡(ğ‘¡) = âˆ’2+ğ‘¡âˆ•20
for ğ‘¡âˆˆ[0,60]
ğœ‡(ğ‘¡) = âˆ’2+ğ‘¡âˆ•20
for ğ‘¡âˆˆ[0,60]
âˆ’0.22
ğ‘ : bifurcation parameter
âˆ’1
1
1
ğœ: shear
âˆ’1
1
1
ğœˆ: noise level
0.18
0.18
0.18
ğœ”: oscillation freq.
1.3
1.3
1.3
ğ›¾: time scale
10
10
10
Table 2. The parameter values chosen for our model in Eqs. (5) to generate different CT-types.
stable and the unstable ğ¿âˆ’is born in a subcritical Hopf bifurcation H+ that occurs when crossing the dashed part of the vertical
red line at ğœ‡= 0. By decreasing ğœ‡further, ğ¿+ and ğ¿âˆ’collide and disappear in a saddle-node of limit cycles bifurcation ğ‘†ğ¿ğ¶
that occurs when crossing the black solid curve given by ğœ‡= âˆ’ğ‘ 2âˆ•4 and ğ‘ > 0; see also the black solution branches in Fig. 2 (c).
Note that ğ‘’1, ğ¿+, and ğ¿âˆ’coincide at a special Bautin (generalised Hopf) bifurcation point GH at (ğœ‡,ğ‘ ) = (0,0).
As emphasised by the different coloured regions in Fig. 3, the bifurcations of ğ‘’1, ğ¿âˆ’, and ğ¿+ define three qualitatively
different types of activity. In the blue region, where ğ‘’1 is the only stable state, there is non-seizure-like activity. In the orange
region, where ğ¿+ is the only stable state, there is seizure-like activity. In the green region of bistability, where both ğ‘’1 and ğ¿+
are stable, either non-seizure-like or seizure-like activity can occur.
In Fig. 3 we also indicate two parameter paths and one special point. The paths ğ‘ƒğµand ğ‘ƒğµğ‘are used to generate many
BCTs and BNCTs from the NS to S state, respectively. This is done by repeatedly increasing ğœ‡from âˆ’2 to 1 for ğ‘ = 1 and
ğ‘ = âˆ’1, respectively, according to
ğœ‡(ğ‘¡) = âˆ’2+ğ‘¡âˆ•20,
(4)
for ğ‘¡â‰¥0. The point ğ‘ƒğ‘at ğ‘ = 1 and ğœ‡= âˆ’0.22 is used to generate a long time series containing many NCTs between the NS
and S states.
In order to provide a direct comparison with the real seizure activity seen in voltage recordings such as Fig. 1, we transform
Eq. (1) into its Cartesian form by setting ğ‘§= ğ‘¥+ğ‘–ğ‘¦, where ğ‘¥and ğ‘¦are real-valued variables, and further augment it by including
additive noise terms to generate the three different CTs mentioned above,
ğ‘‘ğ‘¥
ğ‘‘ğ‘¡= ğ›¾
(
ğœ‡ğ‘¥+ğ‘ (ğ‘¥2 +ğ‘¦2)
(ğ‘¥âˆ’ğœğ‘¦)âˆ’ğœ”ğ‘¦âˆ’(ğ‘¥2 +ğ‘¦2)2 ğ‘¥
)
+ğœˆğœ‚ğ‘¥(ğ‘¡),
ğ‘‘ğ‘¦
ğ‘‘ğ‘¡= ğ›¾
(
ğœ‡ğ‘¦+ğ‘ (ğ‘¥2 +ğ‘¦2)
(ğ‘¦+ğœğ‘¥)+ğœ”ğ‘¥âˆ’(ğ‘¥2 +ğ‘¦2)2 ğ‘¦
)
+ğœˆğœ‚ğ‘¦(ğ‘¡).
(5)
Here, ğœ‚ğ‘¥(ğ‘¡) and ğœ‚ğ‘¦(ğ‘¡) are independent Gaussian random variables (white Gaussian noise with zero mean and unit variance)
and ğœˆis the noise level (standard deviation of the noise). To generate solutions for ğ‘¥and ğ‘¦at a given time ğ‘¡, we discretise
System (5) according to the Euler-Maruyama method for a fixed time step ğ›¿ğ‘¡. In our numerical experiments, we set ğ›¿ğ‘¡= 0.001,
ğ‘¥(0) = ğ‘¥0 = 0.1, and ğ‘¦(0) = ğ‘¦0 = 0.1. This value of ğ›¿ğ‘¡is chosen to prevent numerical errors from arising when ğœis relatively
large. The choice of initial condition (ğ‘¥0,ğ‘¦0) ensures that all simulations of System (5) begin from the NS state.
Throughout the rest of the paper, we refer to System (5) as the â€˜modelâ€™, and to a time series of ğ‘¥(ğ‘¡) as the â€˜modelâ€™s outputâ€™.
Furthermore, the units of quantities related to voltage recordings are millivolts (mV) and time is in seconds (s), whereas the
units of quantities related to the mathematical model are arbitrary. This applies to quantities specified in the text, in tables, and
in figures and their captions.
Seizures in the modelâ€™s output resemble voltage recordings of real seizure activity
Earlier in the paper, we identified three characteristics of the real seizure activity that we want our mathematical model to
mimic:
7/42

( i ) The amplitude and correlation of oscillation in the NS and S states.
( ii ) The range of residence times in the NS and S states.
(iii) Intrinsic time scales of the NS and S states.
When tuning the model parameters we found that trade-offs emerge and we have to make choices on what characteristics should
be prioritised. Optimising ğœ‡for characteristic (i) makes it difficult to find a ğœˆto mimic characteristic (ii). Similarly, optimising
ğ›¾for characteristic (iii) makes it difficult to mimic characteristic (ii), and vice-versa. We find that characteristics (i) and (ii)
are the most critical for training the SVM classifier to accurately classify seizure generation mechanisms in time series. We
therefore choose to prioritise characteristics (i) and (ii) and accept a trade-off in characteristic (iii). The model parameters in
Table 2 have been chosen accordingly.
Characteristic (i): The top row in Fig. 4 shows that the modelâ€™s output (a) closely resembles real voltage recordings in GAERS
(b). Characteristic (i) is reasonably well satisfied since the relative change in the amplitude of oscillation for a CT from the NS
to S state in the modelâ€™s output is similar to the seizure onset in the voltage recordings. The same can be said forf the CT from
the S to NS state and the seizure offset. Figures 4 (a) and (b) also highlight one benefit of including shear, quantified by ğœin the
model: the model exhibits variations in local maxima/minima observed in real voltage recordings beyond what can be achieved
with noise alone.
Characteristic (ii): In the bottom row, Fig. 4 (e) shows the probability density of residence times in the S state for rat S (blue)
and the model (orange). Figure 4 (f) shows the same in the NS state. These residence times were obtained by applying our CT
detection algorithm, introduced in the next subsection, to the voltage recordings for rat S, and to the modelâ€™s output containing
several NCTs between the NS and S states. Figures 4 (e) and (f) show that the model exhibits a range of residence times in the
NS and S state, which is the same or wider than those in the voltage recordings. However, the model tends to spend longer in
the S state than the voltage recordings, resulting in some differences in the magnitude of the probability density.
Characteristic (iii): In the middle row, Figs. 4 (c) and (d) show spectrograms of the time series from Figs. 4 (a) and (b),
respectively. While in the S state, the modelâ€™s output has a component at the same frequency as the voltage recording. However,
the dominant component in the voltage recording is about twice that of the modelâ€™s output.
Step 2: Detecting CTs and identifying distinctive properties of each CT-type in the modelâ€™s output
The aim of this paper is to classify the types of CT that are responsible for seizure onset in voltage recordings of real seizure
activity using a classifier trained on the modelâ€™s output. To ensure consistency, we now develop a single algorithm that detects
CTs between the NS and S states in both the voltage recordings and the modelâ€™s output.
To detect CTs in a noisy time series, one typically specifies a threshold and defines a CT to a different state as a crossing of
the threshold. However, such a single threshold approach has its shortcomings. First, a single threshold can be crossed multiple
times in a short time interval due to noise, leading to false CTs. Second, if one of the states is oscillatory, the threshold can be
crossed multiple times while the system is in that state, also leading to false CTs. Both shortcomings arise in the case of CTs to
the S state and we address them as follows:
â€¢ We use the concept of a â€˜non-ideal relayâ€™ with two thresholds35 and define CTs between the NS and S states in terms of
successive crossings of the two thresholds.
â€¢ We use a â€˜moving window analysisâ€™ to assess how long the system remains in the new state after successive crossings of
the two thresholds.
Constructing a CT detection algorithm
The time resolution of the voltage recordings determines the time step size ğ›¿, i.e., the time interval between two consecutive
points in the time series. The modelâ€™s output is generated with the same time step size. In the case of voltage recordings in
GAERS, we have ğ›¿= 0.001.
Additionally, we introduce six algorithm parameters: the â€œonâ€ threshold ğ›¼> 0, the â€œoffâ€ threshold 0 < ğ›½< ğ›¼, the size of the
moving window ğœğ‘¤> 0, the time step size of the moving window ğ›¿â‰¤Î” â‰¤ğœğ‘¤, the minimum time duration of larger-amplitude
oscillations, ğœS > ğœğ‘¤, expressed as ğœS = ğ‘›SÎ”+ğœğ‘¤, where ğ‘›S is an integer, and the minimum time duration of lower-amplitude
oscillations, ğœNS â‰¥ğœS, expressed similarly as ğœNS = ğ‘›NSÎ”+ğœğ‘¤, where ğ‘›NS is an integer. We list these parameters and specify
their values in Table 3.
The starting point: We start in the NS state, where |ğ‘¥(ğ‘¡)| < ğ›½for the time duration of at least ğœNS.
The moving window: When the system is in the NS state and |ğ‘¥(ğ‘¡)| exceeds ğ›¼at time ğ‘¡= ğ‘¡ğ‘—, the moving window is activated
and |ğ‘¥(ğ‘¡)| is examined within consecutive windows of duration ğœğ‘¤that are shifted in time by Î”, starting with [ğ‘¡ğ‘—, ğ‘¡ğ‘—+ğœğ‘¤], then
[ğ‘¡ğ‘—+Î”, ğ‘¡ğ‘—+ğœğ‘¤+Î”], [ğ‘¡ğ‘—+2Î”, ğ‘¡ğ‘—+ğœğ‘¤+2Î”], and so on. The moving window is deactivated in two cases: (i) the system is in
the NS state, the window is activated, but the algorithm does not detect a CT to the S state, and (ii) the system is in the S state
8/42

0
10
20
30
-1.0
0.0
1.0
x [arb.]
t [arb.]
(a)
0
10
20
30
-0.1
0.0
0.1
t [s]
LFP [mV]
(b)
0
10
20
30
0
3
6
Freq. [arb.]
t [arb.]
(c)
-30
0
Intensity [dB]
0
10
20
30
0
3
6
Freq. [Hz]
t [s]
(d)
-50
-35
Intensity [dB]
100
101
102
103
residence times in S state
-1
-3
-5
log10(prob. density) [arb.]
(e)
Rat S
Model
100
101
102
103
residence times in NS state
-1
-3
-5
log10(prob. density) [arb.]
(f)
Rat S
Model
Figure 4. A comparison of the characteristics of (a) the modelâ€™s output and (b) the actual voltage recordings for rat S. (c)-(d)
The spectrograms of the time series from (a) and (b), respectively. (e)-(f) The probability density of residence times in the S and
NS state, respectively, for (blue) rat S and (orange) the modelâ€™s output.
and the algorithm detects a CT to the NS state.
Critical transitions: The algorithm detects a CT from the NS to S state at time ğ‘¡= ğ‘¡1 if:
(a1) The system is in the NS state just before ğ‘¡1.
(a2) |ğ‘¥(ğ‘¡)| exceeds ğ›¼at time ğ‘¡= ğ‘¡1, i.e., |ğ‘¥(ğ‘¡1)| = ğ›¼and |ğ‘¥(ğ‘¡1 +ğ›¿)| > ğ›¼.
(a3) Each of the ğ‘›S consecutive positions of the moving window contains an |ğ‘¥(ğ‘¡)| > ğ›½.
The algorithm detects a CT from the S to NS state at time ğ‘¡= ğ‘¡2 if:
(b1) The system is in the S state just before ğ‘¡2.
(b2) |ğ‘¥(ğ‘¡)| falls below ğ›½at time ğ‘¡= ğ‘¡2, i.e., |ğ‘¥(ğ‘¡2)| = ğ›½and |ğ‘¥(ğ‘¡2 +ğ›¿)| < ğ›½.
(b3) Each of the ğ‘›NS consecutive positions of the moving window contains no |ğ‘¥(ğ‘¡)| â‰¥ğ›¼.
In other words, the algorithm detects a CT from the NS to S state if |ğ‘¥(ğ‘¡)| exceeds the upper threshold ğ›¼and then continues to
exceed the lower threshold ğ›½frequently enough for a period of at least ğœS. Similarly, the algorithm detects a CT from the S to
NS state if |ğ‘¥(ğ‘¡)| falls below the lower threshold ğ›½and then does not exceed the upper threshold ğ›¼for a period of at least ğœNS.
In Fig. 5 we provide an illustrative example of how the above algorithm detects CTs in the modelâ€™s output from Fig. 4 (a) using
the algorithm parameters specified in Table 3.
Almost-occurring critical transitions: The algorithm detects an almost-occurring CT from the NS to S state at time ğ‘¡= Ìƒğ‘¡1 if
9/42

-1.0
0.0
1.0
Potential CT at t = t1
Moving window activated
t1
Ï„w
t1 + Ï„S
x [arb.]
(a)
-1.0
0.0
1.0
Window shifted by mâˆ†
t1 + mâˆ†
Ï„w
x [arb.]
(b)
22
25
-1.0
0.0
1.0
CT from the NS to S state
detected at t = t1
t1 + nSâˆ†
Ï„w
x [arb.]
t [arb.]
(c)
Potential CT at t = t2
Moving window already active
t2
Ï„w
t2 + Ï„NS
(d)
Window shifted by mâˆ†
t2 + mâˆ†
Ï„w
(e)
37
40
CT from the S to NS state
detected at t = t2
Moving window deactivated
t2 + nNSâˆ†
Ï„w
t [arb.]
(f)
Figure 5. Illustrating how the CT detection algorithm works using the modelâ€™s output from Fig. 4 (a) as an example. (a)-(c)
Detecting a CT from the NS to S state at time ğ‘¡= ğ‘¡1. (d)-(f) Detecting a CT from the S to NS state at time ğ‘¡= ğ‘¡2. The detection
algorithm parameters are specified in Table 3, and we used ğ‘š= 300 in (b) and (e). The red and green horizontal lines indicate
the thresholds ğ›¼and ğ›½, respectively.
(a1) and (a2) are satisfied but (a3) is not. Similarly, the algorithm detect an almost-occurring CT from the S to NS state at time
ğ‘¡= Ìƒğ‘¡2 if (b1) and (b2) are satisfied but (b3) is not.
Applying the CT detection algorithm to the modelâ€™s output and voltage recordings
We refer to S1 and S2 in [30] for examples of CTs and almost-occurring CTs detected by the algorithm in both the modelâ€™s
output and the voltage recordings, and examples of artefacts in the voltage recordings and how we prevent the algorithm from
detecting these as CTs. In S3 and S4 in [30] we show how the shear parameter ğœinfluences the detected residence times in the
NS and S states and the times at which the algorithm detects CTs in the modelâ€™s output. In S5 in [30] we describe how the
algorithm parameters are tuned to maximise the agreement between the CTs detected by the algorithm and the CTs annotated
by the expert in the voltage recordings. These parameters are specified in Table 3.
Identifying distinctive properties of the modelâ€™s output that differentiate between the three CT-types
When a system is perturbed away from its stable state â€” whether stationary or not â€” it will attempt to return to this state.
However, the closer the system is to a bifurcation point at which the state loses stability, the less stable it becomes and the
longer it takes to return. Therefore, as a noisy system gradually approaches a bifurcation point, one would expect to see an
increase in the variance and autocorrelation of the observed time series. This phenomenon is known as â€˜critical slowing downâ€™
(CSD) and the accompanying increases in the variance and autocorrelation are commonly referred to as â€˜early warning signalsâ€™
(EWSs). Scheffer et al.9 recognised that EWSs can be detected in time series recordings of a single observable of a complex
system, allowing researchers to detect CSD without knowledge of the entire state of the system. This approach has been widely
successful in studies of climate and environmental systems, enabling researchers to forecast when BCTs may occur36,37. For
10/42

Time series and detection algorithm parameters
Modelâ€™s output
Voltage recordings
ğ›¿: time step size
0.001
0.001
ğ›¼: on threshold
0.55
[0.03, 0.1]
ğ›½: off threshold
0.45
ğ›¼- 0.01
ğœğ‘¤: size of the moving window
1
1
Î”: time step size of the moving window
0.001
0.001
ğœS: minimum time duration of the S state
2
2
ğœNS: minimum time duration of the NS state
5
3
Table 3. The values of the time step size and CT-detection algorithm parameters used for the modelâ€™s output and voltage
recordings. ğ›¼is chosen differently for each voltage recording session according to the method outlined in S5 in [30].
recent publications on this area we refer to Dakos et al.17 and Ashwin et al.38. Furthermore, in terms of classifying CTs in a
noisy time series, Bury et al.39 classified different bifurcation-induced CTs based on characteristic increases in the variance and
autocorrelation of the time series prior to the different CTs.
CTs in the brain occur on timescales that are several orders of magnitude faster than those in climate and environmental
sciences and could also be noise-induced. Therefore, our approach differs from previous work on EWSs in three key ways:
First, we analyse the time series around a CT, that is, shortly before and shortly after a CT, rather than just prior to a CT. This is
because our primary goal is to identify the CT-type rather than to foresee one that is about to occur within seconds. (We will
discuss EWSs for the onset of seizures and their predictive power in future work.) Second, we consider the following properties
of the time series and their slopes around a CT from the NS to S state; see also Table 5:
(TSP1) Gaussian variance (GV).
(TSP2) Base-10 logarithm of the Gaussian variance (log10GV).
(TSP3) Lag-1 autocorrelation (AC).
(TSP4) Base-10 logarithm of the Gaussian variance of the lag-1 autocorrelation (log10GV(AC)).
We collectively refer to (TSP1)-(TSP4) as the â€˜time series propertiesâ€™ and denote them by TSPs. We also consider the â€˜slopes of
the TSPsâ€™, and denote them by ğ‘š(TSPs, ğ‘¡ğ‘š), where ğ‘¡ğ‘šis the chosen time interval over which the slope is calculated. In other
words, the ğ‘š(TSPs, ğ‘¡ğ‘š) carry information about the trends in TSPs over the past ğ‘¡ğ‘šseconds. Third, and most importantly, we
use the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) to classify CTs from the NS to S state as (i) bifurcation-induced, (ii) noise-induced, or (iii)
bifurcation/noise-induced.
Selection criteria for CTs: We select 100 examples of each CT-type from the NS to S state in the modelâ€™s output according
to the following criteria, bearing in mind that the algorithm detects each CT at a time ğ‘¡= ğ‘¡1,
â€¢ The model is in the NS state for all ğ‘¡âˆˆ[ğ‘¡1 +ğ‘‡âˆ’, ğ‘¡1
), where ğ‘‡âˆ’= âˆ’30.
|ğ‘‡âˆ’| is the length of the time series considered prior to each detected CT.
â€¢ The model is in the S state for all ğ‘¡âˆˆ[ğ‘¡1, ğ‘¡1 +ğ‘‡+], where ğ‘‡+ = 10.
ğ‘‡+ is the length of the time series considered after each detected CT.
Note that these criteria exclude some CTs detected by the algorithm. For instance, if insufficient time is spent in the NS state
prior to a CT, the CT is excluded; see S10 in [30] for further information. Furthermore, while our CT-type classifier requires
sufficient time prior to a CT, there is also a biological reason to exclude such CTs: the brain may not have sufficient time to fully
return to the NS state beyond what can be seen in the voltage recording.
11/42

0.0
0.2
0.4
0.6
GV
(a)
BCT
BNCT
NCT
0
1
2
m(GV, 4) x10âˆ’1
(b)
0
4
8
m(GV, 8) x10âˆ’2
(c)
0
4
8
m(GV, 12) x10âˆ’2
(d)
-3
-2
-1
log10GV
(e)
-4
0
4
8
m(log10GV, 4)
(f)
-2
0
2
4
m(log10GV, 8) x10âˆ’1
(g)
-2
0
2
4
m(log10GV, 12) x10âˆ’1
(h)
0.97
0.98
0.99
1.00
AC
(i)
-4
-2
0
2
4
m(AC, 4) x10âˆ’3
(j)
-1
0
1
2
m(AC, 8) x10âˆ’3
(k)
-1
0
1
2
m(AC, 12) x10âˆ’3
(l)
-10
-5
0
5
-10
-8
-6
log10GV(AC)
T
(m)
-10
-5
0
5
-1
0
1
m(log10GV(AC), 4)
T
(n)
-10
-5
0
5
-8
-4
0
4
m(log10GV(AC), 8) x10âˆ’1
T
(o)
-10
-5
0
5
-8
-4
0
4
m(log10GV(AC), 12) x10âˆ’1
T
(p)
Figure 6. Mean values (solid curves) and range of values (shaded regions surrounding each curve) of the different TSPs and
ğ‘š(TSPs, ğ‘¡ğ‘š) (specified on each vertical axis) before and after (in blue) BCTs, (in red) NCTs, and (in green) BNCTs plotted
versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1, for the ğ‘¡1 detected by our algorithm for each CT. The first column corresponds to the TSPs, the remaining
three columns correspond to the ğ‘š(TSPs, ğ‘¡ğ‘š) for ğ‘¡ğ‘š= 4, 8, and 12 respectively.
Computing TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) near selected CTs: When computing TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) near each CT, it is
convenient to work with a new time, ğ‘‡= ğ‘¡âˆ’ğ‘¡1, where ğ‘‡âˆˆ[ğ‘‡âˆ’,ğ‘‡+]
and each CT is detected at ğ‘‡= 0. A single value of each
TSP is computed over a time interval denoted by ğ‘¡ğ‘¤and referred to as the â€˜window lengthâ€™. We consider a fixed ğ‘¡ğ‘¤= 1 for
both the modelâ€™s output and voltage recordings; see Fig. S-12 in [30] for justification. A single value of each ğ‘š(TSPs, ğ‘¡ğ‘š) is
computed over a longer time interval, ğ‘¡ğ‘š> ğ‘¡ğ‘¤, referred to as the â€˜slope lengthâ€™, and we examine different values of ğ‘¡ğ‘š. Thus, for
a given choice of ğ‘‡âˆ’and ğ‘‡+, we obtain values of (TSP1)-(TSP3) in the interval [ğ‘‡âˆ’+ğ‘¡ğ‘¤,ğ‘‡+] = [ğ‘‡âˆ’+1,ğ‘‡+], (TSP4) in the
interval [ğ‘‡âˆ’+2ğ‘¡ğ‘¤,ğ‘‡+] = [ğ‘‡âˆ’+2,ğ‘‡+], and all ğ‘š(TSPs, ğ‘¡ğ‘š) in the interval [ğ‘‡âˆ’+2ğ‘¡ğ‘¤+ğ‘¡ğ‘š,ğ‘‡+] = [ğ‘‡âˆ’+2+ğ‘¡ğ‘š,ğ‘‡+]; see M2
of the Methods section and S6 in [30] for additional details.
We obtain all the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) for the 100 selected examples of each CT-type, summarising the results in Fig. 6.
We plot the mean values (solid curves) and spread (shaded bands) as a function of ğ‘‡âˆˆ[âˆ’10,10], where the most prominent
changes occur, and use a different colour for each CT-type. More specifically, in the first column of Fig. 6 we plot the mean
value taken by each of the TSPs versus ğ‘‡. In the remaining three columns, we plot the mean value taken by each of the ğ‘š(TSPs,
ğ‘¡ğ‘š) versus ğ‘‡for ğ‘¡ğ‘š= 4 (second column), ğ‘¡ğ‘š= 8 (third column), and ğ‘¡ğ‘š= 12 (fourth column). In Fig. S-11 in [30], we show
how the TSPs behave around a single example of each CT-type.
Differentiating between the three CT types based on TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š): While some aspects of Fig. 6 are similar to
previous findings by Milanowski and Suffczynski12, we focus on how the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) can be used to distinguish
between the three CT-types rather than identify any EWSs, which we leave for future work. For instance, we see in Fig. 6 (c) that
three separate (non-overlapping) bands of ğ‘š(GV, ğ‘¡ğ‘š) emerge near ğ‘‡= 5, where each band corresponds to a different CT-type.
This allows one to visually distinguish between each CT type. Comparing Figs. 6 (b), (c), and (d), we see that as ğ‘¡ğ‘šincreases,
12/42

-2
0
2
m(AC, 8)âˆ—
0
2
m(GV, 8)âˆ—
(a)
T = -4
BCT
BNCT
NCT
-2
0
2
m(AC, 8)âˆ—
0
2
(b)
T = 0
0
2
m(log10GV(AC), 8)âˆ—
-1
0
1
(c)
T = 4
Figure 7. An illustrative example of how a two-feature SVM works, with the features specified on each axis. The different
coloured points correspond to (blue) the true BCTs, (green) the true BNCTs, and (red) the true NCTs. This information is
known to us but not to the SVM. The three different coloured regions are where the SVM classifies CTs as (blue) BCTs, (green)
BNCTs, and (red) NCTs. The accuracy of the classification depends on the chosen features and whether classification is
performed (a) before a detected CT, (b) at a detected CT, or (c) after a detected CT.
the width of these bands decreases, thereby improving our ability to visually distinguish between different CT types. As another
example, we can distinguish between the three CT types by combining the following two observations when ğ‘‡> 2: BNCTs can
be distinguished from BCTs and NCTs based on the separation between corresponding GV bands in Fig. 6 (a), and NCTs can
be distinguished from BCTs and BNCTs based on the separation between corresponding log10GV(AC) bands in Fig. 6 (m). By
considering several TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) simultaneously we may distinguish between the three CT-types at earlier ğ‘‡values,
even before a CT.
Step 3: Training and optimising a Support Vector Machine on CTs of known type in the modelâ€™s output.
It is convenient to automate the process of differentiating between the three CT-types based on their distinctive TSPs and
ğ‘š(TSPs, ğ‘¡ğ‘š) using a machine learning classifier. We chose a Support Vector Machine (SVM) because it is an explainable
machine learning technique, i.e., there is no ambiguity about how it works. An SVM provides an optimal separatrix (e.g. a line
or a hyperplane) that separates each class (in our case, the CT-type) in an ğ¹-dimensional space, where ğ¹âˆˆâ„•+ is the number of
â€˜featuresâ€™. In other words, the ğ¹-dimensional space is divided into different regions that correspond to each class and the feature
data is classified according to which region of the ğ¹-dimensional space it resides in.
Illustrating how SVMs work: For illustrative purposes, we consider a simple SVM with only two features (ğ¹= 2), to
show how CTs in the modelâ€™s output are classified and how the classification changes for different values of ğ‘‡. The three panels
in Fig. 7 are obtained for ğ‘‡= âˆ’4 in (a), ğ‘‡= 0 in (b), and ğ‘‡= 4 in (c). The features in each panel are chosen from Fig. S-13 (b)
in [30] as the two most important features at these values of ğ‘‡. Each panel is divided into three regions separated by the optimal
lines, and these regions are coloured according to each classification class: blue for BCT, green for BNCT and red for NCT. The
different coloured points correspond to the true BCTs in blue, true BNCTs in green, and true NCTs in red. These are known to
us but not to the SVM. The âˆ—superscript on each axis label indicates that the corresponding quantities have been scaled as
described in M3 of the Methods section.
Figure 7 shows that even a two-feature SVM can distinguish between seizure generation mechanisms in noisy time series
with less CTs misclassified as ğ‘‡increases. Figure 7 (a) shows that shortly before a CT, when ğ‘‡= âˆ’4, a small number of BCT
points are misclassified as BNCTs, some BNCT points are misclassified as NCTs, and only two NCT points are misclassified.
Figure 7 (b) shows that at a CT, when ğ‘‡= 0, there is a smaller number of misclassifications of NCT and BNCT points, while
only one BCT point is misclassified. Figure 7 (c) shows that shortly after a CT, when ğ‘‡= 4, only two BNCT points are
misclassified as NCTs.
Training SVMs: We use the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) of the modelâ€™s output, where we know how each CT is generated, as
features to train different SVMs to classify the three CT-types from the NS to S state. To determine how classifications vary
depending on what combination of features are used, we train three types of SVM:
â€¢ Type-1 using the four TSPs, so that ğ¹= 4.
â€¢ Type-2 using the four ğ‘š(TSPs, ğ‘¡ğ‘š), so that that ğ¹= 4.
â€¢ Type-3 using the four TSPs and the four ğ‘š(TSPs, ğ‘¡ğ‘š), so that ğ¹= 8.
13/42

-10
-5
0
5
0.6
0.8
1.0
SVM accuracy
T
(a)
type-1
, type-2: tm = 4
, 8
, 12
-10
-5
0
5
T
(b)
type-3: tm = 4
, 8
, 12
Figure 8. SVM accuracy in classifying unseen data from the modelâ€™s output vs. ğ‘‡for (a) SVM type-1 and 2 and (b) SVM
type-3. For SVM type-2 and 3, we include the dependence on different values of ğ‘¡ğ‘šspecified in the plot legends. The dots
indicate when each SVM achieves perfect accuracy for the first time.
The training procedure is the same for each SVM and is outlined in M3 of the Methods section. The combinations of features
used for each classifier type are listed in Table 6 as a point of reference.
Testing and optimising SVMs:
We test each SVM by examining its ability to classify CTs in unseen data from the
modelâ€™s output. This helps us decide which SVM type is most suitable for classifying seizure generation mechanisms in voltage
recordings. SVM performance is quantified as the fraction of correctly classified samples. We refer to this quantity as the SVM
accuracy, and examine how it changes with ğ‘‡for each SVM type. When the SVM accuracy reaches 1, we say that the SVM
achieves perfect accuracy at this value of ğ‘‡. In Fig. 8 we plot the SVM accuracy versus ğ‘‡for SVMs type-1 and 2 in (a) and
SVM type-3 in (b). The value of ğ‘‡at which a given SVM achieves perfect accuracy for the first time is indicated by a coloured
dot. An additional analysis of â€˜feature importanceâ€™ in each SVM type is provided in Figs. S-13 and S-14 in [30].
Figure 8 (a) shows that SVM type-1 performs poorly for most ğ‘‡< 1, achieves perfect accuracy before SVM type-2 for
each ğ‘¡ğ‘š, and maintains its perfect accuracy for ğ‘‡> 1. Figures 8 (a) and (b) show that increasing ğ‘¡ğ‘šincreases the accuracy of
SVMs type-2 and 3, however, SVM type-2 requires larger ğ‘‡values to achieve perfect accuracy for the first time for larger ğ‘¡ğ‘š.
Additionally, while SVM type-3 maintains its perfect accuracy after first achieving it, SVM type-2 does not beyond a certain
ğ‘‡and starts to decrease in accuracy much sooner (at values of ğ‘‡closer to 0) when smaller ğ‘¡ğ‘šis used. Figures 8 (a) and (b)
also show that SVM type-3 consistently outperforms SVM type-2 for the same values of ğ‘‡and ğ‘¡ğ‘š. It is clear that SVM type-3
benefits from the larger accuracy of SVM type-2 for ğ‘‡< 1 and the consistently perfect accuracy of SVM type-1 for ğ‘‡> 1.
Crucially, Fig. 8 shows that we do not need to be as restrictive with our choice of ğ‘‡âˆ’and ğ‘‡+ as each SVM achieves perfect
accuracy near ğ‘‡= 0. This result works to our advantage in the following subsection because choosing values of ğ‘‡âˆ’and ğ‘‡+
closer to 0 enables us to classify a larger number of CTs in the voltage recordings.
Based on the above discussion we conclude that SVM type-3 is the most suitable choice for classifying seizure generation
mechanisms in the voltage recordings. However, an optimal choice of ğ‘¡ğ‘šmust be made. Although SVM type-3 is more accurate
for ğ‘‡< 1 when ğ‘¡ğ‘š= 12 than for ğ‘¡ğ‘š= 8, we can be less restrictive with our choice of ğ‘‡âˆ’and ğ‘‡+ for smaller ğ‘¡ğ‘š, allowing more
CTs to be classified. This results in a trade-off between maximising either the accuracy or the number of CTs that can be
classified. In the interest of classifying as many seizure generation mechanisms as possible with a reasonably high level of
accuracy, we choose to use the SVM type-3 with ğ‘¡ğ‘š= 8 and refer to it as the â€˜model-trained SVMâ€™ to emphasise that it is trained
on the modelâ€™s output.
Step 4: Using model-trained CT-type classifier to classify unknown seizure generation mechanisms in the
voltage recordings of real seizure activity
This subsection presents our main results: the classification of seizure generation mechanisms in voltage recordings of real
seizure activity using the model-trained SVM. We use the same time notation as for the modelâ€™s output to describe the voltage
recordings near CTs from the NS to S state. Specifically, we use a new time ğ‘‡âˆˆ[ğ‘‡âˆ’,ğ‘‡+], where ğ‘‡= ğ‘¡âˆ’ğ‘¡1, and ğ‘‡= 0 is the
time when the algorithm detects a CT from the NS to S state.
Stage 1 (Detection of CTs): We begin by using our detection algorithm to detect CTs between the NS and S states in the
voltage recordings of rats S, T, and K. The algorithm parameters, specified in Table 3, are chosen according to the procedure
outlined in S5 in [30]. We denote the total number of detected CTs from the NS to S state for a given rat by ğ‘det, where â€˜detâ€™
stands for detected. We refer to this set of CTs as the â€˜detected CTsâ€™.
Stage 2 (Filtering of detected CTs): We classify a detected CT only if it satisfies the following conditions:
14/42

C1:
The brain remains in the NS state for all ğ‘‡âˆˆ[ğ‘‡âˆ’, 0).
C2:
The brain remains in the S state for all ğ‘‡âˆˆ[0, ğ‘‡+].
C3:
There is some overlap between what the algorithm identifies as the S state and the expert annotation of the S state.
C4:
No artefacts appear for ğ‘‡âˆˆ[ğ‘‡âˆ’, 0).
C5:
The threshold ğ›¼is not crossed for any ğ‘‡âˆˆ[ğ‘‡âˆ’, 0).
The conditions C1 and C2 are consistent with those used when selecting CTs from the modelâ€™s output and ensure that the
analysed TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) correspond to an individual CT. C3 ensures that we only classify CTs where our algorithm
agrees with the expert annotation that there is a CT. The algorithm parameters used in Stage 1 are chosen to maximise this
agreement as per the steps outlined in S5 in [30]. C4 and C5 exclude scenarios in which artefacts or almost-occurring CTs
are found just prior to a detected CT, that is for ğ‘‡âˆˆ[ğ‘‡âˆ’,0]; see Figs. S-16 and S-21 in [30] for more details. The subset of
detected CTs that satisfy all five conditions are referred to as the â€˜filtered CTsâ€™ and the number of filtered CTs for a given rat
and choice of ğ‘‡âˆ’and ğ‘‡+ is denoted by ğ‘filt(ğ‘‡âˆ’,ğ‘‡+), where â€˜filtâ€™ is short for filtered.
Stage 3 (Classification of filtered CTs): For our choice of ğ‘‡âˆ’, ğ‘‡+, ğ‘¡ğ‘¤= 1 and ğ‘¡ğ‘š= 8, we obtain all TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š)
of the filtered CTs for ğ‘‡âˆˆ[ğ‘‡âˆ’+2ğ‘¡ğ‘¤+ğ‘¡ğ‘š,ğ‘‡+] = [ğ‘‡âˆ’+10,ğ‘‡+]
. This is the time interval where we can apply our model-trained
SVM classifier to the filtered CTs. Next, we use ğ‘type(ğ‘‡) to denote the number of CTs classified at time ğ‘‡as type = BCT,
BNCT or NCT , and compute the proportions of:
â€¢ Filtered CTs to the total number of detected CTs: ğ‘filt(ğ‘‡âˆ’,ğ‘‡+)âˆ•ğ‘det âˆˆ[0,1].
â€¢ Filtered CTs classified as a particular type at time ğ‘‡to the total number of filtered CTs: ğ‘type(ğ‘‡)âˆ•ğ‘filt(ğ‘‡âˆ’,ğ‘‡+) âˆˆ[0,1],
bearing in mind that ğ‘‡âˆˆ[ğ‘‡âˆ’+10,ğ‘‡+].
We refer to Figs. S-17, S-18 and S-19 in [30] for details of classification dependence on ğ‘‡.
The main experiment
We now have all the ingredients needed to design our main experiment and address our main research question: What CT-types,
or generation mechanisms, are responsible for seizure onset in GAERS. To demonstrate the robustness of our findings with
respect to the selection of time intervals around the CTs, we consider a fixed value for ğ‘‡+ and different values for ğ‘‡âˆ’. More
specifically, we set ğ‘‡+ = ğœS = 2 and consider ğ‘‡âˆ’= âˆ’16,âˆ’14,âˆ’12,âˆ’10, and âˆ’8, which are chosen differently from the ğ‘‡+ and
ğ‘‡âˆ’used for training the SVM classifier in order to to maximise ğ‘filt(ğ‘‡âˆ’,ğ‘‡+). The proportions ğ‘type(ğ‘‡)âˆ•ğ‘filt(ğ‘‡âˆ’,2), which
is the main quantity of interest, can be obtained at any ğ‘‡âˆˆ[âˆ’6, 2] if ğ‘‡âˆ’= âˆ’16, and at just one value of ğ‘‡= 2 if ğ‘‡âˆ’= âˆ’8.
Therefore, to make a meaningful comparison of CT classifications for different values of ğ‘‡âˆ’, we focus on the classifications
at ğ‘‡= 2, that is two seconds after the time at which a CT is detected. In other words, we examine ğ‘type(2)âˆ•ğ‘filt(ğ‘‡âˆ’,2) for
ğ‘‡âˆ’= âˆ’16,âˆ’14,âˆ’12,âˆ’10, and âˆ’8, where type = BCT, BNCT or NCT is given by the SVM classifier.
Key findings from the main experiment
The key findings from our main experiment are presented in the two rows in Fig. 9, bearing in mind that the model-trained
SVM uses the eight features obtained from the portion of the voltage recording that starts |ğ‘‡âˆ’| seconds prior to and always
ends ğ‘‡+ = 2 seconds after the detection of each filtered CT. The results presented in Fig. 9 are based on the classifications
obtained at time ğ‘‡= ğ‘‡+ = 2.
The top row of Fig. 9 shows how the proportions of all filtered CTs that are classified as (red) NCTs, (blue) BCTs, and
(green) BNCTs vary with ğ‘‡âˆ’. We show this for rat S in panel (a), rat T in panel (b) and rat K in panel (c). The key finding is
that NCTs are the dominant seizure generation mechanism in all three rats for all values of ğ‘‡âˆ’. More specifically,
â€¢ NCTs account for approximately 41âˆ’51% of the filtered CTs,
â€¢ BCTs account for approximately 22âˆ’34% of the filtered CTs, and
â€¢ BNCTs account for approximately 22âˆ’27% of the filtered CTs.
In the bottom row of Fig. 9, we show how the proportion of filtered CTs to the total number of detected CTs increases with
ğ‘‡âˆ’for a fixed ğ‘‡+ = 2. For the minimum value of ğ‘‡âˆ’, which is âˆ’8 and is a technical restriction of our classifier, approximately
50% of the detected CTs are retained by filtering (i.e., are classifiable) for rat S in panel (d), compared to approximately 40% for
rat T in panel (e), and approximately 35% for rat K in panel (f) partially due to many artefacts. Finally, a numerical summary of
the key findings is provided in Table 4.
15/42

0.2
0.3
0.4
0.5
Ntype(2)/Nï¬lt(T âˆ’, 2)
(a)
Rat S
NCT
BCT
BNCT
(b)
Rat T
(c)
Rat K
-15
-12
-9
-6
0.2
0.4
0.6
0.8
Nï¬lt(T âˆ’, 2)/Ndet
T âˆ’
(d)
-15
-12
-9
-6
T âˆ’
(e)
-15
-12
-9
-6
T âˆ’
(f)
Figure 9. (Top row) The proportion of filtered CTs from the voltage recordings in GAERS that are classified by the
model-trained SVM, i.e. SVM type-3 with ğ‘¡ğ‘¤= 1 and ğ‘¡ğ‘š= 8, at time ğ‘‡= ğ‘‡+ = 2 as (red) NCT, (blue) BCT and (green) BNCT
vs. ğ‘‡âˆ’for (a) rat S, (b) rat T and (c) rat K. Note that NCT emerges as the dominant CT-type responsible for the onset of
seizures in all three rats. (Bottom row) The proportion of filtered CTs to the total number of CTs detected in the voltage
recordings vs. ğ‘‡âˆ’for (d) rat S, (e) rat T and (f) rat K. Other parameters are specified in Table 3, where ğ›¼is chosen according to
the method outlined in S5 in [30].
The bottom row in Fig. 9 shows that many detected CTs were excluded from classification due to filtering, i.e., they did
not meet the technical requirements of our classifier, namely conditions C1-C5 with ğ‘‡âˆ’â‰¤âˆ’8. This raises the question of how
the exclusion of these detected CTs impacted the key findings. To address this question, we make three observations. First,
if ğ‘‡âˆ’were increased to âˆ’3, over 80% of the detected CTs for rat S would meet the filtering criteria C1-C5. Second, the top
row in Fig. 9 shows that the proportion of classified CTs of a given type in all three rats remains largely unchanged as ğ‘‡âˆ’is
varied from âˆ’16 to âˆ’8. This suggests that the trend continues for larger values of ğ‘‡âˆ’, but this cannot be verified due to the
technical requirements of our classifier. Third, McCafferty et al.22 found that approximately 50% of seizures in GAERS can be
interrupted by applying a suitably chosen auditory stimulus. This suggests that, like in our dominant case of noise-induced
CT, and the least dominant case of bifurcation/noise-induced CT, epileptic brain frequently operates in a multistable regime
whereby disturbances can force transitions between coexisting stable NS and S states. For these reasons, we conjecture that the
key findings persist for most of the excluded detected CTs.
Discussion
This paper presents a general framework for classifying seizure generation mechanisms as different types of critical transition
(CT) between the non-seizure state (NS state) and the seizure state (S state) in the epileptic brain. The framework consists of
four steps. The first step is to construct a canonical mathematical model which displays CTs that (i) closely resemble voltage
recordings of real seizures and (ii) can be bifurcation-induced (BCTs), noise-induced (NCTs), or a combination of both (BNCTs).
The second step is to construct a detection algorithm that detects CTs between the NS and S states in both the modelâ€™s output
and voltage recordings of real seizures. The third step is to construct a machine learning CT-type classifier. The classifier is
trained using selected time series properties of the modelâ€™s output where the CT-types are known. In the fourth step, we use the
model-trained classifier on voltage recordings, where CT-types are unknown, in order to classify the onset of real seizures as
BCTs, NCTs or BNCTs.
We applied the above framework to voltage recordings taken from inside the brain of Genetic Absence Epilepsy Rats from
Strasbourg (GAERS) - a well-established model of absence epilepsy - and, for the first time, demonstrated that consensus can
16/42

Rat S
Rat T
Rat K
No. of CTs detected by expert
621
1593
1136
No. of CTs detected by the algorithm
(ğ‘ğ‘‘ğ‘’ğ‘¡)
466
1115
874
No. of filtered CTs
(ğ‘filt(âˆ’8,2))
226
454
304
Percentage of filtered CTs classified
as type = BCTs, BNCTs, or NCTs
((ğ‘type(2)âˆ•ğ‘filt(âˆ’8,2))Ã—100)
29.6, 22.6, ğŸ’ğŸ•.ğŸ–
33.5, 24.0, ğŸ’ğŸ.ğŸ“
23.0, 27.0, ğŸ“ğŸ.ğŸ
Table 4. Our key findings in numbers. In the bottom row, the dominant CT-type for each rat is highlighted by the bold font.
be reached on which CT-types are predominantly responsible for seizure onset. We found NCTs to be the dominant seizure
generation mechanism - they accounted for almost 50% of the seizures we classified, with BCTs and BNCTs accounting
for the remainder. Our findings show that (i) seizures in the brain appear to be generated by different CT-types40â€“45, and
(ii) multistability and noise play a significant role in the epileptic brain16. They also challenge the view that seizures are
predominantly bifurcation-induced15.
An important practical implication of our findings is that different CT-types may reflect distinct neural processes involved
in generating seizures. From this perspective, a seizure that is classified as an NCT is likely to start locally with respect to a
recording electrode. Furthermore, there are constitutively pro-ictal neurons in the epileptic brain that can initiate a seizure based
solely on noisy input in the form of exogenous or endogenous disturbances to these neurons. The hyper-excitable neurons found
in layer V of the perioral somatosensory cortex46, from which GAERS seizures appear to originate47, may be one such example.
On the other hand, a seizure that is classified as a BCT is likely to emerge gradually, or propagate to the recording electrode
having started elsewhere. In this case, the slow drift in brain state is considered as the ictogenic culprit. The observed changes
in brain state tens of seconds prior to seizure onset in rats and humans48,49 may reflect such a gradual drift towards seizure
onset. Therefore, identifying and arresting this slow drift may prevent seizure onsets. BNCTs may represent combinations of
the neural processes discussed above.
Classifying seizure generation mechanisms in a given brain is not only an important fundamental question. It could transform
how we model, detect, prevent, and treat seizures, with the ultimate aim of informing new clinical decision support systems
and tailored seizure prevention45 and control50 strategies. While studies in GAERS have identified specific neurons capable
of seizure initiation, human epilepsy may have multiple potential cortical sites of origin, likely related to cortical-thalamic
interactions19. Furthermore, these sites may differ between babies and adults. It remains to be seen whether different CT-types
correspond to how people with epilepsy experience their seizures, or whether NCTs play a dominant role. Our framework
provides proof of concept and represents an important first step towards analysing those more complex seizures. The next
step is to apply it to the less well-understood forms of human epilepsy. This may involve updating our mathematical model of
seizure activity with a more realistic one, augmenting our CT detection algorithm with additional time series properties such as
the dynamical eigenvalue early warning signal proposed by Grziwotz et al.51, incorporating different experimental data such as
heart rate variability52, and developing more advanced filtering techniques to remove artefacts53. Furthermore, our framework
can be extended to include other types of CT, such as rate-induced CTs54, identify new forms of early warning signals for
different CT-types, and classify seizure termination mechanisms55,56 to gain new insight into how the brain changes in response
to a seizure and how an ongoing seizure can be terminated. Finally, it can also be applied to complex systems beyond the brain.
17/42

Methods
M1: Obtaining and annotating seizure data from GAERS
The data discussed here was obtained by Cian McCafferty, FranÃ§ois David, and Vincenzo Crunelli and was presented in 18.
Obtaining the data:
The electrophysiological data was acquired from male Genetic Absence Epilepsy Rats from Stras-
bourg (GAERS) aged between 4-7 months when in a state of relaxed wakefulness where seizures were experienced more often.
Silicon-site electrodes were used to sample voltages (20000/second) from the ventrobasal thalamus while the rats were able to
move freely and alternate between waking, sleeping, and seizing states. This data was processed with a Plexon HST/32V-G20
VLSI-based preamplifier and associated digitization system and subsequently down-sampled to 1000 samples/second.
Labelling the recording session: Each recording session was labelling using the following convention described through
example, â€˜S1Kâ€™: voltage recordings from rat S on day 1 of recording during the Kğ‘¡â„recording session on that day.
Annotating the data: Spike-wave discharges (SWDs) were identified using Cambridge Electronic Designâ€™s software, â€˜Spike2â€™.
In all cases, EEG at 1000Hz was used for this step. EEG was acquired at 1000Hz for fMRI (see McCafferty et al.48) and
behaviour was down-sampled by averaging for neuronal activity. Briefly, smoothening (voltage at time ğ‘¡is set to mean of
voltages from time ğ‘¡âˆ’10ms to time ğ‘¡+10ms) and DC removal (voltage at time ğ‘¡is set to original voltage minus mean of voltages
from time ğ‘¡âˆ’0.1s to time ğ‘¡+0.11s) functions were used to reversibly visually clean the frontoparietal differential EEG. Then,
a negative amplitude threshold (mean voltage minus 5-7 standard deviations of baseline non-SWD EEG) was used to detect
putative spike-wave crossing points, defined as whenever the signal crossed this amplitude threshold. The crossing points were
then grouped into events based on the intervals between them (maximum time between initial two crossings 0.2s, maximum
time between any two crossings within an event 0.35s, minimum of 5 crossings per event) and the defined properties of SWDs
(minimum duration 0.5s, minimum inter-SWD interval 0.5s merging any SWDs with shorter intervals), and subsequently, using
a frequency threshold, these events were classified as SWDs (if > 75% of intercrossing intervals were within a 5â€“12Hz range)
or other (e.g., noise, sleep). Labelled SWDs were then visually inspected for accuracy of SWD detection, as well as onset
and offset times. Periods of sleep were identified based on sharp increases in the 1â€“4Hz frequency band and were excluded
from analysis. Periods of non-REM sleep were rare in the recordings due to their relatively short duration and therefore not
informative for analysis.
Artefacts and how they are accounted for: Artefacts in the voltage recordings can appear for different reasons, the most
common being (i) movement of electrodes/wiring and (ii) movement of muscles close to the electrodes (generally for chewing).
Some artefacts are generated by signal overload and noise from electrical devices around the recording setup, however, these
are less common as the recording process was generally appropriately amplified and shielded. Artefacts are accounted for
through the method described above since high-amplitude events that do not align with the frequency profile of SWDs were not
annotated. Additionally, the visual inspection described above also includes the manual rejection of artefacts which could be
excised from recordings. Entire recording session were excluded if more than 5% of the session consisted of artefact activity.
Remark (comparison with human data):
GAERS can express multiple absence seizures per minute57, far exceeding
the frequency of any absence seizure syndrome in humans. For instance, GregorÄiÄ et al.58 found that, for children with
treatment-resistant childhood absence epilepsy, the median number of seizures per day was three. For a review of the GAERS
model, with attention to its similarities and differences to human absence seizures, see Depaulis et al.59.
M2: Computing the time series properties and their slopes
Summary of procedures used to compute different properties of time series of both the modelâ€™s output and the voltage recordings.
Detrending the time series: We first detrend a given time series using the Gaussian filtering approach mentioned in Dakos
et al.60. This involves fitting a Gaussian kernel smoothing function to the time series and then subtracting the fit from the
time series to obtain the detrended time series. As further detailed in Lenton et al.61, this process involves choosing a suitable
bandwidth (a fitting parameter) for the kernel which determines the degree of smoothing. The bandwidth is typically chosen to
neither over-fit the data nor filter out low frequencies from the time series, in our case we choose a bandwidth of 30. In our
experiments we use the â€˜gaussian_filter1dâ€™ function from the â€˜scipy.ndimageâ€™ Python library to detrend the time series. See
Fig. 10 for an example of a detrended time series of voltage recordings. In our experiments we find that, whether the data is
detrended or not, there are minor changes in the main results presented in Figs. 9 (a)-(c), these changes are on the order of 2-3%
in terms of the percentage of CTs classified as BCTs, BNCTs, and NCTs specified in Table 4. Note, we only detrend voltage
recordings as no trends are present in our modelâ€™s output.
Variance: We compute the Gaussian rolling variance at time ğ‘¡â‰¥ğ‘¡ğ‘¤using data points of the time series within the interval,
[ğ‘¡âˆ’ğ‘¡ğ‘¤, ğ‘¡], which we call â€˜the windowâ€™ and ğ‘¡ğ‘¤> 0 is the â€˜window lengthâ€™. The variance is calculated as the weighted variance of
a Gaussian distribution of the data points centered at ğ‘¡âˆ’ğ‘¡ğ‘¤âˆ•2 with mean equal to zero. This involves assigning weights to data
points within the window and these weights are determined from the probability density function of a Gaussian distribution
18/42

-0.1
0
0.1
LFP (a)
original time series
Gaussian kernel smoothing function
-4
-3
-2
-1
0
1
-0.1
0
0.1
T
LFP (b)
detrended time series
Figure 10. Detrending voltage recordings using a Gaussian kernel smoothing function. (a) shows (in blue) an example of
voltage recordings of real seizure activity before detrending and (in orange) the corresponding function fitted to these voltage
recordings, and (b) shows the detrended voltage recordings. The vertical red solid line indicates when ğ‘‡= 0. Horizontal lines
indicate the thresholds of (in red) ğ›¼and (in green) ğ›½used by the algorithm. The other algorithm parameters are specified in
Table 3.
centered at the middle of the window with standard deviation taken as ğ‘¡ğ‘¤âˆ•6. Based on this procedure we define GV(ğ‘¡) âˆ¶â„â†’â„
as the Gaussian variance of the time series at ğ‘¡â‰¥ğ‘¡ğ‘¤.
Logarithm of the variance: Considering the results presented by Milanowski and Suffczynski12, we also find it useful to
record the base-10 logarithm of the GV in each of the moving windows described above. This allows us to more closely examine
the smaller changes in the variance. We define log10GV(ğ‘¡) âˆ¶â„â†’â„as the base-10 logarithm of the Gaussian variance of the
time series at ğ‘¡â‰¥ğ‘¡ğ‘¤.
Autocorrelation: We compute the autocorrelation at ğ‘¡â‰¥ğ‘¡ğ‘¤using a rolling window technique and data points of the time
series within the same intervals as above, [ğ‘¡âˆ’ğ‘¡ğ‘¤,ğ‘¡]. More specifically, we compute the lag-1 autocorrelation at a given time ğ‘¡by
computing the Pearson correlation between data points in the following two intervals, [ğ‘¡âˆ’ğ‘¡ğ‘¤+ğ‘¡ğ‘,ğ‘¡] and [ğ‘¡âˆ’ğ‘¡ğ‘¤,ğ‘¡âˆ’ğ‘¡ğ‘] where
ğ‘¡ğ‘> 0 is the time between two successive points in the time series. Based on this procedure we define AC(ğ‘¡) âˆ¶â„â†’â„as the
lag-1 autocorrelation of the time series at ğ‘¡â‰¥ğ‘¡ğ‘¤.
Variance of the autocorrelation: Prior to a BCT or BNCT from the NS to S state we observe a decrease in the fluctuations
of AC(ğ‘¡) and, after the CT, AC(ğ‘¡) remains relatively constant and close to 1. To more closely examine this behaviour, we
compute the base-10 logarithm of the Gaussian variance of the time series obtained for AC(ğ‘¡) using the same procedure
outlined above. From this we define log10GV(AC)(ğ‘¡) âˆ¶â„â†’â„as the base-10 logarithm of the Gaussian variance of the auto-
correlation of a given time series at ğ‘¡â‰¥2 ğ‘¡ğ‘¤(since AC(ğ‘¡) is defined for ğ‘¡â‰¥ğ‘¡ğ‘¤, the resulting log10GV(AC)(ğ‘¡) is defined for ğ‘¡â‰¥2 ğ‘¡ğ‘¤).
For convenience we refer to GV(ğ‘¡) as GV, and similarly for log10GV, AC, and log10GV(AC). We collectively refer to these four
quantities as the time series properties (TSPs). From several trial runs we find that setting ğ‘¡ğ‘¤= 1 provides relatively robust
estimates of the variance and autocorrelation of the time series. Each window of length ğ‘¡ğ‘¤is shifted forward in time by a factor
Î”ğ‘¤> 0 which we choose as 0.001, the time between two successive points in the original time series.
Slopes: We also analyse how the slopes of the TSPs change over time by fitting the function, ğ‘“(ğ‘¥) = ğ‘šğ‘¥+ğ‘, to the values of,
for example, GV within the interval [ğ‘¡âˆ’ğ‘¡ğ‘š,ğ‘¡], where ğ‘¡ğ‘šis the â€˜slope lengthâ€™. We use the â€˜curve_fit()â€™ function in the SciPy
Python library to perform a least squares fit of this function to the data via the Levenbergâ€“Marquardt algorithm. Based on
this procedure we define ğ‘š(GV,ğ‘¡ğ‘š) âˆ¶â„Ã—â„â†’â„as the slope of the Gaussian variance of time series at ğ‘¡â‰¥ğ‘¡ğ‘¤+ğ‘¡ğ‘š. The same
procedure is applied to all TSPs. Note, ğ‘š(log10GV(AC), ğ‘¡ğ‘š) is defined for ğ‘¡â‰¥2 ğ‘¡ğ‘¤+ğ‘¡ğ‘š. We collectively refer to these four
quantities as â€˜the slopes of the TSPs for a given ğ‘¡ğ‘šâ€™, denoted by ğ‘š(TSPs, ğ‘¡ğ‘š). Each window of length ğ‘¡ğ‘šis shifted forward in
time by a factor Î”ğ‘š> 0 which we choose as Î”ğ‘š= 100Î”ğ‘¤to reduce computational time.
In Figs. 11 and 12 we illustrate the different time intervals over which TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) are computed using an ex-
ample of the modelâ€™s output nearby a BCT from the NS to S state. This helps clarify (i) how values of TSPs and ğ‘š(TSPs,
19/42

Gaussian
variance
base-10 logarithm of
the Gaussian variance
lag-1
autocorrelation
base-10 logarithm of the Gaussian
variance of the lag-1 autocorrelation
Time series properties
(TSPs)
GV
log10GV
AC
log10GV(AC)
Slopes of time series
properties (ğ‘š(TSPs, ğ‘¡ğ‘š))
ğ‘š(GV, ğ‘¡ğ‘š)
ğ‘š(log10GV, ğ‘¡ğ‘š)
ğ‘š(AC, ğ‘¡ğ‘š)
ğ‘š(log10GV(AC), ğ‘¡ğ‘š)
Table 5. Quantities used as features to classify seizure generation mechanisms in a noisy time series. These quantities are
dependent on time, i.e., GV = GV(ğ‘¡), and are computed using data points of a given time series within the interval [ğ‘¡âˆ’ğ‘¡ğ‘¤,ğ‘¡]
where ğ‘¡ğ‘¤> 0 is the â€˜window lengthâ€™. ğ‘¡ğ‘šdenotes the slope length, the duration of time over which the slope is calculated.
-1
0
1
tw
âˆ†w
x
(a)
0
0.1
tm
âˆ†m
GV
(b)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
0
0.01
t
m(GV,tm)
(c)
Figure 11. Illustrating process for computing values of GV and ğ‘š(GV, ğ‘¡ğ‘š) using moving windows of lengths ğ‘¡ğ‘¤and ğ‘¡ğ‘š, which
are advanced along the respective time series in steps of Î”ğ‘¤and Î”ğ‘š. The shaded region shows values of: (a) the modelâ€™s
output used to compute (red point) the first GV value in (b) and (b) GV used to compute (red point) the first ğ‘š(GV, ğ‘¡ğ‘š) value in
(c). The time series in (a) is an example of the modelâ€™s output around a BCT from the NS to S state at ğ‘¡= 8 (inward black tick),
generated using parameters specified in Table 2 and plotted here for a new time ğ‘¡= ğ‘¡â€² +38, where ğ‘¡â€² is the original time, to
better illustrate the above process. ğ‘¡ğ‘¤= 1, Î”ğ‘¤= 0.001, ğ‘¡ğ‘š= 8, and Î”ğ‘š= 0.1.
ğ‘¡ğ‘š) are obtained from moving windows and (ii) the correspondence in time between TSPs, ğ‘š(TSPs, ğ‘¡ğ‘š), and the original
time series from which they are obtained. More specifically, Fig. 11 illustrates the above for GV and ğ‘š(GV, ğ‘¡ğ‘š), the same
procedure is used to compute AC, log10GV, and their corresponding slopes. Fig. 12 illustrates the above for log10GV(AC) and
ğ‘š(log10GV(AC), ğ‘¡ğ‘š). In both Figs. 11 and 12, ğ‘¡ğ‘¤= 1 and ğ‘¡ğ‘š= 8.
M3: SVM training and testing
We use the â€˜sklearn.svmâ€™ Python library to construct a linear support vector machine (SVM) that classifies CTs from the NS to
S state according to the CT-type where type = BCT, BNCT, or NCT. The SVM classifier is obtained by finding an optimal
line/hyperplane that maximises the distance between each class, i.e., CT-type, in an ğ¹-dimensional space where ğ¹âˆˆâ„•+ is the
number of â€˜featuresâ€™.
Preparing the data: We use values of the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) at a time ğ‘¡as features. ğ‘†âˆˆâ„•samples of each feature are
used to train the SVM. Since (i) some features are orders of magnitude larger than others and (ii) we would like to construct a
classifier which treats features with equal importance, each of the features are scaled to be of zero mean and unit variance using
the â€˜preprocessing.scaleâ€™ function from the â€˜sklearnâ€™ Python library. The data used to construct the SVM classifier is structured
as follows, the â€˜feature matrixâ€™ is defined as ğ‘‹âˆˆâ„ğ‘†Ã—ğ¹, and the â€˜class vectorâ€™ is defined as ğ‘¦âˆˆâ„ğ‘†. The information contained
20/42

-1
0
1
tw
âˆ†w
x
(a)
0.98
0.99
1.00
tw
âˆ†w
AC
(b)
-9
-5
tm
âˆ†m
log10GV(AC)
(c)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
-0.35
0.00
t
m(log10GV(AC),tm)
(d)
Figure 12. Illustrating process for computing values of log10GV(AC) and ğ‘š(log10GV(AC), ğ‘¡ğ‘š) using moving windows of
lengths ğ‘¡ğ‘¤and ğ‘¡ğ‘š, which are advanced along the respective time series in steps of Î”ğ‘¤and Î”ğ‘š. The shaded regions indicate
values of: (a) the modelâ€™s output used to compute (red point) the first AV value in (b), (b) AC used to compute (red point) the
first log10GV(AC) value in (c), and (c) log10GV(AC) used to compute (red point) the first ğ‘š(log10GV(AC), ğ‘¡ğ‘š) value in (d).
The time series in (a) is the same as Fig. 11 (a) (see caption for details), ğ‘¡ğ‘¤= 1, Î”ğ‘¤= 0.001, ğ‘¡ğ‘š= 8, and Î”ğ‘š= 0.1.
in ğ‘‹and ğ‘¦define our data set.
Training and testing:
We split the above data set into a training and testing data sets whereby 70% of the data is used
for training and 30% for testing (a convention commonly followed when training and testing SVMs.) The SVM classifier is
constructed based on the training data set and the accuracy of the SVM classifier is determined based on the fraction of correctly
classified samples in the testing data set.
Feature importance: We use the â€˜permutation_importanceâ€™ function from the â€˜sklearn.inspectionâ€™ Python library to determine
what features have the greatest impact on the accuracy. This function constructs different random permutations of a given
feature (i.e., reorders elements in a column of ğ‘‹reserved for testing while the corresponding ğ‘¦remains unchanged) and returns
the resulting decrease in accuracy; the greater the decrease the more dependent the SVM classifierâ€™s accuracy is on the feature.
This process is repeated 100 times for each feature to obtain an estimate of the â€˜mean permutation importanceâ€™ (MPI) where
âˆ’1 â‰¤MPI â‰¤1. We also obtain the corresponding standard deviations from this process. Therefore, if, for a given feature,
â€¢ MPI > 0, the SVM classifierâ€™s accuracy depends on this feature and the greater the MPI the more important the feature.
â€¢ MPI â‰ˆ0, the SVM classifierâ€™s accuracy does not depend on this feature.
â€¢ MPI < 0, the SVM classifierâ€™s accuracy increases when using the permuted data set as opposed to the original data set.
21/42

Features used
SVM type-1
TSPs: GV, log10GV, AC, and log10GV(AC).
SVM type-2
ğ‘š(TSPs, ğ‘¡ğ‘š) for a given ğ‘¡ğ‘š: ğ‘š(GV, ğ‘¡ğ‘š), ğ‘š(log10GV, ğ‘¡ğ‘š), ğ‘š(AC, ğ‘¡ğ‘š), and ğ‘š(log10GV(AC), ğ‘¡ğ‘š).
SVM type-3
All TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) for a given ğ‘¡ğ‘š.
Table 6. Features used to train and test different types of Support Vector Machines (SVMs). The time series properties (TSPs)
and their slopes, ğ‘š(TSPs, ğ‘¡ğ‘š), are specified in Table 5. ğ‘¡ğ‘šdenotes the duration of time over which the slope is calculated.
References
1. Beniczky, S. et al. Updated classification of epileptic seizures: Position paper of the International League Against Epilepsy.
Epilepsia 66, 1804â€“1823 (2025).
2. Kaye, D. et al. Impact of Prolonged Seizures on Patientsâ€™ and Caregiversâ€™ Quality of Life (P1-9.014). In Neurology, vol.
104, 2708 (Lippincott Williams & Wilkins Hagerstown, MD, 2025).
3. GonzÃ¡lez, O. C., Krishnan, G. P., Timofeev, I. & Bazhenov, M. Ionic and synaptic mechanisms of seizure generation and
epileptogenesis. Neurobiol. Dis 130, 104485 (2019).
4. Ashwin, P., Perryman, C. & Wieczorek, S. Parameter shifts for nonautonomous systems in low dimension: bifurcation-and
rate-induced tipping. Nonlinearity 30, 2185 (2017).
5. Lenton, T. M. et al. Tipping elements in the Earthâ€™s climate system. Proc. Natl. Acad. Sci. 105, 1786â€“1793 (2008).
6. Lenton, T. M. Tipping positive change. Philos. Trans. R. Soc. B 375, 20190123 (2020).
7. Armstrong McKay, D. I. et al. Exceeding 1.5 C global warming could trigger multiple climate tipping points. Science 377,
eabn7950 (2022).
8. Lenton, T. M. et al. Global tipping points report 2025 (2025). University of Exeter.
9. Scheffer, M. et al. Early-warning signals for critical transitions. Nature 461, 53 (2009).
10. McSharry, P. E., Smith, L. A. & Tarassenko, L. Prediction of epileptic seizures: are nonlinear methods relevant? Nat. Med.
9, 241â€“242 (2003).
11. Meisel, C. & Kuehn, C. Scaling effects and spatio-temporal multilevel dynamics in epileptic seizures. PLoS One 7, e30371
(2012).
12. Milanowski, P. & Suffczynski, P. Seizures start without common signatures of critical transition. Int. J. Neural Syst. 26,
1650053 (2016).
13. Jirsa, V. K., Stacey, W. C., Quilichini, P. P., Ivanov, A. I. & Bernard, C. On the nature of seizure dynamics. Brain 137,
2210â€“2230 (2014).
14. Wilkat, T., Rings, T. & Lehnertz, K. No evidence for critical slowing down prior to human epileptic seizures. Chaos 29,
091104 (2019).
15. Maturana, M. I. et al. Critical slowing down as a biomarker for seizure susceptibility. Nat. Commun. 11, 2172 (2020).
16. da Silva, F. L. et al. Epilepsies as Dynamical Diseases of Brain Systems: Basic Models of the Transition Between Normal
and Epileptic Activity. Epilepsia 44, 72â€“83 (2003).
17. Dakos, V. et al. Tipping point detection and early warnings in climate, ecological, and human systems. Earth Syst. Dyn. 15,
1117â€“1135 (2024).
18. McCafferty, C. et al. Cortical drive and thalamic feed-forward inhibition control thalamic output synchrony during absence
seizures. Nat. neuroscience 21, 744â€“756 (2018).
19. Crunelli, V. et al. Clinical and experimental insight into pathophysiology, comorbidity and therapy of absence seizures.
Brain 143, 2341â€“2368 (2020).
20. Zhou, Z. et al. A generalized seizure type: Myoclonic-to-tonic seizure. Clin. Neurophysiol. 164, 24â€“29 (2024).
21. Depaulis, A., van Luijtelaar, G., Pitkanen, A., Schwartzkroin, P. & Moshe, S. Models of seizures and epilepsy (2006).
22. McCafferty, C. P., Zheng, X., Tung, R., Gruenbaum, B. F. & Blumenfeld, H. Interruption of rat absence seizures by auditory
stimulation. bioRxiv 2025â€“10 (2025).
22/42

23. da Silva, F. H. L. et al. Dynamical diseases of brain systems: different routes to epileptic seizures. IEEE Trans. Biomed.
Eng. 50, 540â€“548 (2003).
24. Suffczynski, P., Kalitzin, S. & da Silva, F. L. Dynamics of non-convulsive epileptic phenomena modeled by a bistable
neuronal network. Neuroscience 126, 467â€“484 (2004).
25. Suffczynski, P., da Silva, F. L., Parra, J., Velis, D. & Kalitzin, S. Epileptic transitions: model predictions and experimental
validation. J. Clin. Neurophysiol. 22, 288â€“299 (2005).
26. Junges, L., Woldman, W., Benjamin, O. J. & Terry, J. R. Epilepsy surgery: Evaluating robustness using dynamic network
models. Chaos 30, 113106 (2020).
27. Harrington, E. G., Kissack, P., Terry, J. R., Woldman, W. & Junges, L. Treatment effects in epilepsy: a mathematical
framework for understanding response over time. Front. Netw. Physiol. 4, 1308501 (2024).
28. Qin, Y., El-Gazzar, A., Bassett, D. S., Pasqualetti, F. & Van Gerven, M. Analytical characterization of epileptic dynamics
in a bistable system. In 2024 IEEE 63rd Conference on Decision and Control (CDC), 583â€“588 (IEEE, 2024).
29. Byrne, Ã., Ross, J., Nicks, R. & Coombes, S. Mean-field models for EEG/MEG: from oscillations to waves. Brain Topogr.
35, 36â€“53 (2022).
30. Flynn, A. et al. Supplementary material from: â€œClassifying seizure generation mechanisms: A critical transitions
frameworkâ€. (Link).
31. Kuznetsov, Y. A. Elements of Applied Bifurcation Theory (Springer Science & Business Media, 2013).
32. Lin, K. K. & Young, L.-S. Shear-induced chaos. Nonlinearity 21, 899 (2008).
33. Wieczorek, S. Stochastic bifurcation in noise-driven lasers and Hopf oscillators. Phys. Rev. E 79, 036209 (2009).
34. Blackbeard, N., ErzgrÃ¤ber, H. & Wieczorek, S. Shear-induced bifurcations and chaos in models of three coupled lasers.
SIAM J. on Appl. Dyn. Syst. 10, 469â€“509 (2011).
35. Krasnoselâ€™skii, M. A. & Pokrovskii, A. V. Systems with Hysteresis (Springer Science & Business Media, 2012).
36. Ditlevsen, P. & Ditlevsen, S. Warning of a forthcoming collapse of the Atlantic meridional overturning circulation. Nat.
Commun. 14, 4254 (2023).
37. Van Westen, R. M., Kliphuis, M. & Dijkstra, H. A. Physics-based early warning signal shows that AMOC is on tipping
course. Sci. Adv. 10, 1189 (2024).
38. Ashwin, P., Bastiaansen, R., von der Heydt, A. S. & Ritchie, P. D. Early warning skill, extrapolation and tipping for
accelerating cascades. Proc. Roy. Soc. A 481, 20250405 (2025).
39. Bury, T. M. et al. Deep learning for early warning signals of tipping points. Proc. Natl. Acad. Sci. 118, e2106140118
(2021).
40. McCormick, D. A. & Contreras, D. On the cellular and network bases of epileptic seizures. Annu. Rev. Physiol. 63, 815â€“846
(2001).
41. Vezzani, A., French, J., Bartfai, T. & Baram, T. Z. The role of inflammation in epilepsy. Nat. Rev. Neurol. 7, 31â€“40 (2011).
42. Kuhlmann, L., Lehnertz, K., Richardson, M. P., Schelter, B. & Zaveri, H. P. Seizure prediction â€” ready for a new era. Nat.
Rev. Neurol. 14, 618â€“630 (2018).
43. GonzÃ¡lez, O. C., Krishnan, G. P., Timofeev, I. & Bazhenov, M. Ionic and synaptic mechanisms of seizure generation and
epileptogenesis. Neurobiol. Dis. 130, 104485 (2019).
44. Jiruska, P., Freestone, D., Gnatkovsky, V. & Wang, Y. An update on the seizures beget seizures theory. Epilepsia 64,
S13â€“S24 (2023).
45. Lehnertz, K., Broehl, T. & von Wrede, R. Epileptic-network-based prediction and control of seizures in humans. Neurobiol.
Dis. 181, 106098 (2023).
46. Polack, P.-O. et al. Deep layer somatosensory cortical neurons initiate spike-and-wave discharges in a genetic model of
absence seizures. J. Neurosci. 27, 6590â€“6599 (2007).
47. Meeren, H. K., Pijn, J. P. M., Van Luijtelaar, E. L., Coenen, A. M. & da Silva, F. H. L. Cortical focus drives widespread
corticothalamic networks during spontaneous absence seizures in rats. J. Neurosci. 22, 1480â€“1495 (2002).
48. McCafferty, C. et al. Decreased but diverse activity of cortical and thalamic neurons in consciousness-impairing rodent
absence seizures. Nat. Commun. 14, 117 (2023).
23/42

49. Bai, X. et al. Dynamic time course of typical childhood absence seizures: EEG, behavior, and functional magnetic resonance
imaging. J. Neurosci. 30, 5884â€“5893 (2010).
50. Gluckman, B. J., Nguyen, H., Weinstein, S. L. & Schiff, S. J. Adaptive electric field control of epileptic seizures. J.
Neurosci. 21, 590â€“600 (2001).
51. Grziwotz, F. et al. Anticipating the occurrence and type of critical transitions. Sci. Adv. 9, eabq4558 (2023).
52. Rezaei, K. et al. Assessing the effectiveness of heart rate variability as a diagnostic tool for brain injuries in infants. In 2024
46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 1â€“4 (2024).
53. Zhang, X., Zhang, X., Huang, Q. & Chen, F. A review of epilepsy detection and prediction methods based on EEG signal
processing and deep learning. Front. Neurosci. 18, 1468967 (2024).
54. Ritchie, P. D., Alkhayuon, H., Cox, P. M. & Wieczorek, S. Rate-induced tipping in natural and human systems. Earth Syst.
Dyn. 14, 669â€“683 (2023).
55. Kramer, M. A. et al. Human seizures self-terminate across spatial scales via a critical transition. Proc. Natl. Acad. Sci. 109,
21116â€“21121 (2012).
56. Schindler, K., Leung, H., Elger, C. E. & Lehnertz, K. Assessing seizure dynamics by analysing the correlation structure of
multichannel intracranial EEG. Brain 130, 65â€“77 (2007).
57. Powell, K. L. et al. Seizure expression, behavior, and brain morphology differences in colonies of Genetic Absence Epilepsy
Rats from Strasbourg. Epilepsia 55, 1959â€“1968 (2014).
58. GregorÄiÄ, S. et al. Difficult to treat absence seizures in children: A single-center retrospective study. Front. Neurol. 13,
958369 (2022).
59. Depaulis, A., David, O. & Charpier, S. The genetic absence epilepsy rat from strasbourg as a model to decipher the neuronal
and network mechanisms of generalized idiopathic epilepsies. J. Neurosci. Methods 260, 159â€“174 (2016).
60. Dakos, V. et al. Slowing down as an early warning signal for abrupt climate change. Proc. Natl. Acad. Sci. 105, 14308â€“14312
(2008).
61. Lenton, T. M., Livina, V., Dakos, V., van Nes, E. H. & Scheffer, M. Early warning of climate tipping points from critical
slowing down: comparing methods to improve robustness. Philos. Trans. R. Soc. A: Math. Phys. Eng. Sci. 370, 1185â€“1204
(2012).
Acknowledgements
This publication has emanated from research conducted with the financial support of Taighde Ã‰ireann â€“ Research Ireland under
grant number [19/FFP/6782].
Author contributions statement
A.F. and S.W. conceived the experiments, A.F. conducted the experiments, A.F. and S.W. analysed the results, C.Mc C., F.D.,
and V.C. provided the voltage recordings, C.Mc C., K.L., and W.P.M. provided valuable discussions. A.F., C.Mc C., K.L.,
W.P.M., and S.W. reviewed the manuscript.
Additional information
The authors have no competing interests to disclose.
24/42

Supplementary information
S1: Applying the algorithm to detect seizure-like activity in the modelâ€™s output
In this subsection we illustrate some common outcomes that arise when using our algorithm to detect CTs in noisy time series
of the modelâ€™s output for each CT-type it can produce, namely BCTs, BNCTs, and NCTs.
NCTs: We first integrate the model from ğ‘¡= 0 to ğ‘¡= 200000 with model parameters specified in Table 2. We then apply our
algorithm to (ğ‘¥) the modelâ€™s output using the parameter values specified in Table 3. The algorithm detected (i) 193 CTs between
the NS and S states (i.e. 193 pairs of ğ‘¡1 and ğ‘¡2 values) and (ii) 86 almost-occurring CTs (i.e., a set of 86 values consisting of
different Ìƒğ‘¡1 and Ìƒğ‘¡2 values), examples of these are shown in Fig. S-1. For our convenience, in Figs. S-1 (a)-(d) we plot ğ‘¥versus
ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for the chosen values of ğ‘¡1, and in Fig. S-1 (e) we plot ğ‘¥versus Ìƒğ‘‡= ğ‘¡âˆ’Ìƒğ‘¡1 for the chosen value of Ìƒğ‘¡1. Solid vertical
lines indicate (in red) when ğ‘‡= 0 and (in green) when ğ‘‡= ğ‘¡2 âˆ’ğ‘¡1. Similarly, dashed vertical lines indicate (in red) Ìƒğ‘‡= 0 and
(in green) Ìƒğ‘‡= Ìƒğ‘¡2 âˆ’Ìƒğ‘¡1. The horizontal lines indicate the values of (in red) ğ›¼and (in green) ğ›½used by the algorithm.
-20
-10
0
10
20
30
40
-1
0
1
x
(a)
T
-20
-10
0
10
20
30
-1
0
1
x
(b)
T
-20
-10
0
10
20
30
40
-1
0
1
x
(c)
T
-20
-10
0
10
20
30
-1
0
1
x
(d)
T
-30
-20
-10
0
10
20
30
-1
0
1
x
(e)
ËœT
Figure S-1. Examples of NCTs in the modelâ€™s output (the variable ğ‘¥from System (5)) that were detected by the algorithm.
The modelâ€™s output is plotted versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for chosen values of ğ‘¡1 in (a)-(d), and versus Ìƒğ‘‡= ğ‘¡âˆ’Ìƒğ‘¡1 for a chosen Ìƒğ‘¡1 in (e).
Vertical lines indicate CTs from the (solid red) NS to S state and (solid green) S to NS state, and almost-occurring CTs from the
(dashed red) NS to S state and (dashed green) S to NS state. The model parameters are specified in Table 2. The algorithm
parameters are specified in Table 3, the horizontal lines indicate the corresponding thresholds of (red) ğ›¼and (green) ğ›½.
Figure S-1 (a) shows a clear-cut example of a CT between the NS and S states detected by the algorithm. Figure S-1 (b)
illustrates a common type of almost-occurring CT where (b1) and (b2) are satisfied but (b3) is not. Another almost-occurring
CT is shown in Fig. S-1 (c), however, even though ğ‘¥only exceeds ğ›¼once after briefly falling below ğ›½at the dashed vertical
green line, the algorithm still considers the model to be in the S state until (b1)-(b3) are satisfied. Figure S-1 (d) shows that the
algorithm does not treat the opposite scenario to (b) in the same way. Regardless of how many Ìƒğ‘¡1s are detected, the algorithm
still considers the model to be in the NS state until (a1)-(a3) are satisfied. The example in Fig. S-1 (e) illustrates the most
common almost-occurring CT from the NS to S state; (a1) and (a2) are satisfied but (a3) is not.
BCTs and BNCTs: We integrate the model from ğ‘¡= 0 to ğ‘¡= 60 with the parameters specified in Table 2, we set ğ‘ = 1 (ğ‘ = âˆ’1)
25/42

1
0
-1
-2
Âµ
(a)
-1
0
1
x
(b)
0
10
20
30
40
50
-1
0
1
x
t
(c)
Figure S-2. Examples of BNCTs and BCTs from the NS to S state in the modelâ€™s output (the variable ğ‘¥from System (5)) that
were detected by the algorithm. The evolution of ğœ‡versus ğ‘¡is shown in (a), an example of a BNCT is shown in (b) and a BCT
is shown in (c). Black vertical lines at ğ‘¡= 40 indicate when the respective bifurcations occur. Red vertical lines indicate CTs
from the NS to S state (ğ‘¡1). The model parameters are specified in Table 2. The algorithm parameters are specified in Table 3,
the horizontal lines indicate the thresholds of (in red) ğ›¼and (in green) ğ›½.
so that a BNCT (BCT) from the NS to S state can occur. Fig. S-2 (a) shows that ğœ‡increases linearly from âˆ’2 to 1 according to
Eq. (4). We apply our algorithm to (ğ‘¥) the modelâ€™s output using the parameters values specified in Table 3. In Figs. S-2 (b) and
(c) we illustrate an example of a BNCT and BCT detected by the algorithm. The vertical lines indicate (in black) when the
bifurcation takes place and (in red) when the algorithm detects a CT from the NS to S state, i.e., the value of ğ‘¡1. Figure S-2 (b)
shows the algorithm may detect a BNCT before the bifurcation takes place. The opposite is shown in Fig. S-2 (c) for the BCT.
We study this difference between values of ğ‘¡1 in greater detail in Fig. S-7 in S4.
S2: Applying the algorithm to detect real seizure activity in the voltage recordings
In this subsection we illustrate that our algorithm detects CTs between the NS and S states in the voltage recordings at times
which closely correspond with the expertâ€™s annotations. We also describe adjusting our algorithm to account for artefacts that
sometimes appear in voltage recordings but are excluded from expert annotations.
Algorithm vs. expert: We first apply our algorithm to the portion of voltage recordings shown in Fig. 4 (b) in the Results
section. To provide a comparison to the CTs detected in Figs. S-1 and S-2, we use the same ğœğ‘¤, Î”, ğœS, and ğœNS, and set ğ›¼= 0.055
and ğ›½= 0.04 since both the NS and S states in the modelâ€™s output are approximately an order of magnitude larger than the
corresponding states in the voltage recordings (see values on vertical axes). In Fig. S-3 we compare the values of ğ‘¡1 and ğ‘¡2
obtained by the algorithm to the corresponding times provided by the expert. The vertical lines indicate the values of ğ‘¡1 and ğ‘¡2
according to (in orange and purple) the expert and (in red and green) the algorithm. The horizontal lines indicate the values of
(in red) ğ›¼and (in green) ğ›½used by the algorithm. Figure S-3 shows that, for this choice of algorithm parameters, the expertâ€™s ğ‘¡1
is less than the algorithmâ€™s, and the algorithmâ€™s ğ‘¡2 is slightly less than the expertâ€™s. In this case, we say there is partial overlap
between the seizure intervals defined by the algorithmâ€™s and expertâ€™s seizure onset and offset times.
Accounting for artefacts: We now describe how we alter our algorithm to account for artefacts that sometimes appear in the
voltage recordings; see M1 of the Methods section for reasons why artefacts appear. In Fig. S-4 we show a typical example of
artefact activity for ğ‘¡âˆˆ[2,5]. We observe that, during this time interval, the LFP jumps between âˆ’0.15 and 0.15 mV in much
short time intervals, sometimes within consecutive measurements (0.001s). The algorithm may consider this as a CT from the
NS to S state since ğ›¼is crossed and the LFP continues to exceed ğ›½for longer than ğœS. Based on this observation, we adapt our
algorithm to account for artefacts by altering (a2) as follows:
(a2-1) |ğ‘¥(ğ‘¡)| exceeds ğ›¼at time ğ‘¡= ğ‘¡1 and |ğ‘¥(ğ‘¡+ğ›¿)âˆ’ğ‘¥(ğ‘¡)| < ğœ‰for all ğ‘¡âˆˆ[ğ‘¡1,ğ‘¡1 +ğ‘¡ğ‘¤
], or,
(a2-2) |ğ‘¥(ğ‘¡)| exceeds ğ›¼at time ğ‘¡= ğ‘¡1 and |ğ‘¥(ğ‘¡+ğ›¿)âˆ’ğ‘¥(ğ‘¡)| â‰¥ğœ‰for any ğ‘¡âˆˆ[ğ‘¡1,ğ‘¡1 +ğ‘¡ğ‘¤
].
If (a2-1) is true then the algorithm continues as before to evaluate whether a CT from the NS to S state occurs at ğ‘¡1. On the
26/42

25
30
0.0
0.1
t [s]
LFP [mV]
Figure S-3. Expert annotations vs. Algorithm. Vertical lines indicate values of ğ‘¡1 and ğ‘¡2 according to the (orange and purple)
expertâ€™s annotations and (red and green) algorithm for the same portion of the voltage recordings shown in Fig. 4 (b).
Algorithm parameters used: ğ›¼= 0.055, ğ›½= 0.04, ğœNS = 5, ğœS = 2, ğœğ‘¤= 1, and Î” = 0.001, horizontal lines indicate the
thresholds of (red) ğ›¼and (green) ğ›½.
0
2
4
6
8
-0.1
0
0.1
t [s]
LFP [mV]
Artefact
Ë†t1
Ë†t2
Figure S-4. Accounting for artefacts in the NS state. Vertical dashed lines indicate the (pink) beginning and (grey) end of
artefact activity according to our algorithm, i.e., the values of Ì‚ğ‘¡1 and Ì‚ğ‘¡2. Vertical solid lines indicate values of ğ‘¡1 according to
the (orange) expertâ€™s annotations and (red) algorithm when applied to a portion of the voltage recordings from session â€˜T8Câ€™.
Algorithm parameters used: ğ›¼= 0.055, ğ›½= 0.04, ğœNS = 5, ğœS = 2, ğœğ‘¤= 1, and Î” = 0.001, horizontal lines indicate the
thresholds of (in red) ğ›¼and (in green) ğ›½.
other hand, if (a2-2) is true, meaning that the difference between two consecutive measurements is greater than ğœ‰> 0, then we
say artefact activity begins at time ğ‘¡= Ì‚ğ‘¡1 = ğ‘¡1 and we continue to monitor |ğ‘¥(ğ‘¡)| in moving windows. We say artefact activity
ends at time ğ‘¡= Ì‚ğ‘¡2 if |ğ‘¥(Ì‚ğ‘¡2)| < ğ›¼and |ğ‘¥(ğ‘¡+ğ›¿)âˆ’ğ‘¥(ğ‘¡)| < ğœ‰for all ğ‘¡âˆˆ[Ì‚ğ‘¡2, Ì‚ğ‘¡2 +ğ‘¡ğ‘¤
]. We find most artefacts are accounted for by
choosing ğœ‰= 0.2.
While artefacts can appear at various locations in the voltage recordings, the example in Fig. S-4 is chosen to show that for
the current choice of ğœS and ğœNS, the algorithm would have considered the time when artefact activity begins as the time when a
CT from the NS to S state occurs. Furthermore, this alteration to the algorithm does not diminish the strong agreement between
the expert and the algorithm; see the corresponding ğ‘¡1 values in Fig. S-4.
Artefacts nearby CTs: After artefact activity has ended we find the following two scenarios unfold, |ğ‘¥(ğ‘¡)| either remains below
ğ›¼or quickly re-exceeds ğ›¼. The second scenario indicates that the artefact activity happens in conjunction with (i) additional
artefact activity, (ii) an almost-occurring CT from the NS to S state, or most importantly (iii) a CT from the NS to S state.
Fig. S-5 shows that for (iii), minor disagreements between the algorithm and the expert arise in terms of ğ‘¡1. More specifically,
Fig. S-5 shows an example of when the expertâ€™s ğ‘¡1 (vertical orange line) is similar to Ì‚ğ‘¡1 (vertical dashed pink line), the time
where the algorithm says that artefact activity begins. It is only after the artefact activity ends (vertical dashed grey line) that
the algorithm says there is a CT from the NS to S state (vertical red line).
While one may argue that further alterations should be made to the algorithm to improve the agreement between the algorithm
27/42

0
2
4
6
8
-0.2
-0.1
0
0.1
0.2
t [s]
LFP [mV]
Figure S-5. Accounting for artefacts in the NS state which happen in conjunction with a CT. Vertical dashed lines indicate the
(pink) beginning and (grey) end of artefact activity according to our algorithm, i.e., the values of Ì‚ğ‘¡1 and Ì‚ğ‘¡2. Vertical solid lines
indicate values of ğ‘¡1 according to the (orange) expertâ€™s annotations and (red) algorithm when applied to a portion of the voltage
recordings from session â€˜K7Eâ€™. Algorithm parameters used: ğ›¼= 0.1, ğ›½= 0.09, ğœNS = 3, ğœS = 2, ğœğ‘¤= 1, and Î” = 0.001,
horizontal lines indicate the thresholds of (in red) ğ›¼and (in green) ğ›½.
101
102
103
residence times in S state
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
probability density [arb. units]
(a)
101
102
103
residence times in NS state
(b)
Ïƒ = 0,
1,
2
Figure S-6. Probability density of residence times in the modelâ€™s (a) S state and (b) NS state for the values of ğœspecified in
the plot legend. The information presented here is based on 700 CTs between the NS and S states in the model as detected by
the algorithm using the parameter values specified in Tables 2 and 3.
and the expertâ€™s annotations in such instances, for the purposes of this paper (classifying seizure generation mechanisms), it is
more important that we detect this artefact activity in order to prevent the SVM from classifying the corresponding CT. See S10
for further details.
S3: Influence of shear on residence times in NS and S states for noise-induced CTs
In this subsection we examine how ğœ, the shear parameter in our model, influences the modelâ€™s residence times in the NS and
S states. More specifically, for ğœ= 0,1, and 2, we integrate the model forward in time up to ğ‘¡= 1Ã—106 with the remaining
model parameters as specified in NCT section of Table 2. We apply our algorithm to ğ‘¥, the modelâ€™s output, using the algorithm
parameter values specified in Table 3. Our algorithm detected the following number of CTs between the NS and S states for
different values of ğœ: 948 CTs for ğœ= 0, 984 CTs for ğœ= 1, and 715 CTs for ğœ= 2. We then compute residence times in the NS
and S states based on the ğ‘¡1 and ğ‘¡2 values obtained for each choice of ğœ. Since the number of CTs vary depending on ğœ, and we
would like to compare residence times across different values of ğœ, we compute the probability density of residence times in
the NS and S states for each ğœfor 700 randomly chosen residence times, reflecting the smallest number of CTs detected when
ğœ= 2. The resulting probability densities of residence times in the S state are shown in Fig. S-6 (a) and NS state in Fig. S-6 (b).
Figures S-6 (a) and (b) show that the model is most likely to spend a short duration of time in a given state and least likely
to spend a large duration of time. Figure S-6 (a) shows that ğœhas a strong influence on residence times in the S state, and
Fig. S-6 (b) shows it has no influence on the NS state. More specifically, Fig. S-6 (a) shows that as ğœincreases the probability
that the model spends (i) a relatively short duration of time in the S state slightly decreases, and (ii) a relatively large duration
28/42

0.0
0.2
0.4
(a)
Ïƒ = 0
0.77
1.00
BCT
BNCT
36
38
40
42
44
46
48
50
Detected values of t1 [arb. units]
0.0
0.2
0.4
(b)
Ïƒ = 2
0.75
1.03
probability density [arb. units]
Figure S-7. Probability density of ğ‘¡1 values for (blue) BCTs and (green) BNCTs with shear parameter set to (a) ğœ= 0 and (b)
ğœ= 2. The vertical black line indicates when the respective bifurcations take place, vertical red lines indicates the mean ğ‘¡1 for
each CT-type, and the vertical blue and green lines indicate the corresponding standard deviation (quoted the left of each red
line). The information presented here is based on 5000 examples of each CT-type.
of time in the S state increases significantly; for the largest recorded residence time in the S state when ğœ= 0, there is an
approximately equal probability that the model is in the S state for nearly ten times longer when ğœ= 2.
S4: Influence of shear on the detected ğ‘¡1 values for bifurcation and bifurcation/noise induced CTs
In this subsection we examine how ğœ, the shear parameter in our model, influences when BCTs and BNCTs from the NS to
S state occur. More specifically, for ğœ= 0 and 2, we generate 5000 different examples of BCTs and BNCTs for the model
parameters specified in Table 2. We apply our algorithm to ğ‘¥, the modelâ€™s output, using the algorithm parameter values specified
in Table 3. We then construct probability density diagrams (based on histograms) of the different ğ‘¡1 values detected by our
algorithm, these are shown in Fig. S-7 (a) for ğœ= 0 and Fig. S-7 (b) for ğœ= 2 where the data in blue corresponds to BCTs and in
green corresponds to BNCTs. In both Figs. S-7 (a) and (b) the vertical black line indicates when both bifurcations happen, the
vertical red lines indicate the mean ğ‘¡1 for both BCTs and BNCTs, the vertical blue and green lines indicate the corresponding
standard deviation (which is quoted to the left of each red line).
BNCTs: Figure S-7 shows that as ğœincreases there are relatively small changes in the statistics of the detected ğ‘¡1 values;
the mean shifts slightly to the left and the standard deviation slightly increases. For both ğœvalues the mean ğ‘¡1 value is relatively
close when the bifurcation happens. From additional experiments we found that decreasing ğœˆresults in the mean ğ‘¡1 value
increasing. If ğœˆis small enough then the CTs becomes purely bifurcation-induced. If ğœˆis too large then few CTs will take place
after the bifurcation. The chosen value of ğœˆin Table 2 enables the model to produce BNCTs with an almost equal likelihood
that noise or the bifurcation triggering the CT.
BCTs: Figure S-7 shows that as ğœincreases there are relatively small, but more noticeable, changes in the statistics of the
detected ğ‘¡1 values than BNCTs. Specifically, the mean ğ‘¡1 value and standard deviation both decrease.
Two aspects of Fig. S-7 are critical for the main aim of the paper of classifying CTs. The first is that there is no overlap
between the different distributions; BNCTs are designed to cluster around the time of bifurcation while BCTs are designed to
occur at a much greater time after the bifurcation. The second, there is no ğ‘¡1 < 35; this informs our choice of ğ‘‡âˆ’= âˆ’30 when
producing Fig. 6, allowing us to experiment with the relatively wide range of ğ‘¡ğ‘švalues in the interest of finding the optimal way
to differentiate between the three seizure generation mechanisms via our SVM classifier.
S5: Tuning algorithm parameters
In this subsection we describe the procedure used to tune the algorithmâ€™s parameters to maximise agreement between the
algorithm and the expertâ€™s annotation. By agreement we mean that, in the sense of a receiver-operator-characteristic (ROC)
analysis, the proportion of correctly labelled seizure intervals is maximal and the proportion of incorrectly labelled non-seizure
intervals is minimal, using the expertâ€™s annotations as the reference. Seizure and non-seizure intervals are defined by seizure
onset and offset times provided by the expert and the values of ğ‘¡1 and ğ‘¡2 obtained by the algorithm when applied to a given set
of voltage recordings. We consider a seizure interval defined by the algorithmâ€™s ğ‘¡1 and ğ‘¡2 values to be correctly labelled if it
(i) is contained within, (ii) contains, or (iii) partially overlaps with a seizure interval defined by the expertâ€™s annotations. See
29/42

100
101
102
103
residence times in S state
10âˆ’6
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
probability density [arb. units]
(a)
Expert
Algorithm
100
101
102
103
residence times in NS state
(b)
Expert
Algorithm
Figure S-8. Probability density of residence times in the (a) S state and (b) NS state for rat S according to the (orange and
purple) expertâ€™s annotations and (red and green) the algorithm.
100
101
102
103
residence times in S state
10âˆ’6
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
probability density [arb. units]
(a)
Expert
Algorithm
100
101
102
103
residence times in NS state
(b)
Expert
Algorithm
Figure S-9. Probability density of residence times in the (a) S state and (b) NS state for rat T according to the (orange and
purple) expertâ€™s annotations and (red and green) the algorithm.
Fig. S-3 in S2 for an example of (iii). Similarly, we consider an incorrectly labelled non-seizure interval to be a seizure interval
defined by the algorithmâ€™s ğ‘¡1 and ğ‘¡2 values that is contained within a non-seizure interval defined by the expertâ€™s annotations.
From several experiments we found that the above agreement is most sensitive to different values of ğ›¼. Therefore, for each
recording session, we select a different ğ›¼from [0.03,0.1] to obtain the best agreement with the expert annotations. ğ›½is adjusted
accordingly so that ğ›½= ğ›¼âˆ’0.01. The values of ğœğ‘¤, Î”, ğœS, and ğœNS specified in Table 3 provide a strong agreement between the
algorithm and expert across all the analysed voltage recordings from rats S, T, and K. This strong agreement is illustrated in
Figs. S-8, S-9, and S-10 in terms of the probability distributions of residence times in the NS and S states for rats S, T, and K.
Figure S-8 shows there is relatively little difference between the probability distributions of residence times in the NS and S
states according to the expert and the algorithm when computed from all the voltage recordings of rat S. Small differences occur
at the end points between the corresponding distributions, this is mainly due to the choice of ğœNS and ğœS being less than the
smallest residence times in the NS and S states according to the expertâ€™s annotations. As a result, a given seizure/non-seizure
interval according to the algorithm may consist of multiple seizure and non-seizure intervals according to the expert. However,
when ğœNS and ğœS are more in line with the expertâ€™s annotations, we find this significantly reduces the agreement between the
algorithm and the expert. Figures S-9 and S-10 show that similar results are obtained for rats T and K.
S6: Additional clarification on how TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) are computed for CTs in the modelâ€™s output
We obtain 100 BCTs, 100 BNCTs, and 100 NCTs from the modelâ€™s NS to the S state as follows. For NCTs, we use the model
parameters specified in Table 2, algorithm parameters specified in Table 3, and integrate the model until our algorithm detects
100 CTs between the NS and S states that satisfy the following criteria: the model is in the NS state for all ğ‘¡âˆˆ[ğ‘¡1 +ğ‘‡âˆ’, ğ‘¡1
)
and
is in the S state for all ğ‘¡âˆˆ[ğ‘¡1, ğ‘¡1 +ğ‘‡+] where ğ‘‡âˆ’= âˆ’30 and ğ‘‡+ = 10. For BCTs and BNCTs, we use the model parameters
30/42

100
101
102
103
residence times in S state
10âˆ’6
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
probability density [arb. units]
(a)
Expert
Algorithm
100
101
102
103
residence times in NS state
(b)
Expert
Algorithm
Figure S-10. Probability density of residence times in the (a) S state and (b) NS state for rat K according to the (orange and
purple) expertâ€™s annotations and (red and green) the algorithm.
specified in Table 2, different noise realisations for each simulation of the model, and detect CTs using algorithm parameters
specified in Table 3. In Fig. S-7 in S4 we show that all these BCTs and BNCTs satisfy the above two conditions as each ğ‘¡1 > 30
and ğ‘¡2 âˆ’ğ‘¡1 > 10. Further, we only consider CTs where no almost-occuring CTs appear for ğ‘¡âˆˆ[ğ‘¡1 +ğ‘‡âˆ’, ğ‘¡1
).
Based on the above we obtain 100 examples of BCTs, NCTs, and BNCTs from the model. When computing the TSPs
and ğ‘š(TSPs, ğ‘¡ğ‘š) near each CT, it is convenient to work with a new time ğ‘‡= ğ‘¡âˆ’ğ‘‡âˆ’1 where ğ‘‡âˆˆ[ğ‘‡âˆ’,ğ‘‡+] = [âˆ’30,10] and
each CT is detected at ğ‘‡= 0. We then follow the steps in M2 of the Methods section to compute the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š)
for a given ğ‘¡ğ‘¤= 1 and chosen values of ğ‘¡ğ‘š. This choice of ğ‘‡âˆ’and ğ‘‡+ results in the GV, log10GV, and AC being defined
from ğ‘‡= ğ‘‡âˆ’+ ğ‘¡ğ‘¤= âˆ’29, and log10GV(AC) is defined from ğ‘‡= ğ‘‡âˆ’+ 2ğ‘¡ğ‘¤= âˆ’28. Further, the first values of ğ‘š(GV, ğ‘¡ğ‘š),
ğ‘š(log10GV, ğ‘¡ğ‘š), and ğ‘š(AC, ğ‘¡ğ‘š) are defined from ğ‘‡= ğ‘‡âˆ’+ğ‘¡ğ‘¤+ğ‘¡ğ‘šand ğ‘š(log10GV(AC), ğ‘¡ğ‘š) is defined from ğ‘‡= ğ‘‡âˆ’+2ğ‘¡ğ‘¤+ğ‘¡ğ‘š.
Thus, if ğ‘¡ğ‘š= 8 then ğ‘š(GV, 8) is defined from ğ‘‡= âˆ’21 and ğ‘š(log10GV(AC), 8) is defined from ğ‘‡= âˆ’20, and so on.
S7: TSPs for a single example of each CT type
In this subsection we show how the time series properties (TSPs) of the modelâ€™s output behave nearby an example of a BCT,
BNCT, and NCT from the NS to S state. Each of these TSPs are specified in Table 5. We generate and detect the BCT and
BNCT in Figs. S-11 (a) and (b) using the same parameters as in Fig. S-2. For the NCT case in Fig. S-11 (c) we generate and
detect a CT for the same parameters from Fig. S-1 and consider a similar example of an NCT to Fig. S-1 (a). For each of the
CTs shown in Figs. S-11 (a)-(c), we plot the modelâ€™s output, ğ‘¥, versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for the corresponding values of ğ‘¡1 detected by
our algorithm. We analyse the TSPs of each CT from ğ‘‡= âˆ’30 in small moving windows of length ğ‘¡ğ‘¤= 1. Thus, for the reasons
outlined in M2 of the Methods section, the GV, log10GV, and AC are defined from ğ‘‡= âˆ’29, and log10GV(AC) is defined from
ğ‘‡= âˆ’28. We now discuss how each of these TSPs behave in relation to the different CT types.
GV: Figures. S-1 (d) to (f) show that the GV of the modelâ€™s output in the S state is much larger than the NS state for each
CT-type. One would naturally expect this to happen when a transition from a small to a large-amplitude of oscillation occurs.
In the BCT case (Fig. S-11 (d)), there is a noticeably large and relatively constant increase in GV from ğ‘‡â‰ˆâˆ’7 onwards, which
is just before the bifurcation point indicated by the inward black tick. For the BNCT and NCT cases (Figs. S-11 (e) and (f)), no
significant change in GV is seen before the CTs are detected. However, after the CTs are detected, the increase in GV is much
greater for the BNCT case than the NCT case. This difference arises because the radius of the limit cycle for the BNCT case is
larger than for the NCT case (see Eq. (3)), thereby resulting in a more dramatic change in the modelâ€™s output.
log10GV: Figures S-11 (g) and (h) show log10GV increases at a relatively small but constant rate before the CT in the BCT and
BNCT cases, whereas Fig. S-11 (i) shows no such increase for the NCT case. Similar to the GV results, there is a sudden and
large increase after the CT for both the BNCT and NCT cases. Notably, for the BCT case, log10GV starts to increase at an
earlier time than GV in Fig. S-11 (a), thus demonstrating the benefit of monitoring log10GV in addition to GV.
AC: Figures S-11 (j) and (k) show AC increases at a relatively constant rate before the BCTs and BNCTs. For the BCT case,
AC remains close to 1 from ğ‘‡â‰ˆâˆ’7 (mirroring the trend seen in Fig. S-11 (d)). On the other hand, it is only after the CT that
AC remains near 1 for the BNCT case. In stark contrast, Fig. S-11 (l) shows that the AC remains relatively close to 1 for the
NCT case, with no trends present long before the CT. Interestingly, there is a local minimum in AC present before each CT. This
occurs because, regardless of the ACâ€™s value before the CT, the modelâ€™s output becomes highly correlated with itself after the
CT due to the inherent periodicity of the limit cycle, thus causing the AC to quickly increase to 1. We also see less fluctuations
in the AC as the CT is approached for the BCT and BNCT cases.
31/42

-1
0
1
x
BCT
(a)
0.0
0.2
0.4
0.6
GV
(d)
-1
-2
-3
log10GV
(g)
0.97
0.98
0.99
1.00
AC
(j)
-30
-20
-10
0
-9
-7
-5
log10GV(AC)
T
(m)
BNCT
(b)
(e)
(h)
(k)
-30
-20
-10
0
T
(n)
NCT
(c)
(f)
(i)
(l)
-30
-20
-10
0
T
(o)
Figure S-11. Time series properties (TSPs) of the modelâ€™s output near an example of a (left-hand column) BCT, (middle
column) BNCT, and (right-hand column) NCT. For each CT-type, the top row shows the modelâ€™s output versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1 with
ğ‘¡1 detected by the algorithm. Inward black ticks in (a) and (b) indicate when the bifurcation occurs. The remaining rows show
how each of the TSPs, specified in the vertical axes labels and in Table 5, behave for the same values of ğ‘‡. Vertical red line in
each panel emphasises when ğ‘‡= 0.
log10GV(AC): Figures. S-11 (m)-(o) provide a more informative picture on the fluctuations mentioned above by examining
how log10GV(AC) changes over time. For the BCT and BNCT cases, there is an overall constant decrease in log10GV(AC)
before the CT. Log10GV(AC) remains at a similar and constant value after both CTs. For the NCT case, log10GV(AC) remains
relatively constant before the CT and quickly decreases to a smaller value after the CT than the BCT and BNCT.
S8: Modelâ€™s TSPs computed for ğ‘¡ğ‘¤= 1 and 2
In this subsection we provide clarification on comments made earlier regarding tuning the modelâ€™s parameters so that its output
mimics the three important characteristics of real seizure activity in the voltage recordings. In particular, we mentioned that we
compromise on characteristic (iii), â€œIntrinsic time scales of the NS and S states.â€. We now show that there are little consequences
from this in terms of the main aim of this paper, to identify the CT-type responsible for generating a given seizure in voltage
recordings.
In Fig.4 (c) and (d), we showed that the timescale of oscillation of the modelâ€™s S state is approximately half that of the
dominant timescale of real seizure activity. Thus, when choosing a ğ‘¡ğ‘¤to compute the TSPs (the time duration of the moving
window), the value of ğ‘¡ğ‘¤for the model should be twice that of the value used for the voltage recordings. However, we show in
Fig. S-12 that there are no major differences in the TSPs of the model when computed using either ğ‘¡ğ‘¤= 1 (in blue) or ğ‘¡ğ‘¤= 2 (in
32/42

0.0
0.2
0.4
0.6
mean(GV)
(a)
BCT
tw = 1
tw = 2
(b)
BNCT
(c)
NCT
-3
-2
-1
mean(log10GV)
(d)
(e)
(f)
0.98
0.99
1.00
mean(AC)
(g)
(h)
(i)
-30
-20
-10
0
-8
-6
mean(log10GV(AC))
T
(j)
-30
-20
-10
0
T
(k)
-30
-20
-10
0
T
(l)
Figure S-12. Comparing different choices of ğ‘¡ğ‘¤when computing the time series properties (TSPs) of modelâ€™s output near
BCTs (left-hand column), BNCTs (middle column), and NCTs (right-hand column). Each panel shows the mean of each TSP
versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for the ğ‘¡1 values detected by the algorithm. Each curve represents the mean computed from 100 examples of
each CT-type for a given ğ‘‡.
orange). We also find that whether the SVM is trained using TSPs that are calculated with ğ‘¡ğ‘¤= 1 or 2, there is little impact on
classifying seizure generation mechanisms in the voltage recordings. For convenience, we choose ğ‘¡ğ‘¤= 1 when computing TSPs
from both the model and the voltage recordings.
S9: Computing SVM feature importance via the mean permutation importance (MPI)
In this subsection we follow the steps outlined in M3 of the Methods section on how to compute the â€˜mean permutation
importanceâ€™ (MPI) in order to quantify the importance of each feature used to construct SVM type-1, 2, and 3 (see Table 6 for
details on which features are used to construct each SVM). The results discussed in this subsection serve to accompany the
results shown in Fig. 8 where SVM accuracy versus ğ‘‡is shown for SVM type-1, 2, and 3 with ğ‘¡ğ‘š= 4,8, and 12. Since we
trained and tested a new SVM for different values of ğ‘‡in Fig. 8, we compute the MPI the corresponding values of ğ‘‡.
Figures S-13 and S-14 show the resulting MPI versus ğ‘‡. Each of the solid and dashed curves are coloured to correspond
with the features specified in the legend. These curves are surrounded by a cloud of the corresponding colour which describes
the standard deviation of the MPI for a given feature. Note, the results shown in Fig. S-13 (c) informed the choice of features
that were used to construct Fig. 7.
Figures S-13 and S-14 show that for each SVM, (i) some features are much more important than others and (ii) the importance
of each feature varies depending on the value of ğ‘‡, (iii) and also varies depending on the value of ğ‘¡ğ‘šin the case of SVM type-2
and 3. For instance, when comparing the results shown in Figs. S-13 (a) and (e) for ğ‘‡> 5, Fig. S-13 (a) shows that when ğ‘¡ğ‘š= 4
the most important feature is ğ‘š(log10GV, ğ‘¡ğ‘š) and least important feature is ğ‘š(log10GV(AC), ğ‘¡ğ‘š), whereas Fig. S-13 (c) shows
33/42

m(GV, tm)
GV
m(log10GV, tm)
log10GV
m(AC, tm)
AC
m(log10GV(AC), tm)
log10GV(AC)
0.0
0.2
0.4
(a) SVM type-2: tm = 4
(b) SVM type-3: tm = 4
0.0
0.2
0.4
Mean permutation importance
(c) SVM type-2: tm = 8
(d) SVM type-3: tm = 8
-10
-5
0
5
0.0
0.2
0.4
T
(e) SVM type-2: tm = 12
-10
-5
0
5
T
(f) SVM type-3: tm = 12
Figure S-13. Feature importance for SVMs type-2 and 3: Mean permutation importance (solid and dashed curves) and its
standard deviation (cloud surrounding each curve) at time ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for a given feature specified in the plot legend for (left-hand
column) SVM type-2 and (right-hand column) SVM type-3 with (a) and (b) ğ‘¡ğ‘š= 4, (c) and (d) ğ‘¡ğ‘š= 8, and (e) and (f) ğ‘¡ğ‘š= 12.
-10
-5
0
5
0.0
0.2
0.4
Mean permutation importance
T
SVM type-1
GV
log10GV
AC
log10GV(AC)
Figure S-14. Feature importance for SVM type-1: Mean permutation importance (dashed curves) and its standard deviation
(cloud surrounding each curve) at time ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for a given feature specified in the plot legend.
the opposite for ğ‘¡ğ‘š= 12.
It is also worth noting the significant changes in the MPI of some features near ğ‘‡= 0 for SVM type-2 and 3. For example,
Fig. S-13 (a) shows that ğ‘š(GV, ğ‘¡ğ‘š) suddenly changes from being the most important to least important and back to the most
34/42

important feature over a small range of ğ‘‡values near 0. On the other hand, Fig. S-13 (a) shows that ğ‘š(log10GV, ğ‘¡ğ‘š) and
ğ‘š(log10GV(AC), ğ‘¡ğ‘š) change from being the least important to most important and back to least important feature over a small
range of ğ‘‡values between 0 and 5. Similar behaviour is seen in Figs. S-13 (c) and (e) but to a much less significant extent.
While Figs. S-13 (a), (c), and (e) show that the most important feature for SVM type-2 often varies for ğ‘‡> 0, a different
picture emerges for SVM type-3 in Figs. S-13 (b), (d), and (f) as log10GV(AC) is the most importance feature for each choice
of ğ‘¡ğ‘šwhen ğ‘‡> 0. Similar behaviour is shown in Fig. S-14 for SVM type-1. Figures S-13 (b), (d), and (f) also show that the
ğ‘š(TSPs, ğ‘¡ğ‘š) play a more significant role in the accuracy of SVM type-3 for larger ğ‘¡ğ‘šbefore the CT. This further illustrates
the effects of combining the TSPs and their slopes in SVM type-3. More specifically, for ğ‘‡< 0 the features which are most
important for SVM type-2 are the most important for SVM type-3, for ğ‘‡> 0 the features which are most important in SVM
type-1 are the most important for SVM type-3.
S10: TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) for CTs not suitable for classification
In this subsection we show why it is not suitable to classify CTs from the NS to S state when artefacts or almost-occurring CTs
happen in close proximity to the CT. More specifically, we show that some of the corresponding TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) listed in
Table 5 deviate significantly from those of a typical CT from the NS to S state when artefacts or almost-occurring CTs happen
nearby. This is important to show because if the SVM classifies CTs using these TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) then the classifications
are much less reliable and may skew the main results presented in Table 4.
In Fig. S-15 we show the examples of CTs that we compute the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) of in this subsection. In each example
we plot the LFP versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1, where ğ‘¡1 corresponds to the time where the algorithm detects a CT from the NS to S state.
The example in Fig. S-15 (a) (same as in Fig. S-3) is a typical example of a CT that satisfies C1-C5 and is chosen as a reference
point to compare with the following examples of CTs that satisfy C1-C3 but dot not satisfy C4 or C5: Fig. S-15 (b) (taken from
recording session â€˜T8Câ€™) shows an example of an almost-occurring CT before a CT from the NS to S state, Fig. S-15 (c) (same
as in Fig. S-4) shows an example of when artefact activity occurs before a CT from the NS to S state, and Fig. S-15 (d) (same as
in Fig. S-5) shows an example of when artefact activity occurs in conjunction with a CT from the NS to S state. Note also that
the example in Fig. S-15 (b) provides similar insight to a CT that does not satisfy C1 and a CT generated by the model that does
not satisfy the corresponding selection criteria. The horizontal red and green lines in each panel of Fig. S-15 indicate the values
of ğ›¼and ğ›½used to detect CTs between the NS and S states, these values are chosen according to the procedure outlined in S5.
The vertical red solid lines indicates a CT from the NS to S state. Vertical dashed lines indicate (in red) an almost-occurring CT
from the NS to S state, (in pink) the beginning and (in grey) end of artefact activity.
In Fig. S-16 we show how each of the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) with ğ‘¡ğ‘š= 8 behave versus ğ‘‡for the voltage recordings shown
in Fig. S-15. More specifically, the different coloured curves in each panel of Fig. S-16 correspond to a given TSPs or ğ‘š(TSPs,
ğ‘¡ğ‘š) of the examples shown in Fig. S-15;:the blue curves correspond to Fig. S-15 (a), the green curves correspond to Fig. S-15 (b),
the orange curves correspond to Fig. S-15 (c), and the red curves correspond to Fig. S-15 (d). The different coloured vertical
lines have the same meaning as in Fig. S-15. The pink and grey vertical dashed lines at ğ‘‡â‰ˆâˆ’5 and âˆ’2.25 correspond with
the example shown in Fig. S-15 (c), the other pink and grey vertical dashed lines at ğ‘‡â‰ˆâˆ’1.25 and âˆ’0.1 correspond with the
example shown in Fig. S-15 (d).
Figure S-16 shows that when ğ‘‡= 0 there is an almost immediate change in the TSPs and a slightly delayed and longer
lasting change in the ğ‘š(TSPs, ğ‘¡ğ‘š) in response to the presence of CTs that almost occur and the different artefacts. This delayed
and longer lasting change in the ğ‘š(TSPs, ğ‘¡ğ‘š) results in major differences from the ğ‘š(TSPs, ğ‘¡ğ‘š) of a typical CT without any
artefacts or almost-occurring CTs nearby, i.e., when comparing the blue curves to the others. These differences are visible
at several values of ğ‘‡and, crucially for our SVM setup, are noticeable for ğ‘‡= 2 which is the value of ğ‘‡we base our final
results on in Table 4, i.e., the value of ğ‘‡where our SVM classifies CTs in the voltage recordings. In the following headings we
highlight these differences.
Almost-occurring CT: (Comparing blue and green curves at ğ‘‡= 2) Considerable differences particularly in ğ‘š(log10GV, ğ‘¡ğ‘š)
and ğ‘š(AC, ğ‘¡ğ‘š).
Artefact before CT: (Comparing blue and orange curves at ğ‘‡= 2) Major differences particularly in ğ‘š(GV, ğ‘¡ğ‘š) and
ğ‘š(log10GV, ğ‘¡ğ‘š) but quite similar in ğ‘š(AC, ğ‘¡ğ‘š) and ğ‘š(log10GV(AC), ğ‘¡ğ‘š).
Artefact in conjunction with CT: (Comparing blue and red curves at ğ‘‡= 2) Major differences in all ğ‘š(TSPs, ğ‘¡ğ‘š), there
appears as if no changes happens in ğ‘š(AC, ğ‘¡ğ‘š) and ğ‘š(log10GV(AC), ğ‘¡ğ‘š).
As a final word, we would like to add the following for clarification. Since the differences highlighted above are only present
in the ğ‘š(TSPs, ğ‘¡ğ‘š) when ğ‘‡= 2 one may argue that in the interest of classifying as many CTs in the voltage recordings as
possible, one should do so using SVM type-1 instead of SVM type-3 so that less CTs like those shown in Fig. S-15 need to be
excluded via conditions C1-C5. However, we found that SVM type-1 provides classifications which are in total disagreement
with the fundamental characteristics of the different CTs that are captured by the ğ‘š(TSPs, ğ‘¡ğ‘š); there were instances when some
CTs were classified as BCTs without the typical the increase in GV and AC before the CT, and other CTs were classified as
35/42

-0.2
-0.1
0
0.1
LFP (a)
Filtered CT
-0.2
-0.1
0
0.1
LFP (b)
Almost CT
-0.2
-0.1
0
0.1
LFP
(c)
Artefact before CT
-6
-4
-2
0
2
-0.2
-0.1
0
0.1
T
LFP (d)
Artefact during CT
Figure S-15. Examples of detected CTs in voltage recordings that (a) are suitable for classification, (b)-(d) are not suitable for
reasons given in each plot legend, and are plotted versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1 for ğ‘¡1 values detected by our algorithm. Note, in (d), the
â€˜artefact during CTâ€™ label refers to the â€˜artefact in conjunction with a CTâ€™ scenario. The vertical lines indicate (solid red) when
ğ‘‡= 0, (dashed red) an almost-occurring CT from the NS to S state, (dashed pink and grey) the beginning and end of artefact
activity, according to our algorithm. Horizontal solid lines indicate thresholds of (red) ğ›¼and (green) ğ›½used by the algorithm.
NCTs with increases in GV and AC before the CT that are typical of BCTs. Therefore it is necessary to consider both the TSPs
and the ğ‘š(TSPs, ğ‘¡ğ‘š) when classifying CTs in voltage recordings of real seizure activity even if this restricts the amount of CTs
that we can classify.
S11: SVM classifications and mean feature fit error vs. T
In this subsection we show how the proportion of filtered CTs classified as a particular CT type for a given ğ‘‡, denoted by
ğ‘type(ğ‘‡)âˆ•ğ‘filt(ğ‘‡âˆ’,ğ‘‡+) âˆˆ[0,1], varies for ğ‘‡âˆˆ[âˆ’10,10] when setting ğ‘‡âˆ’= âˆ’20 and ğ‘‡+ = 10. These classification are provided
by the SVM type-3 with ğ‘¡ğ‘š= 8 setup. The TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) with ğ‘¡ğ‘š= 8 we use for this SVM are computed from the
voltage recordings of rat K. CTs between the NS and S states are detected by our algorithm (using parameters in Table 3) and
the CTs that are classified all satisfy C1-C5. In total there are ğ‘filt(âˆ’20,10) = 95 CTs eligible for classification, â‰ˆ10% of the
detected CTs. Despite only classifying such a small amount of the detected CTs, the proportion of CTs classified as a particular
CT-type when ğ‘‡= 2 in this subsection are consistent with those presented in Table 4 for rat K (â‰ˆ25% BCT, â‰ˆ25% BNCT,
â‰ˆ50% NCT). We also assess the accuracy of these classifications in terms of how the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) of the classified
voltage recordings compare to those of the modelâ€™s output for each CT type.
SVM classifications: Figure S-17 (a) shows how ğ‘type(ğ‘‡)âˆ•ğ‘filt(âˆ’20,10) varies for different values of ğ‘‡âˆˆ[âˆ’10,10], this
36/42

.00
.01
GV
(a)
Filtered CT
Almost CT
Artefact before CT
Artefact during CT
-1
0
1
m(GV, 8) x10âˆ’3
(b)
-4
-3
-2
log10GV
(c)
0
2
m(log10GV, 8) x10âˆ’1
(d)
0.6
0.8
1.0
AC
(e)
-2
0
2
4
m(AC, 8) x10âˆ’2
(f)
-6
-4
-2
0
2
-6
-4
-2
log10GV(AC)
T
(g)
-6
-4
-2
0
2
-4
-2
0
m(log10GV(AC), 8) x10âˆ’1
T
(h)
Figure S-16. TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) of the voltage recording examples shown in Fig. S-15 and plotted versus the same ğ‘‡
values where the colour of each curve corresponds to the different examples specified in plot legend. The vertical lines indicate
(solid red) when ğ‘‡= 0, (dashed red) an almost-occurring CT from the NS to S state, (dashed pink and grey) the beginning and
end of artefact activity, according to our algorithm.
tells us what proportion of the filtered CTs are classified as a particular CT type; BCT in blue, BNCT in green, and NCT in red.
Figure S-17 (a) shows that long before the CTs, i.e., for ğ‘‡< âˆ’5, ğ‘type(ğ‘‡)âˆ•ğ‘filt(âˆ’20,10) is relatively similar for each CT type,
i.e., the SVM cannot distinguish between CT types for ğ‘‡< âˆ’5. For ğ‘‡âˆˆ[âˆ’5,0), Fig. S-17 (a) shows that ğ‘type(ğ‘‡)âˆ•ğ‘filt(âˆ’20,10)
varies significantly and there are times where different CT types appear to be the dominant seizure generation mechanism.
However, just after the CT takes place, specifically, for ğ‘‡âˆˆ[0,1], there is a major increase in the proportion of CTs classified as
NCT, balanced by a significant decrease in the proportion of CTs classified as either BCT or BNCT. For ğ‘‡âˆˆ(1,5] we note that
ğ‘type(ğ‘‡)âˆ•ğ‘filt(âˆ’20,10) remains relatively constant, thus providing a range of ğ‘‡values where more reliable classifications can
be obtained. Similar to the results presented in Fig. 9, we see that for ğ‘‡= 2 in Fig. S-17 (a), NCTs are also the dominant seizure
generation mechanism for this portion of rat Kâ€™s CTs. For ğ‘‡> 5, the proportion of CTs classified as NCT remains relatively
close to 0.5, BCT starts to increase and is balanced by a near equal decrease in BNCT. Fig. 6 shows this happens because the
TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) of each CT-type are much less distinguishable from each other for ğ‘‡> 5. Thus, the classification are less
reliable for ğ‘‡> 5.
Mean feature fit error: While our main results are computed using an SVM that achieves perfect accuracy when classifying
the modelâ€™s CTs, we are not able to quantify the accuracy of the SVM when used to classify CTs in the voltage recordings in the
same way since their generation mechanisms are unknown. Instead, we take the following steps to compare the classifications
of CTs in the voltage recordings to the corresponding CTs in the modelâ€™s output:
1. After classifying CTs in the voltage recordings, organise their features (i.e., the TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š)) into groups based
on the classifications, i.e., groups of TSPs and ğ‘š(TSPs, ğ‘¡ğ‘š) that correspond to BCTs, BNCTs, and NCTs.
37/42

0.2
0.4
0.6
Ntype(T)/Nï¬lt(âˆ’20, 10)
(a)
BCT
BNCT
NCT
-10
-5
0
5
0.2
0.4
MFFE
T
(b)
Figure S-17. Classification of seizure generation mechanisms and MFFE (mean feature fit error) versus ğ‘‡. (a) shows
ğ‘type(ğ‘‡)âˆ•ğ‘filt(âˆ’20,10) versus ğ‘‡for rat K, i.e., proportion of filtered CTs that are classified as (blue) BCT, (green) BNCT, or
(red) NCT, for a given ğ‘‡with ğ‘‡âˆ’= âˆ’20 and ğ‘‡+ = 10. (b) shows the MFFE computed for each CT-type versus ğ‘‡, inward
coloured ticks indicate the smallest MFFE for the corresponding CT-type.
2. Compute the mean value of each feature in the different groups for a given ğ‘‡, e.g., compute the mean GV at a given ğ‘‡for
the group of CTs classified as BCTs, BNCTs, and NCTs and repeat for each feature.
3. Compare each mean to the equivalent quantities computed from the modelâ€™s output (first and third columns of Fig. 6) by:
(i) Performing a min-max normalisation of the means of each feature.
(ii) Computing the root mean square error (RMSE) between corresponding normalised curves obtained from (i), see
Figs. S-18 and S-19 for an example.
(iii) Computing the â€˜mean feature fit errorâ€™ (MFFE): the mean of the RMSEs over all features for a given ğ‘‡.
In Fig. S-17 (b) we plot the MFFE versus ğ‘‡having applied steps 1-3 listed above to the data used to generate Fig. S-17 (a).
Figure S-17 (b) shows there is a relatively strong agreement between NCTs in the model and CTs in the voltage recordings that
are classified as NCT. Figure S-17 (b) also shows there is an apparent weaker agreement between CTs that are classified as
BNCT and an even weaker agreement for BCT.
To more closely investigate how well the features from the classified CTs in the voltage recordings mimic their counterparts
in the modelâ€™s output, we choose ğ‘‡= âˆ’1.3 as a point to visually compare the normalised means of the features from the model
and the voltage recordings in Figs. S-18 and S-19. This time corresponds to when the MFFE is nearest to 0 for the BCT case.
Figures S-18 and S-19 show there are several qualitative similarities between the classified CTs in the voltage recordings
and the corresponding CTs generated by the model. For instance, the first column in both Figs. S-18 and S-19 show that CTs
classified as BCTs exhibit strong signatures of critical slowing down (CSD), i.e., increases in GV and AC before the CT, thus
indicating that the ratâ€™s brain has drifted towards and then past a bifurcation point that generates the seizure.
38/42

0
1
GVâˆ—
0.1649
(a) BCT
GAERS
Model
0.1254
(b) BNCT
GAERS
Model
0.1135
(c) NCT
GAERS
Model
0
1
log10GVâˆ—
0.2512
(d)
0.0983
(e)
0.0695
(f)
0
1
ACâˆ—
0.3992
(g)
0.3171
(h)
0.1286
(i)
-10
-5
0
5
0
1
log10GV(AC)âˆ—
0.3772
T
(j)
-10
-5
0
5
0.206
T
(k)
-10
-5
0
5
0.1728
T
(l)
Figure S-18. Comparing normalised values of TSPs of (dashed curves) the model to the corresponding quantities obtained
from the (solid curves) voltage recordings that were classified by the SVM for ğ‘‡= âˆ’1.3 (the value of ğ‘‡where the best fit of the
BCTs occur, shown in Fig. S-17 (b)). Each curve is coloured according to the class specified in the top left corner of panels
(a)-(c). RMSE between the curves is quoted beside each pair of curves.
Figures S-18 and S-19 also show that, in comparison to the BCT case, there are significantly smaller RMSEs between the
curves in the BNCT and NCT cases. For instance, panels (e) and (f) in Figs. S-18 and S-19 illustrate the strong agreement
between the model and the voltage recordings in terms of the normalised log10GV and ğ‘š(log10GV,8), further indicated by the
small value of the RMSE quoted beside these curves. However, the same cannot be said for all features. For instance, panel (h)
in Figs. S-18 and S-19 shows there is much poorer agreement between BNCTs in terms of the normalised AC and ğ‘š(AC,8)
curves. The large RMSE quoted beside these curves provides the greatest contribution to the MFFE shown in Fig. S-17 (b). The
same issue arises for the BCT case as panel (g) in Figs. S-18 and S-19 show the RMSE for the normalised AC and ğ‘š(AC,8)
curves is also significantly large. What is important in both BCT and BNCT cases is that there is a clear increase in AC before
the CT, however, it is the rate of increase that differs from the modelâ€™s. Importantly, panel (i) in Figs. S-18 and S-19 show no
gradual increase in AC before NCTs in the voltage recordings, which is consistent with the corresponding CTs generated by the
model.
Figure S-20 shows the TSPs for a single example of each CT-type in the voltage recordings of rat K which were classified
with the same SVM as in Figs. S-18 and S-19. In these examples, one can see a gradual increase in variance and autocorrelation
prior to the BCT and BNCT, and no such increase in variance and autocorrelation prior to the NCT. When compared to Fig. S-11,
one can observe in greater detail how TSPs corresponding to different CT-types in the voltage recordings align with those of the
39/42

0
1
m(GV, 8)âˆ—
0.3409
(a) BCT
GAERS
Model
0.1943
(b) BNCT
GAERS
Model
0.1216
(c) NCT
GAERS
Model
0
1
m(log10GV, 8)âˆ—
0.4326
(d)
0.0339
(e)
0.042
(f)
0
1
m(AC, 8)âˆ—
0.6687
(g)
0.5788
(h)
0.0855
(i)
-10
-5
0
5
0
1
m(log10GV(AC), 8)âˆ—
0.6214
T
(j)
-10
-5
0
5
0.2746
T
(k)
-10
-5
0
5
0.1352
T
(l)
Figure S-19. Comparing normalised values of ğ‘š(TSPs, ğ‘¡ğ‘š) with ğ‘¡ğ‘š= 8 of the (dashed curves) model to the corresponding
quantities obtained from (solid curves) the voltage recordings that were classified by the SVM for ğ‘‡= âˆ’1.3 (the value of ğ‘‡
where the best fit of the BCTs occur, shown in Fig. S-17 (b)). Each curve is coloured according to the class specified in the top
left corner of panels (a)-(c). RMSE between the curves is quoted beside each pair of curves.
modelâ€™s output.
S12: Applying filtering conditions to detected CTs
In this subsection we provide a more detailed account on how the conditions C1-C5 contribute to the information presented in
Figs. 9 (d)-(f). More specifically, we compare the proportion of detected CTs that satisfy C1-C3 to those that satisfy C1-C5.
In Fig. S-21 we plot the proportion of detected CTs versus ğ‘‡âˆ’for each rat for detected CTs that satisfy (i) C1-C3 (in blue)
and (ii) C1-C5 (in black). From Fig. S-21 it is clear that rat K is the most affected by C4 and C5, followed closely by rat T. Rat
S is least affected.
From further calculations we found that by changing ğ‘‡âˆ’= from âˆ’8 to âˆ’ğœNS = âˆ’3, the proportion of CTs that satisfy C1-C3
increased from 75âˆ’85% to 99.7âˆ’100%. By imposing C4 and C5 we found that these proportions of detected CTs fell to 80%
for rat S and 66% for rats T and K.
40/42

-0.1
0
0.1
LFP
BCT
(a)
0.0
0.001
GV
(d)
-4
-3
log10GV
(g)
0.6
0.8
1.00
AC
(j)
-10
-5
0
5
-5
-3
log10GV(AC)
T
(m)
BNCT
(b)
(e)
(h)
(k)
-10
-5
0
5 T
(n)
NCT
(c)
(f)
(i)
(l)
-10
-5
0
5 T
(o)
Figure S-20. Time series properties (TSPs) of voltage recordings for rat K near an example of a (left-hand column) BCT,
(middle column) BNCT, and (right-hand column) NCT as classified by the same model-trained SVM used to generate
Figs. S-18 and S-19. For each CT-type, the top row shows the LFP versus ğ‘‡= ğ‘¡âˆ’ğ‘¡1 with ğ‘¡1 detected by the algorithm. The
remaining rows show how each of the TSPs, specified in the vertical axes labels and in Table 5, behave for the same values of ğ‘‡.
Vertical red line in each panel emphasises when ğ‘‡= 0.
41/42

-14
-10
0.5
1.0
Prop. of detected CTs
T âˆ’
(a)Rat S
C1-C3
C1-C5
-14
-10
T âˆ’
(b) Rat T
-14
-10
T âˆ’
(c) Rat K
Figure S-21. Complimentary figure to Figs. 9 (d)-(f) showing the influence of artefacts and almost-occurring CTs happening
nearby the CT when filtering the detected CTs of (a) Rat S, (b) Rat T, and (c) Rat K. Each panel shows the proportion of
detected CTs that satisfy conditions (blue) C1-C3 and (black) C1-C5, i.e., proportion of detected CTs with/without artefacts and
almost-occurring CTs happening nearby the CT.
42/42
