Relative Error Streaming Quantiles with Seamless Mergeability via
Adaptive Compactors∗
Tomáš Domes and Pavel Veselý
Computer Science Institute of Charles University, Czech Republic
{domestomas,vesely}@iuuk.mff.cuni.cz
Quantile summaries provide a scalable way to estimate the distribution of individual attributes in
large datasets that are often distributed across multiple machines or generated by sensor networks.
ReqSketch [Cor+23] is currently the most space-efficient summary with two key properties:
relative error guarantees, offering increasingly higher accuracy towards the distribution’s tails,
and mergeability, allowing distributed or parallel processing of datasets. Due to these features
and its simple algorithm design, ReqSketch has been adopted in practice, via implementation
in the Apache DataSketches library. However, the proof of mergeability in [Cor+23] is overly
complicated, requiring an intricate charging argument and complex variance analysis.
In this paper, we provide a refined version of ReqSketch, by developing so-called adaptive
compactors. This enables a significantly simplified proof of relative error guarantees in the
most general mergeability setting, while retaining the original space bound, update time, and
algorithmic simplicity. Moreover, the adaptivity of our sketch, together with the proof technique,
yields near-optimal space bounds in specific scenarios – particularly when merging sketches of
comparable size.
1
Introduction
Quantile summaries, or sketches, are data structures of sublinear size, typically just polylogarith-
mic in the input length, allowing to efficiently process large streaming or distributed datasets,
and approximate their distribution, by estimating ranks and quantiles. Here, the rank of an
item y with respect to S, denoted R(y, S), is the number of items in S that are smaller than or
equal to y, and quantiles are essentially the inverse of ranks, i.e., a φ-quantile for φ ∈[0, 1] is
the ⌈φ · |S|⌉-th smallest input item. Quantile summaries are essential for many applications in
network monitoring, e.g., tracking latencies [MRL19; Ten15] or round trip times [Cor+05], or in
sensor networks [Shr+04; He+15].
Many of these applications require two main properties from the summaries that are both
challenging to achieve while preserving high space efficiency: First, the quantile summary
should provide an accurate distribution approximation, especially for the tails. In particular,
network latencies are typically heavily long-tailed [MRL19], and understanding the tail requires
increasingly more accurate estimates of the 95-th, 99-th, 99.5-th, etc., percentiles. The notion of
relative error captures this scenario by requiring that the rank error for an item of rank R is at
most ε · R, for an accuracy parameter ε > 0; equivalently, for a quantile query φ ∈[0, 1], the
summary should return an ˆφ-quantile, for ˆφ = (1 ± ε) · φ. While this yields lower error for items
of small rank, high accuracy for the other tail, as desired in monitoring latencies, can be achieved
∗Supported by the ERC CZ project LL2406 of the Czech Ministry of Education, Youth and Sports, by the grant
SVV–2025–260822 and by Center for Foundations of Modern Computer Science (Charles Univ. project UNCE
24/SCI/008).
1
arXiv:2511.17396v1  [cs.DS]  21 Nov 2025

by simply flipping the comparator. A long line of work [GZ03; Cor+05; Cor+06; Zha+06;
ZW07; Cor+23] studied relative-error quantile summaries, and recently Gribelyuk et al. [Gri+25]
developed a randomized streaming algorithm using near-optimal space of eO(ε−1 · log εN)1.
Second, it is in many cases not feasible to process the dataset in one pass as it may be
distributed across many machines or consist of observations aggregated by sensors [Shr+04].
To allow for distributed or parallel processing of large datasets, the sketch should come with
a merge operation that takes two sketches representing datasets S1 and S2 and outputs one
sketch representing the multiset union of S1 and S2. A quintessential property of summaries
is full mergeability [Aga+13], stating that even when the sketch is built by any sequence of
pairwise merge operations on single-item sketches obtained from a set S, its error should be
bounded, ideally achieving the same accuracy-space trade-off as in the streaming model. The
mergeability setting is more general, since the streaming model can be seen as a repeated merge
with a single-item sketch.
The state-of-the-art relative-error and fully-mergeable quantile summary is ReqSketch [Cor+23]
that consists of a sequence of relative compactors. These are buffers that randomly subsample the
stream in a way that facilitates the relative error; see Section 1.1. Due to its overall simplicity,
mergeability, and fast update time, compared to widely-used t-digest [Cor+21], ReqSketch has
been implemented within the Apache DataSketches library [Rho+13]. While ReqSketch only
achieves a suboptimal space bound of O(ε−1 · log1.5 εN) [Cor+23], the recent improvement
in [Gri+25] does not come with any mergeability guarantees (or even a merge operation) and
moreover, the new algorithm from [Gri+25] is also arguably much more involved than ReqSketch.
Indeed, to the best of our knowledge, there is no implementation, and it seems likely that
ReqSketch would still perform better than the algorithm of [Gri+25] for data streams in practice.
Overall, it is still open how to achieve full mergeability for relative-error sketches in space close
to the lower bound of Ω(ε−1 · log εN) memory words that holds for any sketch, even if computed
offline [Cor+23].
The key obstacle to progress appears to be proving mergeability for relative-error sketches.
Indeed, despite the simplicity of the ReqSketch algorithm and its relatively accessible analysis in
the streaming setting, the proof of full mergeability in [Cor+23] involves an intricate charging
argument and even more complex bounds on the variance across relative compactors. The
especially tricky part is that the internal parameters of the sketch change as the sketch summarizes
a larger and larger input. This makes it difficult to either improve the space bound for fully-
mergeable sketches or generalize the proof for other error guarantees.
Here, we develop adaptive compactors that, when used instead of relative compactors in
ReqSketch, allow for a significantly simplified proof of full mergeability while preserving its
other properties like space requirements, update time, and relative-error guarantees. The new
compactors naturally capture the charging required to bound the error, and in effect make
the analysis of the error in the full mergeability setting as accessible as in the streaming
model. Furthermore, the adaptivity of the compactors allows for getting better space bounds
in special cases, obtained as a simple corollary of our analysis. We demonstrate it by showing
that for a reverse-sorted input stream, the required space is optimal with respect to both ε
and N. Nevertheless, the adaptivity makes bounding the space used by the compactors less
straightforward compared to ReqSketch; we present an analysis of space requirements using a
suitable potential function, yielding the same bound of O(ε−1 · log1.5 εN) as in [Cor+23].
Finally, we confirm the intuition that certain “nice” cases of the mergeability setting admit
better space bounds than streaming. Specifically, we prove that our sketch created using merge
operations organized in an approximately balanced merge tree (i.e., always merging sketches
summarizing a similar number of items), admits a space bound of O(ε−1 · log(εN) · √log log N),
which is only a factor of √log log N from the optimum.
1Notation e
O(f) hides factors poly-logarithmic in f, and the dependency on δ, the probability of a too large
error.
2

Capacity C = 40 is exceeded, compaction happens
Smallest C/2 items are never removed
The size of the compaction T = 18
Figure 1
The compaction operation of relative and adaptive compactors for a compactor that overflows
its capacity (dashed items). Items are first sorted from the largest to the smallest. The compaction evicts
the crossed items from the memory and “promotes” the items with arrows to the next level, while the
gray ones are not involved in the compaction, so they remain in the buffer. The size T of the compacted
part is, however, computed differently in relative and adaptive compactors.
Organization of the paper
In the remaining part of Section 1, we explain the intuition behind adaptive compactors (Sec-
tion 1.1) and review related work (Section 1.2). We introduce main notation in Section 2. In
Section 3, we describe our sketch in detail, particularly the new compaction algorithm (Sec-
tion 3.2) and we state a few crucial observations and invariants of the algorithm. In Section 4, we
prove the space bound and the error guarantee of our sketch. In Section 5, we present two special
cases where our sketch performs particularly well. We conclude in Section 6 with a discussion of
the main open problem.
1.1
Technical overview
The basic sketch design comes from the KLL sketch [KLL16]. Namely, our sketch consists
of a series of compactors, which are essentially buffers, arranged into levels. Items from the
input stream are added to the level-0 compactor and when any compactor exceeds its capacity
C, we perform the compaction operation. The compaction sorts the items in the compactor
(non-increasingly), deletes odd-indexed or even-indexed items with equal probability, and moves
the rest to the compactor one level higher. Therefore, each item at level h “represents” 2h items
of the original stream. Then the rank estimate for a query item x is simply the sum of weights of
all stored items y ≤x. Moreover, as the items are sorted and we delete each odd/even-indexed
item, each promoted item represents a deleted item of a similar rank. Each compaction affects
the error of rank estimates for some universe items, namely, by either increasing, or decreasing
their rank estimate by 2h with equal probability, where h is the level. This implies that the
expected error change is zero, and then the error analysis for any item x mainly boils down to
bounding the number of compactions affecting the estimation error for x.
In relative compactors by Cormode et al. [Cor+23], and also in our adaptive compactors, the
compaction operation is performed only on some T largest items of the compactor, for an even
T; see Figure 1. This is based on the observation that if x is smaller than all compacted items,
the compaction does not affect the error for x. The size T of the compaction is determined in
such a way that the smallest C/2 items are never removed from the compactor. This is related
to the relative error guarantee as the accuracy of the sketch must be higher for items of smaller
rank. Particularly for the smallest ε−1 items the error must equal zero and so we must store
them all (and indeed, the level-0 compactor with capacity C ≥2ε−1 always stores the smallest
ε−1 inserted items).
The main novelty of our work is the choice of the size of the compaction. Cormode et al.
[Cor+23] use a deterministic process called compaction schedule, based on derandomizing a
suitable geometric distribution. The compactor is split into sections of size K, and the schedule
is determined by K and by the binary representation of the number P of already performed
compactions; namely, if Z is the number of trailing ones of P, they have T = (Z + 1)K.
To see the motivation behind the compaction schedule, let us number the sections starting
3

Before:
Compactor is full; K = 4, P = 0 . . . 001011, Z = 2
The compaction:
j = 3
2
1
0
Never compacted
T = (1 + Z)K
After:
P = 0 . . . 001100
Empty slots
Newly marked items
j
2
1
0
Figure 2
The compaction operation in a relative compactor with capacity C = 40, where P is the
number of already performed compactions (written in binary), and Z equals the number of trailing ones
of P.
from zero in such a way that the largest items are in section 0. Similarly, let us index the bits of
the binary representation of P from the least significant bit starting from one. The i-th bit of P
corresponds to the i-th section of the compactor, and we say that section j ≥1 is marked if the
j-th bit is set to 1. During a compaction, we remove items from section 0 and from the marked
sections [1, Z], where Z is the trailing ones of P (the range can be empty); as in KLL, a half of
these removed items promoted to the next level and discarded otherwise. Then we increment P,
which essentially means marking the previously unmarked section j; see Figure 2.
At time t0 of marking, all items in section j are of smaller rank than all items removed
from the compactor by the compaction. After another 2j compactions at time t1, the marked
section j is compacted. Generally, the items present in section j at time t1 can be different
than items there at time t0. However, due to the sorting before each compaction, all the new
items have smaller or equal rank than all the old ones. This implies the key property that for
each compaction, there are some K unique items of smaller rank than all items removed by the
compaction which are either later removed by another compaction or remain in the compactor at
the end of the algorithm. This can be used to bound the number of compactions deleting small
items, which is essential for the relative error guarantee. We refer to [Cor+23] for more details.
Unfortunately, the notion of compaction schedule becomes much less elegant under merge
operations. Cormode et al. [Cor+23] developed a merging algorithm that has two deficiencies:
First, when merging two relative compactors, they take binary OR of their compaction schedules,
which is efficient but the nice intuition behind the compaction schedule is somehow lost and
the previously easy-to-see bounds on the number of compactions affecting the error require an
involved charging argument (Lemma 6.4 in [Cor+23]). Second, the parameters of each sketch are
recomputed multiple times, every time the summarized input size of the sketch gets squared,
and this yields a complicated tight analysis of the variance (e.g., Lemma 6.12 in [Cor+23]).
We propose an alternative to the compaction schedule which is a bit less elegant in the
streaming setting, but naturally covers full mergeability with much simpler proofs. The core of
our method is to tie the marks not to the sections of the compactor, but directly to the items.
Then we can simply merge two compactors by merging the two sorted arrays – all the marked
items simply stay marked. However, in this model the marked items no longer form contiguous
sections of the compactor, due to future item additions. It is possible to analyze the error without
enforcing contiguous sections of marked items, but the resulting compaction algorithm and its
analysis are more complicated, requiring the notions of dense and sparse sections with respect to
the marking.
4

Instead, we further modify the algorithm to recover contiguous sections of marked and
unmarked items, which yields simpler analysis. Suppose the lowest ranked item removed during
a particular compaction has rank r0 and the items marked after the compaction have rank
r1 > r2 > · · · > rK. Then a new item with rank rx arrives such that r0 > rx > rK. Such
item can be surely marked instead of the item rK. With such remarking, the K items marked
for a particular compaction always form a contiguous section in the compactor. To do this
remarking efficiently, we do not actually mark the items, but remember for each compaction
the smallest-ranked removed item, called the ghost item, and the current value of K. From this
information, we can in linear time recover the marking of items. We perform the “remarking” so
that the contiguous sections of marked items become aligned, i.e., start on positions divisible
by K, which further simplifies the analysis and it allows us to have marked sections instead of
marked items, similarly as for relative compactors.
Given the marking of sections, determining the size of the compaction is conceptually the
same as for relative compactors. We look for the first unmarked section j, remove all items to
the left of this section, and then mark j, as depicted in Figure 2.
Our strategy preserves the property of the relative compactors that for each compaction, there
are some K unique items that are smaller than or equal to those removed by the compaction and
that will either be removed by another compaction, or remain in the compactor at the end of
the algorithm. Moreover, with adaptive compactors, this property is a simple observation, even
considering the change of parameters over time and full mergeability, which are both naturally
covered by our algorithm. As mentioned above, proving such a property for relative compactors
from [Cor+23] under full mergeability requires intricate analysis. The rest of the error analysis is
similar to that of ReqSketch in the streaming setting but in our case, it covers full mergeability
for free. We partially pay for this simplicity in the analysis of the space bound, which was
trivial for relative compactors. For adaptive compactors, we define a suitable potential function
which connects the capacity of the compactor with the number P of performed compactions; see
Section 4.1. However, while relative compactors always use the same space for a fixed input size
no matter the input structure, the advantage of our approach is that it adapts, allowing us to
prove near-optimal space bounds in special cases.
To sum up, our analysis of the error under arbitrary merge operations is now comparable to
the relatively simple analysis of ReqSketch in the streaming setting, while the space analysis
requires a new potential-function argument.
1.2
Related work
Here, we focus on state-of-the-art quantile summaries with various error guarantees, with a
particular focus on their mergeability properties. The space bounds are given in memory words
with Θ(log U + log N) bits, where N is the input size and U is the universe of input items. For
randomized algorithms, we present the space bounds assuming a constant probability δ of a too
large error for any fixed query.
Besides relative error, there is a large body of work on quantile summaries with the addi-
tive (uniform) error of ±εN, i.e., without higher accuracy for the tails. The state-of-the-art
additive-error sketch is the randomized comparison-based KLL sketch by Karnin, Lang, and
Liberty [KLL16] achieving full mergeability in the optimal space Θ(ε−1). For deterministic
comparison-based sketches, the optimal space of Θ(ε−1 · log(εN)) is achieved by the Greenwald-
Khanna (GK) summary [GK01]; its optimality has only been shown recently [CV20] (for a
simplified version of the GK summary, see [Gri+24]). However, the Greenwald-Khanna summary
is not fully mergeable, only “one-way mergeable” as discussed in [Aga+13]. An older deterministic
algorithm by Manku, Rajagopalan, and Lindsay [MRL99] achieves full mergeability in space
O(ε−1 · log2(εN)).
The non-comparison-based deterministic additive-error q-digest by Shrivastava et al. [Shr+04]
achieves full mergeability in space O(ε−1 ·log(|U|)), that is, depending on the universe size, which
5

must be known in advance. In this model, Gupta et al. [GSW24] recently achieved the optimal
space Θ(ε−1) deterministically, using a compressed version of q-digest, but they did not prove
that their sketch is fully mergeable.
For deterministic comparison-based sketches with the relative error guarantee, we have a
lower bound of Ω(ε−1 · log2(εN)) by Cormode and Veselý [CV20], while the state-of-the-art
sketch by Zhang and Wang [ZW07] achieves space O(ε−1 · log3(εN)); however, it is also not fully
mergeable.
There is a modified version of the aforementioned q-digest by Cormode et al. [Cor+06]
achieving relative error in space O(ε−1 · log(εN) · log(U)).
Like the original q-digest, it is
deterministic, non-comparison-based, fully mergeable, and requires the prior knowledge of the
universe U.
In the randomized comparison-based setting, the state-of-the-art relative-error algorithms
are ReqSketch by Cormode et al. [Cor+23] and the recent work of Gribelyuk et al. [Gri+25].
ReqSketch is fully mergeable but requires O(ε−1 · log1.5(εN)) space, still by
p
log(εN) larger
than the lower bound of Ω(ε−1 · log(εN)) that holds even for non-comparison-based randomized
algorithms [Cor+23]. The work of Gribelyuk et al. [Gri+25] almost closes this gap by designing
a streaming algorithm using space O(ε−1 · log(εN) · log ε−1 · (log log N + log ε−1)). However, the
sketch was not proven to be mergeable; in fact, no merge operation was designed to preserve
properties required in the analysis.
Finally, besides algorithms designed with theoretical guarantees in mind, it is worth mentioning
that many practitioners actually use t-digest [Dun21], which is fully mergeable and usually highly-
accurate in practice, aiming at uniform or relative error depending on its parameter setup, but
providing no theoretical worst-case guarantees. Indeed, Cormode et al. [Cor+21] demonstrated
that for adversarially constructed inputs or even samples from certain distributions, the error of
t-digest can be almost arbitrarily large.
2
Preliminaries
Here, we outline notation and terminology used throughout the paper:
• By item, we always mean an arbitrary item from a universe U with a total order. For
simplicity, we assume in the whole paper that all the input items are different. However, all
the definitions can be extended to support equal items and all the algorithms and proofs
are correct even without this assumption.
• An input S consists of items from U. We denote the input size by |S| = N.
• For any set of items S, let R(y, S) be the rank of item y in S, and let R(y) = R(y, S) be
the rank of y in the input.
• We use Rˆ(y) for the answer returned by the sketch for a rank query y.
• The error of a rank query y is Err(y) = Rˆ(y) −R(y).
• By log x we always mean log2 x and by ln x the natural logarithm loge x.
• We denote the ranges of arrays as mathematical intervals, thus ) means exclusion and ]
inclusion. For example A[3, 5] are elements A[3], A[4] and A[5] while A[3, 5) means only
A[3] and A[4].
3
Description of the Algorithm
The high-level design of the sketch is analogous to ReqSketch [Cor+23], namely that it consists
of compactors arranged into levels. However, our adaptive compactors are more independent
6

of each other, having their own parameters, and use a more flexible strategy for performing
compactions. In fact, the most significant change is in determining the size of the compaction
(Section 3.2).
Basic sketch design
The sketch consists of a sequence of H adaptive compactors indexed from 0 to H −1. We imagine
the compactors as arranged in levels – the compactor at level 0 is at the bottom, the compactor
at level H −1 at the top.
An adaptive compactor has some internal state, a capacity C, an input stream I, an output
stream O, and contains a set B of items (the buffer). The input of the compactor on level 0
is the input of the sketch and the input of any compactor on level ℓ> 0 is the output of the
compactor on level ℓ−1.
Any set of items received from the input stream is simply added to B. Whenever the size
of B reaches the capacity C (we say that the compactor is full), the compactor performs a
compaction operation, which removes some items from B, a half of them are evicted from memory
and the other half are sent to the output stream (this can trigger a compaction operation on the
next level). We stress that the items are sent in batches – if a compaction operation outputs
a set of items, all the items are atomically added to the next-level compactor before any other
compaction happens.
Any time the compactor on the highest level H −1 performs a compaction, the number of
levels H is increased by one, and a new compactor on the highest level is created to receive the
output stream of the previously highest compactor.
Apart from the sequence of compactors, the sketch consists of the error bound 0 < ε < 1 and
the failure probability 0 < δ ≤1
8.
Merging two sketches and stream updates
The merge operation is defined on two sketches S1 and S2 with parameters ε1 = ε2 and δ1 = δ2.
Without loss of generality (w.l.o.g.), assume that H1 ≥H2. The parameters ε and δ are kept
unchanged and for each level h < H2 the compactor on level h of the new sketch is given by a
merge of the two compactors on level h from sketches S1, S2. We describe the merge operation on
compactors in Section 3.1. For h ≥H2 the resulting compactor is simply the compactor on level
h from the sketch S1. After the merge, we go through the compactors bottom-up and perform a
compaction on each full compactor.
The update operation takes the next item from the input stream of the algorithm and sends
it to the input stream of the level-0 compactor. Note that this operation can be viewed as a
merge with a trivial sketch containing one element.
Answering queries
For the rank query y the sketch returns value Rˆ(y), which is defined as
Rˆ(y) def
=
H−1
X
h=0
2h R(y, Bh).
To implement queries more efficiently, we sort items from all the compactors together with their
weights and precompute weighted prefix sums. Then we answer a rank query by a single binary
search over the sorted items (and one lookup to the prefix sums) and a quantile query by a single
binary search over the prefix sums (and one lookup to the items).
7

3.1
Adaptive compactor
The adaptive compactor consists of the buffer B of items, capacity C, section length K, and a
stack M of markers. We often use subscript to denote the level of the compactor (thus Ch is
capacity of the level-h compactor).
Each marker is a tuple (length, ghost item) where length is an integer and ghost item is a copy
of some item previously removed from B. The comparison of two markers (ℓ1, g1), (ℓ2, g2) ∈M
is given by the comparison of the ghost items g1, g2 and the comparison of a marker m = (ℓ, g)
and item y is given by the comparison of ghost item g and item y (thus we can say that “marker
m is smaller than item y”).
For the analysis, we assume that both B and M are always sorted2 with the largest item on
position B[0] and the largest marker on the top of M.
The initial setting of parameters is the same regardless of the level. At start, we have naturally
B0 = ∅and M0 = ∅. Let us denote the closest larger power of two by
2
⌈x
2
⌉def
= 2⌈log x⌉. The initial
values of parameters C and K are chosen as follows:
K0 =
2
⌈max

ε−1√
ln δ−1, 4 ln δ−1 2
⌉;
C0 = 8K0.
The definition may seem cryptic, but we later see that C0 and K0 are defined exactly such that
invariants (I1), (I3), (I4) and (I6) hold (defined in section 3.2) and C0 is the smallest possible.
Compaction
The compaction operation is performed on a single compactor and it happens only when the
compactor is full, i.e., |B| ≥C. First, the size T of the compaction is determined, as described in
Section 3.2. If T is odd, we temporarily remove the largest item from B, store it and decrease T
by one (and we return the item back to B after the compaction is performed). Then, we remove
the largest T items from B and with probability 1/2, the odd-indexed removed items are sent
to the output stream of the compactor, and in the other case, the even-indexed are sent to the
output. The other half of the largest T items is evicted from memory.
Note that the operation itself is defined exactly as in the relative compactor of ReqSketch
[Cor+23]. However, the key difference is in determining the size of the compaction described in
Section 3.2.
Merging two adaptive compactors
The merge of two compactors is straightforward: We merge the lists B and M and inherit
the parameters C and K from the compactor with larger C and smaller K (this is needed for
invariant (I2) stating that K never increases and C never decreases). We note that the compactor
with larger C has always smaller K, as by invariant (I6) the expression KC is constant.
Thus, the result of the merge of two adaptive compactors C1(B1, C1, K1, M1), C2(B2, C2, K2, M2)
where C1 ≥C2 is defined by B = B1 ∪B2, C = C1, K = K1, and M = M1 ∪M2.
Sections
For the sake of later analysis, we divide B into sections of length K, where each section is a
range B
|B| −(i + 1)K, |B| −iK
 for some integer i ≥0.
Our algorithm maintains the invariants that C is always a multiple of 2K (I5) and that
C ≥8K (I3). This allows for the following notation: We index the sections by consecutive
increasing integers such that the index of the last section is C/K −1. Thus, when |B| = C, the
2In practice we keep M sorted and we sort B before every compaction.
8

the left half
the right half
the tail
the head
S[−1]
S[0]
S[1]
S[2]
S[3]
S[4]
S[5]
S[6]
S[7]
Figure 3
Sections of a buffer with K = 4 and C = 32
largest K items lay exactly in section 0 and when |B| > C, there are some items in negative-
indexed sections (and those items always participate in the compaction). We denote the i-th
section of B by S[i] and analogously for ranges of sections (thus S[i, j] are all sections from i-th
to j-th). Let us name the range S[−∞, C/2K) as the left part of B and the range S[C/2K, C/K)
as the right part of B. Moreover, let S[−∞, 0] be the tail of B, and let the head consist of the
last 2 sections of B (see Figure 3). Note that as C ≥8K (I3), the tail is contained in the left
part and the head is contained in the right part.
Marking
We use the markers from definition of an adaptive compactor to mark the items in B. For any
marker m with ghost item g and length ℓwe mark some ℓitems of B that are smaller than g
such that each item of B is marked by at most one marker.
In Section 3.2 we explain our algorithm which for each compaction creates a marker of length
K where the ghost item is the smallest of the items removed during the compaction. Thus, as
we formally prove in Observation 2, for each compaction there are K unique “small” marked
items and this can be used to bound the number of compactions affecting the error of a given
item y (see Observation 9 and Lemma 5).
Let us now formally define the marking, before we dive into our algorithm.
Definition 1 (Marking). For given buffer B and list of markers M, marking of B by M is
defined as a function M from markers to sets of items in B such that:
• for each m ∈M and y ∈M(m) : y ≤m
• |M(ℓ, g)| = ℓ
• for different markers m1, m2 ∈M : M(m1) ∩M(m2) = ∅
For a given marking M, the items from B that are contained in some set of the image of M are
marked and the rest of the items are unmarked.
Definition 2 (Canonical marking). For sorted buffer B, section length K and sorted stack of
markers M, canonical marking is obtained by the following greedy procedure: Remove the largest
marker m = (ℓ, g) from M, find the first section s = B[i, i + K) such that all items in s are
unmarked and smaller than m and set M(m) = B[i, i + ℓ). Repeat while there are some markers
in M left.
We say that a section of B is full if it contains K items, and it is marked if it contains marked
items only; otherwise, it is unmarked and contains unmarked items only, by the definition of the
marking algorithm. We maintain an invariant that for each marker m = (ℓ, g), the length ℓis a
multiple of K (I9). Thus, in the canonical marking, each section is either marked or unmarked.
9

3.2
The size of the compaction
In this section, we present our algorithm for determining the size T of the compaction which uses
the notion of marking defined above. This algorithm is the key difference between the relative
compactors by Cormode et al. [Cor+23] and our adaptive compactor.
The main idea is to find the leftmost unmarked section s to the right of the tail, compact all
the items from the previous sections (together with the markers that marked them) and create a
new marker that marks s. The pseudocode is shown in Algorithm 1.
The core of the algorithm is on algorithms 1 to 1. We go through the sections of B and
for each section s we check whether it is marked in the canonical marking. When we find an
unmarked section in the left part of B (but not on the tail), we compact all items before s
(algorithm 1) and create a new marker to mark s. This is called a standard compaction. Note
that we do not mark the items explicitly, as it is in fact not needed, they are only marked for the
sake of the analysis.
The special compaction (algorithm 1) corresponds to the case when all sections in the left
part that are not on the tail are marked. In this case, we compact the whole left part and mark
the first unmarked section of the right part. It is possible that in this situation, not all of the
items marked by the last-removed marker get removed (as part of them may lay in the right
part). If so, we increase the length of the new marker accordingly (algorithm 1).
Algorithms 1 to 1 take care of change of parameters of the compactor – if the number of
unmarked sections is too small, we double C and halve K. Algorithm 1 implement a naive
algorithm that we use when K becomes 1.
Algorithm 1
Determining the size of a compaction
Require: |B| ≥C
▷this procedure runs only if buffer is full
Require: B is sorted with largest item B[0], M is sorted with largest item at the top
1: if K = 1:
2:
return |B| −C/2
▷naive compaction
3: T ←|B| mod K
4: while true:
5:
if M is empty or B[T] > M.peek():
▷B[T, T + K) are unmarked
6:
if T < |B| −C + K:
▷T is too small
7:
T ←T + K
8:
else:
▷standard compaction
9:
push (K, B[T −1]) to M
10:
return T
11:
else:
12:
(length, ghost) ←M.pop()
▷B[T : T + length) are marked
13:
T ←T + length
14:
overlap ←T −(|B| −C/2)
15:
if overlap ≥0:
▷special compaction
16:
T ←T −overlap
17:
insert (K + overlap, B[T −1]) to M
▷keep M sorted
18:
unmarked ←C/2 −sum of lengths of all markers in M
19:
if unmarked < 2K:
▷in fact we have unmarked = K
20:
K ←K/2
21:
C ←2C
Our algorithm maintains the following invariants:
(I1) The parameters K and C are both always powers of two by algorithm 1 and by
the initial setting of K0, C0.
10

(I2) The parameter K is nonincreasing in time and the parameter C is nondecreasing
in time3 by algorithm 1.
(I3) We have always C ≥8K by the initial setting of C0 and K0 and by invariant (I2).
(I4) We have always C ≥25 ln δ−1 by the initial setting of C0 and K0 and by invariant (I2).
(I5) C is always a multiple of 2K by invariants (I1) and (I3).
(I6) The value of the expression KC never changes and we have KC ≥23ε−2 ln δ−1
and KC ∈Θ(ε−2 ln δ−1 + ln2 δ−1). This is by algorithm 1 and by the initial setting of
C0 and K0.
(I7) After the compaction, the tail contains at most one item and the number of
removed items is at least K, as each compaction removes all items from the tail (by
algorithm 1) and after the compaction one temporarily removed item can be added back
to B (as explained in the description of the compaction operation) only if T was odd and
consequently the number of items of B was at least C + 1.
(I8) No compaction removes items from the right part of B. This is clear for naive
and special compaction. For standard compaction, consider when T attained its final value.
If it was on algorithm 1, at least C −2K items remain in B and by invariant (I3) this
is larger than C/2. If it was on algorithm 1, we immediately checked this constraint on
algorithm 1 and if it was violated we would perform a special compaction.
(I9) The length of each marker is always divisible by K. By induction over the steps
of the algorithm. As K is initially a power of two, dividing K by 2 on algorithm 1 does
not violate the invariant. Creating a new marker does not violate the invariant if and only
if the overlap is always divisible by K. This is indeed true as C/2 is always divisible by
K and |B| −T is always divisible by K due to algorithm 1 and by induction hypothesis
(algorithm 1).
(I10) All the compacted items not lying on the tail are marked. This is because when we
find unmarked section with positive index (thus not on the tail), we stop the compaction
before this section (algorithm 1).
(I11) The canonical marking of B by M always exists and leaves the head unmarked.
This is clearly true for empty M.
In case of a standard compaction, the new marker marks a previously unmarked section of
the left part and all the old markers mark the same sections as before the compaction.
In case of a special compaction, the length of the new marker is equal to the number of
items of the right part marked by the last removed marker plus K. Thus, the number of
marked items of the right part increases by K. Since the new marker is larger than all the
items present in B after the compaction, the new marking of the right part differs only on
the first previously unmarked section from the old one. If this section was on the head, the
invariant is broken and we immediately restore it on algorithm 1 by halving the section
size and thus halving the head size.
We need for the invariants to hold even after merging two compactors. This is easy to check
for each but the last one. The validity of invariant (I11) under the merge operation is a direct
corollary of the following observation which we use again later in the analysis of the space bound
in Section 4.1. Informally, the observation states that after a merge, none of the markers moves
to the right.
3Note that this holds also after merge, as we always pick the parameters from the sketch with smaller K and
larger C.
11

Observation 1. For a marker m let its rank in B be defined as R(m, B) def
= R(z, B) where z is
the largest (thus least-indexed) item marked by m in the canonical marking.
For each marker m, its rank after a merge is at least its rank before a merge.
Proof. Consider a merge of C1(B1, C1, K1, M1) and C2(B2, C2, K2, M2) resulting in C(B, C, K, M).
We assume that the canonical marking of B1 by M1 and canonical marking of B2 by M2 both
exist. We prove the statement by induction over the number of markers after the merge.
When |M| = 1 the statement clearly holds, as the new section length is a divisor of the old
one and for each item x from any of the buffers B1, B2 its rank does not decrease after a merge.
For the induction step, let m = (ℓ, g) be the smallest marker in M and w.l.o.g., assume that
m ∈M1. Let us remove m from M1 and remove the smallest R(m, B1) items from B1. Note
that R(m, B1) is a multiple of K and also that the canonical marking of B1 by M1 still exists.
By the induction hypothesis, the statement of the lemma now holds.
Let us now add all the removed items back to B1 and B. After this operation, rank of all
the markers in M1 and M (w.r.t. their buffers) increases by R(m, B1), as in both cases all the
added items are smaller than all the markers. Thus, the statement of the lemma still holds and
the smallest R(m, B1) items of B are unmarked. Let us add m to M1 and to M. This can not
influence which items are marked by the rest of the markers, as m is the smallest one. Thus, the
smallest R(m, B1) items of M are still unmarked by the rest of the markers and if m does not
mark any larger items, it surely marks the largest ℓof the smallest R(m, B1) items of B. We
conclude that R(m, B) ≥R(m, B1) and the statement of the lemma holds.
Following observation is crucial in bounding the error of the sketch (we use it in Observation 9
and Lemma 5).
Observation 2. For a compaction P, let KP be the value of K when the compaction happens
and let yP be the smallest item removed from B by compaction P. For each compaction P there
exists a set WP of items of the input stream of the compactor such that:
(R1) |WP| = KP,
(R2) all the items of WP are smaller or equal to yP, and
(R3) for any two distinct compactions P, Q the sets WP and WQ are disjoint.
Proof. We prove the lemma by distributing coins. For a compaction P, we create KP P-coins
and distribute them as described below, ensuring that each item from WP receives exactly one
P-coin and no coins from other compactions.
If KP = 1 we send one P-coin to item yP (which is removed from B during compaction P).
For KP > 1, we send KP P-coins to marker ℓcreated during compaction P. If marker ℓis later
deleted during compaction P′, we send one coin from ℓto each item marked by ℓin the optimal
marking and removed from B during compaction P′; if P′ is special and not all of these items are
removed, we send the rest of the coins to the newly created marker. Observe that the amount of
coins on a marker is always exactly its length. Moreover, for each P-coin on marker m = (g, l) we
have always g ≤yP, showing that property (R2) holds. By invariant (I11) the optimal marking
of B by M always exists and thus, we can distribute the coins remaining in M of the final sketch
to the items remaining in the buffer of the final sketch. Therefore, this process of distributing
coins maintains all properties (R1), (R2), and (R3).
Remark (Time complexity). Note that apart from sorting B, the time complexity of any com-
paction, including Algorithm 1, is linear in the number of items removed from B. If we slightly
modify the algorithm and let the compaction happen only when |B| ≥2C, we get amortized
update time O(log C) per item removed from B, due to sorting B; note that the sketch works in
the comparison-based model. Considering the time over all levels, since a half of the removed
12

items from a compactor is deleted forever, the amortized update time of the whole sketch
is only O(log Cmax) where Cmax is the largest capacity achieved by a compactor of the final
sketch. In Section 4.1, we prove that on any level, C ∈O(
p
log(εN) · (ε−1√
ln δ−1 + ln δ−1)),
obtaining amortized update time of O(log ε−1 + log log(εN) + log δ−1), which is the same as for
ReqSketch [Cor+23].
4
Analysis
In this section, we give analysis of the sketch under full mergeability. We prove the space bound
in Section 4.1 (Lemma 2) and the error guarantee in Section 4.2 (Lemma 5). These together give
the main theorem, which is analogous to Theorem 1 in [Cor+23].
Theorem 1. For any parameters 0 < δ < 1/8 and 0 < ε < 1 there is a randomized, comparison-
based, fully mergeable streaming algorithm that, when processing an input consisting of N items
from a totally-ordered universe U, produces a summary S satisfying the following property. Given
S, for any y ∈U one can derive an estimate Rˆ(y) of R(y) such that
P
Rˆ(y) −R(y)
 > ε R(y)

< δ ,
where the probability is over the internal randomness of the algorithm. The size of S is
O

log1.5(εN) · (ε−1 ·
√
ln δ−1 + ln δ−1)

.
Remark. Assuming that ε−1 ≥
√
ln δ−1, which is typically4 the case for many applications, the
space bound becomes simply
O

log1.5(εN) · ε−1 ·
√
ln δ−1

,
which is exactly the same as for ReqSketch [Cor+23].
Furthermore, similarly as for ReqSketch, the sketch also satisfies the space bound of
O
 log(εN) · ε−2 ln δ−1, which is only better than the bound in Theorem 1 for ε−1 ·
√
ln δ−1 ≤
p
log(εN), i.e., for a very large input size N, exponential in ε−2 · ln δ−1.
4.1
The space bound
In this section, we bound the space consumed by the sketch. The main part is analyzing the
space required for each compactor as the number of compactors can be bounded by O(log εN)
in a straightforward way (see Observation 4). The relative compactors from [Cor+23] were
designed such that each of them fits into space of O(ε−1 ·√log εN), while the hard part is proving
the right properties regarding the error, particularly the bound on the number of important
compactions that involves convoluted charging (see Lemma 6.4 in [Cor+23] which is an analogy
of our Observation 9).
The adaptive compactors are, in a sense, opposite: They minimize the space requirements
while maintaining several invariants, outlined in Section 3.2, which make bounding the number
of important compactions straightforward (Observation 2 and Observation 9). However, the
space bound is not obvious on first sight. With stream updates only, it is quite intuitive that at
the asymptotic behaviour is the same as for relative compactors, but proving the bound under
full mergeability is more challenging.
The cornerstone of the analysis is Lemma 1, which gives a lower bound on the section length
K. With this lemma holding, the rest of the analysis is straightforward. However, the lemma
4For the standard choice ε = 0.01 we can set the failure probability as low as 2−1000 without violating the
inequality. For large ε = 0.1 we can still have δ = 2−32.
13

itself is the most challenging part of our analysis, requiring a careful argument using a suitable
potential function. The proof builds upon the intuition that before doing a compaction of size
XK, we must do ≈2X compactions of smaller size. However, we need to deal with the fact
that the parameters K and C can change after a merge. To formalize our intuition, we define a
potential Φ that bounds the amount of compactions that happened on a given compactor since
the last change of parameters.
For simplicity of notation, we sometimes write ≲, ≳, ≈instead of O, Ω, Θ, respectively.
Lemma 1 (Lower bound on K). Consider an adaptive compactor at any level. Let P be the
number of compactions performed on the compactor. For P ≥2, we always have that
K ∈Ω

C
log P

,
while K ∈Ω(C) for P ≤1.
Proof. We prove the statement by defining a suitable potential function Φ that for given state of
a compactor returns a nonnegative real number. Then we prove that the potential of a compactor
has following five properties.
Five properties of Φ
(P1) Adding new items to B does not increase Φ
(P2) Immediately before changing the values of K and C we have Φ ∈2Ω( C
K)
(P3) Immediately after changing the values of K and C we have Φ = 0
(P4) The compaction operation increases Φ at most by an additive constant
(P5) Merging compactors Ca, Cb with potentials Φa, Φb results in potential at most max(Φa, Φb)
These properties together imply that between changing K to K′, 2Ω( C
K) compactions happen.
As the number of compactions between changing from K to K′ is at most P and K = 2K′, we
have always P ∈2Ω( C
K) and taking logarithm of both sides gives us the desired statement. Note
that the lemma holds even before any change of K happens, as we initially set K ≈C. This also
covers the case P ≤1.
Definition of Φ
For a section S[i] of B let us denote by i that it is marked (in the canonical marking) and by i
that it is unmarked. For any full section S[i] we define its auxiliary potential φ as follows (to
recall naming of parts of B see Figure 3):
• if i lies in the left part, then
φ(i) = 2i/2
and
φ(i) = −
√
2 · 2i/2,
• if i lies in the right part but not on the head, then
φ(i) = 0
and
φ(i) = −(1 +
√
2) · 2
C
4K ,
• otherwise, i lies on the head and we define
φ(i) = (1 +
√
2) · 2
C
4K
and
φ(i) = 0.
14

For section that is not full, the auxiliary potential φ is always zero. Now we define Φ for each
prefix of sections of B. The potential of a prefix with no full section is 0 and for prefix S[−∞, i]
where i is a full section we have
Φ(i) = max(0, Φ(i −1) + φ(i)).
The final potential of B is naturally Φ(C/K −1) = Φ.
Key observations
(O1) Φ is always nonnegative.
(O2) Let ∆φ(i) = φ(i) −φ(i). Notice that ∆φ forms a geometric series with quotient
√
2 in the
left part and remains constant in the right part. More precisely, for each section S[i] in the
left part, we have
√
2 · φ(i) = φ(i + 1);
√
2 · φ(i) = φ(i + 1);
√
2 · ∆φ(i) = ∆φ(i + 1)
and for all sections i in the right part, we have
∆φ(i) = ∆φ(i + 1).
(O3) For i < j suppose that section S[i] is marked while S[j] is unmarked.
As ∆φ(i) is
nondecreasing, if we unmark S[i] and mark S[j], the potential Φ does not decrease.
Consequently, if we mark S[i] and unmark S[j], Φ does not increase.
(O4) We always have Φ(i) ≤(2 +
√
2) · 2i/2, particularly the potential of the tail is at most
2 +
√
2.
It remains to prove all the five properties (P1-P5) of Φ.
(P1) Adding new items to B does not increase Φ.
After adding an item to B, each marker either marks the same sections as before, or it moves
one section to the left. Thus by observation (O3), Φ does not increase.
(P2) Immediately before changing the values of K and C we have Φ ∈2Ω( C
K).
We change K only when the number of unmarked sections equals one. Thus, one of the sections
in the head is marked, and by the definition of the potential we have Φ ≥2
C
4K ∈2Ω( C
K).
(P3) Immediately after changing the values of K and C we have Φ = 0.
By invariant (I11), all the items in the head are unmarked, thus contribute 0 potential. The
items in the rest of the right part never contribute a positive potential. The left part is empty as
it was just compacted by a special compaction. Finally, increasing C preserves these properties.
(P4) The compaction operation increases Φ at most by an additive constant.
For K = 1 this is trivial since there is no new marker, so we assume that K > 1.
Let S[i] be the first section not removed by the compaction and let Φ0 be the potential before
the compaction. The potential of the compacted prefix S(−∞, i) was originally some number ΦC
and after the compaction it is surely 0. Let the potential after the compaction be Φ0 −ΦC + Φm
where Φm is the potential increase over the not compacted part caused by addition of the new
marker. Our goal is to bound the overall potential increase Φm −ΦC.
15

If the compaction is standard, then the newly created marker marks section S[i] and all the
other remaining sections are marked exactly as before the compaction (while items from sections
S[−∞, i) are removed from B during the compaction). Thus, we have Φm = ∆φ(i) = (1+
√
2)·2i/2.
If the compaction is special, the number of marked sections in the right part increase by one and
thus again we have Φm = ∆φ(i) = (1 +
√
2) · 2i/2.
It remains to bound ΦC from below. Recall, that by invariant (I10), all the compacted items
not lying on the tail are marked. By observation (O2), the marked sections form a geometric
series with quotient
√
2 and sum (1 +
√
2) · 2i/2, thus if the number of marked sections was
infinite, we had exactly (1 +
√
2) · 2i/2 of potential from them. While we have guaranteed marked
sections only in the left part without the tail, the potential of the tail is at most 2 +
√
2 by
observation (O4). Hence, we have ΦC ≥(1 +
√
2) · 2i/2 −(2 +
√
2) and the overall potential
increase is at most 2 +
√
2.
(P5) Merging compactors Ca, Cb with potentials Φa, Φb results in potential at most
max(Φa, Φb)
Let us recall that for two compactors C1(B1, C1, K1, M1), C2(B2, C2, K2, M2) with K1 ≤K2, their
merge is defined as B = B1 ∪B2, C = C1, K = K1, and M = M1 ∪M2. If K1 = K2, let us
name the compactors such that the smallest marker in M1 is smaller than the smallest marker in
M2 Let us denote the potentials of the compactors C1, C2 by Φ1 and Φ2 respectively and let us
denote the potential of the new compactor by Φnew. We prove that Φnew ≤Φ1.
Let us temporarily remove the smallest 2K2 items from B2 before merging the compactors.
By Observation 1, the canonical marking of B exists (as the removed items were unmarked in C2)
and for each marker m′ ∈B1 we have R(m′, B) ≥R(m′, B1). Let us add the removed 2K2 items
to B and let m be the smallest of the markers from B2. After the addition of items, R(m, B)
increases by 2K2. Let S[i] be the last (rightmost) section marked by m after the addition.
If there are some markers originally from B1 smaller than m, rank of some of them may also
increase by the addition. For the sake of analysis, let us remark the sections marked by those
markers such that the sections are marked as before the addition. By observation (O3), this can
only increase Φ. After the remarking, the sections S[i + 1, i + 2] are unmarked (as K2 ≥K) and
we still have R(m′, B) ≥R(m′, B1) for each marker m′ from B1.
Now it suffices to prove, that Φ(i+2) = 0. Then we have Φnew ≤Φ1, as K = K1, the markers
from M1 have the same or larger rank in B and the markers from M2 contribute zero potential.
First note, that sections S[i + 1, i + 2] do not lay on the head. If K2 > K1 (and thus
K2 ≥2K1), this is because R(m, B) increased by at least 4K after the addition and the canonical
marking existed before the addition. If K1 = K2 = K, this is because in M there exists some
marker m′ smaller then m that marks some sections to the right of S[i + 2] and there are at least
2 unmarked sections to the right of the sections marked by m′ as R(m′, B) ≥R(m′, B1).
If both sections S[i + 1, i + 2] lay in the right part (but not on the head), we have
φ(i + 1) + φ(i + 2) = −(2 + 2
√
2) · 2
C
4K and Φ(i) ≤(2 +
√
2) · 2
C
4K −1
2
by observation (O4). This gives us Φ(i + 2) = 0.
If we have i = C/2K −2 (thus S[i + 1] is the last section of the left part), we get
φ(i + 1) + φ(i + 2) = −(2 +
√
2) · 2
C
4K and Φ(i) ≤(2 +
√
2) · 2
C
4K −1
and finally if both sections S[i + 1, i + 2] lay in the left part, we have
φ(i + 1) + φ(i + 2) = −(2 + 2
√
2) · 2
i
2 and Φ(i) ≤(2 +
√
2) · 2
i
2 .
This concludes the proof.
16

Lemma 1 together with invariant (I7), stating that each compaction removes at least K
items from B, implies the following bound on P, the number of compactions performed by the
compactor:
P ≤N
K ≲N log P
C
≲εN log P ,
which gives:
Observation 3 (Bound on P). log P ∈O (log(εN)) .
We now bound the number of compactors H, which is also the number of levels. Recall
that a new compactor is created when the highest compactor performs its first compaction and
compactor never performs compaction before it is full.
Since for each x items moved from level h to level h + 1, there are exactly x items deleted
from level h, the input stream Ih of the level-h compactor has at most half the number of items
inserted to level h −1. By induction, we obtain |Ih| ≤N/2h. Using this together with C ≥ε−1,
we get:
Observation 4 (Number of compactors). H ≤log(εN) + 2 ,
Lemma 2 (The space bound). The space bound on the whole sketch after processing N items is
SPACE ∈O

min
n
log1.5(εN) ·

ε−1√
ln δ−1 + ln δ−1
, log(εN) · ε−2 ln δ−1 o
.
Proof. By invariant (I6), we have CK ≈ε−2 ln δ−1 + ln2 δ−1. This together with Lemma 1 and
Observation 3 gives a bound on C:
C ≲ε−2 ln δ−1 + ln2 δ−1
K
C ≲(ε−2 ln δ−1 + ln2 δ−1) · log P
C
C ≲
p
log P · (ε−1√
ln δ−1 + ln δ−1)
C ≲
q
log(εN) · (ε−1√
ln δ−1 + ln δ−1).
At the same time we have C ≲ε−2 ln δ−1 as K ≥1. Surely C bounds the space occupied by
each compactor and as by Observation 4 the number of compactors is O(log(εN)), the statement
of the lemma follows.
4.2
The error bound
In this section, we prove the error guarantee of the sketch. The analysis is adapted from [Cor+23],
but is largely simplified. It consist of three lemmas: Lemma 3 (analogy to Lemma 6.7 in [Cor+23])
states, that for arbitrary item y, its rank decreases exponentially as we go up the levels. In
Lemma 4 (analogy to Lemma 6.11 in [Cor+23]) we prove a weaker error guarantee of ± R(y)
and in Lemma 5 we use the weaker guarantee to obtain the final bound ±ε R(y). However, we
avoid the most complicated parts of the proof in Section 6 of [Cor+23], namely, Lemma 6.12
in [Cor+23], and require less definitions, e.g., we do not need the quantities qi
h and zi
h.
The following fact is a simple corollary of Hoeffding bound.
Fact 1 (Corollary of Hoeffding bound). Let X1 . . . Xn be independent random variables such
that each Xi attains values ai, −ai with equal probability and let X = P
i Xi. Then for all t > 0
we have
P
X ≥t
 ≤exp
 
−
2t2
Var[X]
!
and
P
|X| ≥t
 ≤2 exp
 
−
2t2
Var[X]
!
.
17

For the whole error analysis, let us fix an arbitrary item y and let us recall that the error
of an item y is defined as Err(y) def
= Rˆ(y) −R(y) where R(y) is the true rank of y in the input
stream and Rˆ(y) def
= PH−1
h=0 2h R(y, Bh). Thus, after each compaction on level h, the error either
increases by 2h or decreases by 2h or stays the same.
Definition 3 (Important items and compactions). Let us call all items z ≤y important items.
We say that compaction is important if it affects Err(y). Let us denote the number of important
compactions on level h by mh.
If the rank of y among the compacted items is even, the compaction is not important, as
regardless the random choice, half of the important compacted items get removed and half is
promoted to the next level. On the other hand, if the rank of y among the compacted items (on
level h) is odd, the error either increases or decreases by 2h.
Observation 5 (Important compactions). Compaction is important if and only if the rank of y
among the compacted items is odd.
The following is a trivial bound on the number of important compaction on given level. We
later devise tighter bound in Observation 9.
Observation 6 (Bound on mh). The previous observation implies that each important compaction
removes at least one important item from B. This gives us for each h
mh ≤R(y, Ih) ,
where Ih is the input stream of level h.
Lemma 3 claims, that for any item y its rank decreases exponentially as we go up the levels.
Although this is intuitively true, it is not trivial to prove that it holds with high probability and
in fact this is the hardest part of the proof of the error bound.
For the proof of the lemma, we need a definition of the critical level. This is intuitively the
highest level such that any important item ever reaches it, conditioning on that the y’s rank
indeed decreases exponentially.
Definition 4. (Critical level) Let Hy be the lowest level h such that
2−h+1 R(y) ≤24 ln δ−1.
For the proof we also need following corollary of the definition of critical level. Informally, it
says that the next-to critical level must contain enough important items (again conditioning on
the statement of the lemma).
Observation 7. From the minimality of Hy, if Hy > 0, we have 2−(Hy−1)+1 R(y) > 24 ln δ−1,
which reduces to
2−Hy−2 R(y) > ln δ−1.
Lemma 3 (Ranks decrease exponentially). With probability at least 1−δ/4 we have simultaneously
for all levels h
R(y, Ih) ≤2−h+1 R(y)
and for all levels ℓ> Hℓ
R(y, Iℓ) = 0.
Proof. The proof is by induction over h, with the base case h = 0 holding as R(y, I0) = R(y).
For the rest of the proof, consider h > 0. First observe, that it suffices to prove the lemma
for h ≤Hy. This is true because conditioning on the statement holding for all h ≤Hy, by
18

Definition 4 we have R(y, IHy) ≤24 ln δ−1 and by invariant (I4) we always have 24 ln δ−1 ≤C/2.
By invariant (I8) the smallest C/2 items are never compacted, thus R(y, IHy+1) = 0.
We prove for all h ≤Hy, that if for all ℓ< h we have
R(y, Iℓ) ≤2−ℓ+1 R(y),
(1)
then with probability at least 1 −δ · 2h−Hy−3 we have
R(y, Ih) ≤2−h+1 R(y).
(2)
This is sufficient to prove the lemma as by the union bound, eq. (2) is simultaneously true for all
h with probability at least 1 −δ PHy
h=0 2h−Hy−3 > 1 −δ/4.
Let us investigate the dependence of R(y, Iℓ) on R(y, Iℓ−1). Letting Oℓ−1 be the output
stream of the compactor at level ℓ−1, it holds that R(y, Iℓ) = R(y, Oℓ−1) so we are comparing
the input and the output of the compactor on level ℓ−1. Some of the items from Iℓ−1 can stay
in the buffer Bℓ−1, the rest are subject to compactions. Recall that an important compaction is a
compaction that affects Err(y) and by Observation 5 these are exactly those compactions where
the rank of y among the compacted items is odd. This means that any non-important compaction
always sends half of the important items to the output, and any important compaction always
sends one important item more or less than half with equal probability. Let us recall that the
number of important compactions on level ℓis mℓ. Neglecting the items staying in the buffer, we
can bound the rank of y in the input stream Iℓas
R(y, Iℓ) = R(y, Oℓ−1) ≤1
2(R(y, Iℓ−1) + Binomial(mℓ−1)),
where Binomial(m) is a sum of m independent random variables taking values from {−1, 1} with
equal probability, generated with the same random bits as used in the level-(ℓ−1) compactor for
selecting even/odd-indexed items.
Let us use this bound recursively for all R(y, Iℓ) where 0 ≤ℓ≤h. Let Y0
def
= R(y) and for
0 < ℓ≤h let
Yℓ
def
= 1
2(Yℓ−1 + Binomial(mℓ−1)).
It follows that R(y, Ih) ≤Yh. Thus, it suffices to prove that with probability at least 1−2h−Hy−3δ
we have
Yh ≤2−h+1 R(y).
By unrolling the definition of Yh we obtain
Yh = 2−h R(y) +
h−1
X
ℓ=0
2−h+ℓBinomial(mℓ).
Note that the first summand has a fixed value and let us denote the second summand by
Zh
def
= Ph−1
ℓ=0 2−h+ℓBinomial(mℓ). It suffices to prove that
P[Zh > 2−h R(y)] ≤2h−Hy−3δ.
(3)
Note that the expression Zh can be viewed as a sum of random variables each attaining values ±ai
for some constant ai with equal probabilities, thus we can use Fact 1 on Zh to prove Equation (3).
To do this, we need to bound Var[Zh]. As each important compaction removes at least one
important item from the buffer, we have for each ℓ< h: mℓ≤R(y, Iℓ) (Observation 6) and by
19

eq. (1) we have R(y, Iℓ) ≤2−ℓ+1 R(y). We also have Var[Binomial(n)] = n. All this together
gives us following bound:
Var[Zh] ≤
h−1
X
ℓ=0
2−2h+2ℓmℓ≤
h−1
X
ℓ=0
2−2h+2ℓR(y, Iℓ) ≤
h−1
X
ℓ=0
2−2h+2ℓ2−ℓ+1 R(y)
=
h−1
X
ℓ=0
2−2h+ℓ+1 R(y)
≤2−h+1 R(y)
Now we apply Fact 1 with t = 2−h R(y) together with Observation 7 (2−Hy−2 R(y) > ln δ−1)
to prove Equation (3):
P[Zh > 2−h R(y)] < exp
 
−2−2h+1 R2(y)
Var[Zh]
!
≤exp
 
−2−2h+1 R2(y)
2−h+1 R(y)
!
= exp

−2−h R(y)

= exp

−2−h+Hy+2 · 2−Hy−2 R(y)

≤exp

−2−h+Hy+2 ln δ−1
= δ2−h+Hy+2 ≤2h−Hy−3δ
The last inequality follows from δ ≤1/2. This concludes the proof.
By a quite straightforward application of Lemma 3 and Fact 1 we obtain a crude error bound,
which is however important for proving Observation 8 which is in turn an essential tool in proving
the tight error bound (Lemma 5).
Lemma 4 (Initial error bound). Conditioning on the bound from Lemma 3 holding, with
probability at least 1 −δ/2 we have
Err(y) ≤R(y).
Proof. From the definition of important compaction, we have Err(y) = PH−1
h=0 2h Binomial(mh)
and by Lemma 3, there are no important compactions above level Hy and so we have
Err(y) =
Hy
X
h=0
2h Binomial(mh)
as we are conditioning on the bound from Lemma 3.
As in the proof of Lemma 3, we can see that Err(y) meets the conditions of Fact 1 and so we
can use it to prove the statement. Thus, we need to bound Var[Err(y)]. For this purpose we
use Observation 6 (mh ≤R(y, Ih)), Lemma 3 (R(y, Ih) ≤2−h+1 R(y)) and then Observation 7
(2−Hy−2 R(y) > ln δ−1):
Var[Err(y)] =
Hy
X
h=0
22hml ≤
Hy
X
h=0
22h R(y, Ih) ≤
Hy
X
h=0
22h2−h+1 R(y) =
Hy
X
h=0
2h+1 R(y)
≤2Hy+2 R(y)
= 2Hy+2 R−1(y) · R2(y)
≤R2(y)/ ln δ−1
20

Finally we apply Fact 1 with t = R(y):
P
Err(y) ≥R(y)
 ≤exp
 
−
2 R2(y)
Var[Err(y)]
!
≤exp

−2 ln δ−1
= δ2 ≤δ/2
The last inequality follows from δ ≤1/2. This concludes the proof.
Observation 8. Let Eh be the number of important items remaining in Bh of the final sketch.
Conditioning on the bound from Lemma 4 we have
X
h
2hEh = Rˆ(y) ≤2 R(y).
We use Observation 2 to devise a better bound on the number of important compactions.
The idea is straightforward – as we remove K important items for each important compactions,
we have mh ≤R(y, Ih)/K. However, the section length K decreases over time and using the
value of K of the final sketch is insufficient for the proof of Lemma 5. Thus, we use the value of
K from the “last” important compaction on level h.
Definition 5. Let KP and CP be respectively the values of K and C during a compaction P
and let Py,h be a level-h important compaction P such that KP is minimal (and consequently CP
is maximal). For simplicity, let us denote Ky,h
def
= KPy,h and Cy,h
def
= CPy,h.
The following bound on the number of important compactions is an analogy of Lemma 6.4 in
[Cor+23], which is proven by an involved charging argument. With adaptive compactors, this is
just a simple corollary of (also simple) Observation 2.
Observation 9 (Better bound on mh). By Observation 2, we have that
mh ≤R(y, Ih)
Ky,h
.
The following observation is the reason why we can not simply use the value of K of the
final sketch in Observation 9. As C grows over time, Observation 10 is not necessarily true for
the final value of C, but it is true for Cy,h. This allows as to bound the number of important
compactions by the number of important items remaining in the buffer of the final sketch and
use Observation 8 to bound the error.
Observation 10. We have that Cy,h/2 ≤Eh.
Proof. After an important compaction P there are at least CP/2 important items in B and as
we never remove the smallest C/2 items by invariant (I8) and C is nondecreasing in time by
invariant (I2), there must be at least CP/2 important items present in the final sketch.
Lemma 5 (Final error estimate). With probability 1 −δ we have
|Err(y)| ≤ε R(y).
Proof. We prove that if both bounds from Lemmas 3 and 4 hold, we have
|Err(y)| ≤ε R(y)
with probability at least 1 −δ/4, and the statement then follows from the union bound. Anal-
ogously to previous proofs, we first bound Var[Err(y)] and then we apply Fact 1 to prove the
desired statement.
21

Using the previously obtained bounds, we start by Observation 9 (which conditions on
Lemma 4), the second inequality (6) is by Lemma 3, the third (7) by invariant (I6), the next
inequality (8) is by Observation 10 and the last one (9) is by Observation 8.
Var[Err(y)] =
X
h
22hmh
(4)
≤
X
h
22h R(y, Ih)
Ky,h
(5)
≤
X
h
22h R(y)2−h+1
Ky,h
=
X
h
2h+1 R(y)
Ky,h
(6)
≤
X
h
2h+1 ε2 R(y)Cy,h
23 ln δ−1
= ε2 R(y)
2 ln δ−1
X
h
2hCy,h/2
(7)
≤ε2 R(y)
2 ln δ−1
X
h
2hEh
(8)
≤ε2 R2(y)
ln δ−1
(9)
Now it suffices to apply second part of Fact 1 with t = ε R(y):
P

|Err(y)| ≥ε R(y)

≤2 exp
 
−2ε2 R2(y)
Var[Err(y)]
!
≤2 exp
 
−2 ln δ−1ε2 R2(y)
ε2 R2(y)
!
= 2δ2 ≤δ/4
The last inequality follows from δ ≤1/4.
5
Space bound improvements in special cases
In this section, we present minor improvements of the space bound over original ReqSketch [Cor+23]
in two special cases. First, our sketch uses near-optimal space if the summary is produced by
merging and the merge tree is balanced, even approximately. Second, we provably achieve the
optimal space bound for any reverse-sorted input.
5.1
Better space bound with balanced merging
As mentioned earlier, the update operation can be viewed as a merge with a trivial sketch
representing one item. Thus, we can assume that the sketch is build only by merges. The history
of the sketch can be visualized by a binary tree where the leaves are trivial one-item sketches,
each internal node represents the state of the sketch after merging its two children, and the
root represents the final state of the sketch. For a fixed level h, at most one compactions is
performed during any merge operation. Thus for fixed h, the internal nodes also represent all
the compactions that ever happen on level h (i.e., each level-h compaction is represented by a
unique node).
In the proof of Lemma 1 we have that the potential Φ after a merge is at most maximum
of the potentials of the two merged compactors. This implies that it is sufficient to count in
variable P only the compactions performed in the subtree of one of the node’s children in the
merge tree. Formally, for any level h we can redefine P from the statement of Lemma 1 such that
for trivial one-item sketch P = 0 and for a sketch represented by internal node i with children l
and r we define P as maximum of Pr, Pl plus 1 if node i represents level-h compaction.
22

For this definition of P, Lemma 1 still holds and for each level h, Ph in the final sketch is
bounded by the depth of the tree. Thus for an approximately balanced merge tree, we have
P ∈O(log N) and so log P ∈log log N. This gives us (by the same calculation as in Lemma 2)
space bound
SPACE ∈O

log(εN) ·
p
log log N · (ε−1√
ln δ−1 + ln δ−1)

for the whole sketch, which is only by factor √log log N from the optimum (for constant δ).
More generally, P is bounded by the height H of the merge tree, which leads to a space
bound of O

log(εN) ·
√
H · (ε−1√
ln δ−1 + ln δ−1)

.
5.2
Optimal space bound for reverse-sorted inputs
We named our new compactors adaptive compactors, because they perform better on “nice”
inputs. Here, we demonstrate it on a simple example of any reverse-sorted input, i.e., a strictly
decreasing permutation.
Let us first consider the behaviour of the level-0 compactor on the reverse-sorted input. When
the compactor contains the largest C items of the input stream, the first compaction removes
the largest K items from the compactor and marks the next K largest ones. The items that
come after the compaction are all smaller than all the present items, thus when the compactor
is full again, the largest K items are marked. The second compaction removes all the marked
items, marks K largest remaining items and we find ourselves in the same situation again. Thus,
each compaction removes all the marked items and marks the K largest remaining items, which
remain the largest to the next compaction and are subsequently removed. Thus the level-0
compactor never performs a special compaction and never changes the value of K and C, so its
size remains C0 ≈ε−1√
ln δ−1 + ln δ−1.
Moreover, observe that for any items y < z, if z comes before y in the input stream, the order
of these two items never changes, meaning that z comes before y in the input of any compactor
(as long as they are both present in the sketch). This is simply because each compaction always
removes the largest items from B. It particularly means that if the input stream is reverse-sorted,
then input streams of all the compactors are reverse-sorted and all the compactors keep their
initial constant size. Thus the space bound becomes
SPACE ∈O

log(εN) · (ε−1√
ln δ−1 + ln δ−1)

which is optimal with respect to ε and N.
A similar analysis can be done for any stream that contains a lot of pairs y, z such that y < z
and z comes before y in the input stream. Whenever a new item which is smaller than some
marks comes to a compactor, all these marks “shift to the left” in the optimal marking. Thus
the closer is the stream to the reverse-sorted case, the closer is the space of the sketch to the
optimum.
6
Conclusions
We have proposed adaptive compactors as a new building block for mergeable relative-error
quantile sketches, replacing relative compactors from [Cor+23], while retaining their main
properties such as accuracy, memory footprint, and update time. The main point is to get more
accessible and intuitive proofs of the space bound of O(ε−1 · log1.5 εn) (with constant probability
δ of a too large error), even when the sketch is created by an arbitrary sequence of pairwise
merge operations. That is, we avoid the most involved calculations required for full mergeability
in [Cor+23], and only need to analyze the space consumed by adaptive compactors using a new
intuitive potential function. The upside of our approach is that it allows to flexibly analyze the
23

sketch with respect to special inputs without too much technical work, as we demonstrate in
Section 5.
The main open problem is to improve the space bound towards the lower bound of Ω(ε−1 ·
log εn) [Cor+23] in the general mergeability setting. We believe our paper makes a step in
this direction by making the analysis for full mergeability more accessible and flexible. More
specifically, we ask if our techniques can be combined with the approach in [Gri+25], which gives
a near-optimal bound of eO(ε−1 · log n) in the streaming setting, but does not deal with merging
sketches. One of the main challenges of using adaptive compactors instead of relative compactors
in the algorithm of [Gri+25] is that adaptive compactors do not straightforwardly support the
reset operation that shrinks the compactor to the initial state.
References
[Aga+13]
Agarwal, Pankaj K; Cormode, Graham; Huang, Zengfeng; Phillips, Jeff M;
Wei, Zhewei; Yi, Ke. Mergeable summaries. ACM Transactions on Database Systems
(TODS). 2013, vol. 38, no. 4, p. 26. Available from doi: 10.1145/2500128.
[Cor+23]
Cormode, Graham; Karnin, Zohar S.; Liberty, Edo; Thaler, Justin; Veselý,
Pavel. Relative Error Streaming Quantiles. J. ACM. 2023, vol. 70, no. 5, 30:1–30:48.
Available from doi: 10.1145/3617891.
[Cor+05]
Cormode, Graham; Korn, Flip; Muthukrishnan, S.; Srivastava, Divesh. Ef-
fective Computation of Biased Quantiles over Data Streams. In: Aberer, Karl;
Franklin, Michael J.; Nishio, Shojiro (eds.). Proceedings of the 21st International
Conference on Data Engineering, ICDE 2005, 5-8 April 2005, Tokyo, Japan. IEEE
Computer Society, 2005, pp. 20–31. Available from doi: 10.1109/ICDE.2005.55.
[Cor+06]
Cormode, Graham; Korn, Flip; Muthukrishnan, S.; Srivastava, Divesh. Space-
and time-efficient deterministic algorithms for biased quantiles over data streams.
Proceedings of the Twenty-Fifth ACM SIGMOD-SIGACT-SIGART Symposium
on Principles of Database Systems. 2006. Available from doi: 10.1145/1142351.
1142389.
[Cor+21]
Cormode, Graham; Mishra, Abhinav; Ross, Joseph; Veselý, Pavel. Theory meets
Practice at the Median: a worst case comparison of relative error quantile algorithms.
KDD ’21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
& Data Mining. 2021. Available from doi: 10.1145/3447548.3467152.
[CV20]
Cormode, Graham; Veselý, Pavel. Tight Lower Bound for Comparison-Based
Quantile Summaries. Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Sym-
posium on Principles of Database Systems. 2020. Available from doi: 10.1145/
3375395.3387650.
[Dun21]
Dunning, Ted. The t-digest: Efficient estimates of distributions. Software Impacts.
2021. Available from doi: 10.1016/j.simpa.2020.100049.
[GK01]
Greenwald, Michael; Khanna, Sanjeev. Space-efficient online computation of
quantile summaries. SIGMOD Rec. 2001. Available from doi: 10.1145/376284.
375670.
[Gri+24]
Gribelyuk, Elena; Sawettamalya, Pachara; Wu, Hongxun; Yu, Huacheng. Simple
& Optimal Quantile Sketch: Combining Greenwald-Khanna with Khanna-Greenwald.
Proc. ACM Manag. Data. 2024, vol. 2, no. 2. Available from doi: 10.1145/3651610.
24

[Gri+25]
Gribelyuk, Elena; Sawettamalya, Pachara; Wu, Hongxun; Yu, Huacheng. Near-
Optimal Relative Error Streaming Quantile Estimation via Elastic Compactors. Pro-
ceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2025, New Orleans, LA, USA, January 12-15, 2025. 2025, pp. 3486–3529. Available
from doi: 10.1137/1.9781611978322.115.
[GZ03]
Gupta, Anupam; Zane, Francis. Counting inversions in lists. Proceedings of the
Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms, January 12-14,
2003, Baltimore, Maryland, USA. 2003, pp. 253–254. Available also from: http:
//dl.acm.org/citation.cfm?id=644108.644150.
[GSW24]
Gupta, Meghal; Singhal, Mihir; Wu, Hongxun. Optimal Quantile Estimation:
Beyond the Comparison Model. 2024 IEEE 65th Annual Symposium on Foundations
of Computer Science (FOCS). 2024, pp. 1137–1158. Available from doi: 10.1109/
FOCS61266.2024.00075.
[He+15]
He, Zaobo; Cai, Zhipeng; Cheng, Siyao; Wang, Xiaoming. Approximate aggregation
for tracking quantiles and range countings in wireless sensor networks. Theor. Comput.
Sci. 2015, vol. 607, pp. 381–390. Available from doi: 10.1016/J.TCS.2015.07.056.
[KLL16]
Karnin, Zohar; Lang, Kevin; Liberty, Edo. Optimal quantile approximation in
streams. 2016 IEEE 57th annual symposium on foundations of computer science
(FOCS). 2016. Available from doi: 10.1109/FOCS.2016.17.
[MRL99]
Manku, Gurmeet Singh; Rajagopalan, Sridhar; Lindsay, Bruce G. Random
sampling techniques for space efficient online computation of order statistics of large
datasets. SIGMOD Rec. 1999. Available from doi: 10.1145/304181.304204.
[MRL19]
Masson, Charles; Rim, Jee E.; Lee, Homin K. DDSketch: A Fast and Fully-Mergeable
Quantile Sketch with Relative-Error Guarantees. PVLDB. 2019, vol. 12, no. 12,
pp. 2195–2205. Available from doi: 10.14778/3352063.3352135.
[Rho+13]
Rhodes, Lee; Lang, Kevin; Malkin, Jon; Saydakov, Alexander; Liberty, Edo;
Thaler, Justin. DataSketches: A library of stochastic streaming algorithms [Open
source software: https://datasketches.apache.org/]. 2013.
[Shr+04]
Shrivastava, Nisheeth; Buragohain, Chiranjeeb; Agrawal, Divyakant; Suri,
Subhash. Medians and beyond: new aggregation techniques for sensor networks.
Proceedings of the 2nd International Conference on Embedded Networked Sensor
Systems. 2004. Available from doi: 10.1145/1031495.1031524.
[Ten15]
Tene, Gil. How NOT to Measure Latency [https://www.youtube.com/watch?v=
lJ8ydIuPFeU]. 2015.
[ZW07]
Zhang, Qi; Wang, Wei. An efficient algorithm for approximate biased quantile
computation in data streams. Proceedings of the Sixteenth ACM Conference on
Information and Knowledge Management. 2007. Available from doi: 10.1145/
1321440.1321601.
[Zha+06]
Zhang, Ying; Lin, Xuemin; Xu, Jian; Korn, Flip; Wang, Wei. Space-efficient
Relative Error Order Sketch over Data Streams. Proceedings of the 22nd International
Conference on Data Engineering, ICDE 2006, 3-8 April 2006, Atlanta, GA, USA.
2006, p. 51. Available from doi: 10.1109/ICDE.2006.145.
25
