Proceedings of Machine Learning Research vol vvv:1â€“15, 2026
8th Annual Conference on Learning for Dynamics and Control
A Hybrid Learning-to-Optimize Framework for Mixed-Integer
Quadratic Programming
Viet-Anh Le
VIETANH@SEAS.UPENN.EDU
Mu Xie
MUX2001@SEAS.UPENN.EDU
Rahul Mangharam
RAHULM@SEAS.UPENN.EDU
Department of Electrical & Systems Engineering, University of Pennsylvania, Philadelphia, PA 19104, USA
Editors: G. Sukhatme, L. Lindemann, S. Tu, A. Wierman, N. Atanasov
Abstract
In this paper, we propose a learning-to-optimize (L2O) framework to accelerate solving parametric
mixed-integer quadratic programming (MIQP) problems, with a particular focus on mixed-integer
model predictive control (MI-MPC) applications. The framework learns to predict integer solu-
tions with enhanced optimality and feasibility by integrating supervised learning (for optimality),
self-supervised learning (for feasibility), and a differentiable quadratic programming (QP) layer,
resulting in a hybrid L2O framework. Specifically, a neural network (NN) is used to learn the
mapping from problem parameters to optimal integer solutions, while a differentiable QP layer
is integrated to compute the corresponding continuous variables given the predicted integers and
problem parameters. Moreover, a hybrid loss function is proposed, which combines a supervised
loss with respect to the global optimal solution, and a self-supervised loss derived from the prob-
lemâ€™s objective and constraints. The effectiveness of the proposed framework is demonstrated on
two benchmark MI-MPC problems, with comparative results against purely supervised and self-
supervised learning models.
Keywords: Learning to optimize, mixed-integer quadratic programming, mixed-integer model pre-
dictive control.
1. Introduction
Mixed-integer optimization is fundamental to many control applications that involve discrete decision-
making, such as autonomous driving (Quirynen et al., 2024), traffic signal coordination with con-
nected automated vehicles (Le and Malikopoulos, 2024), multi-robot pickup and delivery (Camisa
et al., 2022), motion planning and task assignment for robot fleets (Salvado et al., 2018), or sig-
nal temporal logic specifications (Belta and Sadraddini, 2019). However, solving a mixed-integer
program (MIP) is computationally NP-hard because it involves combinatorial search over discrete
decision variables. Consequently, the computation time can increase exponentially with problem
size or constraint complexity, making MIPs generally unsuitable for real-time control or decision-
making.
The advancements in machine learning and differentiable programming provide a promising
opportunity for accelerating MIP solvers through learning-to-optimize (L2O) frameworks. The lit-
erature on L2O for MIP problems is limited but has gained increasing attention in recent years. The
current state of the art can be categorized into two main approaches: (i) supervised learning (SL)
and (ii) self-supervised learning (SSL). Classical SL approaches, e.g., (Cauligi et al., 2021, 2022;
Le et al., 2025), aim to train neural networks (NNs) to minimize the supervised loss between the
predictions and the optimal integer solutions generated by an optimization solver such as GUROBI
Â© 2026 V.-A. Le, M. Xie & R. Mangharam.
arXiv:2511.19383v1  [eess.SY]  24 Nov 2025

LE XIE MANGHARAM
Diff
QP Layer
ğœ½
Neural 
Network
ğœ¹
ğ’™
ğœ¹âˆ—
Hybrid Loss
ğœ½
ğœ¹âˆ—
ğœ½
Neural 
Network
Supervised
Loss
ğœ¹
ğœ½
Neural 
Network
ğœ¹, ğ’™
Self-
Supervised
Loss
(a) Supervised learning
(b) Self-supervised learning
(c) Hybrid approach (proposed) 
Figure 1: Architecture of the proposed hybrid framework (c) compared with supervised learning (a)
and self-supervised learning (b). In our framework, the NN takes the problem parameters
Î¸ to predict the integer solution Î´, while the QP layer computes the continuous solution
x based on Î¸ and Î´. In conventional SL and SSL, the NN is trained to predict the integer
solution without considering the continuous solution or to predict both the integer and
continuous solutions, respectively.
(Gurobi Optimization, LLC, 2021). A major drawback of SL approaches is that they do not guaran-
tee feasibility, i.e., the resulting convex continuous problems obtained by fixing the learned integer
variables can be infeasible. SSL approaches (Tang et al., 2024; Boldock`y et al., 2025), on the other
hand, does not rely on labeled data and can improve feasibility by using a loss function that is a
linear combination of the objective function and a penalty for constraint violation. For example,
(Tang et al., 2024) proposed a framework for mixed-integer nonlinear programming that solves the
integer-relaxed problem, combined with integer correction layers to ensure integrality and a pro-
jection step to improve solution feasibility. (Boldock`y et al., 2025) considered parametric MIQP
problems within a differentiable predictive control framework, which constrains the integer solu-
tions and optimal control inputs to a neural state-feedback law. However, the trained model from
SSL may produce feasible but suboptimal solutions, since differentiable programming techniques
such as gradient descent may converge to locally optimal solutions.
Addressing the limitations of both SL (infeasibility) and SSL (suboptimality), we propose a
novel hybrid L2O framework that strategically combines both training paradigms with an integrated
differentiable QP layer. The hybrid approach proposed in this paper shares conceptual similarities
with physics-informed machine learning (PINN) by embedding the problemâ€™s known optimization
structure (the QP layer) as an inductive bias, much as PINNs embed physical laws (the PDEs).
First, we propose a novel architecture where a differentiable QP layer is integrated into the network
2

A HYBRID L2O FRAMEWORK FOR MIQP
to better incorporate the optimization structure. During training, the QP layer, which takes the NN
predictions for the integer decision variables as input and outputs the corresponding optimal solu-
tions for continuous variables, might be infeasible. To address this issue and ensure that gradients
can always be computed, we propose a simple yet effective approach that introduces a differen-
tiable layer for the relaxed QP problem. We prove that, if the penalty weight is chosen sufficiently
large, the relaxed problem yields either the exact optimal solution if the original QP is feasible or
the minimally infeasible solution otherwise. Second, we propose a new loss function design, called
a hybrid loss function, which is defined as a weighted sum of SL and SSL losses. This approach
allows the framework to balance the feasibility-optimality trade-off in L2O. The overall architecture
of our framework in comparison with SL and SSL can be illustrated in Fig. 1.
Our framework differs from existing work in the relevant literature in the following aspects.
First, a major discrepancy lies on the way we encode the dependency between continuous variables,
integer variables, and problem parameters into training. In SL approaches (Cauligi et al., 2021,
2022), the goal is to learn the mapping from problem parameters to the optimal integer variables,
while disregarding the continuous variables during training. In online prediction, the continuous
variables are obtained by solving a QP given the NN predictions of the integer variables. In con-
trast, SSL (Tang et al., 2024; Boldock`y et al., 2025) aims to train NNs to predict both the continuous
and integer variables given the problem parameters. Thus, the prediction obtained directly from the
NN does not explicitly account for the dependency between continuous and integer variables. In
our approach, we incorporate the continuous variables into the training process by integrating a QP
layer, based on the fact that the optimal continuous variables are the solutions of a parametric QP
given the integer variables and the MIQP problem parameters. Therefore, our approach is capable
of better incorporating the underlying optimization structure into both the training and prediction
than supervised and self-supervised learning. Second, we combine supervised and self-supervised
learning objectives to define a hybrid loss function, rather than relying solely on either one. This
design of the hybrid loss function allows the framework to balance between the optimality of su-
pervised learning with training labels and the feasibility improvement of self-supervision during
training. Thus, our proposed framework can be viewed as a compromise between SL and SSL. We
show by some numerical examples that the hybrid loss function achieves near-global optimality and
minimal constraint violation for most problem instances.
2. Preliminaries
This section provides a brief discussion on the parametric MIQPs and L2O for accelerating solving
MIQP problems.
2.1. Parametric MIQPs
We consider a parametric MIQP problem that takes the following form:
minimize
xâˆˆX,Î´âˆˆI
f(x, Î´; Î¸),
(1a)
subject to g(x, Î´; Î¸) â‰¤0,
(1b)
where x is the vector of continuous optimization variables, Î´ is the vector of integer optimization
variables, and Î¸ is the vector of problem parameters. We let X and I be the domains for continuous
and integer optimization variables, and g(Â·) = [g1(Â·), . . . , gr(Â·)] be the vector of r linear constraints.
3

LE XIE MANGHARAM
We assume that X and I are non-empty. In this work, we consider a convex quadratic objective
function and linear constraints, i.e.,
f(x, Î´; Î¸) = 1
2
x
Î´
âŠ¤
Q(Î¸)
x
Î´

+ p(Î¸)âŠ¤
x
Î´

,
g(x, Î´; Î¸) = G(Î¸)
x
Î´

âˆ’h(Î¸).
(2)
Note that given known parameters Î¸ and integer variables Î´, the optimal solution of the continuous
variables can be obtained by solving a QP problem, if it is feasible, as follows,
minimize
xâˆˆX
f(x; Î¾),
(3a)
subject to gc(x; Î¾) â‰¤0,
(3b)
where gc(Â·) denotes the components of g(Â·) that involve at least one continuous decision variable,
and Î¾ = [Î´âŠ¤, Î¸âŠ¤]âŠ¤. The objective function and constraint function can be expressed as:
f(x; Î¾) = 1
2xâŠ¤Qx(Î¾)x + qx(Î¾)âŠ¤x,
(4)
gc(x; Î¾) = Gx(Î¾)x âˆ’hx(Î¾).
(5)
Mixed-integer MPC: A typical application that can greatly benefit from L2O approaches is
model predictive control (MPC). In MPC, we solve a parametric optimization problem at every
time step, in which the problem parameters may include, for instance, a given initial state, target
state, to name a few. Using machine learning to solve or assist in solving parametric MPC problems
enables real-time implementation of complex MPC, such as nonlinear MPC or mixed-integer MPC.
We consider the state xt âˆˆX âŠ‚Rnx and control inputs ut âˆˆU âŠ‚Rnu as the continuous decision
variables and let Î´t âˆˆI âŠ‚ZnÎ´ be the integer decision variables. Let H be the control horizon
length, and x0:H, u0:Hâˆ’1, Î´0:Hâˆ’1 be the concatenated vectors over the control horizon. Given a
vector of problem parameters Î¸ âˆˆRnp, a parametric MI-MPC can be written as:
minimize
x0:H,u0:Hâˆ’1,Î´0:Hâˆ’1 cH(xH; Î¸) +
Hâˆ’1
X
t=0
ct(xt, ut, Î´t; Î¸),
subject to
x0 = xinit(Î¸),
xt+1 = f(xt, ut, Î´t; Î¸), t = 0, . . . , H âˆ’1,
gt(xt, ut, Î´t; Î¸) â‰¤0, t = 0, . . . , H âˆ’1,
gH(xt; Î¸),
(6)
where the stage cost ct(Â·) and terminal cost cH(Â·) are convex quadratic, while the dynamics f(Â·),
the inequality constraints gt(Â·) and gH(Â·) are assumed to be linear functions. The objective function
and constraints are functions of the parameter vector Î¸ âˆˆÎ˜, where Î˜ âŠ†Rnp is the admissible set
of parameters.
4

A HYBRID L2O FRAMEWORK FOR MIQP
2.2. Learning to Optimize for MIQPs
Due to the combinatorial nature, the problem (1) is computationally demanding to solve, where
finding the optimal solution may scale exponentially with the problem size. Meanwhile, solving (3)
while the integers are fixed is significantly cheaper to solve than the MIQP. An interesting approach
to accelerating solving problems of the form (1) is to learn a map between the vector of problem
parameters Î¸ and the discrete optimizer Î´âˆ—by an NN, Î´âˆ—= Ï€Ï‰(Î¸), where Ï‰ is the vector of network
weights. Overall, there are two main approaches for learning Ï€Ï‰, including (i) supervised learning
and (ii) self-supervised learning.
Supervised Learning: The NN Ï€Ï‰ can be trained by a classical SL approach. In SL, we collect
the optimal solutions Î´i,âˆ—corresponding to each Î¸i, for i = 1, . . . , M, obtained from a solver. We
then use the dataset {Î¸i, Î´i,âˆ—} to train a NN that minimizes the following loss function:
minimize
Ï‰
1
M
M
X
i=1
L(Ï€Ï‰(Î¸i), Î´i,âˆ—),
(7)
where L denotes a supervised loss (e.g., Huber or cross-entropy) between the predicted outputs and
labels. However, the prediction from an SL model may not ensure that the resulting QP is feasible,
although the original MIQP is feasible.
Self-Supervised Learning: Self-supervised learning, contrary to SL, does not rely on labeled
data for training the model. It instead trains models by minimizing the objective function and
constraint violation directly from model predictions. In generic SSL, an NN is trained to predict
both the integer and continuous variables, i.e., (Î´i, xi) = Ï€Ï‰(Î¸i), and to train the NNs given a
dataset with M training instances, the following self-supervised loss function is used:
minimize
Ï‰
1
M
M
X
i=1
f(xi, Î´i; Î¸i) + Î»âŠ¤max
 0, g(xi, Î´i; Î¸i)

,
(8a)
subject to (Î´i, xi) = Ï€Ï‰(Î¸i),
(8b)
In (8), we include a penalty for constraint violation with max penalty (implemented via a ReLU)
function. Î» âˆˆRr
>0 is a vector of penalty parameters that balances the trade-off between minimizing
the objective function and satisfying the constraints. Although the penalty methods lack formal
guarantees, they often outperform their hard constraint counterparts in practice. Training the NN
with a self-supervised loss function can improve feasibility. Nevertheless, a major drawback of this
approach is that since the self-supervised loss (8) is non-convex, gradient-based methods may not
converge and may converge to a sub-optimal solution. Moreover, in MPC applications, designing an
NN to predict the optimal continuous decision variables from the problem parameters is generally
challenging, as it is difficult to enforce the system dynamics on the network outputs (Cauligi et al.,
2021), unless the optimal integer and continuous solutions at each time step are constrained to
follow a state-feedback law, as in differentiable predictive control (Boldock`y et al., 2025).
Remark 1 (Differentiating through discrete operations) In many approaches, the NNs are de-
signed to directly output the discrete values, where the discrete operations are used to produce
discrete outputs like rounding operation. Those discrete operations lead to non-differentiability
and hinder the use of standard differentiable programming for training the network. A common
approach to overcome this issue is the straight-through estimator (STE) (Bengio et al., 2013), a
5

LE XIE MANGHARAM
technique for enabling backpropagation through discrete operations. During the forward pass,
STE applies a non-differentiable operation to obtain discrete values. During the backward pass,
it bypasses the non-existent gradients of these operations by replacing them with those of smooth
surrogate functions. This approach was used in self-supervised learning frameworks for mixed-
integer programming (Tang et al., 2024; Boldock`y et al., 2025). We also use this technique in our
framework.
We observe that the strength of SL in finding global solutions corresponds to the weakness of
SSL, and vice versa, the strength of SSL in improving feasibility corresponds to the weakness of SL.
Therefore, an interesting idea is to combine SL and SSL to exploit the benefits of both approaches.
3. Proposed Framework
In this section, we present an L2O framework for MIQPs in which a differentiable QP layer is
incorporated, and a hybrid loss function combining SL and SSL is proposed.
3.1. Differentiable QP Layers for Feasible and Infeasible Problems
Given known Î´âˆ—and Î¸, the optimal solution of the continuous variables can be obtained by solving
the QP problem (3), if it is feasible. Thus, we can consider it as a QP layer within deep learning
architectures, denoted by x = QP(Î´, Î¸). Therefore, it leads to the following problem in which we
approximate the integer solutions by NNs, and the continuous solutions by a QP layer,
minimize
Ï‰
f(x, Î´; Î¸),
(9a)
subject to Î´ = Ï€Ï‰(Î¸),
(9b)
x = QP(Î´, Î¸),
(9c)
g(x, Î´; Î¸) â‰¤0.
(9d)
To ensure the validity of the L2O framework using a QP layer and differentiable programming, we
need the following assumption.
Assumption 1 The QP problem (3) is strictly convex.
As stated in Amos and Kolter (2017, Theorem 1), Assumption 1 is needed to ensure that the QP
layer is subdifferentiable everywhere, and differentiable at all but a measure-zero set of points,
because the solution of a strictly convex QP is continuous. The question is how to compute the
gradient of the optimal solution xâˆ—with respect to the argument, i.e., âˆ‚xâˆ—
âˆ‚Î¾ . If the problem is feasible,
these derivatives can be obtained by differentiating the KKT conditions (sufficient and necessary
conditions for optimality) of (3). However, since the methods in (Amos and Kolter, 2017; Agrawal
et al., 2019) rely on KKT conditions, it assume the QP problem is feasible. On the other hand, in our
framework, the optimization problems might be infeasible during training, given different values of
the integer variables from the NN. Thus, we cannot directly incorporate the differentiable QP layer
in (Amos and Kolter, 2017; Agrawal et al., 2019) into our framework. In this section, we present a
simple yet efficient way to handle infeasibility during training, as described below.
6

A HYBRID L2O FRAMEWORK FOR MIQP
To this end, we introduce slack variables s for the constraints, leading to the following QP:
minimize
xâˆˆX,s
f(x; Î¾) + Ï 1âŠ¤s,
(10a)
subject to gc(x; Î¾) â‰¤s,
(10b)
s â‰¥0.
(10c)
For ease of notations, in the rest of this section, we omit the argument Î¾ while mentioning the
terms involving it. Since (10) is feasible given any realization of Î¾ as long as the domain for x is
non-empty, we can apply the KKT conditions. First, we formulate the Lagrangian of (10) as
L(x, s, Âµ, Îº) = 1
2xâŠ¤Qxx + qâŠ¤
x x + Ï 1âŠ¤s + ÂµâŠ¤(Gxx âˆ’hx âˆ’s) âˆ’ÎºâŠ¤s
(11)
where Âµ â‰¥0 and Îº â‰¥0 are the dual variables on the constraints, and Ï > 0 is a penalty weight
for the slack variables. The KKT conditions for stationarity, primal feasibility, and complementary
slackness are
Qxxâˆ—+ qx + GâŠ¤
x Âµâˆ—= 0,
(12a)
Ï 1 âˆ’Âµâˆ—âˆ’Îºâˆ—= 0,
(12b)
D(Âµâˆ—)
 Gxx âˆ’hx âˆ’s

= 0,
(12c)
D(Îºâˆ—)sâˆ—= 0,
(12d)
where D(Â·) is the operation creating a diagonal matrix from a vector. Taking the differentials of the
KKT conditions (12), we obtain
dQxxâˆ—+ Qxdx + dqx + dGâŠ¤
x Âµâˆ—+ GâŠ¤
x dÂµ = 0,
(13a)
dÂµ + dÎº = 0,
(13b)
D
 Gxxâˆ—âˆ’hx âˆ’sâˆ—
dÂµ + D(Âµâˆ—)(dGxxâˆ—+ Gxdx âˆ’dhx âˆ’ds) = 0,
(13c)
D(sâˆ—)dÎº + D(Îºâˆ—)ds = 0,
(13d)
or in the matrix form as follows:
ï£®
ï£¯ï£¯ï£°
Qx
0
Gx
0
D(Âµâˆ—)Gx
âˆ’D(Âµâˆ—)
D
 Gxxâˆ—âˆ’hx âˆ’sâˆ—
0
0
D(Îºâˆ—)
0
D(sâˆ—)
0
0
I
I
ï£¹
ï£ºï£ºï£»
ï£®
ï£¯ï£¯ï£°
dx
ds
dÂµ
dÎº
ï£¹
ï£ºï£ºï£»=
ï£®
ï£¯ï£¯ï£°
âˆ’dQxxâˆ—âˆ’dqx âˆ’dGâŠ¤
x Âµâˆ—
âˆ’D(Âµâˆ—)(dGxxâˆ—âˆ’dhx)
0
0
ï£¹
ï£ºï£ºï£».
(14)
Using these equations, we can form the Jacobians of xâˆ—and sâˆ—with respect to any of the problem
parameters. The details of this process can be found in (Amos and Kolter, 2017), (Agrawal et al.,
2019).
Note that given any x, sâˆ—= max(0, gc(x)). In other words, the use of slack variables in (10)
is equivalent to using the max penalty function. The following theorem shows that if the original
QP (3) is feasible, then by choosing a sufficiently large value for Ï, solving (10) yields the same
solution as (3).
Theorem 2 If the original QP (3) is feasible and let x
â€²âˆ—and Âµ
â€²âˆ—be the optimal solutions and
multipliers of the problem, respectively. If the penalty weight Ï is chosen such that Ï >
Âµ
â€²âˆ—
âˆ,
then the optimal solutions of (10) and the original QP (3) are the same.
7

LE XIE MANGHARAM
The proof of this theorem follows directly from Proposition 5.25 in (Bertsekas, 2014) and is
therefore omitted. Therefore, if the original QP is feasible, we get the exact derivative of the optimal
solution by using the KKT conditions of the relaxed problem. In the infeasible case, we show in the
following theorem that, under some mild conditions, the obtained solution from (10) is a point that
minimizes the constraint violation, and among all the solution with minimal constraint violation,
the obtained solution also minimizes the original objective function.
Theorem 3 If the original QP (3) is infeasible and assume that either one of the following proper-
ties hold:
â€¢ Qx â‰»0, which means f(x) is coercive.
â€¢ X is compact (closed and bounded).
Let denote v(x) := 1âŠ¤max(0, gc(x)). If the penalty weight Ï is chosen sufficiently large, then (i)
a minimizer (xâˆ—
Ï, sâˆ—
Ï) of (10) achieves minimal total violation, i.e., v(xâˆ—
Ï) = vâˆ—and (ii)
xâˆ—
Ï âˆˆarg min
xâˆˆX
{f(x) : v(x) = vâˆ—}.
(15)
The proof for Theorem 3 is given in Appendix A.
3.2. Hybrid Training Loss
Our training framework relies on a hybrid loss function that combines supervised and self-supervised
losses. The main advantage of this hybrid loss is that it leverages the strengths of supervised learn-
ing (SL) in achieving global solution optimality and self-supervised learning (SSL) in improving
constraint satisfaction. The hybrid loss used to train the neural network can be defined as follows:
minimize
Ï‰
1
M
M
X
i=1
Î³objf(xi, Î´i; Î¸i) + Î³conÎ»âŠ¤max
 0, g(xi, Î´i; Î¸i)

+ Î³supL(Î´i, Î´i,âˆ—),
(16)
where Î³obj, Î³con, and Î³sup âˆˆRâ‰¥0 are the weights for the objective value, constraint violation, and
supervised loss, respectively. Note that in our framework, the constraints involving at least one
continuous variable can be directly handled using the differentiable QP layer, while the constraints
involving only the integer variables must be incorporated into the loss function.
4. Results and Discussions
4.1. Numerical Examples
We validate our hybrid L2O framework on two benchmark MI-MPC problems: (i) collision avoid-
ance for robot navigation and (ii) simplified thermal energy tanks (Boldock`y et al., 2025). In the
first example, binary variables are used to formulate the collision-avoidance constraints, thus, there
are several coupling constraints between the integer and continuous variables. Meanwhile, the sec-
ond example involves integer decision variables and demonstrates the case where the integers ap-
pear in the objective function. The details of the two examples are provided in Appendix B. For
8

A HYBRID L2O FRAMEWORK FOR MIQP
0.0
1.0
Values
0
2500
5000
7500
Count
Violation rate
SL: 0.1%
SSL: 0.0%
H-L2O: 0.0%
Constraint violation
(Integer)
SL
SSL
H-L2O
SL
SSL
H-L2O
Models
0
1
2
3
4
Values
Violation rate
SL: 71.7%
SSL: 0.6%
H-L2O: 8.0%
Constraint violation
(Continuous)
SL
SSL
H-L2O
Models
0
50
100
Values
Optimality gap (%)
Figure 2: Statistical comparison of the three models: hybrid L2O (H-L2O), supervised learning
(SL), and self-supervised learning (SSL), for the robot navigation example.
0.0
1.0
2.0
3.0
Values
0
5000
10000
Count
Violation rate
SL: 5.7%
SSL: 0.0%
H-L2O: 1.1%
Constraint violation
(Integer)
SL
SSL
H-L2O
SL
SSL
H-L2O
Models
0
10
20
Values
Violation rate
SL: 4.8%
SSL: 11.0%
H-L2O: 5.6%
Constraint violation
(Continuous)
SL
SSL
H-L2O
Models
0
50
100
Values
Optimality gap (%)
Figure 3: Statistical comparison of the three models: hybrid L2O (H-L2O), supervised learning
(SL), and self-supervised learning (SSL), for thermal energy tank example.
each example, a multilayer perceptron network is constructed with four hidden layers, 128 neu-
rons per layer, and ReLU activation functions. Our implementation and examples are available at
https://github.com/mlab-upenn/L2O-MIQP.
We compare the proposed hybrid L2O framework with an SL model and an SSL model. Note
that the considered SSL model follows the architecture of our framework with the differentiable QP
layer, rather than the conventional design in which the NN is trained to predict both integer and
continuous solutions. However, the loss function for training the SSL model does not include the
supervised term. We evaluate the trained models using two metrics: constraint violation and opti-
mality gap. We separate the violations into those associated with constraints involving continuous
variables and those with only integer variables. The optimality gap is expressed as a percentage,
computed as the ratio between the objective gap and the optimal objective value. We show the sta-
tistical comparison for the two examples in Figures 2 and 3, respectively. In each figure, the left
panel shows the violations for integer-only constraints in the form of barplots, while the boxplots
in the middle and right panels show the continuous-constraint violations and the optimality gap,
respectively. For the figures showing constraint violations, we also report the violation rate, i.e., the
percentage of validation problems in which the obtained solutions violate the constraints. For the
9

LE XIE MANGHARAM
robot navigation example, the results indicate that all three models satisfy the constraints involving
only integer variables. However, for continuous-constraint violations, the SL model fails to ensure
constraint satisfaction in 71.7% of the test cases (a consequence of SL training only for the integer
solution, which does not guarantee a feasible continuous solution from the subsequent QP), whereas
the hybrid L2O and SSL models exhibit much smaller violation rates of 8% and 0.6%, respectively,
demonstrating improved constraint satisfaction. The plot of the optimality gap shows that the hybrid
L2O model achieves better optimality than the SSL model, although it exhibits a slightly larger gap
than the SL model. A similar trend in optimality is observed in the second example. Regarding
constraint satisfaction, the hybrid L2O model outperforms the SL model for constraints involving
only integer variables, achieving a lower violation rate of 1.1% compared to 5.7%. Meanwhile, the
two models achieve comparable levels of satisfaction for continuous constraints (5.6% vs. 4.8% vi-
olation rate). In contrast, the SSL model perfectly satisfies the integer-only constraints but performs
poorly on the continuous ones. In summary, the results reveal that the proposed hybrid L2O frame-
work effectively balances feasibility and optimality, achieving optimality performance comparable
to supervised learning while improving constraint satisfaction.
Finally, we report the computation times in Table 1, comparing the L2O approach (either SL,
SSL, or the hybrid L2O) with the GUROBI solver (Gurobi Optimization, LLC, 2021). Note that the
computation time for L2O approach includes both the NN prediction time and the time required to
solve the relaxed QP problem. The results confirm that the L2O approach significantly reduces the
overall solving time compared to a state-of-the-art MIQP solver. In particular, the L2O approach
achieves approximately 9Ã— and 12Ã— faster computation for the robot navigation and energy tank
examples, respectively.
4.2. Limitations
Although our proposed hybrid L2O framework demonstrates some benefits over SL and SLL ap-
proaches, it still has certain limitations that can be addressed in future research. First, the choice of
weights in the hybrid loss function significantly affects the optimality and feasibility performance
and currently requires manual tuning. Second, integrating the differentiable QP layer considerably
increases the training time compared to purely supervised learning. Finally, the current framework
is limited to MIQPs, while integrating differentiable optimization layers into general mixed-integer
convex or nonlinear programming problems remains an open challenge.
5. Conclusions
In this work, we developed a hybrid L2O framework for MIQPs that integrates a differentiable QP
layer, and a hybrid loss function combining supervised learning and self-supervised learning. We
designed a model architecture in which a neural network learns the mapping from problem param-
eters to optimal integer variables, while the differentiable QP layer computes the corresponding
continuous variables. This architecture enables the integration of optimization structure into the
learning process. To balance solution optimality and constraint feasibility during training, we de-
fined a hybrid loss that linearly combines supervised and self-supervised terms. We validated the
framework on two benchmark MPC examples, showing that the hybrid L2O approach effectively
trades off optimality and feasibility: it achieves better constraint satisfaction than purely supervised
learning and better optimality than purely self-supervised learning. Our future work will focus on
extending the framework to mixed-integer convex and nonlinear programming problems.
10

A HYBRID L2O FRAMEWORK FOR MIQP
Table 1: Average solving time and standard deviation for GUROBI solver and the L2O solver.
Problem
Solver
Avg. time (ms)
Std. dev. (ms)
Robot navigation example
GUROBI
69.49
19.06
L2O
7.55
1.31
Energy tank example
GUROBI
15.40
8.61
L2O
1.31
0.39
Acknowledgments
This work was partially supported by US DoT Safety21 National University Transportation Center
and NSF grants CISE-2431569.
References
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and J Zico Kolter.
Differentiable convex optimization layers. Advances in neural information processing systems,
32, 2019.
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In International conference on machine learning, pages 136â€“145. PMLR, 2017.
Calin Belta and Sadra Sadraddini. Formal methods for control synthesis: An optimization perspec-
tive. Annual Review of Control, Robotics, and Autonomous Systems, 2(1):115â€“140, 2019.
Yoshua Bengio, Nicholas LÂ´eonard, and Aaron Courville.
Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Dimitri P Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press,
2014.
JÂ´an Boldock`y, Shahriar Dadras Javan, Martin Gulan, Martin MÂ¨onnigmann, and JÂ´an DrgoË‡na. Learn-
ing to solve parametric mixed-integer optimal control problems via differentiable predictive con-
trol. arXiv preprint arXiv:2506.19646, 2025.
Andrea Camisa, Andrea Testa, and Giuseppe Notarstefano. Multi-robot pickup and delivery via
distributed resource allocation. IEEE Transactions on Robotics, 39(2):1106â€“1118, 2022.
Abhishek Cauligi, Preston Culbertson, Edward Schmerling, Mac Schwager, Bartolomeo Stellato,
and Marco Pavone. Coco: Online mixed-integer control via supervised learning. IEEE Robotics
and Automation Letters, 7(2):1447â€“1454, 2021.
Abhishek Cauligi, Ankush Chakrabarty, Stefano Di Cairano, and Rien Quirynen. Prism: Recurrent
neural networks and presolve methods for fast mixed-integer optimal control. In Learning for
Dynamics and Control Conference, pages 34â€“46. PMLR, 2022.
Gurobi Optimization, LLC.
Gurobi optimizer reference manual, 2021.
URL http://www.
gurobi.com.
11

LE XIE MANGHARAM
Viet-Anh Le and Andreas A Malikopoulos.
Distributed Optimization for Traffic Light Control
and Connected Automated Vehicle Coordination in Mixed-Traffic Intersections. IEEE Control
Systems Letters, 8:2721â€“2726, 2024.
Viet-Anh Le, Panagiotis Kounatidis, and Andreas A. Malikopoulos. Combining Graph Attention
Networks and Distributed Optimization for Multi-Robot Mixed-Integer Convex Programming. In
2025 64th IEEE Conference on Decision and Control, 2025.
Rien Quirynen, Sleiman Safaoui, and Stefano Di Cairano. Real-time mixed-integer quadratic pro-
gramming for vehicle decision-making and motion planning.
IEEE Transactions on Control
Systems Technology, 2024.
JoËœao Salvado, Robert Krug, Masoumeh Mansouri, and Fedorico Pecora.
Motion planning and
goal assignment for robot fleets using trajectory optimization. In 2018 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), pages 7939â€“7946. IEEE, 2018.
Bo Tang, Elias B Khalil, and JÂ´an DrgoË‡na. Learning to optimize for mixed-integer non-linear pro-
gramming. arXiv preprint arXiv:2410.11061, 2024.
Appendix A. Proof of Theorem 3
Proof We first prove (ii) given assuming that xâˆ—
Ï âˆˆSv. Let Sv = {x | v(x) = vâˆ—} be the set of
all points that achieve this minimum violation. Sv is closed since it is a level set of a continuous
function. Combining with the condition that either f(x) is coercive or X is compact, there exist a
minimizer of f(x) on Sv. From the definition of xâˆ—
Ï, we have
xâˆ—
Ï âˆˆarg min
xâˆˆX
f(x) + Ïv(x),
(17)
which means xâˆ—
Ï is also a minimizer of f(x) + Ïv(x) on Sv, i.e.,
xâˆ—
Ï âˆˆarg min
xâˆˆSv
f(x) + Ïv(x) = arg min
xâˆˆSv
f(x) + Ïvâˆ—.
(18)
Since Ïvâˆ—is a constant, then xâˆ—
Ï âˆˆarg min
xâˆˆSv
f(x), or xâˆ—
Ï is a minimizer of f(x) on Sv.
For (i), we prove that for a sufficiently large but finite Ï, xâˆ—
Ï must be in Sv. Let us assume, for
the sake of contradiction, xâˆ—
Ï /âˆˆSv. From (17), we have
f(xâˆ—
Ï) + Ïv(xâˆ—
Ï) â‰¤f(xâˆ—âˆ—) + Ïv(xâˆ—âˆ—) = f(xâˆ—âˆ—) + Ïvâˆ—,
(19)
for any xâˆ—âˆ—âˆˆSv, which leads to
Ï â‰¤f(xâˆ—âˆ—) âˆ’f(xâˆ—
Ï)
v(xâˆ—Ï) âˆ’vâˆ—
.
(20)
Next, since v(Â·) is a convex piecewise linear function, the Hoffman error bound property holds, i.e.,
v(xâˆ—
Ï) âˆ’vâˆ—â‰¥c dist(xâˆ—
Ï, Sv),
(21)
12

A HYBRID L2O FRAMEWORK FOR MIQP
with c > 0. Moreover, f(x) is a quadratic function, so it is Lipschitz continuous, and we have the
following inequality
|f(xâˆ—âˆ—) âˆ’f(xâˆ—
Ï)| â‰¤Lf Â· âˆ¥xâˆ—âˆ—âˆ’xâˆ—
Ïâˆ¥.
(22)
Therefore, if we choose xâˆ—âˆ—âˆˆSv such that dist(xâˆ—
Ï, Sv) = âˆ¥xâˆ—âˆ—âˆ’xâˆ—
Ïâˆ¥, i.e., xâˆ—âˆ—is the closest point
in Sv to x, then from (19), we obtain that Ï â‰¤Lf
c must be satisfied. Thus, we can select Ï such that
Ï > Lf
c , which leads to a contradiction. The proof is thus complete.
Appendix B. Details of Numerical Examples
B.1. Collision Avoidance for Robot Navigation
In this example, we consider a navigation problem for a single robot operating in an environment
with stationary obstacles. The robot is required to move from its initial position to a designated
goal while avoiding collisions with obstacles. This problem is formulated as an MIQP, where bi-
nary variables are used to formulate the collision avoidance constraints between the robot and the
obstacles. The corresponding MI-MPC formulation follows the setup described in (Le et al., 2025).
We consider an MPC problem with a single robot and no obstacles, and let O be the set of
obstacles. We formulate the MPC problem with a control horizon of length H. Let t âˆˆZ+ be
the current time step. At every time step k âˆˆZ+, let p(k) = [px(k), py(k)]âŠ¤âˆˆR2, v(k) =
[vx(k), vy(k)]âŠ¤âˆˆR2, and u(k) = [ux(k), uy(k)]âŠ¤âˆˆR2 be the vectors of positions, velocities,
and accelerations for robot, respectively. Let x(k) = [p(k), v(k)]âŠ¤be the state vector of robot. The
dynamics of each robot are governed by a discrete-time double-integrator model as follows,
p(k + 1) = p(k) + Ï„v(k) + 1
2Ï„ 2u(k),
v(k + 1) = v(k) + Ï„u(k),
(23)
where Ï„ âˆˆR>0 is the sampling time period, and compactly expressed as x(k+1) = f(x(k), u(k)).
We assume that the states and control inputs of robots are subjected to the following bound con-
straints:
px
min â‰¤px(k) â‰¤px
max,
py
min â‰¤py(k) â‰¤py
max,
âˆ’vmax â‰¤vx(k), vy(k) â‰¤vmax,
âˆ’amax â‰¤ux(k), uy(k) â‰¤amax,
(24)
where [px
min, px
max, py
min, py
max]âŠ¤âˆˆR4 is the boundary of the environment, vmax âˆˆR>0 and amax âˆˆ
R>0 are the maximum speed and acceleration of the robots, respectively. More compactly, (24) is
expressed as x(k) âˆˆX and u(k) âˆˆU.
The mixed-integer constraints for collision avoidance between the robot and obstacle o âˆˆO at
time-step k are formulated by big-M formulation as follows,
cos Î±o(px(k + 1) âˆ’px
o) + sin Î±o(py(k + 1) âˆ’py
o) â‰¥Lo + dmin âˆ’Mb1,o(k),
âˆ’sin Î±o(px(k + 1) âˆ’px
o) + cos Î±o(py(k + 1) âˆ’py
o) â‰¥Wo + dmin âˆ’Mb2,o(k),
âˆ’cos Î±o(px(k + 1) âˆ’px
o) âˆ’sin Î±o(py(k + 1) âˆ’py
o) â‰¥Lo + dmin âˆ’Mb3,o(k),
sin Î±o(px(k + 1) âˆ’px
o) âˆ’cos Î±o(py(k + 1) âˆ’py
o) â‰¥Wo + dmin âˆ’Mb4,o(k),
(25)
13

LE XIE MANGHARAM
where b1,o(k), b2,o(k), b3,o(k) and b4,o(k) are binary decision variables satisfying
b1,o(k) + b2,o(k) + b3,o(k) + b4,o(k) â‰¤3,
(26)
[px
o, py
o]âŠ¤is the center location, Î±o is the rotation angle, and 2Lo and 2Wo are the length and width
of obstacleâ€“o âˆˆO, respectively, while dmin is the minimal distance between robot and obstacle to
be considered as no collision. We define the binary decision variables Î´(k) âˆˆ{0, 1}4no at each time
step k as the concatenated vector of b1,o(k), b2,o(k), b3,o(k), b4,o(k), for all o âˆˆO, and rewrite all
the collision avoidance constraints as go(xk+1, Î´k) â‰¤0.
The objective for the robot is to reach the goal, i.e., minimize the distance to the goal, while
maintaining the minimum effort. Thus, we consider the following MPC cost given by a weighted
sum of terminal cost Â¯c and running cost c over the horizon i.e.,
minimize
x(k+1)âˆˆX,
u(k)âˆˆU
Â¯c
 x(t + H)

+
t+Hâˆ’1
X
k=t
c
 u(k), x(k)

,
(27)
where
Â¯c(x(t + H)) = Ï‰pt
p(t + H) âˆ’pg
2
2 ,
c(u(k), x(k)) = Ï‰p
p(k) âˆ’pg
2
2 + Ï‰u âˆ¥u(k)âˆ¥2
2
(28)
with pg being the vector of goal positions, while Ï‰pt, Ï‰p, and Ï‰u are positive weights. Consequently,
the cost function is quadratic in the continuous decision variables.
Therefore, the parametric MI-MPC problem can be given by
minimize
Â¯c(xH; Î¸) +
Hâˆ’1
X
k=0
c(xk, uk; Î¸)
subject to
x0 = xinit(Î¸),
xk+1 = f(xk, uk),
go(xk+1, Î´k) â‰¤0,
x âˆˆX H+1, u âˆˆUH, Î´k âˆˆ{0, 1}4no.
(29)
where the parameter vector Î¸ = [xâŠ¤
0 , pâŠ¤
g ]âŠ¤âˆˆR6 contains the initial state and goal position.
We consider an MPC problem with three obstacles and a control horizon of length 20, leading
to 3 Ã— 20 Ã— 4 = 240 binary decision variables. An NN Ï€Ï‰ : R6 â†’{0, 1}240 is employed to pre-
dict the binary decision variables. The parameters for the simulation are set as follows: Ï„ = 0.25 s,
[px
min, px
max, py
min, py
max]âŠ¤= [âˆ’0.5 m, 3 m, âˆ’3 m, 0.5 m]âŠ¤, vmax = 0.5 m/s , amax = 0.5 m/s2,
dmin = 0.25 m, M = 103, Ï‰pt = 10, Ï‰p = 1, Ï‰u = 1, ws = 104. The information of the three
obstacles is:
1. Obstacle 1: px
1 = 1.0 m, py
1 = 0.0 m, L1 = 0.8 m, W1 = 1.0 m, Î±1 = 0.0 rad.
2. Obstacle 2: px
2 = 0.7 m, py
2 = âˆ’1.1 m, L2 = 1.0 m, W2 = 0.8 m, Î±2 = 0.0 rad.
3. Obstacle 3: px
3 = 0.4 m, py
3 = âˆ’2.5 m, L3 = 0.8 m, W3 = 1.0 m, Î±3 = 0.0 rad.
14

A HYBRID L2O FRAMEWORK FOR MIQP
B.2. Simplified Thermal Energy Tank System
In the second example, we examine a simplified thermal energy tank system, adapted from (Boldock`y
et al., 2025). The system dynamics are represented by a discrete-time linear time-invariant (LTI)
model with both continuous and discrete control inputs, described as follows:
xk+1 = Axk + Buuk + BdÎ´k + Edk,
(30)
where xk âˆˆR2 is the state vector, uk âˆˆR2 is the continuous control input, Î´k âˆˆ{0, 1, 2, 3} is the
discrete control input, and dk âˆˆR2 represents known disturbances at time step k. The dynamics
matrices A, Bu, Bd, and E are:
A =
0.9983
0.001
0
0.9966

,
Bu = 0.075 I2,
Bd =

0
0.0825

,
E = âˆ’0.0833 I2.
(31)
The system is subject to the following state and input constraints:
0 â‰¤x1,k â‰¤8.4,
0 â‰¤x2,k â‰¤3.6,
0 â‰¤u1,k, u2,k â‰¤8.
(32)
In addition to the constraints on the continuous control inputs, we also impose constraints on the
changes between consecutive time steps of the discrete control input as follows,
âˆ’1 â‰¤Î´k âˆ’Î´kâˆ’1 â‰¤1, k = 1, . . . , H âˆ’1
(33)
The control objective is to minimize the expected cumulative cost over the prediction horizon, which
includes both state tracking and control effort penalties. The stage cost function and terminal cost
function are defined as
Â¯c(xk, uk, Î´k; rk) = âˆ¥xk âˆ’rkâˆ¥2
Q + âˆ¥ukâˆ¥2
R + Ï âˆ¥Î´kâˆ¥2
2 ,
c(xH; rH) = âˆ¥xH âˆ’rHâˆ¥2
Qt ,
(34)
where rk is the reference state at time step k, and Q, R, Qt and Ï are weighting matrices and
vectors, respectively, given by Q = Qt = I2, R = 0.5 I2, and Ï = 0.1. For simplicity, we assume
that the reference state rk remains constant over the prediction horizon, i.e., rk = r for all k,
where r = [4.2, 1.8]âŠ¤. Thus, the optimization problem is parameterized by the current state xt and
the sequence of known future disturbances over the prediction horizon, [dk, dk+1, . . . , dk+Hâˆ’1].
Accordingly, the parameter vector is defined as Î¸ = [xâŠ¤
k , dâŠ¤
k , dâŠ¤
k+1, . . . , dâŠ¤
k+Hâˆ’1] âˆˆR2H+2.
The parametric MI-MPC problem is given by
minimize
Â¯c(xH; rH) +
Hâˆ’1
X
k=0
c(xk, uk, Î´k; rk)
subject to
xk+1 = Axk + Buuk + BdÎ´k + Edk,
âˆ’1 â‰¤Î´k âˆ’Î´kâˆ’1 â‰¤1, k = 1, . . . , H âˆ’1
xk âˆˆX, uk âˆˆU, Î´k âˆˆ{0, 1, 2, 3}.
(35)
An NN is trained to predict the integer control inputs, i.e., Ï€Ï‰ : R2H+2 â†’{0, 1, 2, 3}H. For this
example, we set the control horizon to H = 20, resulting in a parameter vector Î¸ âˆˆR42 and an NN
output dimension of 20.
15
