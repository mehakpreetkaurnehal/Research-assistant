Highlights
A Physics Informed Machine Learning Framework for Optimal Sensor Placement and
Parameter Estimation
Georgios Venianakis, Constantinos Theodoropoulos, Michail Kavousanakis
â€¢ A PINN framework for joint sensor placement and parameter estimation is presented.
â€¢ Sensor locations are optimized using sensitivity functions and D-optimal design.
â€¢ The framework is demonstrated for reaction-diffusion-advection systems.
â€¢ Optimal placement yields improved parameter estimates over heuristic choices.
arXiv:2511.15543v1  [stat.ML]  19 Nov 2025

A Physics Informed Machine Learning Framework for Optimal
Sensor Placement and Parameter Estimation
Georgios Venianakisa, Constantinos Theodoropoulosa,b,âˆ—and Michail Kavousanakisa,âˆ—
aSchool of Chemical Engineering, National Technical University of Athens, Iroon Polytechneiou 9, Athens, 15772, Greece
bDepartment of Chemical Engineering, University of Manchester, Manchester, M13 9PL, United Kingdom
A R T I C L E I N F O
Keywords:
Machine Learning
Physics Informed Neural Networks
D-optimality
distributed-parameter systems
Fisher Information matrix
automatic differentiation
A B S T R A C T
Parameter estimation remains a challenging task across many areas of engineering. Because
data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it
is crucial to identify sensor configurations that provide the maximum amount of information
about the unknown parameters, in particular for the case of distributed-parameter systems, where
spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently
emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in
cases with sparse or noisy measurements, overcoming some of the limitations of traditional
optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving
inverse problems, relatively little attention has been given to how their performance depends on
sensor placement. This study addresses this gap by introducing a comprehensive PINN-based
framework that simultaneously tackles optimal sensor placement and parameter estimation. Our
approach involves training a PINN model in which the parameters of interest are included
as additional inputs. This enables the efficient computation of sensitivity functions through
automatic differentiation, which are then used to determine optimal sensor locations exploiting
the D-optimality criterion. The framework is validated on two illustrative distributed-parameter
reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our
PINNs-based methodology consistently achieves higher accuracy compared to parameter values
estimated from intuitively or randomly selected sensor positions.
â‹†
âˆ—Corresponding authors
k.theodoropoulos@manchester.ac.uk (C. Theodoropoulos); mihkavus@chemeng.ntua.gr (M. Kavousanakis)
ORCID(s):
Venianakis et al: Preprint submitted to Elsevier
Page 1 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
Nomenclature
NN
Neural Network
PINN
Physics Informed Neural Network
FIM
Fisher Information Matrix
AD
Automatic Differentiation
RAR-D
Residual-based Adaptive Refinement with Distribution
FEM
Finite Element Method
ğ®
General state variable(s)
ğœ†
Unknown parameters vector
ğœ†ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ
A priori parameter estimate vector
ğ‘ ğœ†ğ‘—
Sensitivity function to the parameter ğœ†ğ‘—
ğ¹
FIM
ğ±âˆ—
1, â€¦ , ğ±âˆ—
ğ‘
Selected spatial coordinates of N sensors
Ìƒâ‹…
Dimensionless form of a variable
Ì‚ğ®
Approximation of Ìƒğ®(Numerical or PINN)
ğœƒ
NNâ€™s trainable parameters (weights and biases)
îˆ¸(ğœƒ)
NNâ€™s total loss function
îˆ¸ğ‘“(ğœƒ)
Loss function term corresponding to the PDE residual(s)
îˆ¸ğ‘–ğ‘(ğœƒ)
Loss function term corresponding to the initial condition residual
îˆ¸ğ‘ğ‘(ğœƒ)
Loss function term corresponding to the boundary conditions residuals
îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ (ğœƒ)
Loss function term corresponding to the sum of partial derivatives of all residuals w.r.t. all the
parameters of interest
ğ‘
Species concentration (ğ‘˜ğ‘”âˆ•ğ‘š3)
ğ¯
Velocity Field (ğ‘šâˆ•ğ‘ )
ğ‘
Pressure (ğ‘ƒğ‘)
ğ·
Diffusion coefficient (ğ‘š2âˆ•ğ‘ )
ğ‘˜
Reaction rate constant of an n-th order reaction ((ğ‘˜ğ‘”âˆ•ğ‘š3)1âˆ’ğ‘›â‹…ğ‘ âˆ’1)
ğ‘ˆ
Fluidâ€™s mean inlet velocity (ğ‘šâˆ•ğ‘ )
ğœŒ
Fluid density (ğ‘˜ğ‘”âˆ•ğ‘š3)
ğœ‡
Fluid viscosity (ğ‘ƒğ‘â‹…ğ‘ )
ğœ
Characteristic time scale (ğ‘ )
ğ‘ƒğ‘’
PÃ©clet number
ğ·ğ‘
DamkÃ¶hler number
ğ‘…ğ‘’
Reynolds number
1. Introduction
Distributed-parameter systems, based on partial differential equations (PDEs), typically describing spatiotemporal
variations, represent a wide range of physical systems and phenomena in science and engineering. These PDE-based
models typically involve multiple unknown parameters, often corresponding to physical properties of the system, whose
accurate identification from available data is essential for reliable prediction and effective system design. In general, the
accuracy of parameter estimation improves as more data is collected. Nevertheless, acquiring large datasets is often
expensive or impractical. This motivates the development of systematic methods for selecting sensor locations that
maximize the information gained about the unknown parameters under constraints on the number of sensors.
Most research on Optimal Sensor Placement has focused on state estimation, while comparatively fewer studies
have addressed parameter estimation directly (AlaÃ±a and Theodoropoulos, 2011, 2012). Existing approaches typically
rely on scalar measures derived from the Fisher Information Matrix (FIM), which captures the sensitivity of observable
quantities with respect to parameters. For example, the modified E-criterion (Mehra, 1974; Walter and Pronzato, 1990;
Nahor et al., 2003) minimizes the ratio of the largest to the smallest eigenvalue of the FIM, while Heredia-Zavoni and
Esteva (1998) proposed minimizing the expected Bayesian loss involving the trace of the inverse FIM. The widely used
D-optimality criterion (Qureshi et al., 1980; Vande Wouwer et al., 2000) maximizes the determinant of the FIM (or
Venianakis et al: Preprint submitted to Elsevier
Page 2 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
the Gram determinant), and a closely related approach minimizes the information entropy (Papadimitriou et al., 2000),
reflecting the uncertainty of the parameter under a given sensor configuration.
In large-scale distributed systems, parameter estimation can be facilitated by combining model reduction with
optimal sensor placement. In Alonso et al. (2004), the proposed workflow is to construct a Proper Orthogonal
Decomposition (POD) reduced-order model that captures the dominant spatio-temporal dynamics and then determine
measurement locations by optimizing an observability-based criterion on this reduced model. AlaÃ±a and Theodor-
opoulos (2011) further streamline this process by proposing sensor placement directly at the extrema of the dominant
POD modes, an approach that circumvents costly sensitivity-matrix computations while still providing near-optimal
parameter estimation performance.
Once sensor locations are defined, the next step is parameter estimation. Optimization-based methods, such as
Maximum Likelihood Estimation (MLE) (Myung, 2003), remain the most established, with Least Squares as the most
common implementation under the assumption of Gaussian measurement noise (Biegler et al., 1986; Englezos and
Kalogerakis, 2000). While effective, these methods typically require repeated model evaluations, resulting in high
computational cost, particularly when gradients with respect to parameters must also be computed. Moreover, the
accuracy of estimated parameters may degrade under sparse or noisy data (Kravaris et al., 2013). Bayesian methods
offer an alternative by incorporating prior information and providing full uncertainty quantification through posterior
distributions (Coleman and Block, 2006; Hermanto et al., 2008; Kalyanaraman et al., 2015). Nevertheless, Bayesian
inference often relies on Markov Chain Monte Carlo (MCMC) sampling (Hastings, 1970), which requires extensive
forward model evaluations and can be sensitive to prior selection in data-limited settings (Lenk and Orme, 2009).
Recent advances in machine learning have substantially enhanced the field of parameter estimation by introducing
data-driven methodologies capable of capturing complex relationships between model parameters and observed
data. Traditional statistical approaches, such as maximum likelihood estimation and Bayesian inference, have been
extended through modern techniques, such as variational inference (Blei et al., 2017) and neural density estimation
(Papamakarios et al., 2021) that enable efficient and accurate parameter recovery, even in complex or high-dimensional
settings. Furthermore, deep neural architectures, coupled with gradient-based optimization algorithms, including
stochastic gradient descent and its adaptive variants (Bottou, 2010), facilitate scalable and robust inference across
a wide range of applications.
Hybrid modeling frameworks that integrate data with first-principles formulations have merged as powerful tools
for scientific computing. Among these, Physics-Informed Neural Networks (PINNs) (Raissi et al., 2019) exploit the
expressivity of deep neural networks (Hornik et al., 1989) while embedding governing PDEs as soft constraints. PINNs
have proven particularly useful in scenarios where traditional solvers face challenges: complex geometries (since PINNs
are mesh-free), high-dimensional problems (Hu et al., 2024). and systems with incomplete physics that require data-
driven enhancement (Raissi et al., 2020). Importantly, PINNs enable parameter estimation by directly incorporating
unknown parameters into the training process, making them robust even under sparse or noisy observations, which is
of particular relevance to this work.
Numerous studies have applied PINNs to inverse problems in chemical engineering and related fields. Examples
include the estimation of thermal properties in additive manufacturing (Liao et al., 2023), thermal diffusivity in two-
phase flows (Cai et al., 2021), kinetic parameters in catalytic reactors (Ngo and Lim, 2021; Huang et al., 2025), and
transport coefficients in porous media (Berardi et al., 2025). PINNs have also been used in fluid mechanics, such as
inferring density fields in high-speed flows (Mao et al., 2020) and thrombus properties in arterial flows (Yin et al.,
2021).
Despite these successes, most studies assume dense observations across the spatial domain, often generated from
numerical simulations, overlooking the constraints of real-world sensor settings. For instance, Ngo and Lim (2021)
observed that parameter estimation accuracy strongly depends on the spatial region from which data is collected,
but this conclusion was based on manual experimentation rather than systematic sensor placement strategies. The
integration of optimal sensor placement with PINNs remains relatively unexplored. A few attempts have been made:
Forootani et al. (2024) proposed a greedy sampling approach (GS-PINN), which selects informative samples via
Proper Orthogonal Decomposition (POD) and the Discrete Empirical Interpolation (DEIM) method (Chaturantabut
and Sorensen, 2010), but their approach requires access to full spatio-temporal data and selects time-varying sensor
positions, limiting practical use. In Chang et al. (2025), they introduce a sensitivity-based sampling method (SBS) that
adapts sensor placement based on gradients of the PINN loss with respect to collocation points. However, this method
was designed for state reconstruction problems in process control, not parameter estimation, as it does not account for
sensitivities with respect to the parameters.
Venianakis et al: Preprint submitted to Elsevier
Page 3 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
In this work, we address this gap by proposing a PINN framework that integrates D-optimal sensor placement
directly into the parameter estimation process. Our approach relies on two complementary neural network models.
The first model incorporates the parameters of interest as inputs, enabling the computation of sensitivity functions via
automatic differentiation (AD). These sensitivities are used to select optimal sensor locations under the D-optimality
criterion. The second, is a standard PINN model trained on data obtained from these optimal locations to perform
parameter estimation and state reconstruction.
We demonstrate our method on two illustrative case studies involving reaction-diffusion-advection systems of
increased complexity: a one-dimensional steady-state problem, and a two-dimensional transient problem. In each case,
the goal is to estimate important dimensionless parameters for the system such as the PÃ©clet number and DamkÃ¶hler
number and to compare their accuracy against estimations made from sub-optimal sensor locations.
The paper is organised as follows: Section 2 presents the formulation of our method, including sensor selection
and PINN training. Section 3 describes the case studies and results. Finally, Section 4 discusses the main conclusions
and the relevant future directions.
2. Methods
2.1. Selection of Optimal Sensor Location for Parameter Estimation
In this work, we adopt the D-optimality criterion, widely used for sensor placement due to its geometric inter-
pretability and invariance to parameter rescaling (Franceschini and Macchietto, 2008; Bard, 1974). For implementation,
we follow the method of Vande Wouwer et al. (2000), chosen for its simplicity. Alternative optimality criteria can also
be applied as discussed in (AlaÃ±a and Theodoropoulos, 2011).
Consider a system with states ğ®(ğ±, ğ‘¡) described by the governing PDE:
ğœ•ğ®
ğœ•ğ‘¡+ îˆº[ğ®; ğœ†] = 0,
ğ±âˆˆÎ©,
ğ‘¡â‰¥0,
(1)
subject to the following initial and boundary conditions:
ğ®(0, ğ±) = ğ (ğ±),
ğ±âˆˆÎ©,
(2)
îˆ®[ğ®] = 0,
ğ±âˆˆğœ•Î©,
ğ‘¡â‰¥0,
(3)
where îˆº[â‹…] is a differential operator, and îˆ®[â‹…] a boundary operator, ğœ†= [ğœ†1, â€¦ , ğœ†ğ‘ƒ
] are the ğ‘unknown parameters,
and Î© the spatial domain.
For state measurements at ğ‘sensor locations ğ±1, â€¦ , ğ±ğ‘we define sensitivity functions as:
ğ‘ ğœ†ğ‘—(ğ±ğ‘–, ğ‘¡) = ğœ•ğ®
ğœ•ğœ†ğ‘—
(ğ±ğ‘–, ğ‘¡), ğ‘–= 1, â€¦ , ğ‘, ğ‘—= 1, â€¦ , ğ‘ƒ,
(4)
and collect them into a vector ğ‘€:
ğ‘€(ğ‘¥1, â€¦ , ğ‘¥ğ‘, ğ‘¡) = [ğ‘ ğœ†1(ğ±1, ğ‘¡) â€¦ ğ‘ ğœ†ğ‘ƒ(ğ±1, ğ‘¡) â€¦ ğ‘ ğœ†1(ğ±ğ‘, ğ‘¡) â€¦ ğ‘ ğœ†ğ‘ƒ(ğ±ğ‘, ğ‘¡)]ğ‘‡.
(5)
The Fisher Information Matrix, ğ¹, is then:
ğ¹(ğ±1, â€¦ , ğ±ğ‘) = âˆ«
ğ‘‡
0
ğ‘€(ğ‘¥1, â€¦ , ğ‘¥ğ‘, ğ‘¡)ğ‘€ğ‘‡(ğ‘¥1, â€¦ , ğ‘¥ğ‘, ğ‘¡) ğ‘‘ğ‘¡,
(6)
where [0, ğ‘‡] is the observation horizon. The matrix ğ¹is positive semi-definite with its diagonal entries quantifying
sensor sensitivity to parameters, while its off-diagonal entries capture correlations between the corresponding
parameters.
The optimal sensor locations ğ±âˆ—
1, â€¦ , ğ±âˆ—
ğ‘maximize the Gram determinant (Courant and Hilbert, 1989):
Venianakis et al: Preprint submitted to Elsevier
Page 4 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
ğ±âˆ—
1, â€¦ , ğ±âˆ—
ğ‘= arg max
ğ±1,â€¦,ğ±ğ‘det [ğ¹(ğ±1, â€¦ , ğ±ğ‘)] .
(7)
Maximizing det(ğ¹) encourages both large sensitivities (via diagonal dominance) and independence (via small
off-diagonal terms).
For steady-state problems the FIM reduces to:
ğ¹(ğ±1, â€¦ , ğ±ğ‘) = ğ‘€(ğ‘¥1, â€¦ , ğ‘¥ğ‘, ğ‘¡)ğ‘€ğ‘‡(ğ‘¥1, â€¦ , ğ‘¥ğ‘, ğ‘¡),
(8)
which is rank-1 with zero determinant, rendering D-optimality unusable. For these cases, we instead select optimal
sensor locations by maximizing the FIM trace:
ğ±âˆ—
1, â€¦ , ğ±âˆ—
ğ‘= arg max
ğ±1,â€¦,ğ±ğ‘tr [ğ¹(ğ±1, â€¦ , ğ±ğ‘)] .
(9)
Our PINN framework as well as the calculation of sensitivities using PINNs is discussed next.
2.2. Physics Informed Neural Networks: General Framework
PINNs are fully connected neural networks that incorporate PDE constraints into their loss function (Wang et al.
(2023)). They address two problem types:
(a) Forward Problems: approximating PDE solutions with known parameters and boundary/initial conditions,
without experimental data.
(b) Inverse Problems: estimating unknown parameters from experimental data, while simultaneously approximating
the PDE solution.
For the forward problem, the system state ğ®(ğ‘¡, ğ±) governed by Eqs. (1)â€“(3) is approximated by a neural network
Ì‚ğ®(ğ‘¡, ğ±, ğœƒ), where ğœƒdenotes trainable weights and biases. Training minimizes a composite loss function, îˆ¸:
îˆ¸(ğœƒ) = îˆ¸ğ‘“(ğœƒ) + îˆ¸ğ‘–ğ‘(ğœƒ) + îˆ¸ğ‘ğ‘(ğœƒ),
(10)
with terms enforcing PDE residuals (îˆ¸ğ‘“), initial conditions (îˆ¸ğ‘–ğ‘), and boundary conditions (îˆ¸ğ‘ğ‘). Automatic
differentiation computes derivatives of the system state required for residual evaluation (PDE and boundary conditions
residuals).
For inverse problems, observational data {ğ®ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘
}ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
ğ‘–=1
at spatio-temporal coordinates {ğ‘¡ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘, ğ±ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘
}ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
ğ‘–=1
introduce
an additional loss term:
îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘(ğœƒ) =
1
ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
âˆ‘
ğ‘–=1
|||Ì‚ğ®(ğ‘¡ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘, ğ±ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘) âˆ’ğ®ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘
|||
2 ,
(11)
leading to the total loss function:
îˆ¸(ğœƒ) = îˆ¸ğ‘–ğ‘(ğœƒ) + îˆ¸ğ‘ğ‘(ğœƒ) + îˆ¸ğ‘“(ğœƒ) + îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘(ğœƒ).
(12)
In this case, PINNs are optimized with respect to both ğœ½and unknown parameters ğ€.
Venianakis et al: Preprint submitted to Elsevier
Page 5 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
â€¦
â€¦
â€¦
Figure 1: Schematic representation of the methodology for computing sensitivity derivatives using PINNs. The neural
network admits as input the spatio-temporal coordinates (ğ±, ğ‘¡) along with the parameters of interest, ğœ†. The loss function
îˆ¸to be minimized is constructed from the system residuals (PDE and boundary/initial conditions) and their derivatives
with respect to the parameters, ğœ†.
2.3. Sensitivity functions with PINNs
As described in Section 2.1, optimal sensor placement requires sensitivity functions (Eq. (4)). Following Hanna
et al. (2024), the PINN input is extended to include both spatio-temporal coordinates and parameters ğœ†, enabling
AD-based computation of ğœ•Ì‚ğ®âˆ•ğœ•ğœ†ğ‘—. Additional loss terms enforce accurate parameter sensitivities calculation:
îˆ¸(ğœƒ) = îˆ¸ğ‘–ğ‘(ğœƒ) + îˆ¸ğ‘ğ‘(ğœƒ) + îˆ¸ğ‘“(ğœƒ) + îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘–ğ‘(ğœƒ) + îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘ğ‘(ğœƒ) + îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘“(ğœƒ),
(13)
where:
îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘–ğ‘(ğœƒ) =
1
ğ‘ğ‘–ğ‘
ğ‘ƒ
âˆ‘
ğ‘—=1
ğ‘ğ‘–ğ‘
âˆ‘
ğ‘–=1
|||||
ğœ•ğ‘Ÿğ‘–ğ‘
ğœ•ğœ†ğ‘—
(0, ğ±ğ‘–
ğ‘–ğ‘, ğœ†, ğœƒ)
|||||
2
,
(14)
îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘ğ‘(ğœƒ) =
1
ğ‘ğ‘ğ‘
ğ‘ƒ
âˆ‘
ğ‘—=1
ğ‘ğ‘ğ‘
âˆ‘
ğ‘–=1
|||||
ğœ•ğ‘Ÿğ‘ğ‘
ğœ•ğœ†ğ‘—
(ğ‘¡ğ‘–
ğ‘ğ‘, ğ±ğ‘–
ğ‘ğ‘, ğœ†, ğœƒ)
|||||
2
,
(15)
îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘“(ğœƒ) =
1
ğ‘ğ‘“
ğ‘ƒ
âˆ‘
ğ‘—=1
ğ‘ğ‘“
âˆ‘
ğ‘–=1
|||||
ğœ•ğ‘Ÿğ‘“
ğœ•ğœ†ğ‘—
(ğ‘¡ğ‘–
ğ‘“, ğ±ğ‘–
ğ‘“, ğœ†, ğœƒ)
|||||
2
.
(16)
Each îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,â‹…term penalizes derivatives of residuals with respect to ğœ†.
Training is performed with prior parameter estimates, ğœ†ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ. The resulting sensitivity functions are then used
to construct the FIM and determine optimal sensor locations. Here we assume that their qualitative structure (e.g.,
locations of maxima) remains the same when evaluated at ğœ†ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿcompared to the true ğœ†. When the computed optima
differ substantially, we use an iterative approach. Fig. 1 illustrates the process.
Once sensor placement is decided, state data are collected from the corresponding locations, and the inverse
problem is solved using PINNs. To accelerate training, we apply transfer learning (Zhuang et al., 2021): the inverse
PINN is initialized from a pretrained forward PINN at prior parameter values, ğœ†ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ. A lightweight fine-tuning strategy
is adopted here (Goswami et al., 2020), which retrains only the last hidden layers while freezing the earlier ones,
reducing both the number of epochs and the per-iteration cost.
Finally, to improve PINN training efficiency and stability we incorporate techniques including:
Venianakis et al: Preprint submitted to Elsevier
Page 6 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
â€¢ Dynamic loss weighting: Following Li and Feng (2022), loss weights ğ‘¤ğ‘˜multiplying each loss term in the loss
function are adaptively updated to balance gradients across different terms, preventing dominance by any single
component (see e.g., Eq. (18) and Eq. (31) below).
â€¢ Residual-based adaptive refinement (RAR-D): Collocation points are adaptively chosen according to the PDE
residual (Wu et al., 2023), focusing training on regions with higher errors. We extend this strategy to boundary
and initial condition sampling (collocation) points as well.
â€¢ Non-dimensionalization: PDEs are solved in dimensionless form, preventing vanishing/exploding gradients
and ensuring balanced loss contributions across variables (Wang et al., 2023).
3. Case Studies
This section presents two illustrative examples that demonstate our PINNs-based methodology and also showcase
how the optimal sensor placement criterion enhances parameter inference accuracy. All PINN models are implemented
in Python using the PyTorchÂ® library (Paszke et al., 2019) and trained on an NVIDIAÂ® RTX A4000 GPU computer.
3.1. 1D Steady State Reactionâ€“Advectionâ€“Diffusion equation
3.1.1. Problem Set-Up
The first simpler example considers a steady state, one-dimensional reaction-advection-diffusion equation, which
in dimensionless form is given by the following PDE and boundary conditions:
ğ‘ƒğ‘’ğ‘‘ğ‘
ğ‘‘ğ‘¥= ğ‘‘2ğ‘
ğ‘‘ğ‘¥2 âˆ’ğ·ğ‘ğ‘2, ğ‘¥âˆˆ[0, 10] ,
ğ‘(0) = 1,
ğ‘‘ğ‘
ğ‘‘ğ‘¥
||||ğ‘¥=ğ¿
= 0,
(17)
where ğ‘ƒğ‘’and ğ·ğ‘denote the PÃ©clet number and the DamkÃ¶hler number, respectively. In this example, ğ·ğ‘is fixed
(ğ·ğ‘= 1.0), and the goal is to estimate the unknown value of ğ‘ƒğ‘’with a single, optimally placed sensor.
3.1.2. PINN Framework
To compute sensitivities for the FIM construction, we employ a Sensitivity PINN model with two inputs (ğ‘¥and
ğ‘ƒğ‘’), four hidden layers of 30 neurons each, and hyperbolic tangent activation. This network structure is chosen to
balance computational efficiency with accuracy. The loss function is
îˆ¸(ğœƒ) = ğ‘¤ğ‘“îˆ¸ğ‘“(ğœƒ) + ğ‘¤ğ‘ğ‘1îˆ¸ğ‘ğ‘1(ğœƒ) + ğ‘¤ğ‘ğ‘2îˆ¸ğ‘ğ‘2(ğœƒ) + ğ‘¤ğ‘ ğ‘’ğ‘›ğ‘ 
(îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘“(ğœƒ) + îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘ğ‘1(ğœƒ) + îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘ğ‘2(ğœƒ)) ,
(18)
where îˆ¸ğ‘“(ğœƒ), îˆ¸ğ‘ğ‘1(ğœƒ), îˆ¸ğ‘ğ‘2(ğœƒ) correspond to the PDE and boundary condition residuals. The terms îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘“(ğœƒ),
îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘ğ‘1(ğœƒ) and îˆ¸ğ‘ ğ‘’ğ‘›ğ‘ ,ğ‘ğ‘2(ğœƒ) represent the corresponding derivatives with respect to ğ‘ƒğ‘’. Analytical expressions are
provided in Section I of the Supplementary Material. We sample 800 collocation points on an equispaced grid over the
spatial domain providing sufficient resolution for our PINN model. The loss weights are dynamically updated following
the gradient ascent scheme of Li and Feng (2022). Training is performed with the Adam optimizer (10âˆ’3 learning rate)
for 2500 epochs. Since the true value of ğ‘ƒğ‘’is to be estimated (unknown), the model is initialized with an a priori
estimate, ğ‘ƒğ‘’= 0.1. As noted in Section 2.3, we assume that the maxima of the Gram determinant (or the FIM trace in
this example) are approximately invariant to whether they are computed at the estimated or true parameter values. This
is equivalent to having a "reasonable" prior estimate. An iterative procedure can be employed if the computed optima
vary significantly. For a single-sensor placement problem, the FIM trace reduces to the squared sensitivity of ğ‘with
respect to ğ‘ƒğ‘’.
ğ‘¡ğ‘Ÿ[ğ¹(ğ‘¥)] =
[ ğœ•ğ‘
ğœ•ğ‘ƒğ‘’(ğ‘¥)
]2
, ğ‘¥âˆˆ[0, 10] .
(19)
Fig. 2 shows the FIM trace obtained from the PINN alongside the one obtained by numerically soving the PDE
(equation 17) The results confirm that PINNs accurately compute sensitivity derivatives via automatic differentiation.
Here, it is easy to locate graphically the trace maximum at ğ‘¥âˆ—= 1.81.
Venianakis et al: Preprint submitted to Elsevier
Page 7 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
Table 1
Hyperparameters of Sensitivity PINNS and Inference PINNs.
PINN Hyperparameter
Sensitivity PINN
Inference PINN
Hidden layers
4
4
Neurons per layer
30
30
Inputs
2 (ğ‘¥, ğ‘ƒğ‘’)
1 (ğ‘¥)
Activation function
tanh()
tanh()
Optimizer
Adam
Adam
Iterations
2500
5000
Collocation points
800
800
Dynamic weight strategy
Yes
Yes
RAR-D
No
No
Table 2
Estimated ğ‘ƒğ‘’values and relative error for optimal versus suboptimal sensor placement.
Sensor
Estimated ğ‘ƒğ‘’(true: 1.000)
Relative Error (%)
Optimal (ğ‘¥âˆ—= 1.81)
0.998
0.20
Outlet (ğ‘¥âˆ—= 10.0)
0.153
84.7
Consequently, pseudo-experimental data are generated numerically using the true value of ğ‘ƒğ‘’= 1.0 at two
locations: the optimal location ğ‘¥âˆ—= 1.81, and a suboptimal point at the outlet, ğ‘¥âˆ—= 10.0, which is an intuitive
choice for placing the sensor. The inference PINN has the same architecture but accepts only ğ‘¥as input, with ğ‘ƒğ‘’
treated as a trainable parameter. The loss function now reads:
îˆ¸(ğœƒ) = ğ‘¤ğ‘“îˆ¸ğ‘“(ğœƒ) + ğ‘¤ğ‘ğ‘1îˆ¸ğ‘ğ‘1(ğœƒ) + ğ‘¤ğ‘ğ‘2îˆ¸ğ‘ğ‘2(ğœƒ) + ğ‘¤ğ‘‘ğ‘ğ‘¡ğ‘îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘(ğœƒ),
(20)
with the data term being:
îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘(ğœƒ) =
1
ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
âˆ‘
ğ‘–=1
|||Ì‚ğ‘(ğ‘¥ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘) âˆ’ğ‘ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘
|||
2 .
(21)
Since a single sensor is used, ğ‘ğ‘‘ğ‘ğ‘¡ğ‘= 1. The model is trained with the Adam optimizer for 5000 iterations. The
hyperparameters for the Sensitivity and Inference PINNs applied to the 1D steady-state reaction-advection-diffusion
equation are summarized in Table 1.
Fig. 3 shows the inferred ğ‘ƒğ‘’during training for data from the optimal and outlet locations. The results highlight
the crucial role of sensor placement: the PINN converges closer to the true ğ‘ƒğ‘’, when data are taken at the optimal
location. At the outlet, the parameter fails to converge for the number of epochs used. Final estimates and relative
errors are listed in Table 2.
After identifying the parameter, we recompute the optimal sensor location using the true ğ‘ƒğ‘’. As shown in Fig. 4, the
maximum shifts slightly to ğ‘¥âˆ—= 2.32, very close to the original ğ‘¥âˆ—= 1.81 supporting the assumption that sensitivity
maxima remain nearly invariant when evaluated with initial parameter estimates.
3.2. 2D Transient Reactionâ€“Advectionâ€“Diffusion, Flow Around a Fixed Obstacle
The 1D example demonstrates how our PINN-based methodology guides sensor placement and enables accurate
estimation of a single parameter. To further test the robustness of the framework, we now consider a more complex
2D system that combines nonlinear reaction, advection and diffusion in the presence of an obstacle. This example
introduces additional challenges: heterogeneity due to flow recirculation, transient behavior, as well as the simultaneous
inference of two parameters.
Venianakis et al: Preprint submitted to Elsevier
Page 8 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
In particular, we address the simultaneous estimation of the dimensionless parameters ğ‘ƒğ‘’and ğ·ğ‘in a two-
dimensional, time-dependent reaction-advection-diffusion system. A reactive species is transported by a steady flow
past a fixed cylindrical obstacle.
3.2.1. Problem Set-Up
We consider a 2D rectangular channel of dimensions ğ¿Ã— ğ»(ğ¿= 0.24 m and ğ»= 0.14 m), with flow from left to
right bounded by impermeable walls at the top and bottom. A fixed cylindrical obstacle of radius ğ‘…= 0.01 m is placed
inside the channel perpendicular to the flow direction, at {ğ‘¥0, ğ‘¦0
} = {0.09, 0.07} m. The fluid is incompressible,
Newtonian, and the flow is considered to be at steady-state. Its velocity, ğ¯, and pressure, ğ‘, fields satisfy the Navier-
Stokes equations (neglecting gravity):
ğœŒ
(ğœ•ğ¯
ğœ•ğ‘¡+ ğ¯â‹…âˆ‡ğ¯
)
= âˆ’âˆ‡ğ‘+ ğœ‡âˆ‡2ğ¯,
(22)
subject to the incompressibility constraint:
âˆ‡â‹…ğ¯= 0.
(23)
where ğœŒ, ğœ‡denote the density and dynamic viscosity, respectively.
At the inlet, a parabolic velocity profile is imposed for the horizontal component of velocity, ğ‘£ğ‘¥:
ğ‘£ğ‘¥(0, ğ‘¦) = 6ğ‘ˆ
[( ğ‘¦
ğ»
)
âˆ’
( ğ‘¦
ğ»
)2]
.
(24)
with no-slip conditions at the walls (including the obstacle surface) and zero pressure at the outlet. The fluid properties
are set to ğœŒ= 1000 [ğ‘˜ğ‘”âˆ•ğ‘š3], ğœ‡= 0.001 [ğ‘ƒğ‘â‹…ğ‘ ] and average inlet velocity ğ‘ˆ= 0.00125 [ğ‘šâˆ•ğ‘ ].
Given the steady flow ğ¯, the concentration ğ‘(ğ‘¥, ğ‘¦, ğ‘¡) of a reactive species evolves according to:
ğœ•ğ‘
ğœ•ğ‘¡+ ğ¯â‹…âˆ‡ğ‘= ğ·âˆ‡2ğ‘âˆ’ğ‘˜ğ‘2, (ğ‘¥, ğ‘¦) âˆˆ[0, ğ¿] Ã— [0, ğ»], ğ‘¡> 0,
(25)
where ğ·is the diffusion coefficient and ğ‘˜the reaction rate constant. The initial condition is:
ğ‘(ğ‘¥, ğ‘¦, 0) = 0, (ğ‘¥, ğ‘¦) âˆˆ(0, ğ¿] Ã— [0, ğ»].
(26)
The inlet concentration profile is set to change linearly with ğ‘¦:
ğ‘(0, ğ‘¦, ğ‘¡) = ğ‘¦
ğ»+ 0.3
[ ğ‘˜ğ‘”
ğ‘š3
]
, ğ‘¦âˆˆ[0, ğ»], ğ‘¡> 0.
(27)
Zero-flux boundary conditions are applied at the impenetrable walls and the (long) outlet. A schematic of the setup
is shown in Fig. 5.
By selecting as characteristic scales, ğ¿, ğ‘ˆ, and ğ‘0 = 1 kg/m3, we derive the following dimensionless variables:
Ìƒğ‘¥= ğ‘¥
ğ¿, Ìƒğ‘¦= ğ‘¦
ğ¿, Ìƒğ¯= ğ¯
ğ‘ˆ, Ìƒğ‘=
ğ‘
ğœŒğ‘ˆ2 and Ìƒğ‘= ğ‘
ğ‘0
.
Typically, time is scaled using
Ìƒğ‘¡= ğ‘¡ğ·
ğ¿2 .
However, in this problem, the diffusion coefficient ğ·is unknown and will be determined implicitly through the
dimensionless parameters. Therefore, it cannot serve as the basis for non-dimensionalizing time. Instead, we define
Venianakis et al: Preprint submitted to Elsevier
Page 9 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
a characteristic time scale, ğœ, chosen to be of the same order of magnitude as ğ¿2âˆ•ğ·0, where ğ·0 is an initial estimate
of the diffusion coefficient. Numerical tests show that setting ğœ= 4000 s yields good results. The dimensionless time
variable is therefore defined as:
Ìƒğ‘¡= ğ‘¡
ğœ.
Thus, in dimensionless form, the governing equations are:
Ìƒğ¯â‹…âˆ‡Ìƒğ±Ìƒğ¯= âˆ’âˆ‡Ìƒğ±Ìƒğ‘+ 1
ğ‘…ğ‘’âˆ‡2
Ìƒğ±Ìƒğ¯,
(28)
âˆ‡Ìƒğ±â‹…Ìƒğ¯= 0,
(29)
ğ‘ƒğ‘’ğ¿
ğœğ‘ˆ
ğœ•Ìƒğ‘
ğœ•Ìƒğ‘¡+ ğ‘ƒğ‘’Ìƒğ¯â‹…âˆ‡Ìƒğ±Ìƒğ‘= âˆ‡2
Ìƒğ±Ìƒğ‘âˆ’ğ·ğ‘Ìƒğ‘2,
Ìƒğ±âˆˆ[0, 1] Ã— [0, ğ»
ğ¿],
Ìƒğ‘¡â‰¥0,
(30)
where ğ‘…ğ‘’= ğœŒğ‘ˆğ¿
ğœ‡, ğ‘ƒğ‘’= ğ‘ˆğ¿
ğ·and ğ·ğ‘= ğ‘˜ğ‘0ğ¿2
ğ·
are the Reynolds, PÃ©clet and DamkÃ¶hler numbers, respectively. From
this point forward, tildes are omitted for convenience.
3.2.2. PINN Set-Up
Following the approach of Laubscher (2021), we construct and train separate PINN models: one for the fluid flow
PDEs (Navier-Stokes, Eqs. (28) and (29)) and another for the species transport PDE (Eq. (30)). Because of the one-way
coupling (fluid flow influences mass transport but not vice versa for a dilute system) and since no flow-field parameter
inference is required, we first train the fluid flow PINN using the fixed parameters and boundary conditions described
earlier. Its output is then used to compute the residual of Eq. (30), which in turn guides the training of two distinct
mass transport PINNs: one for computing sensitivity derivatives and another for parameter estimation.
Fluid Flow PINN
The fluid flow PINN admits two inputs, ğ‘¥and ğ‘¦, and produces: Ì‚ğ¯, and Ì‚ğ‘. The Ì‚ symbol is used to denote the PINN
approximation. The architecture is a fully connected neural network with 8 hidden layers of 60 neurons each, using
hyperbolic tangent activation functions. The loss function minimized during training is:
îˆ¸(ğœƒ) = ğ‘¤ğ‘–ğ‘›îˆ¸ğ‘–ğ‘›(ğœƒ) + ğ‘¤ğ‘œğ‘¢ğ‘¡îˆ¸ğ‘œğ‘¢ğ‘¡(ğœƒ) + ğ‘¤ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘¢ğ‘îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘¢ğ‘(ğœƒ) + ğ‘¤ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘™ğ‘œğ‘¤îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘™ğ‘œğ‘¤(ğœƒ)
+ ğ‘¤ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘œğ‘ğ‘ ğ‘¡.îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘œğ‘ğ‘ ğ‘¡.(ğœƒ) + ğ‘¤ğ‘›ğ‘ îˆ¸ğ‘›ğ‘ (ğœƒ) + ğ‘¤ğ‘ğ‘œğ‘›ğ‘¡.îˆ¸ğ‘ğ‘œğ‘›ğ‘¡.(ğœƒ),
(31)
where îˆ¸ğ‘›ğ‘ (ğœƒ) and îˆ¸ğ‘ğ‘œğ‘›ğ‘¡.(ğœƒ) correspond to the residuals of Eqs. (28) and (29), and îˆ¸ğ‘–ğ‘›(ğœƒ), îˆ¸ğ‘œğ‘¢ğ‘¡(ğœƒ), îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘¢ğ‘(ğœƒ), îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘™ğ‘œğ‘¤,
and îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘œğ‘ğ‘ ğ‘¡. correspond to the inlet, outlet and wall boundary condition residuals, respectively. Analytical expressions
for each loss term are provided in Section II.1 of the Supplementary Material. The PINN model is trained for 35,000
iterations with the Adam optimizer, followed by 2,000 iterations with L-BFGS for improved accuracy. We also
employ RAR-D (adaptive collocation point sampling) and an adaptive weight strategy (see Section 2.3). Complete
hyperparameter settings are listed in Table 3. Training requires approximately 95 minutes. The magnitude of the
predicted flow field, |Ì‚ğ¯| =
âˆš
Ì‚ğ‘£2
ğ‘¥+ Ì‚ğ‘£2
ğ‘¦, is visualized and compared with the corresponding numerical solution in Fig. 6
showing the high accuracy of the fluid flow PINN model.
Mass-Transport Sensitivity PINN
We next construct a PINN model to compute the sensitivity of species concentration with respect to the
dimensionless parameters ğ‘ƒğ‘’and ğ·ğ‘. This model admits five inputs: ğ‘¥, ğ‘¦, ğ‘¡, ğ‘ƒğ‘’, and ğ·ğ‘, and outputs Ì‚ğ‘. Its architecture
consists of 5 hidden layers with 100 neurons each and tanh activations. The loss function is:
Venianakis et al: Preprint submitted to Elsevier
Page 10 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
Table 3
Hyperparameters for different PINN architectures and training settings in the 2D reaction-diffusion-advection problem.
PINN hyperparameter
Fluid Flow PINN
Mass Transport Sensitivity PINN
Mass
Transport
Inference
PINN
Hidden layers
8
5
5
Neurons per layer
60
100
100
Inputs
2 (ğ‘¥, ğ‘¦)
5 (ğ‘¥, ğ‘¦, ğ‘¡, ğ‘ƒğ‘’, ğ·ğ‘)
3 (ğ‘¥, ğ‘¦, ğ‘¡)
Outputs
Ì‚ğ‘£, Ì‚ğ‘)
Ì‚ğ‘
Ì‚ğ‘
Activation function
tanh
tanh
tan
Optimizer
Adam and L-BFGS
Adam and L-BFGS
L-BFGS
Optimizer iterations
35000 (Adam) and 2000 (L-BFGS)
35000 (Adam) and 1000 (L-BFGS)
2000
Number of initially sampled training
points
ğ‘ğ‘“= 40000 (collocation)
ğ‘ğ‘–ğ‘›= 1500 (inlet)
ğ‘ğ‘œğ‘¢ğ‘¡= 1500 (outlet)
ğ‘ğ‘¤ğ‘ğ‘™ğ‘™= 500 (channel wall)
ğ‘ğ‘œğ‘ğ‘ ğ‘¡= 100 (obstacle wall)
ğ‘ğ‘“= 55000 (collocation)
ğ‘ğ‘–ğ‘›= 7000 (inlet)
ğ‘ğ‘œğ‘¢ğ‘¡= 3000 (outlet)
ğ‘ğ‘¤ğ‘ğ‘™ğ‘™= 4000 (channel wall)
ğ‘ğ‘œğ‘ğ‘ ğ‘¡= 3000 (obstacle wall)
ğ‘ğ‘–ğ‘›ğ‘–ğ‘¡= 15000 (initial condition)
ğ‘ğ‘“= 75000 (collocation)
ğ‘ğ‘–ğ‘›= 8000 (inlet)
ğ‘ğ‘œğ‘¢ğ‘¡= 8000 (outlet)
ğ‘ğ‘¤ğ‘ğ‘™ğ‘™= 8000 (channel wall)
ğ‘ğ‘œğ‘ğ‘ ğ‘¡= 3000 (obstacle wall)
ğ‘ğ‘–ğ‘›ğ‘–ğ‘¡= 15000 (initial condition)
Dynamic Weight Strategy
Yes
Yes
No
RAR-D
Yes
Yes
Yes
Frequency of sampling new training
points
Every 2000 (Adam) training itera-
tions
Every 2000 (Adam) training iterations
Every 100 (L-BFGS) training it-
erations
Number of added training points with
RAR-D
300 collocation points
20 inlet points
30 wall points
1000 collocation points
300 inlet points
300 wall points
800 initial condition points
750 collocation points
50 inlet points
50 wall points
150 initial points
îˆ¸(ğœƒ) = ğ‘¤ğ‘“îˆ¸ğ‘“(ğœƒ) + ğ‘¤ğ‘–ğ‘›ğ‘–ğ‘¡îˆ¸ğ‘–ğ‘›ğ‘–ğ‘¡(ğœƒ) + ğ‘¤ğ‘–ğ‘›îˆ¸ğ‘–ğ‘›(ğœƒ) + ğ‘¤ğ‘œğ‘¢ğ‘¡îˆ¸ğ‘œğ‘¢ğ‘¡(ğœƒ) + ğ‘¤ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘¢ğ‘îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘¢ğ‘(ğœƒ)
+ ğ‘¤ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘™ğ‘œğ‘¤îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘™ğ‘œğ‘¤(ğœƒ) + ğ‘¤ğ‘œğ‘ğ‘ ğ‘¡îˆ¸ğ‘œğ‘ğ‘ ğ‘¡(ğœƒ) + ğ‘¤sens,ğ‘ƒğ‘’îˆ¸sens,ğ‘ƒğ‘’+ ğ‘¤sens,ğ·ğ‘îˆ¸sens,ğ·ğ‘,
(32)
where îˆ¸ğ‘“(ğœƒ) is the residual of Eq. (30), îˆ¸ğ‘–ğ‘›ğ‘–ğ‘¡(ğœƒ) enforces the initial condition, îˆ¸ğ‘–ğ‘›(ğœƒ), îˆ¸ğ‘œğ‘¢ğ‘¡(ğœƒ), îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘¢ğ‘(ğœƒ), îˆ¸ğ‘¤ğ‘ğ‘™ğ‘™,ğ‘™ğ‘œğ‘¤(ğœƒ),
îˆ¸ğ‘œğ‘ğ‘ ğ‘¡(ğœƒ) correspond to boundary condition residuals for the species concentration at the inlet, outlet and walls of the
channel. The sensitivity terms îˆ¸sens,ğ‘ƒğ‘’, îˆ¸sens,ğ·ğ‘are sums of the derivatives of all residuals with respect to ğ‘ƒğ‘’and
ğ·ğ‘, respectively. Details are given in Section II.2 of the Supplementary Material; full training parameters are listed
in Table 3. Since ğ‘ƒğ‘’and ğ·ğ‘are initially unknown, this PINN model is trained using initial estimates (here, we use
ğ‘ƒğ‘’= 7.0 and ğ·ğ‘= 18.0). As discussed in Section 2.3, we assume that the maxima locations of the Gram determinant
remain nearly unchanged when computed with these estimates instead of true values. Training requires approximately
270 minutes.
Once trained, the model provides sensitivity functions ğ‘ ğ‘ƒğ‘’(ğ‘¥, ğ‘¦, ğ‘¡) =
ğœ•Ì‚ğ‘
ğœ•ğ‘ƒğ‘’(ğ‘¥, ğ‘¦, ğ‘¡) and ğ‘ ğ·ğ‘(ğ‘¥, ğ‘¦, ğ‘¡) =
ğœ•Ì‚ğ‘
ğœ•ğ·ğ‘(ğ‘¥, ğ‘¦, ğ‘¡)
computed via automatic differentiation. These functions are used to assemble the FIM (Eqs. (5) and (6)), and evaluate
its determinant for sensor placement. For a single sensor, the Gram determinant depends only on (ğ‘¥1, ğ‘¦1), and its spatial
distribution is illustrated in Fig. 7.
When multiple sensors are considered, the optimization of the Gram determinant becomes multidimensional. To
address this, we employ the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) (Hansen, 2016), a derivative-
free algorithm well suited for high-dimensional, non-convex, and nonlinear problems (Python package ğ‘ğ‘¦ğ‘ğ‘šğ‘(Hansen
et al., 2019)). We first examine the case of three optimal sensor locations and compare the inference results against
a configuration of three intuitively placed sensors around the obstacle (see Fig. 8). Pseudo-experimental data are
generated by computational experiments using the finite element based commercial software COMSOL Â® setting
ğ‘ƒğ‘’= 12.0 and ğ·ğ‘= 22.0 (target values to be inferred by our PINN-based framework). Virtual sensors record species
concentration every 3 seconds.
Venianakis et al: Preprint submitted to Elsevier
Page 11 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
Table 4
Inferred ğ‘ƒğ‘’and ğ·ğ‘values and relative error from ground truth using data (noise-free) for different sensor counts.
Number of sensors
ğ‘ƒğ‘’(12.0)
ğ·ğ‘(22.0)
Optimal sensors
Intuitive sensors
Optimal sensors
Intuitive sensors
1
11.36 (5.33%)
11.09 (7.58%)
20.79 (5.50%)
20.33 (7.59%)
2
11.62 (3.17%)
10.65 (11.25%)
21.08 (4.18%)
19.28 (12.36%)
3
11.80 (1.67%)
11.09 (7.58%)
21.49 (2.32%)
20.23 (8.05%)
5
11.98 (0.17%)
11.34 (5.50%)
21.78 (1.00%)
20.72 (5.82%)
Mass-Transport PINN for Parameter Estimation
Here, we train a PINN to infer the unknown parameters. This neural network has the same architecture as the
sensitivity PINN, but with only spatio-temporal inputs (ğ‘¥, ğ‘¦, ğ‘¡). Its loss function excludes the sensitivity terms (îˆ¸sens,ğ‘ƒğ‘’
and îˆ¸sens,ğ·ğ‘) and instead incorporates a data loss term:
îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘(ğœƒ) =
1
ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
ğ‘ğ‘‘ğ‘ğ‘¡ğ‘
âˆ‘
ğ‘–=1
|||Ì‚ğ‘(ğ‘¥ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘, ğ‘¦ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘, ğ‘¡ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘) âˆ’ğ‘ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘
|||
2 .
(33)
For computational efficiency, we first pre-train this network on the forward problem with initial parameter estimates,
then reuse this state for parameter estimation. During fine-tuning, the weights and biases of the first three layers
are frozen (see Section 2.3). The dynamic weight strategy is disabled, and to emphasize observational data we set
ğ‘¤ğ‘‘ğ‘ğ‘¡ğ‘= 100 (weight of data loss term), while keeping all other weights equal to 1. Further training details are provided
in Table 3.
The inference results for the three-sensor configurations are shown in Fig. 9. Both parameters are estimated with
higher accuracy when using optimally placed sensors. In addition, the three optimally placed sensors provide sufficient
information for the PINN to reconstruct the concentration field, as illustrated in Fig. 10.
To study the effect of the number of sensors, we repeat the procedure for 1, 2 and 5 sensors, each time optimizing
sensor placement using CMA-ES and comparing the results with intuitive arrangements. For completeness, we show
the locations of optimally placed sensors and the intuitively selected arrangements for 1,2,3 and 5 sensors in Fig. 11.
Table 4 summarizes the inferred parameter values, showing that optimally placed sensors consistently yield more
accurate estimates. Notably, adding more intuitively placed sensors does not necessarily improve performance, while
accuracy increases monotonically with the number of optimally chosen sensors.
We further tested robustness to noise adding Gaussian noise to the COMSOL-generated pseudo-experimental
concentration data. Each measurement was drawn from:
Ìƒğ‘ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘âˆ¼îˆº(ğ‘ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘, (0.1ğ‘ğ‘–
ğ‘‘ğ‘ğ‘¡ğ‘)2).
(34)
To account for the noisy data, we repeated the parameter estimation training five times for each configuration. Fig. 12
depicts results for the three-sensor case, showing mean trajectories and standard deviations. Optimal sensors again
led to more accurate averages, particularly for ğ·ğ‘, which corresponds to the nonlinear term of Eq. (30). The same
trends hold for the 1-, 2-, and 5-sensor cases (Table 5). Fig. 13 compares the PINN solution (fitted to noisy data from
3 optimal sensors) with the FEM solution at the true parameters. One can observe that the PINN predictions are in
close agreement with the sensor data in terms of overall trends. Each estimation experiment required approximately
100 minutes of training.
4. Conclusions
In this work, we develop a novel ML-based methodology for optimal sensor placement in large-scale distributed
parameter systems for efficient parameter estimation. We integrate the D-optimal criterion for sensor placement into the
PINN framework, with the goal of leveraging PINNsâ€™ parameter estimation capabilities using minimal observational
Venianakis et al: Preprint submitted to Elsevier
Page 12 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
Table 5
Mean Inferred ğ‘ƒğ‘’and ğ·ğ‘values (with standard deviation) using noisy data for different sensor counts.
Number of sensors
ğ‘ƒğ‘’(12.0)
ğ·ğ‘(22.0)
Optimal sensors
Intuitive sensors
Optimal sensors
Intuitive sensors
1
10.73 Â± 0.61
10.73 Â± 0.19
20.30 Â± 1.14
19.47 Â± 1.02
2
11.38 Â± 0.35
11.38 Â± 0.76
20.20 Â± 0.97
20.01 Â± 1.07
3
12.01 Â± 0.26
11.18 Â± 0.60
21.97 Â± 0.75
19.59 Â± 1.94
5
11.96 Â± 0.43
11.12 Â± 0.38
21.26 Â± 0.55
20.11 Â± 0.97
data. Our approach first determines optimal sensor locations based on an a priori estimate for the parameters of interest,
and then identifies these parameters using data collected from those locations.
To solve the optimal sensor placement problem, we train a PINN that takes as input both the parameters of
interest and the spatio-temporal coordinates. This design enables the computation of sensitivity functions via automatic
differentiation, which are then used to select the optimal sensor positions. For parameter inference, we employ a
standard PINN architecture trained on data sampled from either optimally placed or intuitively chosen virtual sensors
generated from numerical solutions.
We evaluate the framework on two illustrative case studies: A 1D steady-state and a 2D transient reaction-diffusion-
advection problem. In the 1D case, we estimate the PÃ©clet number using a single sensor. Placement at the optimal
location yields a relative error of only 0.20%, whereas placement at the outlet produces a substantially larger error of
84.7%, underscoring the importance of optimal sensor positioning. In the 2D transient case, we jointly estimate the
PÃ©clet and DamkÃ¶hler numbers using sensor configurations of 1, 2, 3, and 5 sensors. With noiseless data, optimal sensor
placements consistently provide more accurate estimates, and accuracy improves as the number of sensors increases.
Introducing measurement noise preserved these trends.
Training a PINN to compute sensitivity functions is computationally intensive at the outset (â‰ˆ270 minutes), owing
to the additional loss terms requiring derivatives of all residuals with respect to all parameters of interest. However,
once trained, transfer learning enables rapid re-training, offering substantial flexibility. This allows the framework to
adapt efficiently to changes in boundary or initial conditions, modifications to system geometry, or variations in the
underlying physics (e.g., altered reaction kinetics), without restarting training from scratch. Moreover, since PINNs
are mesh-less, sensitivity functions can be computed at any spatio-temporal location.
Overall, our findings demonstrate that PINNs can be effectively used to compute sensitivity functions for optimal
sensor placement in parameter estimation. Data collected at these locations significantly improve estimation accuracy,
even under measurement noise. Beyond the benchmark problems considered here, the framework can be extended
to more complex systems with multiple unknown parameters or partially defined models (e.g., unknown kinetics in
plasma reactors), where data acquisition is costly or limited.
Acknowledgements
CT acknowledges funding through the program YÎ 2TA-0559703 implemented within the framework of the
National Recovery and Resilience Plan "Greece 2.0" and financed by the European Union (NextGeneration EU)
Venianakis et al: Preprint submitted to Elsevier
Page 13 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
0
2
4
6
8
10
x
0.00
0.05
0.10
0.15
 âˆ‚c
âˆ‚P e(x)
2
PINN Solution
Numerical Solution
Figure 2: Spatial profile of the FIM trace computed by PINNs and numerically for ğ‘ƒğ‘’= 0.1 (a priori estimate) and
ğ·ğ‘= 1.0.
0
1000
2000
3000
4000
5000
Training Epoch
0.2
0.4
0.6
0.8
1.0
Pe value
True value of Pe
Optimal sensor location
Sensor at outlet
Figure 3: Inference of ğ‘ƒğ‘’vs. PINN training epochs with sensor placed at the FIM trace maximum (ğ‘¥âˆ—= 1.81 -blue curve)
versus at the outlet (ğ‘¥âˆ—= 10.0 -red curve)
Venianakis et al: Preprint submitted to Elsevier
Page 14 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
0
2
4
6
8
10
x
0.00
0.05
0.10
0.15
 âˆ‚c
âˆ‚P e(x)
2
Sensitivity, Pe = 1.0
Sensitivity, Pe = 0.1
Figure 4: FIM trace for the a priori estimate ğ‘ƒğ‘’= 0.1 and true value ğ‘ƒğ‘’= 1.0.
Inlet 
velocity 
profile
0.24
0.14
0.07
y
x
0.09
0.01
Inlet 
concentration 
profile
Wall
Wall
Wall
Zero pressure and 
zero flux outlet
Figure 5: Computational domain: the fluid enters the channel with a parabolic velocity profile at the horizontal direction
carrying a reactive species, flows past a fixed obstacle, and undergoes transport and reaction.
(a) |Ë†v| - PINN Solution
(b) |Ë†v| - FEM Solution
(c) Absolute Error
0.0005
0.0010
0.0015
0.0020
0.0005
0.0010
0.0015
0.0020
0.00000
0.00005
0.00010
0.00015
0.00020
Figure 6: Magnitude of the velocity field |Ì‚ğ¯|: (a) PINN prediction of the fluid flow, (b) FEM numerical solution and (c)
absolute error of PINN prediction relative to the FEM solution.
Venianakis et al: Preprint submitted to Elsevier
Page 15 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
0.00
0.05
0.10
0.15
0.20
0.25
x (m)
0.000
0.025
0.050
0.075
0.100
0.125
y (m)
0.0001
0.0002
0.0003
0.0004
0.0005
Sensitivity
Figure 7: Spatial profile of Gram determinant computed with sensitivity PINN for a priori estimate of ğ‘ƒğ‘’(7.0) and ğ·ğ‘
(18.0)
0.00
0.05
0.10
0.15
0.20
0.25
x (m)
0.00
0.02
0.04
0.06
0.08
0.10
0.12
0.14
y (m)
Optimal sensor locations
Intuitive sensor locations
Figure 8: Optimally selected and intuitively placed sensor locations for the 2D set-up
Venianakis et al: Preprint submitted to Elsevier
Page 16 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
0
500
1000
1500
2000
Training epoch
7
8
9
10
11
12
Pe Value
(a) Pe
0
500
1000
1500
2000
Training epoch
17
18
19
20
21
22
Da Value
(b) Da
Optimal sensor locations
Intuitive sensor locations
Ground truth
Figure 9: Parameter inference using data from optimally placed sensors versus intuitive placements: (a) ğ‘ƒğ‘’, (b) ğ·ğ‘.
t = 25 s
(a) Ë†c - PINN solution
(b) Ë†c - FEM solution
(c) Absolute Error
t = 50 s
t = 120 s
0.00
0.25
0.50
0.75
1.00
1.25
[kg/m3]
0.00
0.25
0.50
0.75
1.00
1.25
[kg/m3]
0.01
0.02
0.03
Absolute Error
0.00
0.25
0.50
0.75
1.00
1.25
[kg/m3]
0.00
0.25
0.50
0.75
1.00
1.25
[kg/m3]
0.01
0.02
0.03
Absolute Error
0.00
0.25
0.50
0.75
1.00
1.25
[kg/m3]
0.00
0.25
0.50
0.75
1.00
1.25
[kg/m3]
0.01
0.02
0.03
Absolute Error
Figure 10: Predicted Concentration field Ì‚ğ‘(ğ‘¥, ğ‘¦, ğ‘¡) at ğ‘¡= 25, 50, 120 s: (a) PINN prediction trained with data from optimal
sensors, (b) FEM numerical solution for true parameter values, and (c) Absolute error (|Ì‚ğ‘ğ‘ƒğ¼ğ‘ğ‘âˆ’Ì‚ğ‘ğ¹ğ¸ğ‘€|).
Venianakis et al: Preprint submitted to Elsevier
Page 17 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
0.00
0.05
0.10
0.15
0.20
0.25
x (m)
0.000
0.025
0.050
0.075
0.100
0.125
y (m)
(a) Optimally placed sensors
0.00
0.05
0.10
0.15
0.20
0.25
x (m)
0.000
0.025
0.050
0.075
0.100
0.125
y (m)
(b) Intuitively placed sensors
1 Sensor
2 Sensors
3 Sensors
5 Sensors
Figure 11: (a) Optimally selected and (b) intuitively placed sensor locations for the 2D set-up using 1 sensor (blue circle),
2 sensors (green triangles), 3 sensors (orange rectangles) and 5 sensors (red crosses).
0
500
1000
1500
2000
Training epoch
7
8
9
10
11
12
Pe Value
(a) Pe
0
500
1000
1500
2000
Training epoch
18
20
22
Da Value
(b) Da
Optimal sensor locations
Intuitive sensor locations
Ground truth
Figure 12: Mean and standard deviation (shaded area) of parameter inference with noisy data from optimal sensor locations
versus intuitive sensor placements: (a) ğ‘ƒğ‘’, (b) ğ·ğ‘
Venianakis et al: Preprint submitted to Elsevier
Page 18 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
0
100
200
300
t (s)
0.00
0.25
0.50
0.75
1.00
c (kg/m3)
Sensor 1 (0.034, 0.139)
0
100
200
300
t (s)
Sensor 2 (0.132, 0.139)
0
100
200
300
t (s)
Sensor 3 (0.24, 0.078)
PINN solution
Numerical solution for Pe = 12 and Da = 22
Noisy data
Figure 13: PINN prediction fitted to noisy data from 3 optimally placed sensors, compared with FEM solution at true
parameters.
Venianakis et al: Preprint submitted to Elsevier
Page 19 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
References
AlaÃ±a, J.E., Theodoropoulos, C., 2011.
Optimal location of measurements for parameter estimation of distributed parameter systems.
Com-
puters & Chemical Engineering 35, 106â€“120. URL: https://www.sciencedirect.com/science/article/pii/S0098135410001547,
doi:https://doi.org/10.1016/j.compchemeng.2010.04.014.
AlaÃ±a, J.E., Theodoropoulos, C., 2012. Optimal spatial sampling scheme for parameter estimation of nonlinear distributed parameter systems.
Computers & Chemical Engineering 45, 38â€“49. doi:https://doi.org/10.1016/j.compchemeng.2012.04.014.
Alonso, A.A., Kevrekidis, I.G., Banga, J.R., Frouzakis, C.E., 2004. Optimal sensor location and reduced order observer design for distributed
process systems. Computers & chemical engineering 28, 27â€“35.
Bard, Y., 1974. Nonlinear parameter estimation. volume 1209. Academic press New York.
Berardi, M., Difonzo, F.V., Icardi, M., 2025. Inverse physics-informed neural networks for transport models in porous materials. Computer Methods
in Applied Mechanics and Engineering 435, 117628.
Biegler, L., Damiano, J., Blau, G., 1986. Nonlinear parameter estimation: a case study comparison. AIChE Journal 32, 29â€“45.
Blei, D.M., Kucukelbir, A., McAuliffe, J.D., 2017. Variational inference: A review for statisticians. Journal of the American statistical Association
112, 859â€“877.
Bottou, L., 2010.
Large-scale machine learning with stochastic gradient descent, in: Proceedings of COMPSTATâ€™2010: 19th International
Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers, Springer. pp. 177â€“186.
Cai, S., Wang, Z., Wang, S., Perdikaris, P., Karniadakis, G.E., 2021. Physics-informed neural networks for heat transfer problems. Journal of Heat
Transfer 143, 060801.
Chang, S., Agarwal, P., McCready, C., Ricardez-Sandoval, L., Budman, H., 2025. Robust pinn modeling via sensitivity-based adaptive sampling:
Integration of optimal sensor placement and structural uncertainty handling. Journal of Process Control 152, 103493. URL: https://www.
sciencedirect.com/science/article/pii/S0959152425001210, doi:https://doi.org/10.1016/j.jprocont.2025.103493.
Chaturantabut, S., Sorensen, D.C., 2010. Nonlinear model reduction via discrete empirical interpolation. SIAM Journal on Scientific Computing
32, 2737â€“2764.
Coleman, M.C., Block, D.E., 2006. Bayesian parameter estimation with informative priors for nonlinear systems. AIChE journal 52, 651â€“667.
Courant, R., Hilbert, D., 1989. The Algebra of Linear Transformations and Quadratic Forms. John Wiley & Sons, Ltd. chapter 1. pp. 1â€“47.
doi:https://doi.org/10.1002/9783527617210.ch1.
Englezos, P., Kalogerakis, N., 2000. Applied parameter estimation for chemical engineers. CRC Press.
Forootani, A., Kapadia, H., Chellappa, S., Goyal, P., Benner, P., 2024. Gs-pinn: Greedy sampling for parameter estimation in partial differential
equations. arXiv preprint arXiv:2405.08537 .
Franceschini, G., Macchietto, S., 2008. Model-based design of experiments for parameter precision: State of the art. Chemical Engineering Science
63, 4846â€“4872. URL: https://www.sciencedirect.com/science/article/pii/S0009250907008871, doi:https://doi.org/10.
1016/j.ces.2007.11.034. model-Based Experimental Analysis.
Goswami, S., Anitescu, C., Chakraborty, S., Rabczuk, T., 2020. Transfer learning enhanced physics informed neural network for phase-field modeling
of fracture. Theoretical and Applied Fracture Mechanics 106, 102447. URL: https://www.sciencedirect.com/science/article/pii/
S016784421930357X, doi:https://doi.org/10.1016/j.tafmec.2019.102447.
Hanna, J.M., Aguado, J.V., Comas-Cardona, S., Askri, R., Borzacchiello, D., 2024. Sensitivity analysis using physics-informed neural networks.
Engineering Applications of Artificial Intelligence 135, 108764.
Hansen, N., 2016. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772 .
Hansen, N., Akimoto, Y., Baudis, P., 2019. CMA-ES/pycma on Github. Zenodo, DOI:10.5281/zenodo.2559634. URL: https://doi.org/10.
5281/zenodo.2559634, doi:10.5281/zenodo.2559634.
Hastings,
W.K.,
1970.
Monte
carlo
sampling
methods
using
markov
chains
and
their
applications.
Biometrika
57,
97â€“109.
URL:
https://doi.org/10.1093/biomet/57.1.97,
doi:10.1093/biomet/57.1.97,
arXiv:https://academic.oup.com/biomet/article-pdf/57/1/97/23940249/57-1-97.pdf.
Heredia-Zavoni, E., Esteva, L., 1998. Optimal instrumentation of uncertain structural systems subject to earthquake ground motions. Earthquake
engineering & structural dynamics 27, 343â€“362.
Hermanto, M.W., Kee, N.C., Tan, R.B., Chiu, M.S., Braatz, R.D., 2008. Robust bayesian estimation of kinetics for the polymorphic transformation
of l-glutamic acid crystals. AIChE Journal 54, 3248â€“3259.
Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feedforward networks are universal approximators. Neural networks 2, 359â€“366.
Hu, Z., Shukla, K., Karniadakis, G.E., Kawaguchi, K., 2024. Tackling the curse of dimensionality with physics-informed neural networks. Neural
Networks 176, 106369.
Huang, H., Li, Y., Song, R., Ren, J., Yu, H., Zhou, X., He, C., 2025. Inverse design of the membrane reactors enabled by an inverse-forward
physics-informed learning framework. Chemical Engineering Science 316, 121910.
Kalyanaraman, J., Fan, Y., Labreche, Y., Lively, R.P., Kawajiri, Y., Realff, M.J., 2015. Bayesian estimation of parametric uncertainties, quantification
and reduction using optimal design of experiments for co2 adsorption on amine sorbents. Computers & chemical engineering 81, 376â€“388.
Kravaris, C., Hahn, J., Chu, Y., 2013. Advances and selected recent developments in state and parameter estimation. Computers & Chemical
Engineering 51, 111â€“123. URL: https://www.sciencedirect.com/science/article/pii/S0098135412001779, doi:https://doi.
org/10.1016/j.compchemeng.2012.06.001. cPC VIII.
Laubscher, R., 2021. Simulation of multi-species flow and heat transfer using physics-informed neural networks. Physics of Fluids 33.
Lenk, P., Orme, B., 2009. The value of informative priors in bayesian inference with sparse data. Journal of Marketing Research 46, 832â€“845.
Li, S., Feng, X., 2022.
Dynamic weight strategy of physics-informed neural networks for the 2d navierâ€“stokes equations.
Entropy 24.
doi:10.3390/e24091254.
Liao, S., Xue, T., Jeong, J., Webster, S., Ehmann, K., Cao, J., 2023. Hybrid thermal modeling of additive manufacturing processes using physics-
informed neural networks for temperature prediction and parameter identification. Computational Mechanics 72, 499â€“512.
Venianakis et al: Preprint submitted to Elsevier
Page 20 of 21

A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation
Mao, Z., Jagtap, A.D., Karniadakis, G.E., 2020. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics
and Engineering 360, 112789.
URL: https://www.sciencedirect.com/science/article/pii/S0045782519306814, doi:https:
//doi.org/10.1016/j.cma.2019.112789.
Mehra, R., 1974. Optimal inputs for linear system identification. IEEE Transactions on Automatic Control 19, 192â€“200.
Myung, I.J., 2003. Tutorial on maximum likelihood estimation. Journal of mathematical Psychology 47, 90â€“100.
Nahor, H., Scheerlinck, N., Van Impe, J.F., NicolaÄ±, B., 2003. Optimization of the temperature sensor position in a hot wire probe set up for estimation
of the thermal properties of foods using optimal experimental design. Journal of food engineering 57, 103â€“110.
Ngo, S.I., Lim, Y.I., 2021. Solution and parameter identification of a fixed-bed reactor model for catalytic co2 methanation using physics-informed
neural networks. Catalysts 11, 1304.
Papadimitriou, C., Beck, J.L., Au, S.K., 2000. Entropy-based optimal sensor location for structural model updating. Journal of Vibration and Control
6, 781â€“800.
Papamakarios, G., Nalisnick, E., Rezende, D.J., Mohamed, S., Lakshminarayanan, B., 2021. Normalizing flows for probabilistic modeling and
inference. Journal of Machine Learning Research 22, 1â€“64.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al., 2019. Pytorch: An
imperative style, high-performance deep learning library. Advances in neural information processing systems 32.
Qureshi, Z., Ng, T., Goodwin, G., 1980. Optimum experimental design for identification of distributed parameter systems. International Journal of
Control 31, 21â€“29.
Raissi, M., Perdikaris, P., Karniadakis, G., 2019.
Physics-informed neural networks: A deep learning framework for solving forward and
inverse problems involving nonlinear partial differential equations. Journal of Computational Physics 378, 686â€“707. URL: https://www.
sciencedirect.com/science/article/pii/S0021999118307125, doi:https://doi.org/10.1016/j.jcp.2018.10.045.
Raissi, M., Yazdani, A., Karniadakis, G.E., 2020.
Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.
Science 367, 1026â€“1030. URL: https://www.science.org/doi/abs/10.1126/science.aaw4741, doi:10.1126/science.aaw4741,
arXiv:https://www.science.org/doi/pdf/10.1126/science.aaw4741.
Vande Wouwer, A., Point, N., Porteman, S., Remy, M., 2000. An approach to the selection of optimal sensor locations in distributed parameter
systems. Journal of Process Control 10, 291â€“300.
Walter, Ã‰., Pronzato, L., 1990. Qualitative and quantitative experiment design for phenomenological modelsâ€”a survey. Automatica 26, 195â€“213.
Wang, S., Sankaran, S., Wang, H., Perdikaris, P., 2023. An expertâ€™s guide to training physics-informed neural networks.
Wu, C., Zhu, M., Tan, Q., Kartha, Y., Lu, L., 2023. A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-
informed neural networks. Computer Methods in Applied Mechanics and Engineering 403, 115671.
Yin, M., Zheng, X., Humphrey, J.D., Karniadakis, G.E., 2021. Non-invasive inference of thrombus material properties with physics-informed neural
networks. Computer Methods in Applied Mechanics and Engineering 375, 113603. URL: https://www.sciencedirect.com/science/
article/pii/S004578252030788X, doi:https://doi.org/10.1016/j.cma.2020.113603.
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., He, Q., 2021. A comprehensive survey on transfer learning. Proceedings of the
IEEE 109, 43â€“76. doi:10.1109/JPROC.2020.3004555.
Venianakis et al: Preprint submitted to Elsevier
Page 21 of 21
