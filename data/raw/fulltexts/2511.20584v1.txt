A Tale of Two Geometries: Adaptive Optimizers and
Non-Euclidean Descent
Shuo Xie*1
Tianhao Wang*2
Beining Wu3
Zhiyuan Li1
1Toyota Technological Institute at Chicago
2University of California, San Diego
3University of Chicago
{shuox,zhiyuanli}@ttic.edu, tianhaowang@ucsd.edu, beiningw@uchicago.edu
Abstract
Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient,
suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however,
lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed
by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend
the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence
of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers
with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain
non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing
adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees
that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.
1
Introduction
Adaptive optimizers such as Adam have been indispensable for training large-scale machine learning models (Bi et al.,
2024; Dubey et al., 2024; Yang et al., 2025; Wen et al., 2025). Their dominance in training efficiency, however, has
recently been challenged by the surprising effectiveness of simpler Normalized Steepest Descent (NSD)-type methods
such as Muon and Lion (Jordan et al., 2024; Chen et al., 2023; Team et al., 2025; Liu et al., 2025; Shah et al.,
2025). Behind this competition of two family of optimizers, a broader consensus has begun to emerge: their superior
performance is critically related to their ability to exploit non-Euclidean geometry of the loss landscape (Balles et al.,
2020; Xie & Li, 2024; Zhang et al., 2024; Pethick et al., 2025).
Recent studies have rigorously characterized how adaptive optimizers exploit non-Euclidean geometry. For ex-
ample, Maladkar et al. (2024) and Xie et al. (2025a) show that AdaGrad and Adam benefit from exploiting the
ℓ∞-geometry of loss functions, and a one-sided variant of Shampoo has been shown to leverage the geometry induced
by the matrix spectral norm (Xie et al., 2025b; An et al., 2025). Notably, Bernstein & Newhouse (2024) proposed a
striking connection between adaptive optimizers and NSD: with exponential moving average (EMA) turned off, certain
adaptive optimizers reduce exactly to their NSD counterparts. For example, without EMA, Adam coincides with NSD
under the ℓ∞norm, and Shampoo coincides with NSD under the matrix spectral norm, which is proposed to be an
independent algorithm Muon. Yet, beyond these connections, there is no formal result that systematically characterizes
the relationship between the two families of algorithms. This naturally motivates the following question:
Q1.
Do adaptive methods (like Adam, Shampoo) and their corresponding non-Euclidean descent (like
Lion, Muon) exploit the non-Euclidean geometry of loss landscape in the same way?
To address this question, we adopt a theoretical perspective and focus on comparing different types of smoothness
assumptions that underpin the analysis of these methods. In fact, even under the same geometry, two distinct notions
of smoothness arise. The first is the standard smoothness under a general norm (cf. Definition 2.3), which governs
the convergence rate of NSD. The second is called the adaptive smoothness (cf. Definition 2.4), introduced by Xie
et al. (2025b) and shown to govern the convergence rate of adaptive optimizers in the convex case. Indeed, a main
contribution of our work is to show that adaptive smoothness also characterizes the convergence rate of adaptive
1
arXiv:2511.20584v1  [cs.LG]  25 Nov 2025

optimizers in the nonconvex setting. Therefore, while both adaptive optimizers and NSD can exploit non-Euclidean
geometry, they rely on fundamentally different smoothness assumptions.
This difference is not merely terminological but quantitative: adaptive smoothness is always no smaller than the
standard smoothness under the same geometry. In other words, from the standpoint of technical conditions, the adaptive
smoothness represents a stronger assumption. This in turn motivates our second question:
Q2.
Does the stronger smoothness assumption in adaptive methods offer any optimization benefits?
We answer this question affirmatively. In particular, we show that by leveraging Nesterov acceleration, adaptive
optimizers can attain an accelerated O(T −2) rate under adaptive smoothness in the convex setting. In sharp contrast,
it has been shown by Guzmán & Nemirovski (2015) that the convergence rate of any optimizer is no better than
Ω(T −1) under the standard ℓ∞smoothness assumption. This establishes a clear separation: adaptive smoothness
enables adaptive optimizers to achieve acceleration under non-Euclidean geometry, while the standard smoothness
fails. Therefore, the stronger adaptive smoothness assumption indeed translates into concrete optimization benefits,
showing its difference from the standard smoothness.
In fact this difference has a direct and interesting analogy in terms of the noise assumption in the stochastic
setting. When gradient noise is present, its variability can be measured in two distinct ways: the standard variance
considers gradient variation under a fixed norm, whereas the adaptive variance (cf. Definition 4.1) measures noise
in a more stringent but also more adaptive way that requires uniform control over the geometry prescribed by each
preconditioner under consideration. By construction, adaptive variance is always no smaller than standard variance,
directly paralleling the relationship between adaptive and standard smoothness. Analogous to adaptive smoothness
that enables acceleration under a stronger requirement, adaptive variance can likewise yield benefits despite being
larger. We demonstrate this through a careful analysis of NSD under two types of noise assumptions: adaptive variance
enables a dimension-free rate, which is not attainable in the worst case under the standard variance condition.
Taken together, our results demonstrate that adaptive smoothness and adaptive variance are different from their
standard counterparts as adaptive smoothness enables an acceleration rate and adaptive noise enables a dimension-free
rate. These findings reveal an intricate interplay between adaptivity and non-Euclidean geometry, deepening our
theoretical understanding of adaptivity in optimization.
Below we summarize our main contributions.
• In Section 3, we show the convergence rate for adaptive optimizers on nonconvex functions (Theorems C.2, C.7
and C.8), which depends on the adaptive smoothness and matches optimal ˜O(T −1/4) rate. It theoretically justifies
that adaptive methods and NSD exploit the geometry through different smoothness notions in the nonconvex
setting.
• In Section 4.2, we identify the benefit of the adaptive smoothness by showing it enables an acceleration rate
˜O(T −2) of adaptive optimizers equipped with Nesterov momentum (Theorem 4.4) in contrast to the convergence
rate Ω(T −1) the standard ℓ∞smoothness.
• In Section 4, we extend the benefit of adaptive geometry to noise assumptions by introducing adaptive noise (Def-
inition 4.1). We show that this stronger notion of noise can provide a new type of convergence rate for NSD
with momentum on nonconvex functions which gets rid of dependence on parameter size d (Theorem 4.6). We
complement its superiorty by providing a lower bound under the standard noise (Theorem 4.9).
• Our analysis of adaptive optimizers is carried out through a unified framework that covers a broad class of
methods, including AdaGrad, AdaGrad-Norm, and one-sided Shampoo. The proof technique developed in this
framework may be of independent interest.
1.1
Notations
Let Md be the set of all d-by-d matrices, Sd ⊂Md be the subset of all symmetric matrices. We use Sd
+ to denote the
set of positive semi-definite matrices. We denote by Id ∈Md the identity matrix. For matrices A, B, we denote their
inner product by ⟨A, B⟩= Tr(A⊤B).
For H ∈Sd
+, ∥x∥H :=
√
x⊤Hx is the (semi-)norm of x ∈Rd with respect to H. For a convex set H ⊆Sd
+, we
define the induced H-norm as
∥x∥H :=
sup
H∈H,Tr(H)≤1
∥x∥H .
(1)
2

Throughout the paper, we reserve f for the loss function and x0 for the initialization of an optimization algorithm.
For convenience, we denote the initial suboptimality as ∆0 = f(x0) −minx f(x).
2
From Adam/SignGD to Adaptive Smoothness
We use the example of Adam and SignGD to motivate the notion of adaptive smoothness in Section 2.1, and then
present the formal definition in Section 2.2, along with some related background.
2.1
Adam and SignGD can exploit ℓ∞geometry, but in different ways
We start by discussing a specific pair of algorithms, Adam and SignGD, to illustrate the problem of interest. It is
known that SignGD can be viewed as Normalized Steepest Descent (NSD) under the ℓ∞norm and its convergence rate
for deterministic nonconvex functions admits the following form (Xie et al., 2025a)
min
t∈[T ] ∥∇f(xt)∥1 ≤O
s
∆0L∥·∥∞(f)
T

where L∥·∥∞(f) is the standard smoothness of f under the ℓ∞norm (see Definition 2.3). Note that SignGD can
also be viewed as a special case of Adam with β1 = β2 = 0. However, the convergence rate of Adam for gen-
eral β1, β2 instead depends on a different diagonal adaptive smoothness notion, which is defined as Ldiag(f) =
minH∈Dd,−H⪯∇2f(x)⪯H Tr(H) in Maladkar et al. (2024); Xie et al. (2025a). In particular, Adam with β1 = 0
(a.k.a.
RMSProp) for deterministic nonconvex functions admits the convergence rate mint∈[T ] ∥∇f(xt)∥1 =
˜O(
p
∆0Ldiag(f)/T) (Xie et al., 2024).
Notably, this diagonal adaptive smoothness is always no smaller than
L∥·∥∞(f) (Balles et al., 2020). This suggests that though both SignGD and Adam admit convergence guarantees for
the ℓ1 norm (the dual norm of ∥· ∥∞) of the gradients, they achieve so under different smoothness notions. This
distinction motivates the following question:
How does the diagonal adaptive smoothness Ldiag(f) emerge as an ℓ∞geometry?
To address this question, let us consider the convergence rate of NSD under any norm ∥· ∥H for H ∈H = Dd
+ (see
Theorem 4.6):
min
t∈[T ] ∥∇f(xt)∥H,∗= O
r
∆0L∥·∥H(f)
T

(2)
where ∥· ∥H,∗is the dual norm of ∥· ∥H. Minimizing both sides of (2) over H ∈Dd
+ with Tr(H) ≤1 yields
inf
diagonal H⪰0,Tr(H)≤1 min
t∈[T ] ∥∇f(xt)∥H,∗= O
s
∆0
T
inf
diagonal H⪰0,Tr(H)≤1 L∥·∥H(f)

= O
r
∆0Ldiag(f)
T

(3)
where the equality can be checked by the definition of Ldiag(f). Now the right-hand side matches the aforementioned
convergence rate of Adam. The adaptivity of Adam is then demonstrated by its ability to automatically identify and
adapt to the best diagonal matrix-induced norm for any given loss function, without the need of knowing H.
Importantly, we point out that the left-hand side of (3) is closely related to the ℓ1 norm of the gradients. This is
because
sup
diagonal H⪰0,Tr(H)≤1
∥· ∥H = ∥· ∥∞,
inf
diagonal H⪰0,Tr(H)≤1 ∥· ∥H,∗= ∥· ∥1.
(4)
We illustrate this fact in Fig. 1. In words, this means that the ℓ∞norm is the pointwise supremum of all weighted ℓ2
norms induced by diagonal matrices with unit trace, whereas its dual, the ℓ1 norm, is the pointwise infimum of all the
corresponding dual norms. Also, the unit ℓ∞ball is the intersection of all unit balls for those ℓ2 norms, and the unit
ℓ1 ball is the union of all dual unit balls.
Indeed, the duality between supremum of a class of primal norms and infimum of the corresponding dual norms in
(4) is not just a coincidence, but rather a special property induced by the structure of the preconditioner set H = Dd
+
for Adam.
This property holds more generally for any well-structured preconditioner set H and we discuss the
corresponding adaptive smoothness in the next subsection.
3

2
1
0
1
2
x1
2
1
0
1
2
x2
1.5
1.0
0.5
0.0
0.5
1.0
1.5
g1
1.5
1.0
0.5
0.0
0.5
1.0
1.5
g2
Figure 1:
Here we demonstrate the duality between the supremum of the primal norms and the infimum of
the corresponding dual norms for any well-structured preconditioner set H.
In particular, we consider H =
{all diagonal PSD matrices}, in which case ∥·∥H = ∥·∥∞and ∥·∥H,∗= ∥·∥1. Left figure: the ∥· ∥∞-unit ball
(black square) in the primal space is the intersection of all ∥· ∥H-unit ball (colored ellipses) for H ∈H with
Tr(H) ≤1, that is, ∥· ∥∞is the supremum of all such primal ∥· ∥H norms. Right figure: the ∥· ∥1-unit ball (dashed
black square) in the dual space is the union of all ∥· ∥H,∗-unit balls (dashed ellipses) for H ∈H with Tr(H) ≤1,
that is, ∥· ∥1 is the infimum of all such dual ∥· ∥H,∗norms.
2.2
Adaptive smoothness associated with well-structured preconditioner sets
The following definition of well-structured preconditioner sets is proposed by Xie et al. (2025b) to unify the analysis
of a broad family of adaptive optimizers with structured preconditioners.
Definition 2.1 (Well-structured preconditioner set). H ⊆Sd
+ is said to be a well-structured preconditioner set if
H = Sd
+ ∩K for some matrix subalgebra1 K ⊆Md with Id ∈K.
As will be discussed in Section 3.1, many commonly used adaptive optimizers, including Adam, AdaGrad, and
their variants, can be cast into the framework of a meta-algorithm (Algorithm 1) with well-structured preconditioner
sets. A specific case is H = Dd
+, the set of all diagonal PSD matrices, which is the running example in the previous
subsection. For any such well-structured preconditioner set H, we have the duality between the supremum of the
primal norms and the infimum of the corresponding dual norms, formalized in the following lemma.
Lemma 2.2. Let H ⊆Sd
+ be any well-structured preconditioner set. Recall that its induced norm is defined as
∥· ∥H = supH∈H,Tr(H)≤1 ∥· ∥H. Then it holds that
∥· ∥H,∗=
inf
H∈H,Tr(H)≤1 ∥· ∥H,∗=
inf
H∈H,Tr(H)≤1 ∥· ∥H−1.
Based on this fact, we can generalize the discussion in the previous subsection to any well-structured preconditioner
set H, showing that NSD and adaptive optimizers with preconditioner set H can exploit the geometry induced by
∥· ∥= ∥· ∥H via two different smoothness notions, the former being the standard smoothness under ∥· ∥H and the
latter being the adaptive smoothness defined in Definition 2.4 below.
Standard and adaptive smoothness.
We proceed to introduce the adaptive smoothness associated with any well-
structured preconditioner set H. We first review the standard smoothness notion under a general norm ∥·∥.
1For a set of d-by-d matrices K ⊆Md, we say that K is a subalgebra if it is closed under scalar multiplication, matrix addition, and matrix
multiplication. More concretely, we require that for any α ∈R and A, B ∈K, it holds that αA, AB, A + B ∈K.
4

Definition 2.3. For a loss function f : Rd →R and any norm ∥·∥, we will use L∥·∥(f) to denote the smoothness of f
with respect to ∥·∥, i.e., the smallest positive constant L such that ∥∇f(x) −∇f(y)∥∗≤L ∥x −y∥for any x, y.
When ∥· ∥= ∥· ∥H for some well-structured preconditioner set H, L∥·∥H(f) is then the standard smoothness of
f under the norm induced by H. In contrast, the adaptive smoothness associated with H is defined as the smallest
smoothness of f under all norms ∥· ∥H induced by H ∈H with Tr(H) ≤1, as formalized below. This term is
introduced as H-smoothness in Xie et al. (2025b). We rename it to highlight that this notion adapts to the structure of
H, in contrast to the standard smoothness.
Definition 2.4 (Adaptive Smoothness, Xie et al. 2025b). The adaptive smoothness of a function f w.r.t. a well-structured
preconditioner set H is defined as the smallest smoothness of f under all ∥· ∥H for H ∈H with Tr(H) ≤1, that is,
ΛH (f) :=
min
H∈H
Tr(H)≤1
L∥·∥H(f) =
min
H∈H
∀x,−H⪯∇2f(x)⪯H
Tr(H).
(5)
In the deterministic convex setting, it has been shown by Xie et al. (2025b) that the convergence rate of an adaptive
optimizer with any well-structured preconditioner set H is of order O(ΛH(f) ∥X∥2
H /T). In Section 3, we extend such
characterization to the nonconvex setting, demonstrating that the adaptive smoothness ΛH(f) governs the convergence
behavior of any adaptive optimizer with well-structured preconditioner set H.
Comparison between two smoothness notions.
For any H ∈H with Tr(H) = 1, it always holds that ∥x −y∥H ≥
∥x −y∥H and ∥∇f(x) −∇f(y)∥H,∗≤∥∇f(x) −∇f(y)∥H,∗. Therefore,
L∥·∥H(f) = sup
x,y
∥∇f(x) −∇f(y)∥H,∗
∥x −y∥H
≥sup
x,y
∥∇f(x) −∇f(y)∥H,∗
∥x −y∥H
= L∥·∥H(f).
Minimizing over H ∈H with Tr(H) = 1 then yields ΛH (f) = L∥·∥H(f) ≥L∥·∥H(f). In other words, as a
condition, the adaptive smoothness is arguably stronger than the standard smoothness. But they can differ by at most a
multiplicative factor of d, as summarized in the following Proposition 2.5.
Proposition 2.5. For any well-structured preconditioner set H ⊆Sd
+ and any loss function f : Rd →R, it always
holds that L∥·∥H(f) ≤ΛH (f) ≤d · L∥·∥H(f).
3
Unified Analysis in the Nonconvex Setting
In the nonconvex setting, we establish a unified analysis that encompasses a broad family of adaptive optimization
algorithms. Our result highlights how the convergence behavior of these methods depends critically on the notion of
adaptive smoothness.
3.1
Adaptive optimizers with well-structured preconditioner sets
We adopt the framework in Gupta et al. (2017) and Xie et al. (2025b) to describe adaptive optimizers in a unified way,
as displayed in Algorithm 1. This meta-algorithm is flexible in two aspects: the way of aggregating past gradients and
the choice of preconditioner set H. First, there are three different ways to aggregate the past gradients in Algorithm 1,
each of which is presented in a separate algorithm block in Section C.1.
• Cumulative accumulation (Algorithm 5) maintains the plain sum of past squared gradients, thereby giving equal
weight to the entire gradient history. A famous example in this category is AdaGrad.
• EMA accumulation (Algorithm 6) computes an exponential moving average of past gradients, which yields a
stationary estimate of recent gradient statistics. Notable examples include Adam and RMSProp.
• Weighted accumulation (Algorithm 7) applies geometrically decaying weights to past gradients, which differs
from EMA accumulation in that it does not normalize the weights to sum to one.
5

Algorithm 1 General Adaptive Optimization Algorithm
Hyperparam: ϵ ≥0, total steps T, learning rate η, convex cone H ⊂S+, decay factor β
Input: initialization x0, stochastic loss functions {ft}T
t=1 : Rd →R
M−1 ←0
for t = 0, 1, · · · , T −1 :
gt ←∇ft(xt)
Mt ←







Mt−1 + gtg⊤
t ,
Cumulative variant,
β Mt−1 + (1 −β) gtg⊤
t ,
EMA variant,
β Mt−1 + gtg⊤
t ,
Weighted variant.
Vt ←arg minH∈H

Mt + ϵId, H−1
+ Tr(H)
xt+1 ←xt −ηV −1
t
gt
return xT
The cumulative and EMA variants are the most common ways, and they are indeed equivalent to the weighted variant
up to hyperparameter transformations. Therefore, it suffices to analyze the weighted variant, and the results for the
other two variants follow as corollaries.
Another flexibility of Algorithm 5 comes from the choice of convex cone H. More specifically, we remark that
Algorithm 1 recovers several standard optimizers by specifying the preconditioner set H as follows:
• H = {all diagonal PSD matrices} recovers AdaGrad (Duchi et al., 2011) and Adam (Kingma & Ba, 2014).
• H = {c Id | c > 0} recovers AdaGrad-Norm (Ward et al., 2020) and AdaSGD (Wang & Wiens, 2020).
• H = Sd
+ recovers full-matrix AdaGrad (Duchi et al., 2011).
• H = SdL
+ ⊗IdR yields one-sided Shampoo/ASGO recently proposed by (Xie et al., 2025a; An et al., 2025)
In particular, based on the notion of well-structured preconditioner sets defined in Definition 2.1, Xie et al. (2025b)
develops a unified convergence analysis for Algorithm 5 in the convex setting, and the convergence rate depends on the
adaptive smoothness with respect to H defined in Definition 2.4.
Additional notations.
We define PH(M) := arg minH∈H

M, H−1
+ Tr(H) for any M ∈Sd
++. Then in
Algorithm 1, Vt = PH(Mt) and Lemma A.4 will show that PH(M)2 is the projection of M onto H. Specifically,
when H contains all the PSD matrices, Vt is M
1
2
t .
3.2
Convergence rate in the deterministic nonconvex setting
Here we only present results for the deterministic case to highlight the role of adaptive smoothness, and the complete
results for the (stochastic) nonconvex setting and corresponding proofs can be found in Section C.2. We first present
the convergence guarantee for the weighted variant of Algorithm 5 in the following theorem.
Theorem 3.1. For any ϵ ≥0, β ∈(0, 1], η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 1 with well-
structured preconditioner set H, where the update of Mt follows the weighted version, i.e., Mt = βMt−1 + gtg⊤
t for
all t ∈[T]. Let ΛH (f) be the adaptive smoothness of the loss f according to Definition 2.4. Then when ft ≡f, it
holds that
1
T
T −1
X
t=0
∥∇f(xt)∥H,∗≤
qPT −1
i=0 βi/2
T
ξ +
√
dϵ1/4
√
T
p
ξ.
where ξ is given by
ξ = 2∆0
η
+ ηΛH (f) ∥ST ∥op
(6)
6

and ST = E PT −1
t=0 V −1
t
(V 2
t −βV 2
t−1)V −1
t
.
For general well-structured preconditioner set, ∥ST ∥op = ˜O (log(d)[(1 −β)T/β + log(d)]). When the precondi-
tioner set only has diagonal matrices, ∥ST ∥op = (1 −β)T + ˜O (1).
The above result for the weight variant can be converted to guarantees for the cumulative and EMA variants.
Specifically, the cumulative variant is equivalent to weighted accumulation with β = 1 while the EMA variant with
learning rate ηE and stability constant ϵE produces identical iterates as weighted accumulation with ηW = ηE/√1 −β
and ϵW = ϵE/(1 −β). Below we present the result for the cumulative variant in Theorem 3.2, and the result for the
EMA variant can be found in Theorem C.8.
Theorem 3.2. For any ϵ ≥0, η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 1 with well-structured
preconditioner set H, where the update of Mt follows the cumulative version, i.e., Mt = Mt−1 +gtg⊤
t for all t ∈[T].
Let ΛH (f) be the adaptive smoothness of the loss f according to Definition 2.4. Then when ft ≡f, it holds that
1
T
T −1
X
t=0
∥∇f(xt)∥H,∗≤
1
√
T

ξ +
√
dϵ1/4p
ξ

where ξ is given by
ξ = ˜O
∆0
η + η · ΛH (f) log2 d

.
Moreover, when setting the learning rate η =
q
∆0
ΛH(f) log2 d, it holds that ξ = ˜O
 p
∆0 · ΛH (f) log d

.
At a high level, Theorem 3.2 shows that, with appropriate choices of hyperparameters, Algorithm 1 with any
well-structured preconditioner set H achieves a convergence rate of order ˜O(log d ·
p
∆0ΛH(f)/T) on deterministic
nonconvex functions where ˜O(·) hides logarithmic factors in problem parameters other than the dimension d. This
result illustrates that the adaptive smoothness ΛH (f) governs the convergence rate of adaptive optimizers in the
nonconvex setting, complementing previous results for the convex setting in Xie et al. (2025b). Moreover, we remark
that when H contains only diagonal matrices, the log d factor disappears, recovering the bounds in Xie et al. (2025a).
It is worth noticing that the convergence guarantees in the above two theorems are concerned with ∥∇f(xt)∥H,∗
depending on specific H rather than ∥∇f(xt)∥2. For the specific case of Adam where H is the set of all diagonal PSD,
this becomes a guarantees in terms of ℓ1 norm of the gradients, as we discussed in Section 2.1. On the other hand,
Pethick et al. (2025); Kovalev (2025a) show that NSD achieves O
 (
∆0L∥·∥H(f)
T
)
1
2 
in the deterministic case. Taken
together, these two upper bounds suggest that adaptive optimizers and NSD exploit different smoothness notions in the
nonconvex setting.
Comparing the upper bounds on the convergence rates of the adaptive optimizer with H and NSD under ∥·∥H, since
the adaptive smoothness is always no smaller than the standard smoothness, we can see that the upper bound on the
convergence rate for NSD is better. Then why not just use NSD under ∥· ∥H instead of the more complicated adaptive
method? We address this question in Section 4.2 by showing that the stronger adaptive smoothness assumption enables
an accelerated rate that cannot be achieved under the standard smoothness assumption. In addition, we propose that
there are two types of gradient variance assumptions that parallel the two smoothness notions and they also lead to
quantitatively different results.
3.3
Technical Contribution: A Novel Matrix Inequality
Previous theoretical results for one-sided Shampoo/ASGO (Algorithm 4) and other well-structured preconditioners have
primarily focused on convex objectives (Xie et al., 2025b; An et al., 2025; Kovalev, 2025a). In the nonconvex regime,
existing convergence analyses apply essentially when the preconditioner set H contains only diagonal matrices2 (Xie
et al., 2025a). In contrast, we provide the first unified convergence analysis that applies to any general well-structured
preconditioner sets H, well beyond the diagonal cases.
A central difficulty in our analysis is the extension from diagonal preconditioners to a general preconditioner
set H. In the diagonal case, the proof basically decomposes to entry-wise analyses, and scalar telescoping readily
2This can be generalized to any commutative well-structured preconditioner set H.
7

yields the desired bounds. However, for general H, noncommutativity prevents such simplification, and bounding
the second-order terms requires handling delicate matrix inequalities. Our resolution of this challenge yields a key
technical contribution, formalized in the following Lemma 3.3.
Lemma 3.3. Let ϵ ≥0 and β ∈(0, 1]. For any T ∈N, consider any sequence of vectors g0, . . . , gT −1 ∈Rd. Let
M−1 = 0, and recursively define Mt = βMt−1 +gtg⊤
t for t = 0, . . . , T −1. For any well-structured preconditioner
set H, define Vt = arg minH∈H⟨Mt + ϵId, H−1⟩+ Tr(H) for each t ∈[T −1].
Then for any H ∈H ∩Sd
++,
T −1
X
t=0
∥V −1
t
gt∥2
H ≤Tr(H) ∥ST ∥op
where ST =
T −1
X
t=0
V −1
t
 V 2
t −βV 2
t−1

V −1
t
.
Moreover, there exists an absolute constant C1, C2 > 0, independent of d, T, ϵ, β and H, such that
∥ST ∥op ≤C1
 
1 + log

1 + d
ϵ
T −1
X
t=0
∥gt∥2
2 + d2(1 −β)T
! 1 −β
β
T + log ∥V 2
T −1/ϵ∥op

+ C2.
In the special case when H is commutative, the above bound can be further improved to
∥ST ∥op ≤(1 −β)T + log ∥V 2
T −1/ϵ∥op.
Specializing gt’s to be the gradients in the weighted adaptive algorithm, Lemma 3.3 provides a general upper
bound on the sum of second-order terms. Meanwhile, it highlights the essential gap between diagonal and general
preconditioner sets: noncommutativity introduces an additional log d factor, making the dependence strictly worse
than in the diagonal case. Nevertheless, this is the first bound that applies to arbitrary well-structured preconditioner
sets, and it plays a central role in extending convex analyses to the nonconvex setting.
The proof of Lemma 3.3 can be found in Section B. A key step in the proof is to establish a novel matrix inequality
that relates the difference between two positive definite matrices to the difference between their logarithms. To illustrate
this, note that in Lemma 3.3, we need to bound the spectral norm of ST , which admits the following form when β = 1:
ST =
T −1
X
t=0
V −1
t
(V 2
t −V 2
t−1)V −1
t
.
When V0, . . . , VT −1 commute, it can be shown that V −1
t
(V 2
t −V 2
t−1)V −1
t
⪯log(V 2
t ) −log(V 2
t−1), and thus we can
apply telescoping to obtain ST ⪯log(V 2
T −1) −log(V 2
0 ). However, this argument breaks down when the matrices do
not commute. To address this, we need to pay some extra cost to control the noncommutativity, which is captured by
the following matrix inequality.
Lemma 3.4. For any positive definite matrices X, Y ∈Rd×d such that Y ⪯X, it holds for any 0 ≤c ≤C that
X−1/2(X −Y )X−1/2 ⪯3(log C −log c)
π2
(log X −log Y ) +

12cd
π2λmin(X)2 + 12C−1d
π2

Tr(X −Y ) · Id.
Roughly speaking, the second term on the right-hand side represents the curse of noncommutativity. However,
thanks to the logarithmic dependence on c, C in the first term, we can choose c = 1/poly(d) and C = poly(d)
to ensure that log C −log c = O(log d), while keeping the second term small. This is the root of the log d factor
in Lemma 3.3 for general well-structured preconditioner sets. We believe the techniques developed here may be of
independent interest.
4
Benefit of Adaptive Geometry
We have shown in Section 3 that the nonconvex convergence rate of adaptive optimizers relies critically on the adaptive
smoothness of the loss, and the bound is worse than that of the corresponding NSD. This naturally raises the concern
posed in question 2: does the stronger assumption of adaptive smoothness lead to stronger results? In this section, we
address this question from two complementary angles:
8

1. Under the adaptive smoothness assumption, adaptive optimizers can achieve faster convergence rates on convex
functions via Nestrov acceleration.
2. The distinction between standard smoothness and adaptive smoothness mirrors a parallel separation in the
assumptions on gradient noise.
At a high level, these two angles share the same underlying mechanism: Under non-Euclidean geometry, averaging
might not be effective in reducing the norm. We illustrate such ineffectiveness in the subsequent sections.
4.1
Adaptive Variance: An Analogue of Adaptive Smoothness
Our main results in this section are concerned about accelerated adaptive algorithms for convex functions and NSD in
the nonconvex setting. Before presenting these results, we introduce a key technical ingredient: adaptive variance, a
quantity that serves as the analogue of adaptive smoothness for gradient noise.
Definition 4.1 (Standard and adaptive gradient variance). For an index set T , let {ft}t∈T be a set of stochastic loss
functions where each ft : Rd →R.
• For any norm ∥· ∥on Rd, the gradient variance of {ft}t∈T under ∥· ∥is defined as
σ∥·∥({ft}t∈T )2 :=
sup
t∈T ,x∈Rd E
∇ft(x) −E[∇ft(x)]
2
(7)
• The adaptive gradient variance of {ft}t∈T with respect to any well-structured preconditioner set H is defined as
σH({ft}t∈T )2 =
min
H∈H,Tr(H)≤1
sup
t∈T ,x∈Rd E
∇ft(x) −E[∇ft(x)]
2
H−1

.
(8)
This adaptive variance is inspired by the noise assumption in Kovalev (2025a), both capturing the overall variation
of gradient noise in the geometry induced by H. Compared with the traditional noise assumption that only characterizes
ℓ2 norm variance, adaptive variance provides a more informative measure. In addition, analogous to the comparison
between ΛH (f) and L∥·∥H(f), the adaptive variance is always no smaller than ∥·∥H,∗-variance, as formalized below.
Proposition 4.2. For any set of stochastic loss functions {ft}t∈T and any well-structured preconditioner set H, it
always holds σ∥·∥H,∗({ft}t∈T )2 ≤σH({ft}t∈T )2 ≤d · σ∥·∥H,∗({ft}t∈T )2.
Here we can compare Definition 4.1 with bounded covariance assumption in Xie et al. (2025b); An et al. (2025)
that there exists Σ ⪰0 such that E[∇f(xt) −∇ft(xt)][∇f(xt) −∇ft(xt)]⊤⪯Σ. When the covariance matrix is
upper bounded by Σ, σH ≤Tr(PH(Σ)) for general H as shown in Proposition A.10. On the other hand, Definition 4.1
doesn’t require the existence of Σ that can upper bound the covariance matrix everywhere. Therefore, Definition 4.1
is a weaker assumption than the bounded covariance assumption.
4.2
Acceleration on Convex Functions
To illustrate the fact that averaging cannot effectively reduce norm under non-Euclidean geometry, consider x1 =
(1, 0, 0)⊤, x2 = (0, 1, 0)⊤, x3 = (0, 0, 1)⊤∈R3, for which ∥1
3
P3
i=1 x1∥2 =
1
√
3 while ∥1
3
P3
i=1 xi∥1 = 1. In
particular, this will hinder the applicability of acceleration techniques, and below we discuss a resulting separation
between the standard smoothness and adaptive smoothness.
We follow the framework in Kovalev (2025a) to formulate a unified class of adaptive optimizers with well-structured
preconditioner sets with Nesterov acceleration in Algorithm 2. Through the perspective introduced by Kovalev &
Borodich (2024), the idea is to interpret each step of Nesterov acceleration as a single step of standard gradient on a
modified loss, i.e., f αt,¯xt in Eq. (9). Here for a constant α ∈(0, 1] and a reference point ¯x, the corresponding modified
loss is defined as
f α,¯x(x) := α−2f(αx + (1 −α)¯x).
(9)
9

Algorithm 2 Accelerated Adaptive Algorithm
Hyperparam: ϵ ≥0, total steps T, learning rate η, convex cone H ⊆S+
Input: initialization x0 = ¯x0, a sequence of positive constants α0, . . . , αT ∈(0, 1]
M−1 ←0
for t = 0, 1, 2, . . . , T −1 :
gt ←∇f αt,¯xt
t
(xt) where f αt,¯xt
t
is defined in (9)
Mt ←Mt−1 + gtg⊤
t
Vt ←arg minH∈H

Mt + ϵId, H−1
+ Tr(H)
xt+1 ←xt −ηV −1
t
gt
¯xt+1 ←αtxt+1 + (1 −αt)¯xt
return ¯xT
For stochastic convex functions satisfying Assumption 4.3, we establish the convergence rate of Algorithm 2 in the
following Theorem 4.4, whose proof is in Section D.
Assumption 4.3. Let f : Rd →R be a convex loss function. For all t ∈[T] and any x, E[∇ft(x)] = ∇f(x).
Theorem 4.4. For a well-structured preconditioner set H, let f be a convex loss function whose H-smoothness constant
is ΛH (f) ∈(0, ∞) according to Definition 2.4. For ϵ > 0, T > 0, consider Algorithm 2 with αt = 2/(t + 2) for
t = 0, 1, . . . , T −1. Suppose x∗is the global minima and maxt=0,1,...,T −1 ∥xt −x∗∥H ≤D for some D > 0 and
Assumption 4.3 holds with adaptive gradient variance σH({ft}T
t=1)2 ≤σ2
H for some σH ∈[0, ∞). Then it holds that
E[f(¯xT ) −f(x∗)] ≤
2D2ϵ
η(T + 1)2 E Tr(V −1
T −1) +
D2
2η −η
2

E
4
(T + 1)2
T −1
X
t=0
g⊤
t V −1
t
gt
+
2η2
(T + 1)2 · ΛH (f) · ˜O(log2 d) +
η
T 1/2 σH · ˜O(log d).
Moreover, when choosing learning rate η = D, the convergence rate becomes
E[f(¯xT ) −f(x∗)] = ˜O
ΛH (f) D2 log2 d + d√ϵD
T 2
+ σHD log d
√
T

.
Remark 4.5. A drawback of Algorithm 2 is that the optimal choice of learning rate η in Theorem 4.4 depends on the
unknown parameter D = maxt ∥xt −x∗∥H. To circumvent this issue, we follow the approach in Kovalev (2025a)
to introduce a projected variant (see Algorithm 8 and discussion in Section D.2), which ensures that all iterates
remain inside a H-ball of radius D. The removes the requirement for prior knowledge of D, and we establish a same
convergence rate in Theorem D.5.
The analysis in Kovalev (2025a) yields similar results as ours when H contains only diagonal matrices. However,
their Assumption 4, which is critical for their analysis, imposes restrictive conditions on both the loss and the gradient
noise for more general H. By contrast, our approach avoids such requirements by leveraging Lemma 3.3.
Our convergence guarantee attains a deterministic component of order ˜O(ΛH (f) D2/T 2), an accelerated rate
governed by the adaptive smoothness ΛH (f). In comparison, Guzmán & Nemirovski (2015) shows that any first
order optimizer can only achieve Ω(
L∥·∥∞(f)
T log T ) for the specific case of ℓ∞norm smoothness. Taken together, these
results show that the adaptive smoothness is necessary to achieve the acceleration, which can’t be replaced by the eaker
non-Euclidean smoothness, highlighting its algorithmic benefit and answering Question 2 in the affirmative.
4.3
Nonconvex Results for NSD under Adaptive Noise Assumption
The ineffectiveness of averaging in the dual space also leads to difficulty in reducing gradient variance via averaging
such as using momentum. To illustrate this, consider a mean-zero random vector x ∈Rd with E[∥x∥2
2] ≤σ2. Then
for i.i.d. copies of x denoted by x1, . . . , xn, we have E[∥1
n
Pn
i=1 xi∥2
2 ≤σ2
n while E[∥1
n
Pn
i=1 xi∥2
1] ≤d σ2
n , and the
extra d factor in the latter bound is tight in the worst case. This causes the dimension-dependent bound on convergence
rate of NSD for in the nonconvex setting, as shown in recent works (Pethick et al., 2025; Kovalev, 2025b).
10

4.3.1
Adaptive gradient variance enables dimension-free rate
In particular, the extra dimension-dependent constant in the bound in Pethick et al. (2025); Kovalev (2025b), ρ =
supx
∥x∥H,∗
∥x∥2 , reflects the mismatch between ∥·∥H,∗and ∥· ∥2. For instance, when H is the set of all the diagonal
PSD matrices, NSD becomes SignGD and ∥·∥H,∗= ∥·∥1, in which case ρ can scale as Θ(
√
d), leading to vacuous
guarantees when T ≪d. Our analysis eliminates this issue by adopting the adaptive variance (Definition 4.1). For
completeness, we restate NSD in Algorithm 3, and provide its convergence rate in Theorem 4.6. The proof is deferred
to Section E.1.
The concurrent work Kovalev & Borodich (2025) also leveraged the adaptive variance assumption to prove a
dimension-free nonconvex convergence rate of NSD. However, they used a smoothness metric similar to adaptive
smoothness while our Theorem 4.6 uses the standard smoothness. Therefore, our rate is strictly better than Kovalev &
Borodich (2025) because of the relationship between standard smoothness and adaptive smoothness.
Algorithm 3 Normalized steepest descent with momentum
Hyperparam: ϵ ≥0, total steps T, learning rate η, norm ∥· ∥on Rd, averaging parameter α
Input: initialization x0, initialization m0, stochastic loss functions {ft}T
t=1 : Rd →R
for t = 0, 1, · · · , T −1 :
gt ←∇ft(xt)
mt ←(1 −α)mt−1 + αgt
ut ←arg max∥u∥≤1 ⟨mt, u⟩
xt+1 ←xt −ηut
return xT
Theorem 4.6. Let H be a well-structured preconditioner set. For any ϵ ≥0, α ∈(0, 1), η > 0, and T ∈N, let
{xt}T
t=0 be the iterates of Algorithm 3 with ∥· ∥= ∥· ∥H and m0 = ∇f0(x0). Let L∥·∥H(f) be the smoothness
of the loss f w.r.t. ∥·∥H according to Definition 2.3. Suppose Assumption 4.3 holds with adaptive gradient variance
σH({ft}T
t=1)2 ≤σ2
H for some σH ∈[0, ∞). Then it holds that
E 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗≤∆0
ηT + 2η
α L∥·∥(f) + 2σH
αT + 2σH
√α.
Let a0 =
q
∆0L∥·∥(f)/σH. If a0 < 1, then
• When T < a−6
0 , we will choose α = T −2/3 and η =
q
∆0
L∥·∥H(f)T −5/12. The rate is O
 σHT −1/3
.
• When T ≥a−6
0 , we will choose α =
a0
√
T and η =
∆3/4
0
L∥·∥H(f)1/4σ1/2
H T −3/4. The rate is O

(∆0L∥·∥H(f))1/4√σH
T 1/4

.
If a0 ≥1, then
• When T ≤a2
0, we will choose α = 1 and η =
q
∆0
L∥·∥H(f)T −1/2. The rate is O
q
∆0L∥·∥H(f)T −1/2
.
• When T ≥a2
0, we will choose α =
a0
√
T and η =
∆3/4
0
L∥·∥H(f)1/4σ1/2
H T −3/4. The rate is O

(∆0L∥·∥H(f))1/4√σH
T 1/4

.
Remark 4.7. When there is no noise, i.e., σH = 0, we have that a0 = ∞. So we are in the regime T ≤a2
0 and we will
choose α = 1 to achieve O
q
∆0L∥·∥H(f)T −1/2
rate, which recovers the standard result in deterministic case.
Theorem 4.6 shows that, with appropriate choices of the learning rate η and averaging parameter α, NSD achieves
a dimension-free rate depending only on the standard smoothness L∥·∥H(f) and the adaptive variance σH, thereby
avoiding the unfavorable ρ factor. However, next we will show that such a dimension-free upper bound is unattainable
under the standard gradient variance assumption.
11

4.3.2
Standard gradient variance yields dimension-dependent rate
We first present the upper bound on the convergence rate of NSD under the standard gradient variance assumption.
The proof is in Section E.2.
Theorem 4.8. For any ϵ ≥0, α ∈(0, 1), η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 3 with any norm
∥·∥. Suppose Assumption 4.3 holds with the gradient variance σ∥·∥∗({ft}T
t=1)2 ≤σ2
∥·∥∗for some σ∥·∥∗∈[0, ∞). Then
it holds that
E 1
T
T −1
X
t=0
∥∇f(xt)∥∗≤∆0
ηT + 2η
α L∥·∥(f) + 2
αT σ∥·∥∗+ 2σ∥·∥∗· min

1, α1/2ψ(∥· ∥∗, ∥· ∥2)

where ψ(∥· ∥∗, ∥· ∥2) = supx
∥x∥∗
∥x∥2 · supx
∥x∥2
∥x∥∗measures the distortion between the two norms.
Here the norm distortion ψ(∥· ∥∗, ∥· ∥2) can grow with the dimension d: For example, in the case of ∥·∥= ∥·∥∞
and ∥·∥∗= ∥·∥1, we have ψ(∥·∥1 , ∥·∥2) =
√
d. Consequently, in such a case Theorem 4.8 gives two kinds of upper
bound for the convergence rate of NSD:
• When T is small (e.g. T < d), the constant term σ∥·∥∗dominates.
• For sufficiently large T and small α (for which the four terms are balanced), the last term in the upper bound
becomes 2σ∥·∥∗α1/2ψ(∥· ∥1, ∥· ∥2), which depends on d.
Moreover, such a dependence on d is inevitable in the worst case, and we prove the corresponding lower bound in
Theorem 4.9. The proofs are deferred to Section E.3.
Theorem 4.9. For any fixed ∆0, L, σ2, d, T, learning rate η, and any averaging parameter α, there exists a loss
function f : Rd →R, a sequence of stochastic iid loss functions f0, f1, · · · , fT −1 and an initialization x0 satisfying
the following conditions
(a) f(x0) −infx f(x) = ∆0 and L∥·∥∞(f) ≤L;
(b) For any x ∈Rd, it holds that E[∇ft(x)] = ∇f(x) and E[∥∇ft(x) −∇f(x)∥2
1] ≤σ2.
When running Algorithm 3 with ∥· ∥= ∥· ∥∞, learning rate η, averaging parameter α and initialization x0 = 0 on
the stochastic functions f0, · · · , fT −1, it holds that
E
h
min
t∈[T ] ∥∇f(xt)∥1
i
= min{e−25−1
4 (dL∆0σ2)
1
4 T −1
2 , e−25−1
2 σ}
Corresponding to Theorem 4.8, here Theorem 4.9 also shows two kinds of lower bound we can achieve on signGD
with momentum:
• When T is not large enough, we can achieve the lower bound Ω(σ), which shows the hardness induced by the
stochasticity and matches the first upper bound in Theorem 4.8.
• On the other hand, if we want to achieve the error ϵ < e−25−1
2 σ, we require the number of steps T =
Ω(ϵ−2(dL∆0σ2)
1
2 ), whose dependence on dimension d is Ω(d
1
2 ).
In conclusion, we see that under the standard gradient variance assumption for ∥· ∥= ∥· ∥∞and ∥· ∥∗= ∥· ∥1,
the d-dependent convergence rate in Theorem 4.8 is inevitable. While we have seen in Theorem 4.6 that there is no
dimension dependence under the adaptive gradient variance assumption, thus illustrating a fundamental gap.
5
Related Work
Adaptive optimizers.
Matrix-valued preconditioning methods such as Shampoo (Gupta et al., 2018) provide
structure-aware updates and a growing body of work refines their preconditioners and practice (e.g., improved analy-
ses/implementations and Adam–Shampoo hybrids) (Morwani et al., 2024; Vyas et al., 2024; Lau et al., 2025; Si et al.,
2025). Variants like one-sided Shampoo/ASGO achieve stronger convex rates than earlier methods (Xie et al., 2025b;
An et al., 2025), but the nonconvex guarantees are missing which we show in Section 3. The recent work Frans et al.
(2025) views adaptive optimizers as a modified matrix whitening technique and points out it can outperform accurate
spectral normalization (NSD in our context). They highlight the variance adaptation is previously overlooked while
we also want to understand the benefit of adaptive optimizers/geometry.
12

NSD.
Cutkosky & Mehta (2020) proves the convergence rate O(1/T 3.5) of normalized gradient descent with mo-
mentum on nonconvex functions. Pethick et al. (2025); Kovalev (2025b) extend the results to any normalized steepest
descent while the smoothness metric becomes smoothness w.r.t. the specific norm NSD uses. Many works analyze
popular optimizers by showing the equivalence to NSD under some norm, e.g. Lion is NSD under ℓ∞-norm (Sfyraki
& Wang, 2025) and Muon is NSD under spectral norm (Chen et al., 2025), and thus apply the classic optimization
results. Jiang et al. (2025) achieves a similar result as our Theorem 4.6 but their noise assumption is stronger than ours
and only states the result for specific SignGD. The difference between the two kinds of smoothness is also discussed
in Balles et al. (2020) in the context for SignGD only.
Acceleration.
Nesterov acceleration can be viewed as the linear coupling between gradient descent and mirror
descent (Kelner et al., 2014; Allen-Zhu & Orecchia, 2014; Cheng et al., 2018). Through this lens, Cutkosky (2019);
Kavis et al. (2019); Joulani et al. (2020); Ene et al. (2021) applied it on adaptive optimizers with specific structures
like diagonal/coordinate-wise and achieve deterministic O(1/T 2) convergence rate.
6
Conclusion
We extend the unified analysis of adaptive optimizers to nonconvex functions, establishing convergence rate depending
on the adaptive smoothness. It strengthens the comparison between smoothnesses that adaptive optimizers and NSD
use in the convex settings. We further show the benefit of adaptive smoothness by showing the accelerated rate of
adaptive optimizers with Nesterov momentum. The benefit of adaptive geometry is also justified by comparing two
kinds of noise.
References
Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent.
arXiv preprint arXiv:1407.1537, 2014. 13
Kang An, Yuxing Liu, Rui Pan, Yi Ren, Shiqian Ma, Donald Goldfarb, and Tong Zhang. Asgo: Adaptive structured
gradient optimization. arXiv preprint arXiv:2503.20762, 2025. 1, 6, 7, 9, 12
Lukas Balles, Fabian Pedregosa, and Nicolas Le Roux. The geometry of sign gradient descent. arXiv preprint
arXiv:2002.08056, 2020. 1, 3, 13
Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325,
2024. 1
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi
Du, Zhe Fu, et al.
Deepseek llm: Scaling open-source language models with longtermism.
arXiv preprint
arXiv:2401.02954, 2024. 1
Lizhang Chen, Jonathan Li, and Qiang Liu. Muon Optimizes Under Spectral Norm Constraints. arXiv preprint
arXiv:2506.15054, 2025. 13
Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang
Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675,
2023. 1
Xiang Cheng, Fred Roosta, Stefan Palombo, Peter Bartlett, and Michael Mahoney. FLAG n’FLARE: Fast Linearly-
Coupled Adaptive Gradient Methods. In International Conference on Artificial Intelligence and Statistics, pp.
404–414. PMLR, 2018. 13
Sinho Chewi, Sébastien Bubeck, and Adil Salim. On the complexity of finding stationary points of smooth functions
in one dimension. In International Conference on Algorithmic Learning Theory, pp. 358–374. PMLR, 2023. 45
Ashok Cutkosky.
Anytime online-to-batch, optimism and acceleration.
In International conference on machine
learning, pp. 1446–1454. PMLR, 2019. 13
13

Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International conference on machine
learning. PMLR, 2020. 13
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil
Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv–2407,
2024. 1
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic opti-
mization. Journal of machine learning research, 2011. 6
Alina Ene, Huy L Nguyen, and Adrian Vladu. Adaptive gradient methods for constrained convex optimization and
variational inequalities. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. 13
Kevin Frans, Pieter Abbeel, and Sergey Levine. What Really Matters in Matrix-Whitening Optimizers? arXiv preprint
arXiv:2510.25000, 2025. 12
Vineet Gupta, Tomer Koren, and Yoram Singer. A unified approach to adaptive regularization in online and stochastic
optimization. arXiv preprint arXiv:1706.06569, 2017. 5, 38
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In Interna-
tional Conference on Machine Learning, 2018. 12
Cristóbal Guzmán and Arkadi Nemirovski. On lower complexity bounds for large-scale smooth convex optimization.
Journal of Complexity, 31(1):1–14, 2015. 2, 10
Wei Jiang, Dingzhi Yu, Sifan Yang, Wenhao Yang, and Lijun Zhang. Improved Analysis for Sign-based Methods with
Momentum Updates. arXiv preprint arXiv:2507.12091, 2025. 13
Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon:
An optimizer for hidden layers in neural networks, 2024. 1
Pooria Joulani, Anant Raj, Andras Gyorgy, and Csaba Szepesvari. A simpler approach to accelerated optimization:
iterative averaging meets optimism. In International conference on machine learning. PMLR, 2020. 13
Ali Kavis, Kfir Y Levy, Francis Bach, and Volkan Cevher. Unixgrad: A universal, adaptive algorithm with optimal
guarantees for constrained optimization. Advances in neural information processing systems, 32, 2019. 13
Jonathan A Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-time algorithm for approximate
max flow in undirected graphs, and its multicommodity generalizations. In Proceedings of the twenty-fifth annual
ACM-SIAM symposium on Discrete algorithms, pp. 217–226. SIAM, 2014. 13
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 6
Dmitry Kovalev. SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration. arXiv preprint
arXiv:2506.23803, 2025a. 7, 9, 10, 34
Dmitry Kovalev. Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimiza-
tion. arXiv preprint arXiv:2503.12645, 2025b. 10, 11, 13, 41
Dmitry Kovalev and Ekaterina Borodich. On linear convergence in smooth convex-concave bilinearly-coupled saddle-
point optimization: Lower bounds and optimal algorithms. arXiv preprint arXiv:2411.14601, 2024. 9
Dmitry Kovalev and Ekaterina Borodich. Non-Euclidean SGD for Structured Optimization: Unified Analysis and
Improved Rates. arXiv preprint arXiv:2511.11466, 2025. 11
Tim Tsz-Kit Lau, Qi Long, and Weijie Su. PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying
Preconditioning Perspective. arXiv preprint arXiv:2505.21799, 2025. 12
Huan Li, Yiming Dong, and Zhouchen Lin. On the O(
√
d
K1/4 ) Convergence Rate of AdamW Measured by ℓ1 Norm.
arXiv preprint arXiv:2505.11840, 2025. 33
14

Elliott H. Lieb. Convex trace functions and the wigner-yanase-dyson conjecture. Advances in Mathematics, 11(3):
267–288, 1973. 18
Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu,
Junjie Yan, et al. Muon is scalable for LLM training. arXiv preprint arXiv:2502.16982, 2025. 1
Devyani Maladkar, Ruichen Jiang, and Aryan Mokhtari. Convergence Analysis of Adaptive Gradient Methods under
Refined Smoothness and Noise Assumptions. arXiv preprint arXiv:2406.04592, 2024. 1, 3
Depen Morwani, Itai Shapira, Nikhil Vyas, Eran Malach, Sham Kakade, and Lucas Janson. A New Perspective on
Shampoo’s Preconditioner. arXiv preprint arXiv:2406.17748, 2024. 12
Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher.
Training deep learning models with norm-constrained lmos. arXiv preprint arXiv:2502.07529, 2025. 1, 7, 10, 11,
13
Maria-Eleni Sfyraki and Jun-Kun Wang. Lions and muons: Optimization via stochastic frank-wolfe. arXiv preprint
arXiv:2506.04192, 2025. 13
Ishaan Shah, Anthony M Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma,
Anil Thomas, Ashish Tanwer, Darsh J Shah, et al. Practical efficiency of muon for pretraining. arXiv preprint
arXiv:2505.02222, 2025. 1
Chongjie Si, Debing Zhang, and Wei Shen. Adamuon: Adaptive muon optimizer. arXiv preprint arXiv:2507.11005,
2025. 12
Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun
Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. 1
Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham Kakade.
Soap: Improving and stabilizing shampoo using adam. arXiv preprint arXiv:2409.11321, 2024. 12
Jiaxuan Wang and Jenna Wiens. AdaSGD: Bridging the gap between SGD and Adam. arXiv preprint arXiv:2006.16541,
2020. 6
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
Journal of Machine Learning Research, 2020. 6
Kaiyue Wen, David Hall, Tengyu Ma, and Percy Liang. Fantastic Pretraining Optimizers and Where to Find Them.
arXiv preprint arXiv:2509.02046, 2025. 1
Shuo Xie and Zhiyuan Li.
Implicit Bias of AdamW: ℓ∞Norm Constrained Optimization.
arXiv preprint
arXiv:2404.04454, 2024. 1
Shuo Xie, Mohamad Amin Mohamadi, and Zhiyuan Li.
Adam Exploits ℓ∞-geometry of Loss Landscape via
Coordinate-wise Adaptivity. arXiv preprint arXiv:2410.08198, 2024. 3
Shuo Xie, Mohamad Amin Mohamadi, and Zhiyuan Li. Adam Exploits $\ell_\infty$-geometry of Loss Landscape via
Coordinate-wise Adaptivity. In The Thirteenth International Conference on Learning Representations, 2025a. 1, 3,
6, 7, 29, 33
Shuo Xie, Tianhao Wang, Sashank J. Reddi, Sanjiv Kumar, and Zhiyuan Li. Structured Preconditioners in Adaptive
Optimization: A Unified Analysis. In Forty-second International Conference on Machine Learning, 2025b. 1, 4, 5,
6, 7, 9, 12, 16, 17, 19, 20
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen
Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 1
Chenyang Zhang, Difan Zou, and Yuan Cao. The implicit bias of adam on separable data. Advances in Neural
Information Processing Systems, 37:23988–24021, 2024. 1
15

Contents
1
Introduction
1
1.1
Notations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
2
From Adam/SignGD to Adaptive Smoothness
3
2.1
Adam and SignGD can exploit ℓ∞geometry, but in different ways
. . . . . . . . . . . . . . . . . . .
3
2.2
Adaptive smoothness associated with well-structured preconditioner sets . . . . . . . . . . . . . . . .
4
3
Unified Analysis in the Nonconvex Setting
5
3.1
Adaptive optimizers with well-structured preconditioner sets . . . . . . . . . . . . . . . . . . . . . .
5
3.2
Convergence rate in the deterministic nonconvex setting . . . . . . . . . . . . . . . . . . . . . . . . .
6
3.3
Technical Contribution: A Novel Matrix Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
4
Benefit of Adaptive Geometry
8
4.1
Adaptive Variance: An Analogue of Adaptive Smoothness . . . . . . . . . . . . . . . . . . . . . . .
9
4.2
Acceleration on Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
4.3
Nonconvex Results for NSD under Adaptive Noise Assumption . . . . . . . . . . . . . . . . . . . . .
10
4.3.1
Adaptive gradient variance enables dimension-free rate . . . . . . . . . . . . . . . . . . . . .
11
4.3.2
Standard gradient variance yields dimension-dependent rate . . . . . . . . . . . . . . . . . .
12
5
Related Work
12
6
Conclusion
13
A Auxiliary Results
16
A.1
Comparison on adaptive geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
B
Proof for the matrix inequality
20
C Unified proof for adaptive algorithms
26
C.1
Relationship between adaptive algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
C.2
Proof for weighted algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
C.3
Proof for the cumulative and EMA variants
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
D Proof for the accelerated algorithm
34
D.1
Stochastic case without projection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
D.2
Stochastic case with projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
E
Proof for normalized steepest descent
40
E.1
Convergence rate with respect to adaptive gradient variance . . . . . . . . . . . . . . . . . . . . . . .
41
E.2
Convergence rate under general-norm assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
E.3
Dimension dependent lower bound for NSD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
A
Auxiliary Results
Recall that we define PH(M) = arg minH∈H

M, H−1
+ Tr(H) for any M ∈Sd
++ in Section 2. In this section,
we will prove several important properties of PH which will be useful in proving the convergence rate for general
well-structured H.
It will help interpret these results when keeping in mind that PH(M) = M 1/2 when H is the PSD matrix cone,
i.e., there is no constraint on the structure.
We will first state Lemma A.2 in Xie et al. (2025b), which will be used a lot in our proof.
Lemma A.1 (Lemma A.2 in Xie et al. (2025b)). For any A, B ∈H, if ⟨A, H⟩= ⟨B, H⟩holds for all H ∈H, then
it must be A = B.
16

Lemma A.2. For any M ∈Sd
++ and any H ∈H, λ ≥0, it always holds that ⟨M, H⟩=

PH(M)2, H

and
PH(M + λId)2 = PH(M)2 + λId.
Proof. By property (c) in Proposition A.3 in Xie et al. (2025b), we know that for any M ≻0,
⟨M −PH(M)2, PH(M)−1HPH(M)−1 −PH(M)−1⟩= 0.
Note that H 7→PH(M)−1HPH(M)−1 is a bijection from H to H by lemma A.1 in Xie et al. (2025b), so we have
⟨M −PH(M)2, H −PH(M)−1⟩= 0 for all H ∈H. H 7→H −PH(M)−1 is also a bijection from H to H. So
we have that ⟨M −PH(M)2, H⟩= 0 for all H ∈H.
For any λ ≥0 and any H ∈H, we have that

PH(M + λId)2, H

= ⟨M + λId, H⟩
= ⟨M, H⟩+ ⟨λId, H⟩
=

PH(M)2, H

+ ⟨λId, H⟩
=

PH(M)2 + λId, H

.
Note that PH(M + λId)2 and PH(M)2 + λId are both in H. We conclude PH(M + λId)2 = PH(M)2 + λId
according to Lemma A.1.
Lemma A.3. For any M1, M2 ∈Sd
++, PH(M1 + M2)2 = PH(M1)2 + PH(M2)2. For any c ≥0, PH(cM)2 =
cPH(M)2. For any 0 ⪯A ⪯B, it always holds PH(A) ⪯PH(B).
Proof. For any H ∈H, we have that

PH(M1 + M2)2, H

= ⟨M1 + M2, H⟩
= ⟨M1, H⟩+ ⟨M2, H⟩
=

PH(M1)2, H

+

PH(M2)2, H

=

PH(M1)2 + PH(M2)2, H

.
Then we have that PH(M1 + M2)2 = PH(M1)2 + PH(M2)2 from Lemma A.1. For any c ≥0, it holds that

PH(cM)2, H

= ⟨cM, H⟩
= c ⟨M, H⟩
= c

PH(M)2, H

=

cPH(M)2, H

.
So it also holds that PH(cM)2 = cPH(M)2 from Lemma A.1.
Moreover, when 0 ⪯A ⪯B, PH(B)2 = PH(A)2 +PH(B −A)2 ⪰PH(A)2. Then PH(B) ⪰PH(A) because
X1/2 is a monotone function.
Lemma A.4. If we define projH(M) = arg minH∈H ∥M −H∥2
F, then it holds that projH(M) = PH(M)2.
Proof. We define dM(H) = ∥M −H∥2
F = Tr((M −H)(M −H)⊤). Then ∇HdM(H) = 2(H −M). Because
H is a cone, by the optimality of projH(M), it always holds for any H ∈H that
0 = ⟨∇HdM(projH(M)), H −projH(M)⟩= 2 ⟨projH(M) −M, H −projH(M)⟩.
For any H′ ∈H, we can always choose H = H′ + projH(M). H ∈H because H is closed under matrix addition.
Then it always holds that ⟨projH(M) −M, H′⟩= 0. On the other hand, Lemma A.2 shows it always holds that

PH(M)2 −M, H′
= 0 for any H′ ∈H. Then we can get projH(M) = PH(M)2 by Lemma A.1.
Lemma A.5. For any A, B ∈Sd
++ and any well-structured preconditioner set H,
∥PH(B) −PH(A)∥op ≤∥B −A∥
1
2
op
17

Proof. We have that
PH(B) = PH(A + B −A) ⪯PH(A + ∥B −A∥op I)
=

PH(A)2 + ∥B −A∥op I
 1
2 ⪯PH(A) + ∥B −A∥
1
2
op I
The last step is by considering
q
λ2
i + ∥B −A∥op ≤λi+
q
∥B −A∥op for every eigenvalue λi of PH(A). Similarly
we can also show PH(A) ⪯PH(B) + ∥B −A∥
1
2
op I, which finishes the proof.
Lemma A.6. Tr[PH(X)] is a concave function.
Proof. For any psd matrices A and B, we have that
2 Tr

PH(A + B
2
)

= min
H∈H
A + B
2
, H−1

+ Tr(H)

= min
H∈H
"
A, H−1
+ Tr(H)
2
+

B, H−1
+ Tr(H)
2
#
≥min
H∈H

A, H−1
+ Tr(H)
2
+ min
H∈H

B, H−1
+ Tr(H)
2
= Tr[PH(A)] + Tr[PH(B)].
Lemma A.7. Let A ⪰0 and x, y ∈Rd. It holds
PH(A + xx⊤) −PH(A + yy⊤)

op ≤
√
2 ∥x −y∥2.
Proof. Set F(X, Y ) := tr
 √
X
√
Y

for X, Y ⪰0. By Lieb’s concavity theorem Lieb (1973), F is jointly concave
and positively homogeneous, hence superadditive:
F(X1+X2, Y1+Y2) ≥F(X1, Y1) + F(X2, Y2).
Define ΠH(H) = PH(H)2. Then ΠH is the pinching onto a unital ∗-subalgebra H.
For any A, P , Q ⪰0,
∥PH(A+P ) −PH(A+Q)∥2
F = tr(A+P ) + tr(A+Q) −2 tr
 PH(A+P )PH(A+Q)

.
Using PH(Z)2 = ΠH(Z), the cross term equals F(ΠH(A)+ΠH(P ), ΠH(A)+ΠH(Q)), which by superadditivity
is at least F(ΠH(A), ΠH(A)) + F(ΠH(P ), ΠH(Q)) = tr A + tr
 PH(P )PH(Q)

. Hence
∥PH(A+P ) −PH(A+Q)∥F ≤∥PH(P ) −PH(Q)∥F.
(10)
Write r = ∥x∥2, s = ∥y∥2, u = x/r, v = y/s, and cos θ = u⊤v. For the unstructured square root,
√
xx⊤−
p
yy⊤2
F = r2 + s2 −2rs cos2 θ ≤2
 r2 + s2 −2rs cos θ

= 2∥x −y∥2
2.
Pinching monotonicity of F yields
∥PH(xx⊤) −PH(yy⊤)∥2
F = ∥
q
ΠH(xx⊤) −
q
ΠH(yy⊤)∥2
F ≤∥
√
xx⊤−
p
yy⊤∥2
F ≤2∥x −y∥2
2.
Apply Eq. (10) with P = xx⊤, Q = yy⊤,
∥PH(A+xx⊤) −PH(A+yy⊤)∥op ≤∥PH(xx⊤) −PH(yy⊤)∥F ≤
√
2 ∥x −y∥2.
18

Lemma A.8. For any well-structured preconditioner set H and any vector x ∈Rd, it holds that
∥x∥H,∗= Tr[PH(xx⊤)].
Proof. We have that
Tr[PH(xx⊤)] = 1
2 min
H∈H x⊤H−1x + Tr(H)
= 1
2
min
H∈H,Tr(H)≤1 min
c>0 c−1x⊤H−1x + c Tr(H)
=
min
H∈H,Tr(H)≤1
√
x⊤H−1x.
Xie et al. (2025b) shows that ∥x∥H,∗= minH∈H,Tr(H)≤1
√
x⊤H−1x in their proof of lemma 3.3, which finishes the
proof.
Lemma A.9.
PH(gtg⊤
t )−1gt = arg min
∥x∥H≤1
⟨x, gt⟩
Proof. First we show that it satisfies that
PH(gtg⊤
t )−1gt

H ≤1. For any H ∈H and Tr(H) ≤1, we have that
PH(gtg⊤
t )−1gt
2
H = g⊤
t PH(gtg⊤
t )−1HPH(gtg⊤
t )−1gt
= Tr(PH(gtg⊤
t )−1HPH(gtg⊤
t )−1gtg⊤
t )
= Tr(PH(gtg⊤
t )−1HPH(gtg⊤
t )−1PH(gtg⊤
t )2)
= Tr(H) ≤1
Then
PH(gtg⊤
t )−1gt

H = supH∈H,Tr(H)≤1
PH(gtg⊤
t )−1gt

H ≤1.
Next, we can employ Lemma A.8 to show that

PH(gtg⊤
t )−1gt, gt

= Tr(PH(gtg⊤
t )−1gtg⊤
t ) = Tr(PH(gtg⊤
t )) = ∥gt∥H,∗.
Because min∥x∥H≤1 ⟨x, gt⟩= ∥gt∥H,∗and there is only one unique vector achieving this optimality, we finish showing
the statement.
Proposition A.10. For any well-structured preconditioner set H, let σH({ft}t∈T ) be adaptive gradient variance
defined in Definition 4.1. It always hold σH({ft}t∈T ) ≤Tr(PH(Σ)) when we assume E[∇f(x) −∇ft(x)][∇f(x) −
∇ft(x)]⊤⪯Σ for any t ∈T .
Proof of Proposition A.10. From Definition 4.1, we can have that
σH({ft}t∈T )2 =
min
H∈H,Tr(H)≤1 sup
x,t E ∥∇f(x) −∇ft(x∥2
H−1
=
min
H∈H,Tr(H)≤1 sup
x,t E

[∇f(x) −∇ft(x)][∇f(x) −∇ft(x)]⊤, H−1
≤
min
H∈H,Tr(H)≤1

Σ, H−1
.
By the definition of PH(Σ), we know that

Σ, PH(Σ)−1
= Tr(PH(Σ)) and
Tr(PH(Σ)) = 1
2 min
H∈H

Σ, H−1
+ Tr(H)
≥min
H∈H
p
⟨Σ, H−1⟩Tr(H)
= min
H∈H
p
⟨Σ, (H/ Tr(H))−1⟩
=
min
H∈H,Tr(H)≤1
p
⟨Σ, H−1⟩
≥σH({ft}t∈T ),
which finishes the proof.
19

A.1
Comparison on adaptive geometry
Proposition 2.5. For any well-structured preconditioner set H ⊆Sd
+ and any loss function f : Rd →R, it always
holds that L∥·∥H(f) ≤ΛH (f) ≤d · L∥·∥H(f).
Proof of Proposition 2.5. For any H ∈H with Tr(H) ≤1, by the definition of ∥·∥H and ∥·∥H,∗, it holds for any
x ∈Rd that ∥x∥H ≥∥x∥H and ∥x∥H,∗≤∥x∥H,∗. Then by the definition of L∥·∥(f) in Definition 2.4,
L∥·∥H(f) = sup
x,y
∥∇f(x) −∇f(y)∥H,∗
∥x −y∥H
≤sup
x,y
∥∇f(x) −∇f(y)∥H,∗
∥x −y∥H
= L∥·∥H(f).
Therefore, further minimizing both sides over H ∈H with Tr(H) ≤1, we obtain the desired result.
On the other hand, we can choose H∗= 1
dId ∈H and Xie et al. (2025b) shows that ∥·∥H∗=
1
√
d ∥·∥2. Then we
have that ΛH (f) ≤L∥·∥H∗(f).
We can alsochooseH∗tobeallPSDmatricessothatweknowH ⊆H∗. Thenweknow∥x∥H = supH∈H,Tr(H)≤1 ≤
supH∈H∗,Tr(H)≤1 ∥x∥H = ∥x∥H∗and ∥x∥H,∗= infH∈H,Tr(H)≤1 ∥x∥H−1 ≥infH∈H∗,Tr(H)≤1 = ∥x∥H∗,∗. Xie
et al. (2025b) also shows that ∥x∥H∗= ∥x∥H∗,∗= ∥x∥2. So we have that
L∥·∥H(f) = sup
x,y
∥∇f(x) −∇f(y)∥H,∗
∥x −y∥H
≥sup
x,y
∥∇f(x) −∇f(y)∥2
∥x −y∥2
= 1
d sup
x,y
√
d ∥∇f(x) −∇f(y)∥2
1
√
d ∥x −y∥2
= 1
d sup
x,y
∥∇f(x) −∇f(y)∥H∗−1
∥x −y∥H∗
≥1
dΛH (f) .
Proposition 4.2. For any set of stochastic loss functions {ft}t∈T and any well-structured preconditioner set H, it
always holds σ∥·∥H,∗({ft}t∈T )2 ≤σH({ft}t∈T )2 ≤d · σ∥·∥H,∗({ft}t∈T )2.
Proof. For any H ∈H with Tr(H) ≤1, it always holds that ∥u∥2
H,∗≤∥u∥2
H−1 for any u ∈Rd, and thus
sup
t∈T ,x∈Rd E
∇ft(x) −E[∇ft(x)]
2
H,∗

≤
sup
t∈T ,x∈Rd E
∇ft(x) −E[∇ft(x)]
2
H−1

.
By minimizing over H ∈H with Tr(H) ≤1, we conclude that σ∥·∥H,∗({ft}t∈T )2 ≤σH({ft}t∈T )2.
Similar with the proof of Proposition 2.5, we can show that σH({ft}t∈T )2 ≤supx,t∈[T ] Ed ∥∇f(x) −∇ft(x)∥2
2
when choosingH∗= 1
dId . Wecanalsoshowthatsupt∈T ,x∈Rd E
∇ft(x)−E[∇ft(x)]
2
H,∗≥supt∈T ,x∈Rd E
∇ft(x)−
E[∇ft(x)]
2
2. Then combining them, we have that d supt∈T ,x∈Rd E
∇ft(x)−E[∇ft(x)]
2
H,∗≥σH({ft}t∈T )2.
B
Proof for the matrix inequality
Lemma 3.3. Let ϵ ≥0 and β ∈(0, 1]. For any T ∈N, consider any sequence of vectors g0, . . . , gT −1 ∈Rd. Let
M−1 = 0, and recursively define Mt = βMt−1 +gtg⊤
t for t = 0, . . . , T −1. For any well-structured preconditioner
set H, define Vt = arg minH∈H⟨Mt + ϵId, H−1⟩+ Tr(H) for each t ∈[T −1].
Then for any H ∈H ∩Sd
++,
T −1
X
t=0
∥V −1
t
gt∥2
H ≤Tr(H) ∥ST ∥op
where ST =
T −1
X
t=0
V −1
t
 V 2
t −βV 2
t−1

V −1
t
.
20

Moreover, there exists an absolute constant C1, C2 > 0, independent of d, T, ϵ, β and H, such that
∥ST ∥op ≤C1
 
1 + log

1 + d
ϵ
T −1
X
t=0
∥gt∥2
2 + d2(1 −β)T
! 1 −β
β
T + log ∥V 2
T −1/ϵ∥op

+ C2.
In the special case when H is commutative, the above bound can be further improved to
∥ST ∥op ≤(1 −β)T + log ∥V 2
T −1/ϵ∥op.
Proof of Lemma 3.3. For each t = 0, . . . , T −1, by the definition of {Mt}T −1
t=0 , we have
V −1
t
gt
2
H =

V −1
t
HV −1
t
, gtg⊤
t

=

V −1
t
HV −1
t
, Mt + ϵId

−β

V −1
t
HV −1
t
, Mt−1 + ϵId

.
Since V −1
t
HV −1
t
∈H, it follows from Lemma A.2 and the definition of {Vt}T −1
t=0 that
V −1
t
gt
2
H =

V −1
t
HV −1
t
, PH(Mt + ϵId)2
−β

V −1
t
HV −1
t
, PH(Mt−1 + ϵId)2
=

V −1
t
HV −1
t
, V 2
t

−β

V −1
t
HV −1
t
, V 2
t−1

=

H, V −1
t
 V 2
t −βV 2
t−1

V −1
t

Then summing over t = 0, . . . , T −1 gives
T −1
X
t=0
V −1
t
gt
2
H =
T −1
X
t=0

H, V −1
t
 V 2
t −βV 2
t−1

V −1
t

≤Tr(H)

T −1
X
t=0
V −1
t
 V 2
t −βV 2
t−1

V −1
t

op
.
This shows the first part of the lemma. We next bound the spectral norm of PT −1
t=0 V −1
t
(V 2
t −βV 2
t−1)V −1
t
.
Note that βV 2
t−1 = βPH(Mt−1 + ϵId)2 = PH(β(Mt−1 + ϵId))2 by Lemma A.3. Since Mt + ϵId ⪰β(Mt−1 +
ϵId), we further have V 2
t ⪰βV 2
t−1 ≻0 by Lemma A.3. Therefore, since λmin(V 2
t ) ≥ϵ, we can apply Lemma 3.4 to
get that for any C ≥c > 0,
V −1
t
(V 2
t −βV 2
t−1)V −1
t
⪯3(log C −log c)
π2
(log(V 2
t ) −log(βV 2
t−1)) +
12cd
π2ϵ2 + 12C−1d
π2

Tr(V 2
t −βV 2
t−1)Id.
= 3(log C −log c)
π2
log 1
β · Id + 3(log C −log c)
π2
(log(V 2
t ) −log(V 2
t−1))
+
12cd
π2ϵ2 + 12C−1d
π2

Tr[Mt + ϵId −β(Mt−1 + ϵId)]Id
= 3(log C −log c)
π2
log 1
β · Id + 3(log C −log c)
π2
(log(V 2
t ) −log(V 2
t−1))
+
12cd
π2ϵ2 + 12C−1d
π2

Tr[gtg⊤
t + (1 −β)ϵId]Id
where the first equality holds by Lemma A.2. Further summing over t = 0, . . . , T −1 gives
T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t
⪯3(log C −log c)
π2
T log 1
β · Id + 3(log C −log c)
π2
(log(V 2
T −1) −log(V 2
−1))
+
12cd
π2ϵ2 + 12C−1d
π2
 T −1
X
t=0
Tr[gtg⊤
t + (1 −β)ϵId]Id
= 3(log C −log c)
π2

T log 1
β · Id + log(V 2
T −1/ϵ)

+
12cd
π2ϵ2 + 12C−1d
π2

d(1 −β)ϵT +
T −1
X
t=0
∥gt∥2
2

Id
21

where the equality holds as V−1 = PH(ϵId) = √ϵId. Note that log(1/β) = log(1 + (1 −β)/β) ≤(1 −β)/β for any
β ∈(0, 1]. Then by triangle inequality for the spectral norm, we have

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤3(log C −log c)
π2
1 −β
β
T + log
V 2
T −1/ϵ

op

+
12cd
π2ϵ2 + 12C−1d
π2

d(1 −β)Tϵ +
T −1
X
t=0
∥gt∥2
2

.
(11)
In particular, we choose
c =
(1 −β)T/β + log
V 2
T −1/ϵ

op
4d(d(1 −β)Tϵ + PT −1
t=0 ∥gt∥2
2)
ϵ2,
C = max(ϵ2/c, c).
(12)
With this, the second term on the right-hand side of (11) satisfies that
12cd
π2ϵ2 + 12C−1d
π2

d(1 −β)Tϵ +
T −1
X
t=0
∥gt∥2
2

≤6
π2
1 −β
β
T + log ∥V 2
T −1/ϵ∥op

.
Plugging this back into (11), we obtain

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤6 + 3 log(C/c)
π2
1 −β
β
T + log ∥V 2
T −1/ϵ∥op

.
Moreover, by the choice of C, c in (12),
log C
c = log(max(ϵ2/c2, 1)) = 2 log
 
max
4d(d(1 −β)Tϵ + PT −1
t=0 ∥gt∥2
2)
(1 −β)T/β + log ∥V 2
T −1/ϵ∥op
, 1
!
.
For convenience, denote A = (1−β)T/β+log ∥V 2
T −1/ϵ∥op and B = 4d(d(1−β)Tϵ+PT −1
T =0 ∥gt∥2
2. Then combining
the above two displays, we get

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤6
π2 A + 3
π2 A log

max
B
A, 1

.
When A > 1, we always have max(B/A, 1) ≤1 + B, which gives rise to the following bound

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤6
π2
 1 + log(1 + B)

A.
When A < 1, using the fact that x log(1/x) ≤1/e, we have
A log

max
B
A, 1

≤A log(1 + B) −A log A ≤A log(1 + B) + 1/e.
In this case, the bound becomes

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤6
π2
h 1 + log(1 + B)

A + 1/e
i
.
Combining the above two case, we can conclude that there exists universal constants C1, C2 > 0 such that

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤C1
 1 + log(1 + B)

A + C2.
22

This completes the proof for general well-structured preconditioner set H.
In this special case where matrices in H are diagonal, we have that
T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t
= (1 −β)TId + β
T −1
X
t=0
(Id −V −1
t
V 2
t−1V −1
t
) ⪯(1 −β)TId + β
T −1
X
t=0
log(VtV −2
t−1Vt)
where the inequality follows from the fact that Id −A−1 ⪯log A for any A ⪰0. Further note that as Vt−1, Vt are
diagonal matrices, they commute with each other. Then using the fact that log(AB) = log A + log B when A and B
commute, we obtain
T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t
⪯(1 −β)TId + β
T −1
X
t=0
[log(V 2
t ) −log(V 2
t−1)]
= (1 −β)TId + log (V 2
T −1/ϵ).
Therefore, in this case it holds that

T −1
X
t=0
V −1
t
(V 2
t −βV 2
t−1)V −1
t

op
≤(1 −β)T + log ∥V 2
T −1/ϵ∥op.
This completes the proof.
Lemma 3.4. For any positive definite matrices X, Y ∈Rd×d such that Y ⪯X, it holds for any 0 ≤c ≤C that
X−1/2(X −Y )X−1/2 ⪯3(log C −log c)
π2
(log X −log Y ) +

12cd
π2λmin(X)2 + 12C−1d
π2

Tr(X −Y ) · Id.
Proof of Lemma 3.4. For any δ ∈(0, 1), since the matrix logarithm is operator concave, it holds that
log((1 −δ)X + δY ) ⪰(1 −δ) log X + δ log Y .
Reorganizing the above inequality yields that for all δ ∈(0, 1),
log X −log Y ⪰−log(X + δ(Y −X)) −log X
δ
.
Taking the limit δ →0, we obtain
log X −log Y ⪰∂log(X)[X −Y ].
The proof is completed by further applying Lemma B.1 with A = X −Y .
Lemma B.1. For any positive definite matrix X ∈Rd×d and any positive semi-definite matrix A ∈Rd×d, it holds for
any 0 ≤c ≤C that
X−1/2AX−1/2 ⪯3(log C −log c)
π2
∂log(X)[A] +

12cd
π2λmin(X)2 + 12C−1d
π2

Tr(A) · Id.
(13)
Proof of Lemma B.1. We consider the following expansion of the matrix logarithm:
log(X + δA) = log X +
Z ∞
0
(X + zI)−1 · δA · (X + zI)−1dz + O(δ2).
This implies that
∂log(X)[A] = lim
δ→0
log(X + δA) −log X
δ
=
Z ∞
0
I
X + zI · A ·
I
X + zI dz.
23

Let v ∈Rd be an arbitrary unit vector. Note that both X−1/2AX−1/2 and ∂log(X)[A] are linear in A, and thus
we first consider A = uu⊤for any unit vector u ∈Rd. We define the following two quantities
f(u, v) = v⊤∂log(X)[uu⊤]v = v⊤
Z ∞
0
(X + zId)−1uu⊤(X + zId)−1dzv,
(14)
g(u, v) = v⊤X−1/2uu⊤X−1/2v.
(15)
Now suppose that the SVD of X is X = Udiag(λ1, . . . , λd)U ⊤where U ∈Rd×d is an orthogonal matrix. Then
X−1/2 = Udiag(λ−1/2
1
, . . . , λ−1/2
d
)U ⊤and (X + zI)−1 = Udiag(1/(λ1 + z), . . . , 1/(λd + z))U ⊤. Writing
˜u = Uu and ˜v = Uv, then f(u, v) becomes
f(u, v) =
Z ∞
0
˜v⊤diag

1
λ1 + z , . . . ,
1
λd + z

˜u˜u⊤diag

1
λ1 + z , . . . ,
1
λd + z

˜vdz
=
Z ∞
0
⟨˜u ⊙˜v, (1/(λ1 + z), . . . , 1/(λd + z))⟩2dz.
Similarly, g(u, v) becomes
g(u, v) = ˜v⊤diag(λ−1/2
1
, . . . , λ−1/2
d
)˜u˜u⊤diag(λ−1/2
1
, . . . , λ−1/2
d
)˜v
=
D
˜u ⊙˜v, (1/λ−1/2
1
, . . . , 1/λ−1/2
d
)
E2
.
Now applying the fact that λ−1/2 = 1
π
R ∞
0
z−1/2
λ+z dz for any λ > 0, we have
g(u, v) =
 1
π
Z ∞
0
D
˜u ⊙˜v, (z−1/2/(λ1 + z), . . . , z−1/2/(λd + z))
E
dz
2
.
To further bound g(u, v), we split the integral into three parts: [0, c], [c, C], and [C, ∞) where c > 0 and C > 0 are
constants to be determined later. That is,
g(u, v) ≤3
π2
 Z c
0
D
˜u ⊙˜v, (z−1/2/(λ1 + z), . . . , z−1/2/(λd + z))
E
dz
2
+ 3
π2
 Z C
c
D
˜u ⊙˜v, (z−1/2/(λ1 + z), . . . , z−1/2/(λd + z))
E
dz
2
+ 3
π2
 Z ∞
C
D
˜u ⊙˜v, (z−1/2/(λ1 + z), . . . , z−1/2/(λd + z))
E
dz
2
=: g1(u, v) + g2(u, v) + g3(u, v)
where we apply the triangle inequality and denote the three terms on the right-hand side by g1(u, v), g2(u, v), g3(u, v)
respectively. We control each term separately.
First, for g1(u, v),
g1(u, v) = 3
π2
 Z c
0
⟨˜u ⊙˜v, (1/(λ1 + z), . . . , 1/(λd + z))⟩z−1/2dz
2
≤3
π2
 Z c
0
∥˜u ⊙˜v∥2

d
X
i=1
1
(λi + z)2
1/2
z−1/2dz
2
≤3
π2
∥˜u ⊙˜v∥2
√
d
mini∈[d] λi
Z c
0
z−1/2dz
2
= 12∥˜u ⊙˜v∥2
2 · cd
π2 · mini∈[d] λ2
i
24

where the first inequality follows from Cauchy-Schwarz inequality and in the second inequality we apply
1
λi+z ≤
1
λi
as each λi is positive. Next, for the integral over [c, C], we have
g2(u, v) = 3
π2
 Z C
c
⟨˜u ⊙˜v, (1/(λ1 + z), . . . , 1/(λd + z))⟩z−1/2dz
2
≤3
π2
 Z C
c
⟨˜u ⊙˜v, (1/(λ1 + z), . . . , 1/(λd + z))⟩2dz
 Z C
c
z−1dz

≤3(log C −log c)
π2
 Z ∞
0
⟨˜u ⊙˜v, (1/(λ1 + z), . . . , 1/(λd + z))⟩2dz

= 3(log C −log c)
π2
f(u, v)
where the first inequality follows from Cauchy-Schwarz inequality, and in the second inequality we relax the domain
of the integral to [0, ∞), which exactly gives us f(u, v). Finally, for g3(u, v),
g3(u, v) = 3
π2
 Z ∞
C
⟨˜u ⊙˜v, (1/(λ1 + z), . . . , 1/(λd + z))⟩z−1/2dz
2
≤3
π2
 Z ∞
C
∥˜u ⊙˜v∥2

d
X
i=1
1
(λi + z)2
1/2
z−1/2dz
2
≤3
π2
 Z ∞
C
∥˜u ⊙˜v∥2
√
dz−3/2dz
2
= 12∥˜u ⊙˜v∥2
2 · C−1d
π2
where the first inequality follows from Cauchy-Schwarz inequality and in the second inequality we apply
1
λi+z ≤1
z as
each λi is positive. Collecting the above bounds, we obtain
g(u, v) ≤3(log C −log c)
π2
f(u, v) + 12∥˜u ⊙˜v∥2
2 · cd
π2 · mini∈[d] λ2
i
+ 12∥˜u ⊙˜v∥2
2 · C−1d
π2
.
(16)
Now for any general positive semi-definite matrix A ∈Rd×d with eigendecomposition A = Pd
i=1 αiuiu⊤
i , we
apply the bound (16) to each ui in the eigendecomposition and sum over all i to get
d
X
i=1
αig(ui, v) ≤3(log C −log c)
π2
d
X
i=1
αif(ui, v) +
12cd
π2 · mini∈[d] λ2
i
d
X
i=1
αi∥˜ui ⊙˜v∥2
2
+ 12C−1d
π2
d
X
i=1
αi∥˜ui ⊙˜v∥2
2.
Note that ∥˜ui ⊙˜v∥2
2 ≤∥˜u∥2
2∥˜v∥2
2 = 1 as both ˜u and ˜v are unit vectors. Therefore, we further have
d
X
i=1
αig(ui, v) ≤3(log C −log c)
π2
d
X
i=1
αif(ui, v) +
12cd
π2 · mini∈[d] λ2
i
d
X
i=1
αi + 12C−1d
π2
d
X
i=1
αi
= 3(log C −log c)
π2
d
X
i=1
αif(ui, v) +
12cd
π2 · mini∈[d] λ2
i
Tr(A) + 12C−1d
π2
Tr(A).
Recall the definition of f(u, v) and g(u, v) in (14) and (15). Then we have shown that for any unit vector v ∈Rd,
v⊤X−1/2AX−1/2v ≤3(log C −log c)
π2
v⊤∂log(X)[A]v +
12cd
π2 · mini∈[d] λ2
i
Tr(A) + 12C−1d
π2
Tr(A).
Therefore, we conclude that
X−1/2AX−1/2 ⪯3(log C −log c)
π2
∂log(X)[A] +

12cd
π2λmin(X)2 + 12C−1d
π2

Tr(A) · Id.
25

C
Unified proof for adaptive algorithms
C.1
Relationship between adaptive algorithms
Algorithm 4 One-sided Shampoo
Hyperparam: ϵ ≥0, total steps T, learning rate η, initial M0, L0 = 0
Input: initialization x0, stochastic loss functions {ft}T
t=1 : RdL×dR →R
for t = 1, 2, · · · , T :
Gt ←∇ft(Xt−1)
Lt ←Lt−1 + GtG⊤
t
Xt ←Xt−1 −η(Lt + ϵIdL)−1
2 Gt
return xT
Algorithm 5 General Adaptive Cumulative Optimization Algorithm
Hyperparam: ϵ ≥0, total steps T, learning rate η, convex cone H ⊂S+, β
Input: initialization x0, stochastic loss functions {ft}T
t=1 : RdL×dR →R
M0 ←0
for t = 1, 2, · · · , T :
gt ←∇ft(xt−1)
Mt ←Mt−1 + gtg⊤
t
Vt ←arg minH∈H

Mt + ϵId, H−1
+ Tr(H)
xt ←xt−1 −ηV −1
t
gt
return xT
Algorithm 6 General Adaptive EMA Optimization Algorithm
Hyperparam: ϵ ≥0, total steps T, learning rate η, convex cone H ⊂S+, β
Input: initialization x0, stochastic loss functions {ft}T
t=1 : RdL×dR →R
M0 ←0
for t = 1, 2, · · · , T :
gt ←∇ft(xt−1)
Mt ←βMt−1 + (1 −β)gtg⊤
t
Vt ←arg minH∈H

Mt + ϵId, H−1
+ Tr(H)
xt ←xt−1 −ηV −1
t
gt
return xT
Algorithm 7 General Adaptive Weighted Optimization Algorithm
Hyperparam: ϵ ≥0, total steps T, learning rate η, convex cone H ⊂S+, β
Input: initialization x0, stochastic loss functions {ft}T
t=1 : RdL×dR →R
M0 ←0
for t = 1, 2, · · · , T :
gt ←∇ft(xt−1)
Mt ←βMt−1 + gtg⊤
t
Vt ←arg minH∈H

Mt + ϵId, H−1
+ Tr(H)
xt ←xt−1 −ηV −1
t
gt
return xT
26

C.2
Proof for weighted algorithm
Before presenting the main theorems, we first clarify our gradient noise assumption below. Assumption C.1 is stronger
than the conventional noise assumption when it assumes the condition holds almost surely. It is required in proving
Lemma C.3. Lemma C.5 can still hold when we only assume the covariance matrix is bounded.
Assumption C.1. For any t ∈[T] and any x ∈Rd, E [∇ft(x)] = ∇f(x) where the expectation is taken with respect
to the randomness in the loss ft. Moreover, there exists Σ ⪰0 such that for all t ≥0, −Σ ⪯∇f(x)∇f(x)⊤−
∇ft(x)∇ft(x)⊤⪯Σ.
With Assumption C.1 in place, Theorem C.2 presents the general result for weighted adaptive algorithms on
stochastic nonconvex functions, which is then specialized to the cumulative and EMA variants in Theorem C.7 and
C.8, respectively. When plugging Σ = 0 in the stochastic rate, we can get the deterministic result in Section 3.2.
In this section, we will define H∗= arg minH∈H,Tr(H)≤1 L∥·∥H(f). For all the Mt and Vt in this section except
the proof of Theorem C.7 and Theorem C.8, they are defined as in Algorithm 7, i.e., Mt = βMt−1 + gtg⊤
t . We will
define ¯gt = ∇f(xt) and
˜
Mt = βMt−1 + ¯gt¯g⊤
t + Σ,
˜Vt = PH( ˜
Mt + ϵId).
(17)
Then it always holds Mt ⪯˜
Mt because of Assumption C.1. By Lemma A.3, it also holds Vt ⪯˜Vt.
Theorem C.2. For any ϵ ≥0, β ∈(0, 1], η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 1 with
well-structured preconditioner set H, where the update of Mt follows the weighted version, i.e., Mt = βMt−1 +gtg⊤
t
for all t ∈[T].
Let ΛH (f) be the adaptive smoothness of the loss f according to Definition 2.4.
Then under
Assumption C.1, it holds that
E
 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗

≤
qPT −1
i=0 βi/2
T
ξ +
q
Tr

PH
 (PT −1
i=0 βi)Σ + ϵId

√
T
p
ξ.
where ξ is given by
ξ = 2∆0
η
+ ηΛH (f) ∥ST ∥op +
√
2d ∥Σ∥1/2
op ∥ST ∥op
(18)
and ST = E PT −1
t=0 V −1
t
(V 2
t −βV 2
t−1)V −1
t
.
For general well-structured preconditioner set, ∥ST ∥op = ˜O (log(d)[(1 −β)T/β + log(d)]). When the precondi-
tioner set only has diagonal matrices, ∥ST ∥op = (1 −β)T + ˜O (1).
Proof of Theorem C.2. With the shorthand ¯gt = ∇f(xt), we need to bound
E
 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗

= E
 1
T
T −1
X
t=0
∥¯gt∥H,∗

= E
 1
T
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )]

(19)
where the second equality holds by Lemma A.8. By employing Lemma C.4 and Lemma C.5, we have that

E
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )]
2
≤

E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ]

E
T −1
X
t=0
Tr[ ˜Vt]

≤

E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ]

T · Tr

PH
 T −1
X
i=0
βi
Σ + ϵId

+
 T −1
X
i=0
βi/2
E
T −1
X
t=0
Tr[¯gt¯g⊤
t ˜V −1
t
]

.
(20)
It then suffices to bound the sum of Tr[ ˜V −1
t
¯gt¯g⊤
t ]. To this end, we apply Lemma C.3 to get
E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ] ≤2E
T −1
X
t=0
Tr[V −1
t
gt¯g⊤
t ] +
q
2 ∥Σ∥opE
T −1
X
t=0
Tr[V −2
t
gtg⊤
t ].
(21)
27

Here, the second term on the right-hand side can be controlled by applying Lemma 3.3. For the first term on the
right-hand side, we need to apply the descent lemma. Specifically, recall that H∗= arg minH∈H,Tr(H)≤1 L∥·∥H(f),
and then by second-order Taylor expansion,
f(xt+1) ≤f(xt) + ⟨∇f(xt), xt+1 −xt⟩+ ΛH (f)
2
∥xt+1 −xt∥2
H∗
= f(xt) −η

¯gt, V −1
t
gt

+ η2ΛH (f)
2
V −1
t
gt
2
H∗.
By taking expectation on both sides and summing over t = 0, 1, . . . , T −1, we get
E[f(xT ) −f(x0)] ≤−ηE
T −1
X
t=0
Tr[V −1
t
gt¯g⊤
t ] + η2ΛH (f)
2
E
T −1
X
t=0
V −1
t
gt
2
H∗.
Rearranging the above inequality, we have
E
T −1
X
t=0
Tr[V −1
t
gt¯g⊤
t ] ≤E[f(x0) −f(xT )]
η
+ ηΛH (f)
2
E
T −1
X
t=0
V −1
t
gt
2
H∗
≤∆0
η + ηΛH (f)
2
E
T −1
X
t=0
V −1
t
gt
2
H∗
(22)
where the second inequality holds by the definition ∆0 = f(x0) −minx f(x). Therefore, combining (21) and (22)
yields
E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ] ≤2∆0
η
+ ηΛH (f) E
T −1
X
t=0
∥V −1
t
gt∥2
H∗+
q
2 ∥Σ∥opE
T −1
X
t=0
Tr[V −2
t
gtg⊤
t ].
(23)
Now, we apply Lemma 3.3 to both E PT −1
t=0
V −1
t
gt
2
H∗and E PT −1
t=0 Tr[V −2
t
gtg⊤
t ] = E PT −1
t=0
V −1
t
gt
2
2, which
gives
E
T −1
X
t=0
V −1
t
gt
2
H∗≤Tr[H∗] · ∥ST ∥op = ∥ST ∥op,
E
T −1
X
t=0
V −1
t
gt
2
2 ≤d∥ST ∥op
where ST = E PT −1
t=0 V −1
t
(V 2
t −βV 2
t−1)V −1
t
. Based on this, we further define
ξ = 2
η [f(x0) −min f(x)] + ηΛH (f) ∥ST ∥op +
q
2 ∥Σ∥opd ∥ST ∥op .
(24)
Applying these bounds to (23) gives
E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ] ≤ξ.
(25)
Now we can plug (25) into (20), and hence it follows from (19) that
E
 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗

= 1
T E
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )] ≤
qPT −1
i=0 βi/2
T
ξ + T −1/2 Tr

PH
 T −1
X
i=0
βi
Σ + ϵId
!# 1
2 p
ξ.
It remains to provide a concrete bound for ∥ST ∥op, which follows from Lemma 3.3 and Lemma C.6. Specifically,
we know from Lemma 3.3 that
∥ST ∥op ≤C1
 
log
d
ϵ
T −1
X
t=0
∥gt∥2
2 + d2(1 −β)T

+ 1
!  
(1 −β)T
β
+ log

V 2
T −1
ϵ

op
!
+ C2.
(26)
28

Moreover, by Lemma C.6, we know that
T −1
X
t=0
∥gt∥2
2 = poly(T, ∥Σ∥op , ∥∇f(x0)∥2 , ΛH (f) , d, η)
∥VT −1∥op = poly(T, ∥Σ∥op , ∥∇f(x0)∥2 , ΛH (f) , d, η, ϵ).
Then the first term in Eq. (26) satisfies
log
d
ϵ
T −1
X
t=0
∥gt∥2
2 + d2(1 −β)T

+ 1 = 1 + log

poly(T, ∥Σ∥op , ∥∇f(x0)∥2 , ΛH (f) , d, η, 1 −β, ϵ−1)]
= ˜O(log d)
where ˜O(·) hides logarithm dependence on all problem parameters except d. Similarly, the second term in Eq. (26)
satisfies
(1 −β)T
β
+ log

V 2
T −1
ϵ

op
= (1 −β)T
β
+ ˜O(log d)
Plugging these two bounds into Eq. (26) yields that for any well-structured H,
∥ST ∥op = ˜O

log d ·
(1 −β)T
β
+ log d

(27)
When H only has diagonal matrices, we can improve Lemma C.6 with results in Xie et al. (2025a). The dependence
on d can be improved such that log(V 2
T −1/ϵ) = ˜O(1). Then Lemma 3.3 gives us
∥ST ∥op ≤(1 −β)T + log

V 2
T −1
ϵ

op
= (1 −β)T + ˜O(1).
This completes the proof.
Lemma C.3. Under the setting of Theorem C.7, let ˜Vt be as defined in Eq. (17). Then it holds that
T −1
X
t=0
Et Tr[V −1
t
gt¯g⊤
t ] ≥1
2
T −1
X
t=0
Et Tr[ ˜V −1
t
¯gt¯g⊤
t ] −
q
2 ∥Σ∥op
2
T −1
X
t=0
Et Tr[V −2
t
gtg⊤
t ].
Proof. First we can compute the gap by replacing Vt with ˜Vt as follows
Tr
h
(V −1
t
−˜V −1
t
)gt¯g⊤
t
i =
Tr
h
˜V −1
t
( ˜Vt −Vt)V −1
t
gt¯g⊤
t
i
≤1
2 Tr
h
˜V −1
t
¯gt¯g⊤
t
i
+ 1
2 Tr
h
( ˜Vt −Vt) ˜V −1
t
( ˜Vt −Vt)V −1
t
gtg⊤
t V −1
t
i
,
(28)
where the second inequality follows from the fact that Tr[A⊤B] ≤1
2 Tr[AA⊤] + 1
2 Tr[BB⊤] with A = ˜V
−1
2
t
¯gt and
B = ˜V
−1
2
t
( ˜Vt −Vt)V −1
t
gt. Note that since ˜Vt ⪰Vt, it holds that 0 ⪯˜Vt −Vt ⪯˜Vt. Then we can upper bound the
second term on the right-hand side of (28) as
Tr
h
( ˜Vt −Vt) ˜V −1
t
( ˜Vt −Vt)V −1
t
gtg⊤
t V −1
t
i
≤Tr
h
( ˜Vt −Vt)V −1
t
gtg⊤
t V −1
t
i
≤∥˜Vt −Vt∥op Tr[V −2
t
gtg⊤
t ]
= ∥PH( ˜
Mt) −PH(Mt)∥op Tr[V −2
t
gtg⊤
t ]
Further applying Lemma A.5 yields
Tr
h
( ˜Vt −Vt) ˜V −1
t
( ˜Vt −Vt)V −1
t
gtg⊤
t V −1
t
i
≤∥˜
Mt −Mt∥1/2
op Tr[V −2
t
gtg⊤
t ]
≤
q
2 ∥Σ∥op Tr[V −2
t
gtg⊤
t ]
(29)
29

where the second inequality holds because 0 ⪯
˜
Mt −Mt = ¯gt¯g⊤
t + Σ −gtg⊤
t ⪯2Σ based on Assumption C.1.
Then plugging (29) back into (28), we get
T −1
X
t=0
Tr[V −1
t
gt¯g⊤
t ] ≥
T −1
X
t=0
Tr[ ˜V −1
t
gt¯g⊤
t ] −
T −1
X
t=0
Tr
h
V −1
t
−˜V −1
t

gt¯g⊤
t
i
≥1
2
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ] −
q
2 ∥Σ∥op
2
T −1
X
t=0
Tr[V −2
t
gtg⊤
t ].
Lemma C.4. Under the setting of Theorem C.7, let ˜Vt be as defined in Eq. (17). Then it holds that
 
E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ]
!  
E
T −1
X
t=0
Tr[ ˜Vt]
!
≥
 
E
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )]
!2
.
Proof of Lemma C.4. For convenience, denote At = PH(¯gt¯g⊤
t ) and Bt = ˜V
1
2
t . Then
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )] =
T −1
X
t=0
Tr[At] =
T −1
X
t=0
⟨At, I⟩=
T −1
X
t=0
⟨B−1
t
At, Bt⟩.
Now taking expectation on both sides and applying Cauchy-Schwarz inequality, we get
E
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )] ≤
T −1
X
t=0
E[∥B−1
t
At∥F ∥Bt∥F ] ≤
T −1
X
t=0
 E∥B−1
t
At∥2
F
1/2 E∥Bt∥2
F )1/2
Applying Cauchy-Schwarz inequality again, we further have
E
T −1
X
t=0
Tr[PH(¯gt¯g⊤
t )] ≤
 T −1
X
t=0
E∥B−1
t
At∥2
F
1/2 T −1
X
t=0
E∥Bt∥2
F
1/2
=

E
T −1
X
t=0
Tr[ ˜V −1
t
PH(¯gt¯g⊤
t )2]
1/2
E
T −1
X
t=0
Tr[ ˜Vt]
1/2
=

E
T −1
X
t=0
Tr[ ˜V −1
t
¯gt¯g⊤
t ]
1/2
E
T −1
X
t=0
Tr[ ˜Vt]
1/2
where the second equality follows from Lemma A.2 as ˜Vt ∈H. This completes the proof.
Lemma C.5. Under the setting of Theorem C.7, let ˜Vt be as defined in Eq. (17). Then it holds that
E
T −1
X
t=0
Tr[ ˜Vt] ≤T · Tr

PH
 T −1
X
i=0
βi
Σ + ϵId

+
 T −1
X
i=0
βi/2
E
T −1
X
t=0
Tr[¯gt¯g⊤
t ˜V −1
t
].
(30)
Proof of Lemma C.5. Recall that ˜Vt = PH( ˜
Mt + ϵId) ∈H. Then applying Lemma A.2, we get
Tr[ ˜Vt] = Tr[ ˜V 2
t ˜V −1
t
] = Tr[(PH( ˜
Mt + ϵId))2 ˜V −1
t
] = Tr[( ˜
Mt + ϵId) ˜V −1
t
].
(31)
Further plugging in ˜
Mt = βMt−1 + ¯gt¯g⊤
t + Σ, we have
Tr[ ˜Vt] = Tr[
 ¯gt¯g⊤
t + Σ + βMt−1 + ϵId
 ˜V −1
t
]
= Tr[(Σ + βMt−1 + ϵId) ˜V −1
t
] + Tr[¯gt¯g⊤
t ˜V −1
t
]
30

Applying Lemma A.2 again, we further have
Tr[ ˜Vt] = Tr[PH (Σ + βMt−1 + ϵId)2 ˜V −1
t
] + Tr[¯gt¯g⊤
t ˜V −1
t
]
≤Tr[PH (Σ + βMt−1 + ϵId)] + Tr[¯gt¯g⊤
t ˜V −1
t
]
(32)
where the inequality holds because ˜Vt = PH(¯gt¯g⊤
t + Σ + βMt−1 + ϵId) ⪰PH(Σ + βMt−1 + ϵId). Next, we will
further control the right-hand side of the above inequality by recursive expansion.
For notational convenience, for any 1 ≤s < t, denote
As =
 s−1
X
i=0
βi
Σ + βsMt−s + ϵId.
Then for any 1 ≤s < t, we have
E Tr[PH(As)] = E Tr
"
PH
 s−1
X
i=0
βi
Σ + βs+1Mt−s−1 + βsgt−sg⊤
t−s + ϵId
#
≤E Tr
"
PH

Et−s−1
 s−1
X
i=0
βi
Σ + βs+1Mt−s−1 + βsgt−sg⊤
t−s + ϵId
#
where the inequality holds because Tr[PH(X)] is concave in X by Lemma A.6. Then since Egt−sg⊤
t−s ⪯¯gt−s¯g⊤
t−s+Σ
by Assumption C.1, we further have
E Tr[PH(As)] ≤E Tr
"
PH

s
X
i=0
βi
Σ + βs+1Mt−s−1 + βs¯gt−s¯g⊤
t−s + ϵId
#
= E Tr

PH(As+1 + βs¯gt−s¯g⊤
t−s)

.
Applying the same trick as in (31) to As+1 + βs¯gt−s¯g⊤
t−s, we obtain
E Tr [PH(As)] ≤E Tr

PH(As+1 + βs¯gt−s¯g⊤
t−s)2PH(As+1 + βs¯gt−s¯g⊤
t−s)−1
= E Tr

(As+1 + βs¯gt−s¯g⊤
t−s)PH(As+1 + βs¯gt−s¯g⊤
t−s)−1
= E Tr

PH(As+1)2PH(As+1 + βs¯gt−s¯g⊤
t−s)−1
+ E Tr

βs¯gt−s¯g⊤
t−sPH(As+1 + βs¯gt−s¯g⊤
t−s)−1
≤E Tr

PH(As+1)

+ E Tr

βs¯gt−s¯g⊤
t−sPH(As+1 + βs¯gt−s¯g⊤
t−s)−1
(33)
where the second equality is again by Lemma A.2 and the second inequality follows from the fact that PH(As+1 +
βs¯gt−s¯g⊤
t−s) ⪰PH(As+1). In addition, note that
As+1 + βs¯gt−s¯g⊤
t−s = βs
Ps
i=0 βi
βs
Σ + βMt−s−1 + ¯gt−s¯g⊤
t−s

+ ϵId ⪰βs( ˜
Mt−s + ϵId).
Thus it follows from Lemma A.3 that
PH(As+1 + βs¯gt−s¯g⊤
t−s) ⪰PH(βs( ˜
Mt−s + ϵId)) = βs/2PH( ˜
Mt−s + ϵId) = βs/2 ˜Vt−s.
(34)
Now combining (33) and (34) yields
E Tr [PH(As)] ≤E Tr

PH(As+1)

+ βs/2E Tr[¯gt−s¯g⊤
t−s ˜V −1
t−s].
Further telescoping over s = 1, . . . , t −1, we obtain
E Tr

PH(Σ + βMt−1 + ϵId)

= E Tr

PH(A1)

≤E Tr

PH(At)

+
t−1
X
s=1
βs/2E Tr
¯gt−s¯g⊤
t−s ˜V −1
t−s

= Tr
"
PH
 t−1
X
i=0
βi
Σ + ϵId
#
+
t−1
X
s=1
βs/2E Tr
¯gt−s¯g⊤
t−s ˜V −1
t−s

(35)
31

Now, plugging (35) back into (32), we obtain
E Tr[ ˜Vt] ≤Tr
"
PH
 t−1
X
i=0
βi
Σ + ϵId
#
+
t−1
X
s=0
βs/2E Tr
¯gt−s¯g⊤
t−s ˜V −1
t−s

= Tr
"
PH
 t−1
X
i=0
βi
Σ + ϵId
#
+
t
X
s=1
β(t−s)/2E Tr
¯gs¯g⊤
s ˜V −1
s

.
Finally, summing over t = 0, 1, . . . , T −1, we get
E
T −1
X
t=0
Tr[ ˜Vt] ≤
T −1
X
t=0
Tr
"
PH
 t−1
X
i=0
βi
Σ + ϵId
#
+
T −1
X
t=0
t
X
s=1
β(t−s)/2E Tr[¯gs¯g⊤
s ˜V −1
s
]
≤T · Tr
"
PH
 T −1
X
i=0
βi
Σ + ϵId
#
+
 T −1
X
i=0
βi/2
E
T −1
X
t=0
Tr[¯gt¯g⊤
t ˜V −1
t
].
This completes the proof.
Lemma C.6. Under the setting of Theorem C.2, for any fixed initialization x0, let g0, . . . , gT −1 and V0, . . . , VT −1 be
given by Algorithm 7. Then the following hold with probability 1:
T −1
X
t=0
∥gt∥2
2 ≤2(∥Σ∥op + ∥∇f(x0)∥2
2 + ΛH (f)2 dη2)T 3,
∥VT −1∥op ≤
q
2(∥Σ∥op + ∥∇f(x0)∥2
2 + ΛH (f)2 dη2)T 3 + ϵd.
Proof. In this proof, we will define H∗∈arg minH∈H,−H⪯∇2f(x)⪯H Tr(H).
We first control PT −1
t=0 ∥gt∥2
2. According to Assumption C.1, we have
T −1
X
t=0
∥gt∥2
2 =
T −1
X
t=0
gtg⊤
t

op ≤
T −1
X
t=0

∥Σ∥op +
¯gt¯g⊤
t

op

= T ∥Σ∥op +
T −1
X
t=0
∥¯gt∥2
2
(36)
So it suffices to control the sum of ∥¯gt∥2
2. By triangle inequality, ∥¯gt∥2
2 ≤2 ∥¯g0∥2
2 + 2 ∥¯gt −¯g0∥2
2, and thus we only
need to bound the distance ¯gt −¯g0. To this end, since −H∗⪯∇2f(x) ⪯H∗for all x, we have
∥¯gt −¯g0∥2 = ∥∇f(xt) −∇f(x0)∥2 ≤∥H∗∥op ∥xt −x0∥2 ≤ΛH(f)
t−1
X
s=0
∥xs+1 −xs∥2.
(37)
Moreover, we can control each ∥xs+1 −xs∥2 as follows:
∥xs+1 −xs∥2
2 = η2 V −1
s
gs
2
2 = η2 Tr(g⊤
s V −2
s
gs)
= η2 Tr(PH(Ms)−2gsg⊤
s )
= η2 Tr(PH(Ms)−2PH(gsg⊤
s )2)
where the last equality holds by Lemma A.2. Then since Ms ⪰gsg⊤
s , we have PH(Ms) ⪰PH(gsg⊤
s ) by Lemma A.3,
and it follows that
∥xs+1 −xs∥2
2 ≤η2 Tr(Id) = dη2.
Plugging this back into Eq. (37), we get ∥¯gt −¯g0∥2 ≤ηtΛH(f)
√
d, so ∥¯gt∥2
2 ≤2∥¯g0∥2
2 + 2η2t2ΛH(f)2d. Now
applying this to Eq. (36), we obtain
T −1
X
t=0
∥gt∥2
2 ≤T ∥Σ∥op +
T −1
X
t=0
(2∥¯g0∥2
2 + 2η2t2ΛH(f)2d)
≤2(∥Σ∥op + ∥¯g0∥2
2 + ΛH (f)2 dη2)T 3.
(38)
32

Finally, we can bound ∥VT −1∥op as follows:
∥VT −1∥2
op =
V 2
T −1

op =
PH(MT −1)2
op ≤Tr(PH(MT −1)2) = Tr(MT −1)
where we apply Lemma A.2 in the last equality. Note that Tr(MT −1) = PT −1
t=0 ∥gt∥2
2 + ϵd, so it follows from (38)
that
∥VT −1∥op ≤
q
2(∥Σ∥op + ∥¯g0∥2
2 + ΛH (f)2 dη2)T 3 + ϵd.
This completes the proof.
C.3
Proof for the cumulative and EMA variants
We will derive the convergence rate of cumulative optimizers and EMA optimizers by reducting from Theorem C.2. In
the stochastic case, the dependence on T of both convergence rate is ˜O(T −1/4), matching previous results on specific
examples AdaGrad and Adam (Xie et al., 2025a; Li et al., 2025).
Theorem C.7. For any ϵ ≥0, η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 1 with well-structured
preconditioner set H, where the update of Mt follows the cumulative version, i.e., Mt = Mt−1 +gtg⊤
t for all t ∈[T].
Let ΛH (f) be the adaptive smoothness of the loss f according to Definition 2.4. Then under Assumption C.1, it holds
that
E
 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗

≤
1
√
T
ξ +
1
T 1/4 Tr[PH(Σ + ϵ
T Id)]
1
2 p
ξ
where ξ is given by
ξ = ˜O
∆0
η +
 η · ΛH (f) + d ∥Σ∥1/2
op

log2 d

.
Moreover, whensettingthelearningrateη =
q
∆0
ΛH(f) log2 d, itholdsthatξ = ˜O
 p
∆0 · ΛH (f) log d+d ∥Σ∥1/2
op log2 d

.
Proof of Theorem C.7. The desired result directly follows from Theorem C.2 with β = 1, where we additionally apply
the bound Tr[PH(TΣ + ϵId)] ≤
√
T Tr[PH(Σ + ϵ
T Id] using Lemma A.3.
Theorem C.8. For any ϵ ≥0, β ∈(0, 1), η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 1 with well-
structured preconditioner set H, where the update of Mt follows the EMA version, i.e., Mt = βMt−1 + (1 −β)gtg⊤
t
for all t ∈[T].
Let ΛH (f) be the adaptive smoothness of the loss f according to Definition 2.4.
Then under
Assumption C.1, it holds that
E
 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗

≤
1
√
T
κ +
1
T 1/4 Tr[PH(Σ + ϵId)]
1
2 √κ
where κ is given by
κ = ˜O
 ∆0
η
√
T
+ η · ΛH (f) + d√1 −β∥Σ∥1/2
op
√
T
 1
β T + log d
1 −β

log d

.
Moreover, when setting 1 −β = Θ( log d
T ) and η =
q
∆0
ΛH(f)·T log2 d, it holds that
κ = ˜O
p
∆0 · ΛH (f) + d ∥Σ∥1/2
op

log d

.
When there is no noise, i.e., ft ≡f, it holds that
E
 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗

≤
1
√
T
κ +
√
dϵ1/4
T 1/4
√κ
33

where κ is given by
κ = ˜O
 ∆0
η
√
T
+ η · ΛH (f)
√
T
 1
β T + log d
1 −β

log d

.
Moreover, when setting 1 −β = Θ( log d
T ) and η =
q
∆0
ΛH(f)·T log2 d, it holds that
κ = ˜O
p
∆0 · ΛH (f) log d

.
Proof of Theorem C.8. As mentioned in Section 3, Algorithm 6 is equivalent to Algorithm 7 with ϵ/(1 −β) in place
of ϵ and η/√1 −β in place of η. Therefore, the convergence rate of Algorithm 6 follows from Theorem C.2 with this
change of hyperparameters. Specifically,
1
T E
T −1
X
t=0
∥¯gt∥H,∗≤
qPT −1
i=0 βi/2
T
ξ +
q
Tr[PH((PT −1
i=0 βi)Σ +
ϵ
1−β Id)]
√
T
p
ξ
(39)
where ξ satisfies
ξ = ˜O
∆0
√1 −β
η
+

η
√1 −β ΛH (f) + d ∥Σ∥1/2
op
1 −β
β
T + log d

log d

.
(40)
We can further simplify the result for β < 1 as follows. Note that PT −1
i=0 βi ≤1/(1 −β) and
T −1
X
i=0
βi/2 ≤
1
1 −√β = 1 + √β
1 −β
≤
2
1 −β .
It then follows from Lemma A.3 that
Tr
"
PH
 T −1
X
i=0
βi
Σ +
ϵ
1 −β Id
#
≤Tr

PH

1
1 −β (Σ + ϵId

=
1
√1 −β Tr[PH(Σ + ϵId)].
Applying these to (39), we get
E 1
T
T −1
X
t=0
∥¯gt∥H,∗≤
√
2
T
ξ
√1 −β +
p
Tr[PH(Σ + ϵId)]
√
T
s
ξ
√1 −β .
Denote κ = ξ/
p
(1 −β)T. Then we have
E 1
T
T −1
X
t=0
∥¯gt∥H,∗≤
√
2
√
T
κ +
p
Tr[PH(Σ + ϵId)]
T 1/4
√κ.
By (40), we know that κ satisfies that
κ = ˜O
 
∆0
η
√
T
+
ηΛH (f) + d√1 −β ∥Σ∥1/2
op
√
T
 1
β T + log d
1 −β

log d
!
.
This completes the proof.
D
Proof for the accelerated algorithm
In this section, we will define H∗= arg minH∈H,Tr(H)≤1 L∥·∥H(f). Our proof follows the strategy developed by
Kovalev (2025a). For completeness, we reproduce some of the arguments to make the exposition self-contained.
34

D.1
Stochastic case without projection
Theorem 4.4. For a well-structured preconditioner set H, let f be a convex loss function whose H-smoothness constant
is ΛH (f) ∈(0, ∞) according to Definition 2.4. For ϵ > 0, T > 0, consider Algorithm 2 with αt = 2/(t + 2) for
t = 0, 1, . . . , T −1. Suppose x∗is the global minima and maxt=0,1,...,T −1 ∥xt −x∗∥H ≤D for some D > 0 and
Assumption 4.3 holds with adaptive gradient variance σH({ft}T
t=1)2 ≤σ2
H for some σH ∈[0, ∞). Then it holds that
E[f(¯xT ) −f(x∗)] ≤
2D2ϵ
η(T + 1)2 E Tr(V −1
T −1) +
D2
2η −η
2

E
4
(T + 1)2
T −1
X
t=0
g⊤
t V −1
t
gt
+
2η2
(T + 1)2 · ΛH (f) · ˜O(log2 d) +
η
T 1/2 σH · ˜O(log d).
Moreover, when choosing learning rate η = D, the convergence rate becomes
E[f(¯xT ) −f(x∗)] = ˜O
ΛH (f) D2 log2 d + d√ϵD
T 2
+ σHD log d
√
T

.
Proof of Theorem 4.4. Combining the results of Lemma D.2 and Lemma D.3, we get
T −1
X
t=0
E[f αt,¯xt(xt+1) −f αt,¯xt(x∗)] ≤E
D2ϵ
2η Tr(V −1
T −1) +
D2
2η −η
2
 T −1
X
t=0
g⊤
t V −1
t
gt

+ E

η
T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩+ ΛH (f) η2
2
T −1
X
t=0
∥V −1
t
gt∥2
H∗

.
(41)
It remains to further bound the last two terms.
For the last term on the right-hand side of (41), we apply Lemma 3.3 to get
T −1
X
t=0
∥V −1
t
gt∥2
H∗≤Tr[H∗] · ˜O(log2 d) = ΛH(f) · ˜O(log2 d)
(42)
where the second step follows from the definition of H∗.
For the third term on the right-hand side of (41), consider any H ∈H with Tr(H) ≤1, and then it follows from
the Cauchy-Schwarz inequality that
E

T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩
 ≤
T −1
X
t=0
 E∥gt −∇f αt,¯xt(xt)∥2
2
1/2 E∥V −1
t
gt∥2
2)1/2
≤
 
E
T −1
X
t=0
gt −∇f αt,¯xt(xt)
2
H−1
!1/2  
E
T −1
X
t=0
∥V −1
t
gt∥2
H
!1/2
.
Here we similarly have E PT −1
t=0 ∥V −1
t
gt∥2
H = Tr[H] · ˜O(log2 d) ≤˜O(log2 d) according to Lemma 3.3. Then we
further minimize over H ∈H with Tr[H] ≤1 to get
min
H∈H,Tr(H)≤1 E
T −1
X
t=0
gt −∇f αt,¯xt(xt)
2
H−1
=
min
H∈H,Tr(H)≤1 E
T −1
X
t=0
1
α2
t
∥∇ft(αtxt + (1 −αt)¯xt) −∇f(αtxt + (1 −αt)¯xt)∥2
H−1
≤σ2
H
T −1
X
t=0
1
α2
t
35

where the last equation follows from Assumption 4.3 and the condition on σH. Therefore, we obtain
E

T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩
 ≤σH
 T −1
X
t=0
α−2
t
1/2
· ˜O(log2 d) ≤σHT 3/2 · ˜O(log2 d)
(43)
where we have plugged in the choice of αt = 2/(t + 2).
Finally, combining (41), (42) and (43), we obtain
E
T −1
X
t=0
(f αt,¯xt(xt+1) −f αt,¯xt(x∗)) ≤D2ϵ
2η E Tr(V −1
T −1) +
D2
2η −η
2

E
T −1
X
t=0
g⊤
t V −1
t
gt
+ η2
2 ΛH (f) · ˜O(log2 d) + ησHT 3/2 · ˜O(log d)
The proof is then completed by further applying Lemma D.1.
Lemma D.1. Under the setting of Theorem 4.4, it holds that
E[f(¯xT ) −f(x∗)] ≤
4
(T + 1)2
T −1
X
t=0
E

f αt,¯xt(xt+1) −f αt,¯xt(x∗)

(44)
Proof of Lemma D.1. Plugging in the definition of f αt,¯xt in (9), we have
T −1
X
t=0
E

f αt,¯xt(x∗) −f αt,¯xt(xt+1)

=
T
X
t=0
α−2
t E[f(αtx∗+ (1 −αt)¯xt) −f(αtxt+1 + (1 −αt)¯xt)]
≤
T −1
X
t=0
α−2
t E[αtf(x∗) + (1 −αt)f(¯xt) −f(¯xt+1)]
=
T −1
X
t=0
α−2
t E[(1 −αt)(f(¯xt) −f(x∗)) −(f(¯xt+1) −f(x∗))]
=
T −1
X
t=1
(α−2
t (1 −αt) −α−2
t−1)E[f(¯xt) −f(x∗)]
+ α−2
0 (1 −α0)E[f(¯x0) −f(x∗)] −α−2
T −1E[f(¯xT ) −f(x∗)]
where the inequality follows from the convexity of f. Plugging in the choice of αt = 2/(t + 2), we further obtain
T −1
X
t=0
E

f αt,¯xt(x∗) −f αt,¯xt(xt+1)

= (T + 1)2
4
E[f(x∗) −f(¯xT )] −1
4
T −1
X
t=1
E[f(¯xt) −f(x∗)]
≤(T + 1)2
4
E[f(x∗) −f(¯xT )].
This completes the proof.
Lemma D.2. Under the setting of Theorem 4.4, it holds that
T −1
X
t=0
E

f αt,¯xt(xt) −f αt,¯xt(x∗)

≤E
D2ϵ
2η Tr(V −1
T −1) +
D2
2η + η
2
 T −1
X
t=0
g⊤
t V −1
t
gt

.
Proof of Lemma D.2. For every t = 0, 1, . . . , T −1, by the convexity of f αt,¯xt, it follows from the Taylor expansion
of f αt,¯xt at xt that
f αt,¯xt(xt) −f αt,¯xt(x∗) ≤⟨∇f αt,¯xt(xt), xt −x∗⟩= ⟨gt, xt −x∗⟩+ ⟨∇f αt,¯xt(xt) −gt, xt −x∗⟩
36

Since gt is an unbiased estimate of ∇f αt,¯xt(xt), taking expectation on both sides yields
E

f αt,¯xt(xt) −f αt,¯xt(x∗)

≤E[⟨gt, xt −x∗⟩]
By definition xt+1 = xt −ηV −1
t
gt, so it follows from Lemma D.4 that
⟨gt, xt −x∗⟩≤1
2η ∥xt −x∗∥2
Vt −1
2η ∥xt+1 −x∗∥2
Vt + η
2∥gt∥2
V −1
t
Define V−1 = 0 for convenience. Then combining the above two equations yields
E

f αt,¯xt(xt) −f αt,¯xt(x∗)

≤E
 1
2η ∥xt −x∗∥2
Vt −1
2η ∥xt+1 −x∗∥2
Vt + η
2∥V −1
t
gt∥2
Vt

= E
 1
2η ∥xt −x∗∥2
Vt−1 −1
2η ∥xt+1 −x∗∥2
Vt

+ E
 1
2η ∥xt −x∗∥2
Vt−Vt−1 + η
2∥V −1
t
gt∥2
Vt

.
Summing over t, we obtain
T −1
X
t=0
E

f αt,¯xt(xt) −f αt,¯xt(x∗)

≤E
"
−1
2η ∥xT −x∗∥2
VT −1 + 1
2η
T −1
X
t=0
∥xt −x∗∥2
Vt−Vt−1
#
+ E
η
2
T −1
X
t=0
∥V −1
t
gt∥2
Vt

.
Since Vt ⪰Vt−1, we have ∥xt −x∗∥2
Vt−Vt−1 ≤∥xt −x∗∥2
H Tr(Vt −Vt−1) ≤D2 ·Tr(Vt −Vt−1), where the second
inequality holds because of the assumption that maxt∈[T ] ∥xt −x∗∥H ≤D. Therefore, we further have
T −1
X
t=0
E

f αt,¯xt(xt) −f αt,¯xt(x∗)

≤E
D2
2η
T −1
X
t=0
Tr(Vt −Vt−1) + η
2
T −1
X
t=0
∥V −1
t
gt∥2
Vt

= E
D2
2η Tr(VT −1) + η
2
T −1
X
t=0
∥V −1
t
gt∥2
Vt

= E
D2
2η ⟨MT −1 + ϵId, V −1
T −1⟩+ η
2
T −1
X
t=0
∥V −1
t
gt∥2
Vt

= E
D2
2η ϵ · Tr(V −1
T −1) + D2
2η
T −1
X
t=0
g⊤
t V −1
T −1gt + η
2
T −1
X
t=0
g⊤
t V −1
t
gt

.
Since Vt ⪯VT −1 for all t ∈[T −1], we have g⊤
t V −1
T −1gt ≤g⊤
t V −1
t
gt for all t ∈[T −1]. Consequently,
T −1
X
t=0
E[f αt,¯xt(xt) −f αt,¯xt(x∗)] ≤E
D2ϵ
2η Tr(V −1
T −1) +
D2
2η + η
2
 T −1
X
t=0
g⊤
t V −1
t
gt

.
This completes the proof.
Lemma D.3. Under the setting of Theorem 4.4, it holds that
T −1
X
t=0
(f αt,¯xt(xt+1) −f αt,¯xt(x∗)) ≤
T −1
X
t=0
(f αt,¯xt(xt) −f αt,¯xt(x∗)) −η
T −1
X
t=0
∥V −1
t
gt∥2
Vt
+ η
T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩+ ΛH (f) η2
2
T −1
X
t=0
∥V −1
t
gt∥2
H∗.
37

Proof of Lemma D.3. By Definition 2.4, we have that
f(y) ≤f(x) + ⟨∇f(x), y −x⟩+ ΛH (f)
2
∥x −y∥2
H∗.
By the definition of f αt,¯xt in (9),
f αt,¯xt(xt+1) ≤f αt,¯xt(xt) + ⟨∇f αt,¯xt(xt), xt+1 −xt⟩+ ΛH (f)
2
∥xt+1 −xt∥2
H∗
= f αt,¯xt(xt) −η⟨∇f αt,¯xt(xt), V −1
t
gt⟩+ ΛH (f) η2
2
V −1
t
gt
2
H∗
= f αt,¯xt(xt) −η⟨gt, V −1
t
gt⟩+ η⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩+ ΛH (f) η2
2
V −1
t
gt
2
H∗
where we plug in the update rule for xt+1 in Algorithm 2. Summing over t yields
T −1
X
t=0
(f αt,¯xt(xt+1) −f αt,¯xt(x∗)) ≤
T −1
X
t=0
(f αt,¯xt(xt) −f αt,¯xt(x∗)) −η
T −1
X
t=0
∥V −1
t
gt∥2
Vt
+ η
T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩+ ΛH (f) η2
2
T −1
X
t=0
∥V −1
t
gt∥2
H∗.
This completes the proof.
The following lemma is a standard result for mirror descent.
Lemma D.4 (Lemma 5, Gupta et al. 2017). For any x0 ∈X, g ∈Rd and H ⪰0, let x1 = arg minx∈X ⟨g, x0⟩+
∥x −x0∥2
H. Then it holds that
⟨g, x0 −x1⟩≤1
2∥H−1g∥2
H + 1
2∥x0 −x1∥2
H.
Moreover, for any x ∈X, it holds that
⟨g, x1 −x⟩≤1
2∥x0 −x∥2
H −1
2∥x1 −x∥2
H −1
2∥x0 −x1∥2
H,
⟨g, x0 −x⟩≤1
2∥x0 −x∥2
H −1
2∥x1 −x∥2
H + 1
2∥H−1g∥2
H.
D.2
Stochastic case with projection
Algorithm 8 Accelerated Adaptive Algorithm with Projection
Hyperparam: ϵ ≥0, total steps T, learning rate η, convex cone H ⊂S+,radius D?
Input: initialization x0 = ¯x0, a sequence of positive constants α0, . . . , αT ∈(0, 1]
M−1 ←0
for t = 0, 1, 2, . . . , T :
gt ←∇f αt,¯xt
t
(xt) where f αt,¯xt
t
is defined in (9)
Mt ←Mt−1 + gtg⊤
t
Vt ←arg minH∈H

Mt + ϵId, H−1
+ Tr(H)
xt+1/2 ←xt −ηV −1
t
gt
xt+1 ←arg min∥x∥H≤D ∥x −xt+1/2∥2
Vt
¯xt+1 ←αtxt+1/2 + (1 −αt)¯xt
return ¯xT
Theorem D.5 shows the convergence rate of Algorithm 8, which is the same as Algorithm 2 while removing the
dependence on prior knowledge of D.
38

Theorem D.5. For a well-structured preconditioner set H, let f be a convex loss function whose H-smoothness
constant is ΛH (f) ∈(0, ∞) according to Definition 2.4. For ϵ > 0, T > 0, consider Algorithm 8 with αt = 2/(t + 2)
for t = 0, 1, . . . , T −1. Suppose the global minima x∗satisfies that ∥x∗∥H ≤D and Assumption 4.3 holds with
adaptive gradient variance σH({ft}T
t=1)2 ≤σ2
H for some σH ∈[0, ∞). Then it holds that
E[f(¯xT ) −f(x∗)] ≤
2D2ϵ
η(T + 1)2 E Tr(V −1
T −1) +
D2
2η −η
2

4
(T + 1)2 E
T −1
X
t=0
g⊤
t V −1
t
gt
+
2η2
(T + 1)2 · ΛH (f) · ˜O(log2 d) +
η
T 1/2 σH · ˜O(log d).
Moreover, when choosing learning rate η = D, the convergence rate becomes
E[f(¯xT ) −f(x∗)] = ˜O
ΛH (f) D2 log2 d + d√ϵD
T 2
+ σHD log d
√
T

.
Proof of Theorem D.5. Combining the results of Lemma D.7 and Lemma D.8, we get
T −1
X
t=0
E[f αt,¯xt(xt+1/2) −f αt,¯xt(x∗)] ≤E
D2ϵ
2η Tr(V −1
T −1) +
D2
2η −η
2
 T −1
X
t=0
g⊤
t V −1
t
gt

+ E

η
T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩+ η2ΛH (f)
2
T −1
X
t=0
∥V −1
t
gt∥2
H∗

.
The rest of the proof will be the same as the proof of Theorem 4.4.
Lemma D.6. Under the setting of Theorem D.5, it holds that
E[f(¯xT ) −f(x∗)] ≤
4
(T + 1)2
T −1
X
t=0
E

f αt,¯xt(xt+1/2) −f αt,¯xt(x∗)

(45)
Proof of Lemma D.6. The proof is the same as the proof of Lemma D.1 with xt+1/2 in place of xt+1.
Define V−1 = 0 for convenience.
Lemma D.7. Under the setting of Theorem D.5, it holds that
T −1
X
t=0
E

f αt,¯xt(xt) −f αt,¯xt(x∗)

≤E
D2ϵ
2η Tr(V −1
T −1) +
D2
2η + η
2
 T −1
X
t=0
g⊤
t V −1
t
gt

.
Proof of Lemma D.7. The proof is the same as the proof of Lemma D.2, except that controlling ⟨gt, xt −x∗⟩requires
some extra work. Specifically, by definition xt+1/2 = xt −ηV −1
t
gt, so it follows from Lemma D.4 that
⟨gt, xt −x∗⟩≤1
2η ∥xt −x∗∥2
Vt −1
2η ∥xt+1/2 −x∗∥2
Vt + η
2∥V −1
t
gt∥2
Vt.
Since xt+1 = arg minx∈X,∥x∥H≤D ∥x −xt+1/2∥2
Vt and ∥x∗∥H ≤D, it holds that
d
dλ∥xt+1 + λ(x∗−xt+1) −
xt+1/2∥2
Vt

λ=0 ≥0, which implies that (x∗−xt+1)⊤Vt(xt+1−xt+1/2) ≥0. Then it follows that ∥xt+1/2−x∗∥2
Vt =
∥xt+1 −x∗∥2
Vt + 2(x∗−xt+1)⊤Vt(xt+1 −xt+1/2) + ∥xt+1 −xt+1/2∥2
Vt ≥∥xt+1 −x∗∥2
Vt. Therefore, applying
this to the above inequality yields
⟨gt, xt −x∗⟩≤1
2η ∥xt −x∗∥2
Vt −1
2η ∥xt+1 −x∗∥2
Vt + η
2∥V −1
t
gt∥2
Vt.
Then the rest of the proof is the same.
39

Lemma D.8. Under the setting of Theorem D.5, it holds that
T −1
X
t=0
(f αt,¯xt(xt+1/2) −f αt,¯xt(x∗)) ≤
T −1
X
t=0
(f αt,¯xt(xt) −f αt,¯xt(x∗)) −η
T −1
X
t=0
∥V −1
t
gt∥2
Vt
+ η
T −1
X
t=0
⟨gt −∇f αt,¯xt(xt), V −1
t
gt⟩+ η2
2
T −1
X
t=0
∥V −1
t
gt∥2
H∗.
Proof of Lemma D.8. The proof is the same as the proof of Lemma D.3 with xt+1/2 in place of xt+1.
E
Proof for normalized steepest descent
Suppose that ∥·∥is a norm on Rd and ∥·∥∗is its dual norm. The following descent lemma characterizes the progress
of one step of steepest descent under ∥· ∥.
Lemma E.1 (Descent lemma). Let f : Rd →R be a differentiable function.
For any m ∈Rd, let u =
arg max∥v∥≤1 ⟨m, v⟩. Then for any η > 0, it holds that
f(x −ηu) ≤f(x) −η ∥∇f(x)∥∗+ 2η ∥∇f(x) −m∥∗+ η2
2 L∥·∥(f) .
Proof of Lemma E.1. By definition of the smoothness in Definition 2.3, we have
f(x −ηu) ≤f(x) −∇f(x)⊤(ηu) + ∥ηu∥2
2
L∥·∥(f)
= f(x) −η ⟨∇f(x) −m, u⟩−η ⟨m, u⟩+ η2
2 L∥·∥(f)
≤f(x) + η ∥∇f(x) −m∥∗−η ∥m∥∗+ η2
2 L∥·∥(f)
where in the last step we use ∥u∥= 1 and ⟨m, u⟩= ∥m∥∗by the definition of u. Then we further apply the triangle
inequality ∥m∥∗≥∥∇f(x)∥∗−∥∇f(x) −m∥∗to get
f(x −ηu) ≤f(x) −η ∥∇f(x)∥∗+ 2η ∥∇f(x) −m∥∗+ η2
2 L∥·∥(f) .
This concludes the proof.
Proposition E.2. For any ϵ ≥0, α ∈(0, 1), η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 3 with ∥· ∥.
Let L∥·∥(f) be the smoothness of the loss f w.r.t. ∥·∥according to Definition 2.3. Suppose Assumption 4.3 holds. Then
it holds that
1
T
T −1
X
t=0
E ∥∇f(xt)∥∗≤∆0
ηT + L∥·∥(f)η
2
+ 2
αT E ∥m0 −∇f(x0)∥∗+ 2(1 −α)η
α
L∥·∥(f)
+ 2α
T
T −1
X
t=0
E

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
Proposition E.2. Apply Lemma E.1 with ∥·∥yields that
f(xt+1) −f(xt) ≤−η ∥∇f(xt)∥∗+ L∥·∥(f)η2
2
+ 2η ∥∇f(xt) −mt∥∗.
(46)
Rearranging the telescoping sum of Eq. (46) from t = 0 to T −1, we have that
1
T
T −1
X
t=0
E ∥∇f(xt)∥∗≤1
ηT [f(x0) −min f(x)] + L∥·∥(f)η
2
+ 2
T
T −1
X
t=0
E ∥∇f(xt) −mt∥∗.
(47)
40

By the update rule of Algorithm 3, we have mt = (1 −α)mt−1 + αgt, so
mt −∇f(xt) = (1 −α)(mt−1 −∇f(xt−1)) + (1 −α)(∇f(xt−1) −∇f(xt)) + α(gt −∇f(xt)).
Unrolling the above recursion, we get that
mt −∇f(xt) = (1 −α)t(m0 −∇f(x0)) +
t
X
i=1
(1 −α)t−i[(1 −α)(∇f(xi−1) −∇f(xi)) + α(gi −∇f(xi))].
and
E∥∇f(xt) −mt∥∗≤(1 −α)tE∥m0 −∇f(x0)∥∗+
t
X
i=1
(1 −α)t−i+1∥∇f(xi−1) −∇f(xi)∥∗
+ αE

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
(48)
We know that
∥∇f(xi−1) −∇f(xi)∥∗≤L∥·∥(f) ∥xi−1 −xi∥≤ηL∥·∥(f) .
(49)
Then plugging (49) into (48), we can get that
E ∥∇f(xt) −mt∥∗≤(1 −α)tE ∥m0 −∇f(x0)∥∗+ (1 −α)ηL∥·∥(f)
t
X
i=1
(1 −α)t−i + αE

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
≤(1 −α)tE ∥m0 −∇f(x0)∥∗+ 1 −α
α
ηL∥·∥(f) + αE

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
.
Plugging it in Eq. (47), we have that
1
T
T −1
X
t=0
E ∥∇f(xt)∥∗≤∆0
ηT + L∥·∥(f)η
2
+ 2
T
T −1
X
t=0
"
(1 −α)tE ∥m0 −∇f(x0)∥∗+ 1 −α
α
ηL∥·∥(f) + αE

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
#
= ∆0
ηT + L∥·∥(f)η
2
+ 2
αT E ∥m0 −∇f(x0)∥∗+ 2(1 −α)η
α
L∥·∥(f)
+ 2α
T
T −1
X
t=0
E

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
E.1
Convergence rate with respect to adaptive gradient variance
The proof of Theorem 4.6 largely borrows from Kovalev (2025b), upon which we change the dependence on adaptive
noise and improve some coefficient constant.
Theorem 4.6. Let H be a well-structured preconditioner set. For any ϵ ≥0, α ∈(0, 1), η > 0, and T ∈N, let
{xt}T
t=0 be the iterates of Algorithm 3 with ∥· ∥= ∥· ∥H and m0 = ∇f0(x0). Let L∥·∥H(f) be the smoothness
of the loss f w.r.t. ∥·∥H according to Definition 2.3. Suppose Assumption 4.3 holds with adaptive gradient variance
σH({ft}T
t=1)2 ≤σ2
H for some σH ∈[0, ∞). Then it holds that
E 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗≤∆0
ηT + 2η
α L∥·∥(f) + 2σH
αT + 2σH
√α.
Let a0 =
q
∆0L∥·∥(f)/σH. If a0 < 1, then
41

• When T < a−6
0 , we will choose α = T −2/3 and η =
q
∆0
L∥·∥H(f)T −5/12. The rate is O
 σHT −1/3
.
• When T ≥a−6
0 , we will choose α =
a0
√
T and η =
∆3/4
0
L∥·∥H(f)1/4σ1/2
H T −3/4. The rate is O

(∆0L∥·∥H(f))1/4√σH
T 1/4

.
If a0 ≥1, then
• When T ≤a2
0, we will choose α = 1 and η =
q
∆0
L∥·∥H(f)T −1/2. The rate is O
q
∆0L∥·∥H(f)T −1/2
.
• When T ≥a2
0, we will choose α =
a0
√
T and η =
∆3/4
0
L∥·∥H(f)1/4σ1/2
H T −3/4. The rate is O

(∆0L∥·∥H(f))1/4√σH
T 1/4

.
Proof of Theorem 4.6. By applying Proposition E.2 with ∥·∥= ∥·∥H, we have that
1
T
T −1
X
t=0
E ∥∇f(xt)∥H,∗≤∆0
ηT + L∥·∥(f)η
2
+ 2
αT E ∥m0 −∇f(x0)∥H,∗+ 2(1 −α)η
α
L∥·∥(f)
+ 2
T
T −1
X
t=0
E

t
X
i=1
(1 −α)t−iα(gi −∇f(xi))

H,∗
.
(50)
We only need to bound E ∥m0 −∇f(x0)∥H,∗and E

Pt
i=1(1 −α)t−iα(gi −∇f(xi))

H,∗
. For notational
simplicity, we define ωi = (1 −α)t−iα and δi = gi −∇f(xi). By Assumption 4.3, we know that E[δj|δi] = 0 for
j > 1. Then we have that
E

t
X
i=1
(1 −α)t−iα(gi −∇f(xi))

2
H,∗
= E

t
X
i=1
ωiδi

2
H,∗
= E
min
H∈H,Tr(H)≤1

t
X
i=1
ωiδi

H−1
t
X
i=1
ωiδi

≤
min
H∈H,Tr(H)≤1 E

t
X
i=1
ωiδi

H−1
t
X
i=1
ωiδi

=
min
H∈H,Tr(H)≤1 E
t
X
i=1
ω2
i δ⊤
i H−1δi
=
min
H∈H,Tr(H)≤1 E
t
X
i=1
ω2
i ∥δi∥2
H−1 .
Now by the condition on σH and the definition of the adaptive gradient variance in Definition 4.1, we know that
minH∈H,Tr(H)≤1 E ∥δi∥2
H−1 ≤σ2
H for each i ∈[t]. Therefore, we further have
E

t
X
i=1
(1 −α)t−iα(gi −∇f(xi))

2
H,∗
≤σ2
H
t
X
i=1
ω2
i ≤σ2
Hα
(51)
where the second inequality holds by the definition of ωi and the condition that α ∈(0, 1). Similarly, we also have
E ∥m0 −∇f(x0)∥2
H,∗= E ∥g0 −∇f(x0)∥2
H,∗≤σ2
H.
(52)
Plugging in Eq. (51) and Eq. (52) into Eq. (50), we can get
E 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗≤∆0
ηT + ηL∥·∥H(f)
2
+ 2σH
αT + 2(1 −α)η
α
L∥·∥H(f) + 2√ασH
≤∆0
ηT + 2η
α L∥·∥H(f) + 2σH
αT + 2√ασH.
(53)
42

Next, we will choose appropriate hyperparameters η and α to achieve the optimal rate.
For any fixed α < 1, to balance the first two terms on the right-hand side of (53), we choose
η =
s
∆0α
2TL∥·∥H(f).
Also, for simplicity, denote a0 =
q
∆0L∥·∥H(f)/σH. Then the convergence rate becomes
E 1
T
T −1
X
t=0
∥∇f(xt)∥H,∗≤2
s
2∆0L∥·∥H(f)
αT
+ 2σH
αT + 2√ασH
≤2
√
2σH(f)
 a0
√
αT
+ 1
αT + √α

.
(54)
We further stratify the values of α, a0 and T to minimize the right-hand side:
• When α ≤T −2/3,
1
αT ≥√α. We only need to minimize
1
αT +
a0
√α
√
T , which is achieved at α = T −2/3 because
it is a monotone decreasing function in α. So the convergence rate is always no better than that in the case of
α ≥T −2/3.
• When α ≥T −2/3,
1
αT ≤√α. We only need to minimize √α +
a0
√
αT . Then
a0
√
T is the global minimizer but
there are constraints T −2/3 ≤α ≤1.
– When a0 ≤T −1/6, we have that
a0
√
T ≤T −2/3. Then we need to choose α = T −2/3 and the rate is
σHT −1/3 +
q
∆0L∥·∥H(f)T −1/6 ≤2σHT −1/3.
– When a0 ≥
√
T, we have that
a0
√
T ≥1.
Then we need to choose α = 1 and the rate is σH +
q
∆0L∥·∥H(f)T −1/2 ≤2
q
∆0L∥·∥H(f)T −1/2.
– When T −1/6 ≤a0 ≤
√
T, we have that T −2/3 ≤
a0
√
T ≤1. We can choose α =
a0
√
T . The rate becomes
(∆0L∥·∥H(f))1/4√σHT −1/4.
This completes the proof.
E.2
Convergence rate under general-norm assumption
Definition E.3 (Distortion of norms). For any target norm ∥·∥and a reference norm ∥·∥ref on a finite-dimensional
vector space, the corresponding distortion of ∥·∥with respect to ∥·∥ref is defined as
ψ(∥·∥, ∥·∥ref) := sup
x̸=0
∥x∥
∥x∥ref
· sup
x̸=0
∥x∥ref
∥x∥.
(55)
Since all norms on the finite-dimensional space are equivalent, ψ(∥·∥, ∥·∥ref) is always finite.
Theorem 4.8. For any ϵ ≥0, α ∈(0, 1), η > 0, and T ∈N, let {xt}T
t=0 be the iterates of Algorithm 3 with any norm
∥·∥. Suppose Assumption 4.3 holds with the gradient variance σ∥·∥∗({ft}T
t=1)2 ≤σ2
∥·∥∗for some σ∥·∥∗∈[0, ∞). Then
it holds that
E 1
T
T −1
X
t=0
∥∇f(xt)∥∗≤∆0
ηT + 2η
α L∥·∥(f) + 2
αT σ∥·∥∗+ 2σ∥·∥∗· min

1, α1/2ψ(∥· ∥∗, ∥· ∥2)

where ψ(∥· ∥∗, ∥· ∥2) = supx
∥x∥∗
∥x∥2 · supx
∥x∥2
∥x∥∗measures the distortion between the two norms.
43

Proof of Theorem 4.8. Since E[∥m0 −∇f(x0)∥∗] ≤σ∥·∥∗, it follows from Proposition E.2 that
1
T
T −1
X
t=0
E ∥∇f(xt)∥∗≤∆0
ηT + L∥·∥(f)η
2
+ 2σ∥·∥∗
αT
+ 2(1 −α)η
α
L∥·∥(f)
+ 2α
T
T −1
X
t=0
E

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
.
(56)
There are two ways to control the last terms, and we discuss them separately below.
For the first approach, we directly apply triangle inequality to get
α · E

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
≤E
t
X
i=1
α(1 −α)t−i ∥gi −∇f(xi)∥∗
≤
t
X
i=1
α(1 −α)t−i
E ∥gi −∇f(xi)∥2
∗
1/2 ≤σ∥·∥∗.
(57)
For the second approach, we first convert ∥· ∥∗to ∥· ∥2 and get
E
α
t
X
i=1
(1 −α)t−i(gi −∇f(xi))

2
∗

≤

sup
x
∥x∥∗
∥x∥2
2
· E

t
X
i=1
α(1 −α)t−i(gi −∇f(xi))

2
2

=

sup
x
∥x∥∗
∥x∥2
2
·
t
X
i=1
α2(1 −α)2(t−i) · E[∥gi −∇f(xi)∥2
2]
≤

sup
x
∥x∥∗
∥x∥2
2 
sup
x
∥x∥2
∥x∥∗
2
·
t
X
i=0
(1 −α)2iα2E[∥gi −∇f(xi)∥2
∗]
≤ψ(∥·∥∗, ∥·∥2)2 · σ2
∥·∥∗· α.
Using Jensen’s inequality, this further implies that
α · E

t
X
i=1
(1 −α)t−i(gi −∇f(xi))

∗
≤α1/2ψ(∥· ∥∗, ∥· ∥2) · σ∥·∥∗.
(58)
Now, by taking the minimum over the two bounds in (57) and (58), we obtain from (56) that
1
T
T −1
X
t=0
E ∥∇f(xt)∥∗≤∆0
ηT + L∥·∥(f)η
2
+ 2σ∥·∥∗
αT
+ 2(1 −α)η
α
L∥·∥(f) + 2σ∥·∥∗· min

1, α1/2ψ(∥· ∥∗, ∥· ∥2)

.
This completes the proof.
E.3
Dimension dependent lower bound for NSD
In this subsection, we construct the example for the lower bound in Theorem 4.9.
Lemma E.4. Fix any ϵ > 0. For any distinct points x0, x1, . . . , xT ∈R where T + 1 ≤
1
4ϵ2 , there exists a function
p : R →R, such that its derivative p′ is 1-Lipschitz, p(x0) −infx p(x) ≤1 and p′(xt) = −ϵ for t = 0, 1, . . . , T.
Proof of Lemma E.4. We use g(x) to denote p′(x) in the proof. For any set of points {xt}T
t=0 ⊆R, we will construct
g(x) such that g(x) = −ϵ for x < x0 and g(x + 1
ϵ ) = g(x) for x ≥x0. We first explain what it requires to construct
this function g: For such a g, because of the periodicity, we can always find x0 = y0 ≤y1 ≤· · · ≤yT ′ < x0 + 1
ϵ for
T ′ ≤T so that for any xt ≥x0, there is a ys such that there exists n ∈N satisfying xt = ys + 1
ϵ · n and therefore
g(xt) = g(ys). For any xt < x0, g(xt) = −ϵ by definition. Also for x ≤x0, it holds that p(x0) ≤p(x) and g(x) is
1-Lipschitz. Then it suffices to construct g(x) in the interval [x0, x0 + 1
ϵ ) to satisfy the following conditions
44

• g(x) is 1-Lipschitz in [x0, x0 + 1
ϵ ].
• For any x > x0, it can be written as x = x0 + δ + nϵ−1 for some δ ∈[0, ϵ−1) and non-negative integer n, and
thus p(x) −p(x0) =
R x0+δ
x0
g(x)dx + n
R x0+ϵ−1
x0
g(x)dx. Therefore, to ensure that p(x0) −infx p(x) ≤1, we
require p(x0) −p(x0 + δ) =
R x0+δ
x0
g(x)dx ≥−1 for any 0 ≤δ ≤1
ϵ and
R x0+ 1
ϵ
x0
g(x)dx ≥0.
• g(ys) = −ϵ for s = 0, 1, . . . , T ′.
We will further define yT ′+1 = x0 + 1
ϵ in the following construction.
For each t = 0, 1, . . . , T ′, we construct g on the interval [yt, yt+1] as follows:
g(x) =
(
−ϵ + (x −yt),
x ∈[yt, (yt + yt+1)/2];
−ϵ −(x −yt+1),
x ∈((yt + yt+1)/2, yt+1].
It is straightforward to see that g is 1-Lipschitz on [x0, x0 + ϵ−1] and g(yt) = −ϵ for t = 0, 1, . . . , T ′ + 1. Also, note
that g(x) ≥−ϵ for all x ∈[x0, x0 + ϵ−1]. Thus, for any 0 ≤δ ≤1
ϵ , it always holds that
R x0+δ
x0
g(x)dx ≥−ϵδ ≥−1.
Then it only remains to show
R x0+ 1
ϵ
x0
g(x)dx ≥0. By direct calculation, we have
Z x0+ 1
ϵ
x0
g(x)dx =
T ′
X
t=0
Z yt+1
yt
g(x)dx =
T ′
X
t=0

−ϵ(yt+1 −yt) + (yt+1 −yt)2
4

= −1 + 1
4
T ′
X
t=0
(yt+1 −yt)2
≥−1 + 1
4
(PT ′
t=0(yt+1 −yt))2
T ′ + 1
= −1 +
1
4ϵ2(T ′ + 1)
where the inequality holds by Cauchy-Schwarz inequality. Since T ′ + 1 ≤T + 1 ≤
1
4ϵ2 , it immediately follows that
R x0+ 1
ϵ
x0
g(x)dx ≥0. This completes the proof.
Theorem 4.9. For any fixed ∆0, L, σ2, d, T, learning rate η, and any averaging parameter α, there exists a loss
function f : Rd →R, a sequence of stochastic iid loss functions f0, f1, · · · , fT −1 and an initialization x0 satisfying
the following conditions
(a) f(x0) −infx f(x) = ∆0 and L∥·∥∞(f) ≤L;
(b) For any x ∈Rd, it holds that E[∇ft(x)] = ∇f(x) and E[∥∇ft(x) −∇f(x)∥2
1] ≤σ2.
When running Algorithm 3 with ∥· ∥= ∥· ∥∞, learning rate η, averaging parameter α and initialization x0 = 0 on
the stochastic functions f0, · · · , fT −1, it holds that
E
h
min
t∈[T ] ∥∇f(xt)∥1
i
= min{e−25−1
4 (dL∆0σ2)
1
4 T −1
2 , e−25−1
2 σ}
Proof of Theorem 4.9. Inspired by Lemma 1 in Chewi et al. (2023), we first rescale the original problem to a parameter-
free scaling for simplicity.
Indeed, for any f, {ft}T −1
t=0 , x0 satisfying the conditions (a) and (b), we can define
h(x) = ∆−1
0
· f(
p
∆0/Lx) and ht(x) = ∆−1
0
· ft(
p
∆0/Lx).
Then it can be verified that h satisfies that
h(x0)−infx h(x) = 1 and L∥·∥∞(h) ≤1. Therefore, it suffices to construct h and associated stochastic loss functions
h0, h1, . . . , hT −1 such that h(x0)−infx h(x) = 1, L∥·∥∞(h) ≤1 and E[∥∇ht(x)−∇h(x)∥1] ≤σ/√L∆0, and then
the construction is completed with the reverse transform f(x) = ∆0 · h(
p
L/∆0x) and ft(x) = ∆0 · ht(
p
L/∆0x).
Correspondingly, we can rescale the learning rate and the iterates of Algorithm 3 on the loss function f to trans-
form {xt}T
t=0 to be iterates obtained by running Algorithm 3 on the loss function h: It can be easily checked that
xt =
p
∆0/L˜xt, where {˜xt}T −1
t=0 are the iterates obtained by running Algorithm 3 on h0, . . . , hT −1 with learning rate
45

η
p
L/∆0 and momentumfactorα. ThenifitholdsthatE[minT −1
t=0 ∥∇h(˜xt)∥1] = min

e−25−1
4 (dσ′2)1/4T −1/2, e−25−1
2 σ′	
where σ′ = σ/√L∆0, we immediately have
E
h
min
t=0,1,...,T −1 ∥∇f(xt)∥1
i
= E
h
min
t=0,1,...,T −1
p
L∆0 · ∥∇h(˜xt)∥1
i
= min{e−25−1
4 (dL∆0σ2)
1
4 T −1
2 , e−25−1
2 σ}
which gives the desired result in Theorem 4.9. Given the above discussion, we only need to construct a loss function
f : Rd →R and associated stochastic loss functions f0, f1, . . . , fT −1 such that f(x0)−infx f(x) = 1, L∥·∥∞(f) ≤1
and E[∥∇ft(x) −∇f(x)∥2
1] ≤σ2 for all t = 0, . . . , T −1, and show that
E
h
min
t=0,...,T −1 ∥∇f(xt)∥1
i
= min

e−25−1
4 d
1
4 σ
1
2
T
1
2
, e−25−1
2 σ

Below we present a construction for such a hard instance.
Construction of loss functions.
We consider initialization x0 = 0. For convenience, we define the target quantity
as
ϵ = min
 d1/4σ1/2
51/4T 1/2 , σ
√
5

.
(59)
We first construct a sequence of independently random noise vectors δ0, δ1, . . . , δT −1 as follows. For constant C = σ2
5ϵ
and constant θ =
5ϵ2
dσ2 , each random vector δt = −C[Rt,1, · · · , Rt,d] where iid variables Rt,i ∼Bernoulli(θ).
The Bernoulli distribution is well-defined because θ ≤1
d ≤1 by the choice of ϵ in Eq. (59). Then we know that
E[δt] = −Cθ1d = −ϵ
d1d,
E[∥δt∥1] = dE[|δt,1|] = Cdθ = ϵ
and
E

∥δt −E[δt]∥2
1

= C2E

 d
X
i=1
|Rt,i −θ|
!2 
= C2 h
dE[|Rt,1 −θ|2] + d(d −1)E[|Rt,1 −θ|]2i
= C2 
d

θ(1 −θ)2 + (1 −θ)θ2
+ d(d −1)[2θ(1 −θ)]2
= C2 
dθ(1 −θ) + d(d −1)4θ2(1 −θ)2
≤C2  dθ + 4d2θ2
≤5C2dθ = σ2
(60)
The last inequality holds because of dθ ≤1 by the definition of θ.
And we can verify that E[∥δt∥1] = ϵ and
E

∥δt −E[δt]∥2
1

≤5C2dθ = σ2 by plugging in C = σ2
5ϵ and θ = 5ϵ2
dσ2 .
Next, we construct the loss function f. Consider the set of points Sη,ϵ = {0, η, 2η, . . . , (N −1)η} with N = ⌊1
4ϵ2 ⌋.
We then invoke Lemma E.4 with set Sη,ϵ and level −ϵ to obtain a one-dimensional function p, such that its derivative
p′ is 1-Lipschitz, p(0) −infx p(x) ≤1 and p′(kη) = −ϵ for all k ≤N. Now we define f : Rd →R as
f(x) = 1
d
d
X
i=1
p(xi)
(61)
Leveraging the properties of p, we can show that for any x, x′ ∈Rd,
f(x0) −inf
x f(x) ≤1
d
d
X
i=1
 p(0) −inf
x p(x)

≤1,
∥∇f(x) −∇f(x′)∥1 = 1
d
d
X
i=1
|p′(xi) −p′(x′
i)| ≤∥x −x′∥∞,
46

where the last inequality holds because p′ is 1-Lipschitz. This shows that f satisfies the desired condition (a). Then
for each t = 0, 1, . . . , T −1, we define the stochastic loss function ft as
ft(x) = f(x) + ⟨δt, x⟩+ ϵ
d⟨1d, x⟩.
(62)
Its gradient is given by
∇ft(x) = ∇f(x) + δt + ϵ
d1d.
By the previous construction of δt, it is clear that E[∇ft(x)] = ∇f(x), and it follows from (60) that the vari-
ance of ∇ft(x) under ∥· ∥1 satisfies E[∥∇ft(x) −∇f(x)∥2
1] = E[∥δt −E[δt]∥2
1] ≤σ2. Therefore, the stochas-
tic loss functions f0, f1, . . . , fT −1 satisfy the condition (b).
Next, we proceed to establish the lower bound for
E[minT −1
t=0 ∥∇f(xt)∥1], where x0, x1, . . . , xT −1 are obtained by running Algorithm 3 on the constructed loss func-
tions f0, f1, . . . , fT −1 with x0 = 0.
Lower bound on gradient norm.
First note that by the construction of the loss functions and the choice x0 = 0,
the coordinate-wise dynamics have the same distribution across coordinates, i.e., (x0,i, x1,i, . . . , xT −1,i) has the same
distribution as (x0,j, x1,j, . . . , xT −1,j) for all i, j ∈[d]. Utilizing this observation, we have
E
h
min
t∈[T ] ∥∇f(xt)∥1
i
= E

min
t∈[T ]
1
d
d
X
i=1
|p′(xt,i)|

≥1
d
d
X
i=1
E
h
min
t∈[T ] |p′(xt,i)|
i
= E
h
min
t∈[T ] |p′(xt,1)|
i
.
(63)
So it suffices to focus on the dynamics in the first coordinate.
By the construction of the function p in the previous step, we have p′(0) = −ϵ. Therefore, when xt,1 = 0, we have
[∇ft(x)]1 = p′(x1)/d + δt,1 + ϵ/d = δt,1. Consequently, since x0,1 = 0, the first coordinate of xt will remain to be
zero until at some step δt,1 ̸= 0, in which case we must have δt,1 = −C. This implies that
min{t ∈[T] : xt,1 ̸= 0} −1 = min

t ∈[T] : [∇ft(xt)]1 = −C

=: τ,
and this stopping time τ follows a geometric distribution with parameter θ =
5ϵ2
dσ2 again by the distribution of δt.
Hence,
P

τ ≥1
θ

= P

τ ≥⌈1
θ⌉

= (1 −θ)⌈1
θ ⌉
≥exp

−θ⌈1
θ⌉

≥e−1−θ ≥e−2.
Now conditioned on the event that τ ≥1
θ, we can show that
τ + N ≥1
θ + 1
4ϵ2 ≥2
r
1
θ · 1
4ϵ2 =
1
√
θϵ
=
√
dσ
√
5ϵ2 ,
which is no smaller than T by the choice of ϵ in Eq. (59). Therefore, we have xt,1 = 0 for all t = 0, 1, . . . , τ and there
are at most T −τ distinct points among x0,1, x1,1, . . . , xT,1. Then since |xt+1 −xt| is either 0 or η by the update rule
of Algorithm 3 with ∥· ∥= ∥· ∥∞, we see that x0,1, x1,1, . . . , xT,1 ∈{0, η, 2η, . . . , (N −1)η} on the event τ ≥1
θ,
47

in which case we have p′(xt,1) = −ϵ for all t = 0, 1, . . . , T by our construction of p. Leveraging this, it follows from
Eq. (63) that
E
h
min
t∈[T ] ∥∇f(xt)∥1
i
≥E
h
min
t∈[T ] |p′(xt,1)|
i
≥P
h
τ ≥1
θ
i
· E
h
min
t∈[T ] |p′(xt,1)|
 τ ≥1
θ
i
≥e−2ϵ
This gives the desired lower bound and completes the proof.
48
