Nonparametric Instrumental Variable Regression with Observed
Covariates
Zikai Shenâˆ—1, Zonghao Chenâˆ—2, Dimitri Meunier3, Ingo Steinwart4, Arthur Grettonâ€ 3, and
Zhu Liâ€ 3
1Department of Statistical Science, University College London
2Department of Computer Science, University College London
3Gatsby Computational Neuroscience Unit, University College London
4Department of Mathematics, University of Stuttgart
Abstract
We study the problem of nonparametric instrumental variable regression with observed
covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental
variable regression (NPIV), the additional observed covariates facilitate causal identification and
enables heterogeneous causal effect estimation. However, the presence of observed covariates
introduces two challenges for its theoretical analysis. First, it induces a partial identity structure,
which renders previous NPIV analysesâ€”based on measures of ill-posedness, stability conditions,
or link conditionsâ€”inapplicable. Second, it imposes anisotropic smoothness on the structural
function. To address the first challenge, we introduce a novel Fourier measure of partial
smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable
algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales
adaptive to the anisotropic smoothness. We prove upper L2-learning rates for KIV-O and
the first L2-minimax lower learning rates for NPIV-O. Both rates interpolate between known
optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap
between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned
to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference,
an emerging framework for causal effect estimation that shares the same conditional moment
restriction as NPIV-O.
1
Introduction
We consider the problem of identifying and estimating the causal effect of a treatment variable X
on an outcome variable Y , where their relationship is confounded by an unobserved confounder Ïµ.
Despite the existence of unobserved confounding, it is nonetheless possible to identify the causal
effect by leveraging an instrumental variable Z. For instance, if one aims to identify the causal
effect of smoking (X) on the risk of lung disease (Y ), which may be potentially confounded by an
individualâ€™s occupation and early childhood environment (Ïµ), the cigarette cost (Z) would be a valid
instrument as it only affects the risk of lung disease via smoking [Leigh and Schembri, 2004]. For
cases where X and Y are continuous (and possibly multivariate) random variables, Nonparametric
Instrumental Variables Regression (NPIV) has received significant attention. NPIV is particularly
âˆ—,â€  Equal contribution in random order.
1
arXiv:2511.19404v1  [stat.ML]  24 Nov 2025

valuable as it avoids imposing potentially misspecified parametric or semiparametric assumptions
when such structure is not warranted [Newey and Powell, 2003, Horowitz, 2011]. Prior literature has
explored various algorithms for NPIV. These include methods based on: kernel density estimation
[Hall and Horowitz, 2005, Darolles et al., 2011], sieve minimum distance estimators [Chen and Pouzo,
2012, Newey and Powell, 2003, Chen and Christensen, 2018] and more recently Reproducing Kernel
Hilbert Spaces (RKHSs) with the Kernel Instrumental Variables (KIV) algorithm [Singh et al.,
2019, Meunier et al., 2024a], which is a nonparametric generalization of the two-stage least squares
(2SLS) algorithm. Another family of nonparametric algorithms is based on min-max optimization
[Bennett et al., 2019, Dikkala et al., 2020, Bennett et al., 2023]. We defer a full discussion of these
approaches to Section 3.
Practitioners often have access to observed covariates O. These observed covariates encode
individual-level characteristics, which allow for the estimation of heterogenous causal effects. Re-
turning to the smoking example, confounders such as an individualâ€™s occupation fall into this
subset as it is readily observable. Such extra information enables the estimation of heterogeneous
treatment effectsâ€”for instance, the causal effect of smoking on lung disease specifically for manual
workers. In this work, we refer to the NPIV framework that incorporates observed covariates as
NPIV-O. Formally, we introduce the following NPIV-O model,
Y = fâˆ—(X, O) + Ïµ,
E[Ïµ | Z, O] = 0.
(1)
We refer to fâˆ—as the heterogeneous dose response curve, and it is our target of interest. Here, Ïµ is an
unobserved confounder that affects Y additively. By Eq. (1), we implicitly assume that Z can only
possibly affect Y through X, a condition known as exclusion restriction. The mean independence
E[Ïµ | Z, O] = 0 is a relaxation of the stronger unconfoundedness assumption, that requires (Z, O) to
be independent of Ïµ. We also require that Z and X are not independent, a condition known as
instrumental relevance. A random variable Z that satisfies these requirements is referred to as a
valid instrumental variable. For a detailed discussion of our assumptions, we refer to Section 4. We
also note that the observed covariates O aid identification by capturing non-linear confounding
effects, thereby relaxing the strict additivity assumption required for all confounders in classical
NPIV.
Incorporating observed covariates in NPIV estimation poses both an algorithmic and a theoretical
challenge. To understand this, define the conditional expectation operator
T : L2(PXO) â†’L2(PZO),
f 7â†’E[f(X, O) | Z, O].
Eq. (1) is equivalent to the following conditional moment restriction for fâˆ—:
E[Y | Z, O] = (Tfâˆ—)(Z, O).
(2)
We refer the reader to Assumption 4.1 for the technical assumption ensuring unique identification
of fâˆ—from this equation. With the presence of O, T acts as an identity operator on the infinite
dimensional function space
F1 = {f : f(X, O) = f1(O), f1 âˆˆL2(PO)} âŠ‚L2(PXO),
(3)
rendering the operator non-compact. A naive application of kernel instrumental variables without
observed covariates [Singh et al., 2019] augments both X and Z to (X, O) and (Z, O), without
exploiting the fact that T is partially an identity operator. As a result, this algorithm is not
consistent, a point we elaborate on in Section 2.1. From a theoretical standpoint, T being partially
2

an identity operator fundamentally alters the statistical properties of NPIV-O estimation compared
with classical NPIV, posing significant challenges as detailed below.
The first challenge is to characterize the degree of ill-posedness of the inverse problem, Eq.
(2), while accounting for the fact that T is partially an identity operator. We consider another
function space F2 = {f : f(X, O) = f2(X), f2 âˆˆL2(PX)} âŠ‚L2(PXO). In this case, under mild
conditions on the conditional distributions [Darolles et al., 2011, Assumption A.1], T when treated
as a mapping from F2 to L2(PZO) is a compact operator whose smoothing effect can be quantified
through the rate of decay of its singular values. This mixed behaviour of T reveals the nature of
NPIV-O as a hybrid between NPIV and Nonparametric Regression (NPR). Existing theoretical
analyses of NPIV estimator mostly preclude the case where T acts as a partial identity operator
[Blundell et al., 2007, Chen and Reiss, 2011, Chen and Pouzo, 2012, Chen and Christensen, 2018,
Meunier et al., 2024a, Kim et al., 2025, Chen et al., 2024], making them unsuitable for the NPIV-O
setting in Eq. (1). We refer the reader to Section 3 and Remark 4.2 for a more in-depth discussion.
Another challenge is the anisotropic smoothness of the heterogenous dose response curve fâˆ—
across the treatment X and observed covariates O. In real-world applications, the treatment
X (e.g. smoking) is often one dimensional, while the observed covariates O (e.g. occupation,
age, gene) are of higher dimensionality. This is because practitioners tend to adjust for as many
observed covariates as possible, in an effort to overcome unobserved confounding. Therefore, a
desirable algorithm would adapt to the intrinsic smoothness of fâˆ—, thereby adapts appropriately
to the high intrinsic smoothness when the directional smoothness is highly anisotropic [Hoffman
and Lepski, 2002]. To achieve this desirable property, we modify the KIV-O algorithm to select
kernel lengthscales adaptively to the varying directional smoothness of fâˆ—. In contrast, existing
NPIV algorithms are typically analyzed under an isotropic smoothness assumption, meaning their
established convergence rates are dictated by the worst smoothness across all dimensions [Singh
et al., 2019, 2024]. This limitation of these theoretical guarantees becomes increasingly severe in
high dimensions.
In this paper, we tackle the above two challenges and make the following contributions.
1. Fourier measure of partial smoothing effect of T: In Section 4.1, we introduce a new framework
based on Fourier spectra which quantifies the partial smoothing effect of T for functions
f : X Ã— O â†’R with only high frequencies on X; while conversely quantifies the partial anti-
smoothing effect of T for functions f : X Ã— O â†’R with only low frequencies on X. Our Fourier
measure of partial smoothing effect resembles existing ones based on sieves [Blundell et al., 2007,
Chen et al., 2024, Kim et al., 2025], but features two key distinctions: 1) Our framework takes
into account the partial identity structure of T caused by the existence of observed covariates
O. 2) Our framework aligns, through Bochnerâ€™s theorem, with the RKHS of a continuous
translational invariant kernel. This alignment will be useful for our next contribution.
2. Upper and minimax lower L2(PXO)-learning rate: Under the above framework that quantifies the
partial smoothing effect of T, we prove an upper learning rate (Theorem 4.1) for a kernel based
algorithm for instrumental variable regression with observed covariates proposed in Singh et al.
[2024], termed KIV-O. Furthermore, we prove the first minimax lower learning rate (Theorem 4.2)
for NPIV-O defined in Eq. (1). All our bounds hold in the strong L2(PXO) norm rather than
the pseudo-metric âˆ¥T(Â·)âˆ¥L2(PZO) considered in Singh et al. [2019]. For the following two edge
cases: 1) No observed covariates: NPIV-O reduces to classical NPIV; 2) No hidden confounding:
instrument variables Z are unnecessary and NPIV-O reduces to NPR, our upper bound of KIV-O
matches the minimax lower bound, recovering earlier results on minimax optimality of classical
kernel instrumental variable regression [Meunier et al., 2024a] and kernel ridge regression [Hang
and Steinwart, 2021, Fischer and Steinwart, 2020], respectively established under analogous
3

assumptions to our work. In the general intermediate case where T exhibits a partial identity
structure, both our upper and lower learning rates interpolate accordingly, however, there exists
a gap between the upper and minimax lower bound. We posit that this gap is fundamental and
we provide insights on why this gap emerges in Section 5.
3. Adaptivity to model intrinsic smoothness: We modify the existing KIV-O algorithm to select
kernel lengthscales separately for each dimension, adaptive to the varying directional smoothness
of fâˆ—. We prove that its learning rate takes into account the anisotropic smoothness of fâˆ—
across the treatment X and observed covariates O. Compared with existing KIV algorithms
and their associated analyses that assume isotropic smoothness [Fischer and Steinwart, 2020,
Meunier et al., 2024a, Singh, 2020, Singh et al., 2024], our learning rate is adaptive to the target
functionâ€™s intrinsic smoothness, and alleviates the slow rate caused by the need to account for
the worst-case smoothness, when the anisotropic smoothness is highly imbalanced.
4. Interpretable anisotropic smoothness assumption: Another key feature of our upper and lower
bounds is that they highlight the separate contribution of the partial smoothing effect of T and
the anisotropic smoothness of fâˆ—, characterized by an anisotropic Besov space. In contrast, much
work in the NPIV literature employ a generalized source condition with respect to the unknown
conditional expectation operator T [Engl et al., 1996, Singh et al., 2019, Mastouri et al., 2021,
Singh, 2020, Bozkurt et al., 2025a, Hall and Horowitz, 2005], which is less interpretable because
T is unknown a priori and cannot reveal the separate contribution of the intrinsic (anisotropic)
smoothness of fâˆ—and the smoothness of T.
1.1
Organization of the paper
An outline of the paper is as follows. In Section 2, we introduce the RKHS-based 2SLS algorithm
for instrumental variable regression with observed covariates, referred to as KIV-O. In Section 3,
we discuss related work on NPIV in the literature. In Section 4, we present the main assumptions
and theoretical results, and discuss the interpretation of our findings. In Section 5, we highlight
the fundamental challenges towards obtaining minimax optimal rates of KIV-O.
2
Setup
Consider P the joint data-generating probability measure over (Z, O, X, Y ), where Z âˆˆZ := [0, 1]dz
denotes the instrument, O âˆˆO := [0, 1]do denotes the observed covariates, Y âˆˆR denotes the
outcome variable, and X âˆˆX := [0, 1]dx denotes the treatment variable. We use p to denote
the probability density functions; for example, p(x | z, o) denotes the density of the conditional
distribution PX|Z=z,O=o. As stated in Section 1, we define the conditional expectation operator T:
T : L2(PXO) â†’L2(PZO),
f 7â†’
 (z, o) 7â†’
R
X f(x, o)p(x|z, o) dx

.
Notations: Let N+ denote the set of positive integers and N = N+ âˆª{0} denote the set of non-
negative integers. We use boldfaced letters, such as x, to denote a vector in Rd for d â‰¥1. Specifically,
x = [x1, . . . , xd]âŠ¤âˆˆX âŠ‚Rd. For a distribution P defined on a measurable space (X, B(X)) and
0 < p < âˆ, Lp(P) is the space of functions h : X â†’R such that âˆ¥hâˆ¥Lp(P) := EXâˆ¼P [|h(X)|p]
1
p < âˆ
and Lâˆ(P) is the space of functions that are bounded P-almost everywhere. When P is the
Lebesgue measure LX over X, we write Lp(X) := Lp (LX ). For H a separable Hilbert space,
we let Lp(X; H) denote the space of Bochner 2-integrable functions from X to H with norm
âˆ¥Fâˆ¥2
L2(X;H) =
R
X âˆ¥F(x)âˆ¥2
H dx. Two Banach spaces E1, E2 are said to be isometrically isomorphic,
4

denoted E1 âˆ¼= E2, if there exists an isometric isomorphism S, such that âˆ¥Shâˆ¥E2 = âˆ¥hâˆ¥E1 for all
h âˆˆE1. Two Banach spaces E1, E2 are said to be norm equivalent, denoted E1 â‰ƒE2, if E1, E2
coincide as sets and there are constants c1, c2 > 0 such that c1âˆ¥hâˆ¥E1 â‰¤âˆ¥hâˆ¥E2 â‰¤c2âˆ¥hâˆ¥E1 holds for
all h âˆˆE1. For an operator T : E1 â†’E2, âˆ¥Tâˆ¥denotes its operator norm and T âˆ—denotes its adjoint.
For two Hilbert spaces H1, H2, S2(H1, H2) is the Hilbert space of Hilbert-Schmidt operators from
H1 to H2. For two numbers Î± and Î², we let Î± âˆ§Î² = min(Î±, Î²) and Î± âˆ¨Î² = max(Î±, Î²). â‰²(resp.
â‰³) means â‰¤(resp. â‰¥) up to positive multiplicative constants.
2.1
Algorithm
In this section, we introduce a kernel two-stage least-squares approach for instrumental variable
regression with observed covariates, which we term the KIV-O algorithm. KIV-O algorithm adopts
a sample splitting strategy. In Stage I, we learn the conditional expectation operator T with dataset
D1 := {(Ëœzi, Ëœoi, Ëœxi)}Ëœn
i=1 (see Eq. (6)); in Stage II, we perform regression of the outcome Y on the
features learned in Stage I with dataset D2 := {(zi, oi, yi)}n
i=1 (see Eq. (7)). The KIV-O algorithm
is a generalization of the KIV algorithm proposed in Singh et al. [2019]. We note that many existing
NPIV learning methods employ a two-stage estimation procedure, see for instance Hartford et al.
[2017], Singh et al. [2019], Xu et al. [2021a], Li et al. [2024b], PetrulionytË™e et al. [2024], Khoury
et al. [2025].
We now briefly review the relevant reproducing kernel Hilbert space (RKHS) theory, following
Berlinet and Thomas-Agnan [2004]. For a domain X âŠ†Rd, a Hilbert space H of functions f : X â†’R
is called a Reproducing Kernel Hilbert Space (RKHS) if the evaluation functional Î´x : H â†’R defined
by f 7â†’f(x) is continuous for every x âˆˆX. Every RKHS H has a unique symmetric, positive definite
reproducing kernel k : X Ã— X â†’R, which satisfies k(x, Â·) âˆˆH for all x âˆˆX and âŸ¨f, k(Â·, x)âŸ©H = f(x)
for all f âˆˆH and x âˆˆX (the reproducing property). To describe the KIV-O algorithm, we introduce
RKHSs HX on X, HO,1 and HO,2 on O and HZ on Z. The reasoning for defining two distinct
RKHSs on O will be clear later in the algorithm. We denote the associated unique reproducing
kernels via kX : X Ã— X â†’R, kO,1 : O Ã— O â†’R, kO,2 : O Ã— O â†’R, kZ : Z Ã— Z â†’R. We denote the
canonical feature map of HX as Ï•X(x) := kX(x, Â·), and similarly for feature maps Ï•O,1, Ï•O,2, Ï•Z.
Assumption 2.1. All kernels (kX, kO,1, kO,2 and kZ) are measurable and bounded.
An immediate consequence of Assumption 2.1 is that the embedding IPX : HX â†’L2(PX),
which maps a function f âˆˆHX to its PX-equivalence class [f]PX is well-defined, compact and
Hilbert-Schmidt [Steinwart and Scovel, 2012, Lemma 2.3]. We define [HX]PX âŠ†L2(PX) as the
image of IPX. For Î² > 0, we denote by [HX]Î²
PX the Î²-th power space, as introduced in Steinwart
and Scovel [2012, Theorem 4.6]. For 0 â‰¤Î² â‰¤1, this space is shown to be isomorphic to the
Î²-interpolation space [L2(PX), [HX]PX]Î²,2 [Steinwart and Scovel, 2012, Theorem 4.6]. It is known
that the 1-interpolation space [HX]1
PX is isometrically isomorphic to the closed subspace (ker IPX)âŠ¥
of HX via IPX [Steinwart and Scovel, 2012, Lemma 2.12]. For Î² â‰¥1, the space contains functions
that are smoother than those in HX. The same definitions and properties hold for HZ, HO,1 and
HO,2 as well.
For two Hilbert spaces H, Hâ€², we let H âŠ—Hâ€² denote their tensor product Hilbert space, defined
as H âŠ—Hâ€² := span{u âŠ—uâ€² : u âˆˆH, uâ€² âˆˆHâ€²}, where u âŠ—uâ€² is the linear rank-one operator Hâ€² â†’H
defined by (u âŠ—uâ€²)vâ€² = âŸ¨uâ€², vâ€²âŸ©Hâ€²u [Aubin, 2011, Section 12]. In the case of RKHSs, the tensor
product HZO,1 := HZ âŠ—HO,1 and HXO,2 := HX âŠ—HO,2 are the unique RKHSs associated
with the product kernels kZO,1((z, o), (zâ€², oâ€²)) = kZ(z, zâ€²) Â· kO,1(o, oâ€²) and kXO,2((x, o), (xâ€², oâ€²)) =
kX(x, xâ€²) Â· kO,2(o, oâ€²), respectively [Berlinet and Thomas-Agnan, 2004]. We define the embedding
IPXO,2 : HXO,2 â†’L2(PXO) which maps a function f âˆˆHXO,2 to its PXO-equivalence class [f]PXO,
5

and define the Î²-th power spaces as [HXO,2]Î²
PXO. An analogous construction applies to HZO,1,
yielding the spaces [HZO,1]Î²
PZO. In the rest of the paper, we omit the subscript and use the notation
[Â·] to denote equivalence classes in L2.
We are now ready to present the KIV-O algorithm.
Stage I. The action of the operator T on the RKHS HXO,2 can be represented with the aid of
the conditional mean embedding (CME) [Song et al., 2009, Park and Muandet, 2020, Klebanov
et al., 2020, Li et al., 2022]. We define the CME Fâˆ—as the mapping from Z Ã— O to HX, given by
(z, o) 7â†’E[Ï•X(X) | Z = z, O = o]. Equipped with the CME, we note that the image of T acting
on a function f âˆˆHXO,2 admits the following representation: for any (z, o) âˆˆZ Ã— O,
(Tf)(z, o) = E[f(X, O) | Z = z, O = o] = E[âŸ¨f, Ï•X(X) âŠ—Ï•O,2(O)âŸ©HXO,2 | Z = z, O = o]
= âŸ¨f, E[Ï•X(X) | Z = z, O = o] âŠ—Ï•O,2(o)âŸ©HXO,2 = âŸ¨f, Fâˆ—(z, o) âŠ—Ï•O,2(o)âŸ©HXO,2,
where the second equality follows from the reproducing property and the third equality requires a
Bochner integrable feature map Ï•X (true for bounded kernels) from Assumption 2.1 [Steinwart and
Christmann, 2008, Definition A.5.20]. Note that the feature map Ï•X is projected by the conditional
expectation of the conditional distribution PX|Z,O, while the feature map Ï•O,2 remains unprojected.
This is the key distinction from classical KIV. In Stage I, our goal is to estimate the CME, Fâˆ—,
by performing a regularized least squares regression in a vector-valued RKHS G induced by the
operator-valued kernel [GrÃ¼newÃ¤lder et al., 2012, Li et al., 2022]
K := kZO,1IdHX : (Z Ã— O) Ã— (Z Ã— O) â†’L(HX),
(4)
where L(HX) denotes the space of bounded linear operators HX â†’HX, and IdHX âˆˆL(HX)
denotes the identity operator on HX. An important property of G is that it is isometrically
isomorphic to the space S2(HZO,1, HX) of Hilbert-Schmidt operators from HZO,1 to HX. On the
other hand, by Aubin [2011, Theorem 12.6.1], S2(L2(PZO), HX) is isometrically isomorphic to the
Bochner space L2(PZO, HX), and we denote this isomorphism as Î¨. We can define vector-valued
Î²-th power spaces [Li et al., 2022, Definition 4]:
[G]Î² := Î¨
 S2
 [HZO,1]Î², HX

=

F | F = Î¨(C), C âˆˆS2
 [HZO,1]Î², HX
	
.
(5)
The space [G]Î² generalizes the definition of scalar-valued power space to vector-valued RKHSs,
quantifying the smoothness of Fâˆ—relative to the RKHS G (see Eq. (79)). We refer the reader to
Carmeli et al. [2006, 2010] for definitions and properties of more general vector-valued RKHSs.
Given D1 = {(Ëœzi, Ëœoi, Ëœxi)}Ëœn
i=1 sampled i.i.d from the joint distribution PXZO, a regularized
estimator of Fâˆ—is obtained as the solution to the following optimization problem:
Ë†FÎ¾ := arg min
FâˆˆG
1
Ëœn
PËœn
i=1 âˆ¥Ï•X(Ëœxi) âˆ’F(Ëœzi, Ëœoi)âˆ¥2
HX + Î¾âˆ¥Fâˆ¥2
G,
(6)
where Î¾ > 0 denotes the Stage I regularization parameter.
Stage II. In Stage II, we perform regularized least squares regression in the RKHS HXO,2, using
features derived from the estimated conditional mean embedding Ë†FÎ¾. Specifically, the features are
Ë†FÎ¾(Z, O) âŠ—Ï•O,2(O). Given D2 = {(zi, oi, yi)}n
i=1 i.i.d sampled from the joint distribution PZOY
and independent of D1, the regularized estimator Ë†fÎ» is defined as:
Ë†fÎ» := arg min
fâˆˆHXO,2
1
n
nP
i=1

yi âˆ’
D
f, Ë†FÎ¾(zi, oi) âŠ—Ï•O,2(oi)
E
HXO,2
2
+ Î»âˆ¥fâˆ¥2
HXâŠ—HO,2,
(7)
6

where Î» > 0 denotes the Stage II regularization parameter.âˆ—Owing to the favourable properties of
kernel ridge regression, Ë†fÎ» admits a closed-form expression, given in Section B in the Supplement.
Upon learning Ë†fÎ», the quantity Ë†fÎ»(xâˆ—, oâˆ—) represents the estimated heterogenous dose response of a
new treatment xâˆ—on a new individual with observed covariates oâˆ—. The estimated dose response
curve evaluated at xâˆ—can then be obtained as the expectation of Ë†fÎ»(xâˆ—, O) with respect to the
marginal distribution of the observed covariates.
Our primary goal is to study the L2(PXO)-risk:
âˆ¥Ë†fÎ» âˆ’fâˆ—âˆ¥L2(PXO).
(8)
To this end, we need to impose regularity conditions on the regression targets in both stages.
Specifically, we characterize the regularity of the conditional mean embedding Fâˆ—: Z Ã— O â†’
HX through a dominating mixed-smoothness Sobolev space, as discussed in Section 2.2; and we
characterize the regularity of the function fâˆ—: X Ã— O â†’R through an anisotropic Besov space, as
introduced in Section 2.3. It is thus natural to use two different kernels kO,1 and kO,2, because the
regularity of Fâˆ—and fâˆ—with respect to O might not be the same. Since the choice of kernel in both
stages is dependent on the regularity of their respective regression targets Fâˆ—and fâˆ—, we provide a
more in-depth description and justification of the kernels we use in stages I and II in Remarks 2.1
and 2.2 respectively.
2.2
Mixed-smoothness Sobolev spaces
In this section, we introduce vector-valued mixed-smoothness Sobolev spaces to characterize the
smoothness of the conditional mean embedding (CME) Fâˆ—: (z, o) 7â†’E[Ï•X(X) | Z = z, O = o] in
Stage I. In fact, the smoothness of Fâˆ—can be identified via the differentiability of the conditional
density.
Let (N+)d be the set of all multi-indices Î± = (Î±1, . . . , Î±d) with Î±i âˆˆN and |Î±| = Pd
i=1 Î±i. For
Î± âˆˆNd and f : Rd â†’R, âˆ‚Î± denotes the classical (pointwise) partial derivative, and DÎ±f denotes
the corresponding weak (distributional) partial derivative.
Assumption 2.2. Let mo, mz âˆˆN+. For any x âˆˆX, the map (z, o) 7â†’p(x | z, o) has bounded,
continuous derivatives of order mo with respect to o and order mz with respect to z on the interior
of Z Ã— O.
Ï := max
|Î±|â‰¤mz
max
|Î²|â‰¤mo
sup
xâˆˆX,zâˆˆZ,oâˆˆO
âˆ‚Î±
z âˆ‚Î²
o p(x | z, o)
 < âˆ.
The differentiability conditions on the conditional density imposed in Assumption 2.2 imply
that Fâˆ—belong to a certain vector-valued dominating mixed-smoothness Sobolev space, as defined
below.
Definition 1 (Vector-valued dominating mixed-smoothness Sobolev space). Let H be a Hilbert
space. Let mz, mo âˆˆN+. We define
MW mz,mo
2
(Z Ã— O; H) :=
n
F | F âˆˆL2(Z Ã— O; H), âˆ¥Fâˆ¥MW mz,mo
2
(ZÃ—O;H) < âˆ
o
.
where âˆ¥Fâˆ¥MW mz,mo
2
(ZÃ—O;H) := P
|Î±|â‰¤mz
P
|Î²|â‰¤mo âˆ¥DÎ±
z DÎ²
o Fâˆ¥L2(ZÃ—O;H).
âˆ—The naive extension to observed covariates in KIV Singh et al. [2019] considers augmenting X, Z to (X, O), (Z, O).
This approach is not consistent because Stage I would then require estimating the conditional mean embedding
(z, o) 7â†’E[Ï•X(X) âŠ—Ï•O(O) | Z = z, O = o], which is not Hilbert-Schmidt and for which vector-valued kernel ridge
regression is not consistent, see also [Mastouri et al., 2021, Appendix B.9] for an illustration.
7

The real-valued dominating mixed-smoothness Sobolev space [Schmeisser, 1987, 2007, Sickel
and Ullrich, 2009] MW mz,mo
2
(Z Ã— O; R) is a special case of MW mz,mo
2
(Z Ã— O; H) when H = R.
When dz = 0 (or do = 0), we recover the vector-valued Sobolev spaces W mz
2
(Z; H) (or W mo
2
(O; H))
as defined in Aubin [2011, Section 12.7]. Assumption 2.2 implies that âˆ¥DÎ±
z DÎ²
o Fâˆ—âˆ¥L2(ZÃ—O;HX) is
bounded for any multi-indices |Î±| â‰¤mz and |Î²| â‰¤mo. Hence, Fâˆ—âˆˆMW mz,mo
2
(Z Ã— O; HX).
Now we are ready to state our choice of kernels kZ, kO,1 in Stage I.
Remark 2.1 (Choice of Stage I kernels kZ, kO,1). We let kZ and kO,1 be any positive definite
kernels such that their associated RKHSs HZ, HO,1 are respectively norm equivalent to real-valued
Sobolev spaces W tz
2 (Z) and W to
2 (O), where tz > dz
2 , to > do
2 . Following Chen et al. [2025], we say
that kZ, kO,1 are Sobolev reproducing kernels of smoothness tz, to. An important example of Sobolev
reproducing kernel is the MatÃ©rn-Î½ kernel whose RKHS is norm equivalent to a Sobolev space W t
2
of smoothness t = Î½ + d/2 [Wendland, 2004, Corollary 10.48]. Since all Sobolev reproducing kernels
are bounded and measurable, kZ, kO,1 satisfy Assumption 2.1.
With the above choice of kZ and kO,1, it follows from Lemma C.8 that the vector-valued RKHS
G associated with the operator-valued kernel in Eq. (4) is norm equivalent to the mixed-smoothness
Sobolev space MW tz,to(Z Ã— O; HX). Since Fâˆ—âˆˆMW mz,mo
2
(Z Ã— O; HX) as established above, Fâˆ—
lies in the appropriate power space of G for suitably chosen (tz, to). Consequently, Li et al. [2022],
Meunier et al. [2024b] show that estimating the CME via Eq. (6) achieves the minimax-optimal rate
in both the L2(Z Ã— O; HX) and G norms, provided that the regularization parameter Î¾ is selected
adaptively with respect to the sample size Ëœn.
2.3
Anisotropic Besov spaces
In this section, we introduce the definition of anisotropic Besov spaces [Leisner, 2003], which is
used to characterize the smoothness of fâˆ—.
Definition 2 (Modulus of smoothness). Let X = Qd
i=1 X (i) âŠ†Rd be a subset with non-empty
interior, Î½ be a product measure on X with Î½ = âŠ—d
i=1Î½i, and f : X â†’R be a function in Lp(Î½) for
some p âˆˆ(0, âˆ]. The r-th modulus of smoothness of f is defined by
Ï‰r,p (f, t, X) = sup0<|hi|â‰¤ti âˆ¥â–³r
hfâˆ¥Lp(Î½) ,
(9)
where the r-th difference of f in the direction h at point x, denoted as â–³r
hf (x), is defined through
recursion: âˆ†0
hf(x) := f(x) and âˆ†r
hf(x) := âˆ†râˆ’1
h
f(x + h) âˆ’âˆ†râˆ’1
h
f(x) if x, x + h, . . . , x + rh âˆˆX
and 0 otherwise.
Definition 3 (Anisotropic Besov space Bs
p,q(Î½)). For p âˆˆ[1, âˆ), q âˆˆ[1, âˆ] and s = (s1, . . . , sd) âˆˆ
Rd
+, the anisotropic Besov space Bs
p,q(Î½) is defined by
Bs
p,q(Î½) :=
n
f âˆˆLp(Î½) : âˆ¥fâˆ¥Bsp,q(Î½) := âˆ¥fâˆ¥Lp(Î½) + |f|Bsp,q(Î½) < âˆ
o
,
(10)
where the Besov semi-norm |f|Bsp,q(Î½) is defined as,
|f|Bsp,q(Î½) :=
hR 1
0

tâˆ’1Ï‰r,p
 f, t1/s1, . . . , t1/sd, X
q
dt
t
i1/q
,
(11)
for r = max{âŒŠs1âŒ‹, . . . , âŒŠsdâŒ‹} + 1. When q = âˆ, we replace the integral by a supremum in Eq. (11).
When Î½ is the Lebesgue measure over X, we use the notation Bs
p,q(X) := Bs
p,q(Î½).
8

If s1 = Â· Â· Â· = sd = s, then the anisotropic Besov space recovers the standard isotropic Besov space
[DeVore and Popov, 1988, DeVore and Sharpley, 1993]. Since fâˆ—takes as input both the treatment X
and observed covariate O, it naturally exhibits different smoothness with respect to X and O. Hence,
as opposed to an isotropic Besov space which imposes uniform smoothness along all directions, an
anisotropic Besov space captures such heterogeneous regularity. To simplify the exposition, we
focus on anisotropic smoothness across X and O, while assuming isotropic smoothness within X
and within O. In other words, we only consider s = (sx, . . . , sx, so, . . . , so) âˆˆRdx+d0
â‰¥0
and denote
Bs
2,âˆ(X Ã— O) as Bsx,so
2,âˆ(X Ã— O). Let U(Bsx,so
2,âˆ(Rdx+do)) denote the unit ball of Bsx,so
2,âˆ(Rdx+do) with
respect to the Besov norm. Let C0(Rdx+do) denote the space of continuous functions Rdx+do â†’R.
Assumption 2.3. fâˆ—âˆˆS where we define S := U(Bsx,so
2,âˆ(Rdx+do)) âˆ©Lâˆ(Rdx+do) âˆ©L1(Rdx+do) âˆ©
C0(Rdx+do).
In particular, under Assumption 2.3, âˆ¥fâˆ—âˆ¥L2(Rdx+do) â‰¤âˆ¥fâˆ—âˆ¥Bsx,so
2,âˆ(Rdx+do) â‰¤1. Moreover, by
continuity, for all o âˆˆRdo the slice function fâˆ—(Â·, o) is well-defined. We assume p = 2 in accordance
with our L2(PXO)-norm learning risk in Eq. (8). We assume q = âˆbecause Bsx,so
2,âˆ(X Ã— O) is the
largest anisotropic Besov space among all Bsx,so
2,q
(X Ã— O) spaces [Triebel, 2011]. To the best of our
knowledge, we are the first to consider anisotropic smoothness in the NPIV literature.
We are now ready to state our choice of kernels kX, kO,2 in Stage II.
Remark 2.2 (Choice of Stage II kernels kX, kO,2). We choose kX and kO,2 to be Gaussian kernels
kÎ³x and kÎ³o with bandwidths Î³x âˆˆ(0, 1), Î³o âˆˆ(0, 1). Denote HÎ³x and HÎ³o as the associated Gaussian
RKHSs; Ï•Î³x and Ï•Î³o as the associated feature maps. The tensor product RKHS HÎ³x,Î³o := HÎ³x âŠ—HÎ³o
is the unique RKHS associated with the product kernel
kÎ³x(x, xâ€²) Â· kÎ³o(o, oâ€²) = exp

âˆ’Pdx
j=1
(xjâˆ’xâ€²
j)2
Î³2x
âˆ’Pdo
j=1
(ojâˆ’oâ€²
j)2
Î³2o

.
(12)
This kernel is called an anisotropic Gaussian kernel and its associated RKHS HÎ³x,Î³o is the corre-
sponding anisotropic Gaussian RKHS. Hang and Steinwart [2021] proves that kernel ridge regression
with anisotropic Gaussian kernel in the form of Eq. (12) is minimax optimal for anisotropic Besov
space target functions, provided that both the regularization parameter and the kernel lengthscale
Î³x, Î³o are adaptive to the number of samples n. Such adaptivity will also be evident in our setting
(see Theorem 4.1). Singh et al. [2019] adopt the median heuristic for selecting the kernel lengthscale
in KIV algorithm, a widely used practical choice. Unlike ours, however, the theoretical relationship
between their heuristic and the underlying smoothness of the target function remains unclear.
Remark 2.3 (Why use different kernels in Stage I and Stage II). We briefly explain the rationale
for selecting different types of kernels for Stage I and Stage II. For Stage I, the regression target
is the conditional mean embedding Fâˆ—. By Assumption 2.2, we have shown that Fâˆ—belongs to the
mixed Sobolev space MW mz,mo(Z Ã— O; H), which can be learned at the minimax optimal rate using
a tensor-product Sobolev RKHS (see Proposition D.4). On the other hand, the Stage II regression
target fâˆ—belongs to an anisotropic Besov space (Assumption 2.3). Hang and Steinwart [2021]
has proved that learning an anisotropic function in a nonparametric regression setting is minimax
optimal via an anisotropic Gaussian RKHS. We have followed their approach with additional
refinements to our setting, that reveals the interplay between the effect of T and the anisotropic
smoothness of fâˆ—. See Section 4 for the details.
9

3
Related work
Early NPIV literature focuses on series estimators [Newey and Powell, 2003, Blundell et al., 2007,
Chen, 2007, Horowitz, 2011] and methods based on kernel density estimation [Hall and Horowitz,
2005, Darolles et al., 2011, Florens et al., 2011].
These works established minimax optimal
convergence rates under various ill-posedness and smoothness conditions [Hall and Horowitz, 2005,
Chen and Reiss, 2011, Chen and Christensen, 2018]. Recent NPIV algorithms leverage modern
machine learning techniques, including RKHSs [Singh et al., 2019, Zhang et al., 2023b, Meunier
et al., 2024a] and neural networks [Hartford et al., 2017, Bennett et al., 2019, Xu et al., 2021a,
PetrulionytË™e et al., 2024, Kim et al., 2025, Sun et al., 2025, Meunier et al., 2025]. These modern
methods mainly fall into two categories: two-stage estimation and min-max optimization. Min-max
approaches [Bennett et al., 2019, Dikkala et al., 2020, Liao et al., 2020, Bennett et al., 2023,
Zhang et al., 2023b, Wang et al., 2022] formulate NPIV as a saddle point optimization problem,
which can be unstable and may fail to converge, especially when deep neural networks are used
as function classes. In contrast, two-stage methodsâ€”such as the KIV-O algorithm studied in this
manuscript (Section 2.1)â€”first estimate the conditional expectation operator T, and then perform
a second-stage regression using the learned operator [Hartford et al., 2017, Singh et al., 2019, Xu
et al., 2021a, Li et al., 2024b, Kim et al., 2025]. One recent paper [Kankanala, 2025] employs a sieve
estimator in the first stage and a Gaussian process (a Bayesian analogue of an RKHS) estimator in
the second stage.
In the introduction, we outlined two challenges for the theoretical analysis of NPIV-O. The
first challenge concerns the fact that T is an identity operator restricted to the infinite dimensional
function space F1 (defined in Eq. (3)). This has the following consequences. The L2-stability
condition imposed in the NPIV literature (cf. Blundell et al. [2007, Assumption 6], Chen and
Pouzo [2012, Assumption 5.2(ii)] and Chen and Christensen [2018, Assumption 4.2]) fails to hold
except for the degenerate case where the sieve measure of ill-posedness is 1 (i.e. Z = X, see also
Remark 4.2). The link condition imposed in the optimal rate literature for NPIV (cf. Hall and
Horowitz [2005], Chen and Reiss [2011, Assumption 2.2], Chen and Christensen [2018, Condition
LB]) implies that âˆ¥Tfâˆ¥L2(PZO) â‰¤âˆ¥Brfâˆ¥L2(PXO) for some known compact operator B, where a
larger r corresponds to a more ill-posed model. However, for any f âˆˆF1 defined in Eq. (3), we
have âˆ¥Tfâˆ¥L2(PZO) = âˆ¥fâˆ¥L2(PXO) so the link condition only holds with r = 0.
The second challenge lies in deriving a unified analysis where fâˆ—lies in an anisotropic Besov
space. Several prior works on NPIV-O and nonparametric proxy methods (see Section 3.1) Singh
et al. [2019], Mastouri et al. [2021], Singh [2020], Bozkurt et al. [2025a], Hall and Horowitz [2005],
Bozkurt et al. [2025b] choose instead to assume the generalized source condition fâˆ—âˆˆR((T âˆ—T)Î²)
for some Î² â‰¥0. However, as we have critiqued in the introduction, such an approach does not shed
light on the separate contribution of the intrinsic smoothness of fâˆ—and the smoothing effect of T.
It also suffers from a lack of interpretability since T is a priori unknown. We also mention that Hall
and Horowitz [2005] derived optimal rates for a kernel density based estimator for NPIV-O, where
the smoothness of fâˆ—(Â·, o) is characterized via a generalized source condition with respect to the
partial conditional expectation operator To [Hall and Horowitz, 2005, Section 4.3] for o âˆˆO. Such
an assumption suffers from similar drawbacks to those outlined above, and it is moreover unclear
how fâˆ—â€™s smoothness in the direction of O impacts learning rates.
To the best of our knowledge, our paper is the first theoretical analysis that simultaneously
addresses the anisotropic smoothness of both fâˆ—and the operator T in the X and O directions. We
address both challenges by (i) introducing a novel Fourier-based measure of partial smoothing of T,
and (ii) employing Gaussian kernel lengthscales that adapt to the anisotropic smoothness of fâˆ—.
10

3.1
Proximal causal learning (PCL)
The two challenges mentioned above also arise in a recent popular framework called proximal causal
learning (PCL), which has gained considerable interest as a framework to identify and estimate
causal effects from observational data, where the analyst only has access to imperfect proxies
of the true underlying confounding mechanism without being able to observe the confounders
directly [Miao et al., 2018, Tchetgen Tchetgen et al., 2024]. Our contributions to NPIV-O can
directly be extended to this context. In the context of PCL, the (heterogeneous) dose response
curve fâˆ—can be identified either via the outcome bridge function [Miao et al., 2018, Deaner, 2018,
Mastouri et al., 2021, Xu et al., 2021b, Kallus et al., 2021, Singh, 2020], which generalizes outcome
regression, or via the treatment bridge function [Cui et al., 2024, Kallus et al., 2021, Bozkurt et al.,
2025a,b], which generalizes inverse propensity weighting estimators. Analogous to the modern NPIV
literature, the nonparametric estimators for bridge functions fall under the 2SLS approach [Deaner,
2018, Mastouri et al., 2021, Singh, 2020, Bozkurt et al., 2025a,b], min-max optimization approach
with either RKHS or deep neural networks as function classes [Mastouri et al., 2021, Ghassami
et al., 2022, Kallus et al., 2021], or via spectral methods [Sun et al., 2025]. Notably, both outcome
bridge function and treatment bridge function are identified via conditional moment constraints
of the same form as NPIV-O (see Eq. (2)), thus our theory in NPIV-O could be extended to the
estimation of bridge functions in PCL.
3.2
Kernel ridge regression (KRR)
Our theoretical analysis of KIV-O builds on and extends existing theory in kernel ridge regres-
sion (KRR). The literature on KRR primarily follows two methodological lines: one based on
empirical process [Steinwart and Christmann, 2008, Steinwart et al., 2009, Eberts and Steinwart,
2013, Hang and Steinwart, 2021, Hamm and Steinwart, 2021] and one based on integral operator
techniques [De Vito et al., 2005, Smale and Zhou, 2005, 2007, Blanchard and MÃ¼cke, 2018, Lin and
Cevher, 2020, Fischer and Steinwart, 2020, Zhang et al., 2023a, 2024]. In the context of learning an
anisotropic Besov space function fâˆ—using KRR, the only available convergence rate is provided
by Hang and Steinwart [2021], which builds on an oracle inequality derived using empirical process
techniques and hence necessitates a clipping operation on the KRR estimator. In our work, we are
the first to remove the clipping step by leveraging integral operator techniques to directly control
the finite sample estimation error. Moreover, our analysis leverages state of the art results on
optimal rates for vector-valued kernel ridge regression [Li et al., 2024a, Meunier et al., 2024b] to
bound the statistical error arising from estimating a conditional mean embedding.
4
Theory
This section presents our main theoretical results on the non-asymptotic convergence rate of the
learning risk defined in Eq. (8). Section 4.1 presents our assumptions on the conditional expectation
operator T. Section 4.2 presents an upper bound. Section 4.3 presents a minimax lower bound.
4.1
Partial smoothing effect of T
The challenge of estimating fâˆ—via the inverse problem E[Y | Z, O] = (Tfâˆ—)(Z, O) arises from its
ill-posed nature: a small error in estimating E[Y | Z, O] may lead to a large error in estimating fâˆ—.
To address this challenge, we make the following assumptions. The first assumption enables unique
identification of fâˆ—.
11

Assumption 4.1 (Lâˆ-completeness). The conditional distribution PX|Z,O satisfies that, for every
bounded measurable function f : X Ã— O â†’R, if E[f(X, O) | Z, O] = 0 holds PZO-almost surely,
then f(X, O) = 0 holds PXO-almost surely.
Assumption 4.1, known as bounded completeness or Lâˆ-completeness [Dâ€™Haultfoeuille, 2011,
Blundell et al., 2007], is weaker than the L2-completeness condition, which assumes that T :
L2(PXO) â†’L2(PZO) is injective. The latter is standard in the NPIV literature [Newey and Powell,
2003, Hall and Horowitz, 2005, Carrasco et al., 2007, Darolles et al., 2011, Andrews, 2017, Chen
et al., 2014, Chen and Christensen, 2018, Chen et al., 2024]. Although we do not assume that
the outcome Y is bounded (see Assumption 4.5), the target heterogenous response curve fâˆ—is
bounded (assumed in Assumption 2.3), hence it suffices to impose the weaker Lâˆ-completeness
identification. We refer the reader to Andrews [2017], Dâ€™Haultfoeuille [2011] for sufficient conditions
on the conditional distribution PX|Z,O such that bounded completeness holds.
Beyond identification, to establish a non-asymptotic rate of convergence for fâˆ—, existing work on
NPIV imposes additional assumptions on the smoothing properties of T which are not compatible
with the partial identity structure of T imposed by the common variable O [Blundell et al., 2007,
Chen and Christensen, 2018, Chen and Reiss, 2011]. In contrast, as highlighted in Section 1, with
the existence of observed covariates O, our T exhibits characteristics of a compact operator in the
X direction and acts as an identity operator in the O direction. We thus propose a novel framework
to characterize the partial smoothing properties of T.
We describe this partial smoothing in terms of the Fourier representation of a function f on
which T acts. For f âˆˆL1(Rdx), its Fourier transform is defined as a Lebesgue integral [Rudin,
1987, 9.1]: Ë†f(Â·) =
R
Rdx f(x) exp(âˆ’iâŸ¨x, Â·âŸ©) dx. One can extend the Fourier transform to L2(Rdx) by
defining it as a unitary operator on L2(Rdx) [Rudin, 1987, Theorem 9.13]. We use F to denote
this operator, and let Fâˆ’1 denote its inverse. In particular, Fâˆ’1[1[A]] is well-defined, where 1[A]
denotes the indicator function of a compact set A âŠ‚Rdx. For any scalar Î³ âˆˆ(0, 1), we define the
following two sets of functions:
LF(Î³) := {f : Rdx+do â†’R
 âˆ€o âˆˆO, [f(Â·, o)] âˆˆL2(Rdx),
supp
 F[f(Â·, o)]

âŠ†{Ï‰x âˆˆRdx : âˆ¥Ï‰xâˆ¥2 â‰¤Î³âˆ’1}}.
HF(Î³) := {f : Rdx+do â†’R
 âˆ€o âˆˆO, [f(Â·, o)] âˆˆL2(Rdx),
supp
 F[f(Â·, o)]

âŠ†{Ï‰x âˆˆRdx : âˆ¥Ï‰xâˆ¥2 â‰¥Î³âˆ’1}}.
(13)
where supp for an element of L2(Rdx) is defined in Definition 5 and Definition 6 in the Supplementary.
The set LF(Î³) (respectively, HF(Î³)) consists of functions such that for every o âˆˆO, the slice
function f(Â·, o) belongs to L1(Rdx) and its Fourier transform is supported inside (respectively,
outside) the centered ball of radius Î³âˆ’1 in the Fourier domain. See Figure 1 for an illustration.
Assumption 4.2 (Fourier measure of partial ill-posedness of T). There exists a constant c0 > 0
and a parameter Î·0 âˆˆ[0, âˆ) depending only on T, such that for all Î³ âˆˆ(0, 1) and all functions
f âˆˆLF(Î³) âˆ©Lâˆ(PXO), the following inequality is satisfied:
âˆ¥Tfâˆ¥L2(PZO) â‰¥c0Î³dxÎ·0âˆ¥fâˆ¥L2(PXO).
In particular, c0 does not depend on Î³.
Assumption 4.3 (Fourier measure of partial contractivity of T). There exists a constant c1 > 0
and a parameter Î·1 âˆˆ[0, âˆ) depending only on T, such that for all Î³ âˆˆ(0, 1) and all functions
12

0
ğ‘¯ğ‘­(ğœ¸)
ğğ’™
ğğ’
ğ‘¯ğ‘­(ğœ¸)
ğ‘³ğ‘­(ğœ¸)
ğ‘³ğ‘­(ğœ¸)
Figure 1: Illustration for LF(Î³) and HF(Î³).
f âˆˆHF(Î³) âˆ©Lâˆ(PXO), the following inequality is satisfied:
âˆ¥Tfâˆ¥L2(PZO) â‰¤c1Î³dxÎ·1âˆ¥fâˆ¥L2(PXO).
In particular, c1 does not depend on Î³.
Assumptions 4.2 and 4.3 are assumptions about the conditional distribution P(X | Z, O). In
Section A in the Supplement, for any positive integer k â‰¥1, we construct a distribution Pk(X, Z, O)
such that, for the conditional expectation operator T defined by Pk(X | Z, O), a weaker version of
Assumptions 4.3 and 4.2 is satisfied with Î·0 = Î·1 = k (where we restrict to considering functions
f(x, o) = g(x)h(o), and impose a further technical restriction for Assumption 4.2). If Assumption
4.2 and 4.3 hold simultaneously, and PXO is absolutely continuous with respect to the Lebesgue
measure on X Ã— O, then Î·0 â‰¥Î·1 and c0 â‰¤c1 (Lemma A.1 in Section A). In the remainder of the
manuscript, we assume that the distribution PZXOY is fixed and we set the constants c0 = c1 = 1
for notational simplicity. Assumption 4.2 and Assumption 4.3 characterize the mildly ill-posed
regime in the NPIV literature.
Assumption 4.3 quantifies the partial smoothing effect of T on the high-frequency components of
a function f with respect to X; while Assumption 4.2 captures the partial anti-smoothing behaviour
of T on the low-frequency components of a function f with respect to X. When the treatment
X is exogenous and we take X = Z, then T is an identity mapping so Î·0 = Î·1 = 0 and NPIV-O
reduces to non-parametric regression from (X, O) to Y .
To motivate the partial smoothing effect of T, notice that the bounded self-adjoint operator
T âˆ—T : L2(PXO) â†’L2(PXO) acts on f âˆˆL2(PXO) as follows:
((T âˆ—T)f)(xâ€², o) =
Z
X
f(x, o)L(x, xâ€²; o) dx,
(14)
where L(x, xâ€²; o) :=
R
Z p(x | z, o)p(z | xâ€², o) dz. Consider two subsets of L2(PXO).
GX =

g âˆˆL2(PXO)
 âˆƒËœg âˆˆL2(PX) such that âˆ€x âˆˆX, o âˆˆO, g(x, o) = Ëœg(x)
	
,
GO =

g âˆˆL2(PXO)
 âˆƒËœg âˆˆL2(PO) such that âˆ€x âˆˆX, o âˆˆO, g(x, o) = Ëœg(o)
	
.
Under mild conditions on the conditional distribution p(x | z, o) [Darolles et al., 2011, Assumption
A.1], T âˆ—T|GX (T âˆ—T restricted to GX) is compact and its smoothing effect can be quantified through
its eigenvalue decay; while in contrast, T âˆ—T|GO is an identity operator: for g âˆˆGO
((T âˆ—T)g)(x, o) =
Z
X
g(x, o)L(x, xâ€²; o) dx = Ëœg(o)
Z
X
L(x, xâ€²; o) dx = g(x, o).
(15)
Therefore, when we incorporate observed covariates O, the conditional expectation operator T
acts as a compact operator in the X direction and as an identity operator in the O direction. As
a result, we propose to characterize its partial smoothing properties through measure of partial
contractivity (Assumption 4.3) and measure of partial ill-posedness (Assumption 4.2).
13

In the literature on NPIV, conditions similar to Assumption 4.3 and Assumption 4.2 have been
employed to quantify the smoothing effect of T. For instance, Chen and Reiss [2011], Meunier et al.
[2024a] use the so-called link condition and reverse link condition which relate the smoothness of
the hypothesis space with that of R(T âˆ—T); Chen and Christensen [2018], Blundell et al. [2007],
Chen et al. [2024] employ the sieve measure of ill-posedness and stability conditions, which quantify
the smoothing effect of T on functions in the hypothesis space spanned by a sieve basis. Our
Assumption 4.3 and Assumption 4.2 share strong resemblance with the latter. To see why, recall
the definition of Gaussian RKHS HÎ³x with length-scale Î³x through Fourier transforms [Wendland,
2004, Theorem 10.12]:
HÎ³x =
n
f : Rdx â†’R |
R
Rdx |F[f](Ï‰x)|2 exp
  1
4Î³2
xâˆ¥Ï‰xâˆ¥2
2

dÏ‰x < âˆ
o
,
where we can see that for f âˆˆHÎ³x, the bulk of its Fourier spectrum would belong to the ball
{Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¤Î³âˆ’1
x } with the remaining spectrum decaying exponentially as âˆ¥Ï‰xâˆ¥2 â†’âˆ. We
formulate our Assumption 4.3 and Assumption 4.2 with Fourier transforms rather than Gaussian
RKHSs for potential applications beyond Gaussian RKHSs. A closely related work is Kankanala
[2025], which employs a local sieve measure of ill-posedness for functions in the RKHS. Unfortunately,
these conditions, including ours, are hard to verify in practice as T is unknown.
Remark 4.1 (Connection with sieve measure of ill-posedness). In this remark, we connect As-
sumption 4.2 to the sieve measure of ill-posedness condition employed in the analysis of sieve
2SLS [Blundell et al., 2007, Chen and Christensen, 2018, Chen et al., 2024, Kim et al., 2025]. For
this remark, we omit observed covariates O, and take T : L2(PX) â†’L2(PZ). The sieve measure of
ill-posedness is defined as
Ï„ sieve
J
:=
sup
0Ì¸=fâˆˆÎ¨J
âˆ¥fâˆ¥L2(PX)
âˆ¥Tfâˆ¥L2(PZ)
,
(16)
where Î¨J denotes the Jth sieve space for the treatment variable [Chen and Christensen, 2018,
Section 3]. For this remark, we let Î¨J be the linear span of cardinal B-splines of order m up to
resolution K with J = (2K + m + 1)dx â‰2Kdx [DeVore and Lorentz, 1993, Section 5]. We note that
the parameter Î³âˆ’1, where Î³ occurs in the definition of the function space LF(Î³) in Eq. (13), plays a
role analogous to the resolution level K for cardinal B-splines. Indeed, LF(Î³) for smaller values of Î³
(or Î¨J for larger values of J) correspond to a class of less smooth functions. The above observation
and the form of Eq. (16) thus suggests an analogous definition:
Ï„ Fourier
Î³
:=
sup
0Ì¸=fâˆˆLF(Î³)âˆ©Lâˆ(PXO)
âˆ¥fâˆ¥L2(PX)
âˆ¥Tfâˆ¥L2(PZ)
.
(17)
We can thus restate Assumption 4.2 as: there exists a constant c0 > 0 and a parameter Î·0 âˆˆ[0, âˆ)
depending only on T, such that for all Î³ âˆˆ(0, 1), the following inequality is satisfied: Ï„ Fourier
Î³
â‰¤
câˆ’1
0 Î³âˆ’dxÎ·0.
In the sieve NPIV literature [Blundell et al., 2007, Chen et al., 2024, Chen and
Christensen, 2018], an NPIV model is said to be mildly ill-posed if Ï„ sieve
J
= O(JÎ·) = O(2Î·Kdx).
By the analogy between Ï„ Fourier
Î³
and Ï„ sieve
J
, we see that our Assumption 4.2 characterizes a mildly
ill-posed regime.
Remark 4.2 (Existing treatment of partial smoothing of T in NPIV-O). Existing work that
concerns NPIV-O in the literature circumvents the partial identity structure of T either by imposing
additional structural assumptions [Blundell et al., 2007, Syrgkanis et al., 2019] or by stratifying the
14

problem on O [Horowitz, 2011] thereby reducing NPIV-O to NPIV, which is statistically inefficient
and scales poorly with the dimension of O. Instead of T, Chen and Christensen [2018, Section 3.3]
consider the compactness and the smoothing effect of the partial conditional expectation operator
To : L2(PX|O=o) â†’L2(PZ|O=o) for each o âˆˆO. Chen and Christensen [2018, Section 3.3] proves
that, if Î¨J (defined in the above remark) equals the span of the first J eigenvectors of T âˆ—
oTo for any
o âˆˆO, then
Ï„ sieve
J
â‰Eoâˆ¼O[Âµ2
J,o]âˆ’1
2 ,
(18)
where ÂµJ,o is the Jth singular value of To arranged in non-increasing order. Chen and Christensen
[2018] then claims that the convergence rates derived for the standard NPIV can be extended to
NPIV-O. However, we identify an essential oversight: Chen and Christensen [2018, Assumption 4
(ii)], which is required for their L2-norm upper bound, cannot hold in the NPIV-O setting. This
assumption states that for Î¨J, fâˆ—satisfies the following inequality:
Ï„ sieve
J
âˆ¥T(fâˆ—âˆ’Î Jfâˆ—)âˆ¥L2(PZO) â‰¤âˆ¥fâˆ—âˆ’Î Jfâˆ—âˆ¥L2(PXO),
where Î J : L2(PXO) â†’Î¨J denotes the L2-orthogonal projection from L2(PXO) onto Î¨J. In the
NPIV-O setting, if fâˆ—depends only on o and PXO is the Lebesgue measure, then the projection
Î Jfâˆ—also depends only on o.
This implies (T âˆ—T)(fâˆ—âˆ’Î Jfâˆ—) = fâˆ—âˆ’Î Jfâˆ—as per Eq. (15),
forcing Ï„ sieve
J
âˆ’1 = O(1), causing a contradiction with the measure of ill-posedness condition that
Ï„ sieve
J
= O(JÎ·0) unless Î·0 = 0. Hall and Horowitz [2005] addresses NPIV-O by assuming smoothness
of fâˆ—(Â·, o) relative to R(T âˆ—
oTo) for each o âˆˆO, which is hard to interpret because To is unknown in
NPIV-O.
4.2
Upper Bound for KIV-O
To obtain the upper learning rate for KIV-O in Section 2.1, we need to impose the following
assumptions about the data-generating distribution.
Assumption 4.4. 1. The joint probability measures PZO and PXO admit probability density
functions pZO and pXO. There exists a universal constant a > 0 such that aâˆ’1 â‰¥pZO(z, o) â‰¥a
for all (z, o) âˆˆ[0, 1]dz+do and aâˆ’1 â‰¥pXO(x, o) for all (x, o) âˆˆ[0, 1]dx+do and pXO(x, o) â‰¥a for all
(x, o) âˆˆ[1/4, 3/4]dx+do. 2. Assumption 2.2 holds with the following mz, mo âˆˆN+:
mo :=
&
do
2
1+2

sx
dx +Î·1

+ do
so

sx
dx +Î·1

1+2

sx
dx +Î·1

'
+ 1,
mz :=
l
dz
do mo
m
.
(19)
The requirement that pXO(x, o) â‰¥a for all (x, o) âˆˆ[1/4, 3/4]dx+do is a mild assumption to
ensure that the HÎ³x,Î³o-norm of the kernel mean embedding of PXO is bounded away from zero.
This plays a role in the control of the estimation error for Stage II regression (see Proposition D.8
in the Supplement). The choice of 1/4, 3/4 is arbitrary and can be replaced by any fixed unequal
values in (0, 1). The constraint on mo depends on (sx, so) because the embedding norm of the
RKHS HFO (defined in Section C) into the mixed-smoothness Sobolev space scales as Î³âˆ’mo
o
, where
Î³o itself depends on both sx and so in Theorem 4.1; this embedding norm must be controlled, a
requirement referred to as the (EMB) condition in Fischer and Steinwart [2020]. See Eq. (89) for
details.
Assumption 4.5. For all (z, o) âˆˆZ Ã— O, the residual Ï… := Y âˆ’(Tfâˆ—)(Z, O) is subgaussian
conditioned on Z = z, O = o with subgaussian norm at most Ïƒ.
15

In the NPIV literature, existing work assumes a moment condition on the residual Ï… [Blundell
et al., 2007, Hall and Horowitz, 2005, Chen and Christensen, 2018, Chen and Reiss, 2011], which is
weaker than our Assumption 4.5. However, their corresponding high probability upper bounds only
guarantee polynomially decaying tails, as a result of Chebyshevâ€™s inequality (see, for example, the
proof of Lemma F.9 in Chen and Christensen [2018], p.40). In contrast, our upper bound holds in
high probability with subexponential tails. Our sharper guarantee is a consequence of our applying
Bernstein concentration inequality and the more advanced techniques in the analysis of kernel ridge
regression [Fischer and Steinwart, 2020, Eberts and Steinwart, 2013, Hang and Steinwart, 2021].
Now we are ready to state the upper learning rate of âˆ¥[ Ë†fÎ»] âˆ’fâˆ—âˆ¥L2(PXO). We remind the reader
that Ëœn, n denote respectively the number of Stage 1 and Stage 2 samples. We let Stage I kernels
kO,1, kZ be MatÃ©rn kernels whose associated RKHSs HO,1 and HZ are respectively norm equivalent
to W to
2 (O) and W tz
2 (Z), for to, tz to be specified. We let Stage II kernels kX, kO,2 be Gaussian
kernels with respective lengthscales Î³x, Î³o.
Theorem 4.1 (Upper learning rate for KIV-O). Suppose Assumptions 2.2, 4.4 hold with parameters
mz, mo âˆˆN+, Assumption 2.3 holds with parameters sx, so > 0, Assumptions 4.2, 4.3 hold with
parameters Î·0, Î·1. We further suppose Assumptions 4.1 and 4.5 hold. We assume to, tz satisfy
2to â‰¥mo > to > do/2, 2tz â‰¥mz > tz > dz/2. We let
Î³x = n
âˆ’
1
dx
1+2( sx
dx +Î·1)+ do
so ( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+2( sx
dx +Î·1)+ do
so ( sx
dx +Î·1) .
(20)
Define mâ€  := (mztâˆ’1
z ) âˆ§(motâˆ’1
o ) and dâ€  := (dztâˆ’1
z ) âˆ¨(dotâˆ’1
o ). Let Stage I regularization parameter
Î¾ be given by Ëœn
âˆ’
1
mâ€ +dâ€ +Î¶ for any Î¶ > 0; and Stage II regularization parameter Î» be given by nâˆ’1.
Suppose that n â‰¥1 is sufficiently large, and Ëœn â‰¥1 satisfies
Ëœn â‰³n
mâ€ +dâ€ /2+Î¶
mâ€ âˆ’1
âˆ¨n2 mâ€ +dâ€ /2+Î¶
mâ€ 
.
(21)
Then with P n+Ëœn-probability at least 1 âˆ’40eâˆ’Ï„, we have

h
Ë†fÎ»
i
âˆ’fâˆ—

L2(PXO) â‰²Ï„n
âˆ’
sx
dx +Î·1âˆ’Î·0
1+2( sx
dx +Î·1)+ do
so ( sx
dx +Î·1) Â· (log n)
dx+do+1+dxÎ·0
2
.
(22)
Remark 4.3. In Theorem 4.1, we present the regime where the Stage I sample size Ëœn is sufficiently
large relative to the Stage II sample size (see Eq. (21)). This is the appropriate regime where we
can study rate-optimality because we present a minimax lower bound in Theorem 4.2 with respect to
the class of estimators that only utilize Stage II samples. It is nevertheless possible to derive upper
bounds with respect to both n and Ëœn without restrictions on the relative size of n, Ëœn. It remains
a challenging open problem, however, to establish rate optimality for estimators utilizing a split
dataset of the form {(Ëœzi, Ëœoi, Ëœxi)}Ëœn
i=1 and {(zi, oi, yi)}n
i=1, even for the standard NPIV setting [Chen
and Reiss, 2011, Chen and Christensen, 2018, Meunier et al., 2024a].
Remark 4.4 (Interpolation between NPIV and non-parametric regression). Our derived upper
rate interpolates between the known optimal L2-rates for NPIV without observed covariates and
anisotropic kernel ridge regression.
1.
When Î·0 = Î·1 = 0 and X = Z, i.e.
T is the identity mapping, our setting reduces to
nonparametric regression where the target function lies in the anisotropic Besov space Bsx,so
2,âˆ(X Ã—O).
The upper rate simplifies to ËœOP (nâˆ’
1
2Ëœs+1 ) with Ëœs = (do/so + dx/sx)âˆ’1 being the intrinsic smoothness,
16

which matches the known optimal learning rate of regression with an anisotropic Besov target
function [Hoffman and Lepski, 2002, Hang and Steinwart, 2021].
2. When do = 0, our setting reduces to NPIV without observed covariates where the target function
fâˆ—lies in an isotropic Besov space Bsx
2,âˆ(X). We take Î·1 = Î·0 = Î· following Chen and Christensen
[2018], Chen and Reiss [2011] which employs a single parameter to characterize both the ill-posedness
and contractivity, then our upper learning rate simplifies to ËœOP (nâˆ’
sx
dx+2(sx+Î·dx) ), which matches the
known optimal rate in NPIV regression [Chen and Christensen, 2018, Corollary 3.1].
4.2.1
Proof sketch
The proof of Theorem 4.1 is given in Section D in the Supplement. Here we give an outline of our
proof to facilitate a deeper understanding of both the assumptions and the results. Define Â¯fÎ» as
the oracle estimator for Stage II with access to the true conditional mean embedding Fâˆ—and recall
Ë†fÎ» for comparison:
Â¯fÎ» := arg min
fâˆˆHÎ³x,Î³o
Î»âˆ¥fâˆ¥2
HÎ³x,Î³o + 1
n
n
X
i=1
(yi âˆ’âŸ¨f, Fâˆ—(zi, oi) âŠ—Ï•O,Î³o(oi)âŸ©HÎ³x,Î³o)2.
(23)
Ë†fÎ» := arg min
fâˆˆHÎ³x,Î³o
Î»âˆ¥fâˆ¥2
HÎ³x,Î³o + 1
n
n
X
i=1
(yi âˆ’âŸ¨f, Ë†FÎ¾(zi, oi) âŠ—Ï•O,Î³o(oi)âŸ©HÎ³x,Î³o)2.
(24)
The proof can be summarized in 3 steps. We upper bound âˆ¥T[ Ë†fÎ»] âˆ’T[ Â¯fÎ»]âˆ¥L2(PZO) in Step 1 and
upper bound âˆ¥T[ Â¯fÎ»]âˆ’Tfâˆ—âˆ¥L2(PZO) in Step 2, which induce an upper bound on âˆ¥T[ Ë†fÎ»]âˆ’Tfâˆ—âˆ¥L2(PZO)
via a triangular inequality. In Step 3, we apply the partial measure of ill-posedness to obtain an
upper bound on âˆ¥[ Ë†fÎ»] âˆ’fâˆ—âˆ¥L2(PXO). We highlight our technical contributions in each step with
Remark 4.5 and Remark 4.6.
Step 1 We upper bound âˆ¥T[ Ë†fÎ»] âˆ’T[ Â¯fÎ»]âˆ¥L2(PZO). By their definition in Eq. (24) and Eq. (23),
the discrepancy between Ë†fÎ» and Â¯fÎ» arises solely from the difference between Ë†FÎ¾ and Fâˆ—; hence it
corresponds to Stage I error. First, we prove in Proposition D.3 that âˆ¥T[ Ë†fÎ»] âˆ’T[ Â¯fÎ»]âˆ¥L2(PZO) can be
upper bounded by an expression involving âˆ¥Ë†FÎ¾ âˆ’Fâˆ—âˆ¥G and âˆ¥Fâˆ—âˆ’Ë†FÎ¾âˆ¥L2(ZÃ—O;HX,Î³x), where we recall
that G denotes the unique vector-valued RKHS induced by the operator-valued kernel K defined
in Eq. (4). To obtain high-probability upper bounds on both of these quantities, we adapt the
existing optimal learning rates on CME from Li et al. [2022] to our setting with a tensor product
RKHS. The fact that we require upper learning rate for âˆ¥Fâˆ—âˆ’Ë†FÎ¾âˆ¥G imposes the conditions mo > to
and mz > tz, so the CME Fâˆ—lies in a smoother space than RKHS G. Specifically, Fâˆ—belongs to the
power space [G]mâ€  (See Eq. (79)). In addition, the constraints 2to > mo and 2tz > mz reflect the
saturation effect inherent in Tikhonov regularization [Bauer et al., 2007, Lu et al., 2024, Meunier
et al., 2024b]. These constraints can be removed to allow for greater smoothness of Fâˆ—by employing
spectral regularization [Meunier et al., 2024b]. The appearance of an arbitrarily small Î¶ > 0 in
Stage I regularization parameter Î¾ reflects the fact that, after reordering, the eigenvalues of the
tensor product operator exhibit slower decay than those of the individual components, owing to an
extra logarithmic term [Krieg, 2018].
Remark 4.5 (Tensor product kernel ridge regression). In the above step, we generalize the upper
learning rate for vector-valued kernel ridge regression to the setting of tensor product kernels (See
Proposition D.4). Although tensor product kernels have been widely used in kernel-based hypothesis
tests [Gretton et al., 2007, Gretton and Gyorfi, 2010, Sejdinovic et al., 2013, Gretton, 2015, Zhang
et al., 2018, Albert et al., 2022, SzabÃ³ and Sriperumbudur, 2018], kernel independent component
17

analysis [Bach and Jordan, 2002, Shen et al., 2009], and feature selection [Song et al., 2012, Li
et al., 2021], they have been less well studied in kernel ridge regression, with the exception of Hang
and Steinwart [2021] for Gaussian kernels. Our analysis is also applicable to real-valued kernel
ridge regression with tensor product kernels.
Step 2 We upper bound âˆ¥T[ Â¯fÎ»] âˆ’Tfâˆ—âˆ¥L2(PZO). We follow the approach in Blanchard and
MÃ¼cke [2018], Meunier et al. [2024a], where it is observed (in the standard NPIV case) that this
term corresponds to the learning risk of a kernel ridge regression problem with an appropriately
defined RKHS HFO âŠ†{Z Ã— O â†’R}, namely the RKHS induced by the feature map (z, o) 7â†’
Fâˆ—(z, o) âŠ—Ï•O,2(o). We refer the reader to Section C in the Supplement for the definition of
HFO. Our construction is adapted from Meunier et al. [2024a, Appendix E.1.2]. However, unlike
Blanchard and MÃ¼cke [2018], Meunier et al. [2024a] who only consider fixed RKHSs, we employ
tensor product Gaussian RKHS HÎ³x,Î³o with adaptive length-scales Î³x, Î³o (as in Eq. (20)) to capture
the anisotropic smoothness of fâˆ—âˆˆBsx,so
2,âˆ(X Ã— O) [Hang and Steinwart, 2021]. When Î·1 = 0, our
choice of length-scales Î³x, Î³o coincides with that of kernel ridge regression in Hang and Steinwart
[2021]. The logarithmic factor (log n)
dx+do+1
2
in Eq. (22) arises from the entropy numbers of the
Gaussian RKHS in this step (see Section C.1 in the Supplement and Hang and Steinwart [2021,
Proposition 1]).
Remark 4.6 (Gaussian kernel ridge regression with Besov space target functions). To establish
learning rates for kernel ridge regression, there are two main techniques in the literature: the empirical
process technique [Steinwart and Christmann, 2008, Steinwart et al., 2009] and the integral operator
technique [Fischer and Steinwart, 2020, Lin et al., 2020, Smale and Zhou, 2007, Caponnetto and
De Vito, 2007, Blanchard and MÃ¼cke, 2018]. Previous works on Gaussian kernel ridge regression
with Besov space targets [Eberts and Steinwart, 2013, Hang and Steinwart, 2021, Hamm and
Steinwart, 2021] rely on an oracle inequality proved via empirical process techniques [Steinwart and
Christmann, 2008, Theorem 7.23], which necessitates a clipping operation on the estimator. On the
other hand, the integral operator technique avoids the clipping operation, but it requires the target fâˆ—
in a power space of the RKHSâ€”a condition known as the source condition [Fischer and Steinwart,
2020, SRC]â€”which does not hold for Besov space targets and Gaussian RKHSs.
In our proof in step 2, we prove an upper learning rate of Gaussian kernel ridge regression
with Besov space target functions without clipping the estimator. Specifically, we combine the two
techniques above, in that we bound the approximation error with Hang and Steinwart [2021], while
we bound the estimation error with the integral operator technique [Fischer and Steinwart, 2020].
To be precise, define
fÎ» := arg min
fâˆˆHÎ³x,Î³o
Î»âˆ¥fâˆ¥2
HÎ³x,Î³o + âˆ¥T([f] âˆ’fâˆ—)âˆ¥2
L2(PZO).
(25)
The learning risk âˆ¥T([ Â¯fÎ»] âˆ’fâˆ—)âˆ¥L2(PZO) can be decomposed into an estimation error term âˆ¥T([ Â¯fÎ»] âˆ’
[fÎ»])âˆ¥L2(PZO) and an approximation error term âˆ¥T([fÎ»]âˆ’fâˆ—)âˆ¥L2(PZO). We upper bound the estimation
error with Fischer and Steinwart [2020, Theorem 16], once we prove that the RKHS HFO satisfies an
embedding property (see Fischer and Steinwart [2020, EMB]), which avoids the clipping operation.
We upper bound the approximation error with Hang and Steinwart [2021, Theorem 4], which avoids
the source condition.
Step 3 We combine the above two terms âˆ¥T[ Ë†fÎ»]âˆ’T[ Â¯fÎ»]âˆ¥L2(PZO) and âˆ¥T[ Â¯fÎ»]âˆ’Tfâˆ—âˆ¥L2(PZO) through
a triangle inequality, which gives an upper bound on the projected risk âˆ¥T[ Ë†fÎ»] âˆ’Tfâˆ—âˆ¥L2(PZO).
âˆ¥T[ Ë†fÎ»] âˆ’Tfâˆ—âˆ¥L2(PZO) â‰²(log n)
dx+do+1
2
n
âˆ’
sx
dx +Î·1
1+2( sx
dx +Î·1)+ do
so ( sx
dx +Î·1) .
(26)
18

To bound the unprojected risk âˆ¥[ Ë†fÎ»] âˆ’fâˆ—âˆ¥L2(PZO), it seems all that remains is to apply the
Fourier measure of partial ill-posedness in Assumption 4.2 to remove T. Unfortunately, however,
Assumption 4.2 only holds for functions in LF(Î³) (Eq. (13)) with low partial Fourier frequency
in X. Notice that for a function f âˆˆHÎ³x,Î³o, its partial Fourier spectrum |F[f(Â·, o)](Ï‰x)| decays
exponentially fast as âˆ¥Ï‰xâˆ¥2 â†’âˆ[Wendland, 2004, Theorem 10.12]
(âˆ€o âˆˆO),
R
Rdx |F[f(Â·, o)](Ï‰x)|2 exp
  1
4Î³2
xâˆ¥Ï‰xâˆ¥2
2

dÏ‰x < âˆ.
(27)
This exponential decay implies that Ë†fÎ»(Â·, o) satisfies the conditions of Assumption 4.2 up to some
logarithmic factors as reflected by (log n)
dxÎ·0
2
in Eq. (22). For fâˆ—, we find an auxiliary function
faux âˆˆHÎ³x,Î³o that is close to fâˆ—in L2(PXO)-norm and agrees with fâˆ—at low frequencies. To be
precise, we require that supp(F[(faux âˆ’fâˆ—)(Â·, o)]) âŠ†{Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¥Î³âˆ’1
x } for any o âˆˆO, and we
refer the reader to Eq. (40) for the exact definition of faux. Hence,
âˆ¥[ Ë†fÎ»] âˆ’fâˆ—âˆ¥L2(PXO) â‰²âˆ¥[ Ë†fÎ»] âˆ’fauxâˆ¥L2(PXO) â‰²Î³âˆ’Î·0dx
x
(log n)
dxÎ·0
2 âˆ¥T[ Ë†fÎ»] âˆ’Tfauxâˆ¥L2(PZO).
In the last step, we utilize the Fourier measure of partial ill-posedness in Assumption 4.2, and the
fact that Ë†fÎ», faux âˆˆHÎ³x,Î³o. The above equation is a sketch where formal derivations can be found
at the beginning of Section D, particularly Eq. (74) and Eq. (75). Combining the above relation
and the choice of Î³x in Eq. (20) concludes the proof of Theorem 4.1.
Remark 4.7 (Extension to more anisotropy). For simplicity of presentation, we focus on the
case where anisotropic smoothness exists across (X, O) but we assume no anisotropy within X
and O. The KIV-O algorithm with adaptive length-scales and its associated learning rate can
both be easily extended to the fully anisotropic setting, where fâˆ—âˆˆBs
2,âˆ(X Ã— O) with s =
[s1, . . . , sdx, sdx+1, . . . , sdx+do] âˆˆRdx+do, a generalization of previous results from anisotropic non-
parametric regression [Hang and Steinwart, 2021, Suzuki and Nitanda, 2021, Hoffman and Lepski,
2002] to anisotropic NPIV-O. This is particularly relevant in applied work, where the observed
covariates O are often of high dimensionality, because practitioners tend to adjust for as many
observed covariates as possible to mitigate unobserved confounding. In such cases, our KIV-O
algorithm with adaptive length-scales adapts to the intrinsic smoothness with respect to O. This
mitigates the slow rates typically caused by a high ambient dimension when the intrinsic smoothness
is high, as our learning rates avoid being limited by the worst-case smoothness across all dimensions.
4.3
Minimax lower bound for NPIV-O
In Theorem 4.2, we prove a minimax lower bound for the NPIV-O problem. We call admissible a
distribution PZXOY over (Z, X, O, Y ) satisfying Assumption 2.2, Assumption 4.1, Assumption 4.4
and Assumption 4.5, inducing a model of the form
Y = fâˆ—(X, O) + Ïµ,
E[Ïµ|Z, O] = 0 ,
where fâˆ—satisfies Assumption 2.3 and the conditional expectation operator T : L2(PXO) â†’L2(PZO)
satisfies Assumption 4.3. For an admissible distribution PZXOY , consider D = (zi, xi, oi, yi)N
i=1, N â‰¥
1 sampled i.i.d from PZXOY and consider D1 = (Ëœzi, Ëœoi, Ëœxi)Ëœn
i=1 and D2 = (zi, oi, yi)n
i=1 with Ëœn, n â‰¤N.
Theorem 4.2 (Minimax lower bound). There exists an admissible distribution PZXOY such that
for all learning methods (D1, D2) 7â†’Ë†f(D1,D2), for all Ï„ > 0, and sufficiently large n â‰¥1, the
following minimax lower bound holds with P n-probability at least 1 âˆ’C1Ï„ 2 and P Ëœn-almost surely ,
 Ë†f(D1,D2) âˆ’fâˆ—

L2(PXO) â‰¥C0Ï„ 2n
âˆ’
sx
dx
1+2( sx
dx +Î·1)+ do
so
sx
dx (log n)âˆ’2sxâˆ’dx.
(28)
19

We remind the reader here of our upper bound in Eq. (22): with P n+Ëœn-probability at least
1 âˆ’40eâˆ’Ï„, we have

h
Ë†fÎ»
i
âˆ’fâˆ—

L2(PXO) â‰²Ï„n
âˆ’
sx
dx +Î·1âˆ’Î·0
1+2( sx
dx +Î·1)+ do
so ( sx
dx +Î·1) Â· (log n)
dx+do+1+dxÎ·0
2
.
There remains a gap between the upper and minimax lower bounds even if we take Î·1 = Î·0 and
ignore the logarithmic terms. We dedicate Section 5 to its discussion.
Remark 4.8 (Interpolation between NPIV and nonparametric regression). Similar to the Re-
mark 4.4 of the upper learning rate, our minimax lower learning rate also interpolates between
the known optimal L2-rates for NPIV without observed covariates (do = 0) and anisotropic non-
parametric regression (Î·1 = Î·0 = 0).
Remark 4.9 (Comparison to existing L2-minimax lower bounds for NPIV regression). The
work Chen and Reiss [2011] established minimax rate-optimality in L2-norm for NPIV under an
approximation condition and link condition. For a conveniently chosen compact operator B, the
approximation condition characterizes the smoothness of the structural function by the optimal L2-
rate approximation by the eigenvectors of B, and the link condition relates the mapping properties of
the conditional expectation operator T to the Hilbert scale generated by B. This framework subsumes
an earlier minimax convergence rate result in Hall and Horowitz [2005], and was subsequently
instantiated in Chen and Christensen [2018] for a B constructed via a wavelet basis, and generalized
to kernel instrumental variables in Meunier et al. [2024a] with a Hilbert scale given by the covariance
operator of the RKHS. A crucial limitation of this framework is the link condition can only hold if
the singular values of the conditional expectation operator are decaying to zero. Thus the minimax
framework given by the link condition is applicable only when T is compact, and is not suitable for
our observed covariates setting.
We give an outline of our proof for Theorem 4.2 to facilitate a deeper understanding of both the
assumptions and the results. The full proof is in Section E.2 in the Supplementary. The primary
strategy in deriving minimax lower bounds is to construct a family of distributions that are similar
enough so that they are statistically indistinguishable, but for which the target function of interest
is maximally separated. This implies no estimator can have error uniformly smaller than this
separation.
Following prior work in the literature [Chen and Reiss, 2011, Meunier et al., 2024a], we observe
that NPIV-O in Eq. (1) is statistically more challenging than the reduced form non-parametric
indirect regression with observed covariates (NPIR-O) with a known operator T : L2(PXO) â†’
L2(PZO). The NPIR-O model is defined below
Y = (Tfâˆ—)(Z, O) + Ï…,
(29)
where Ï… is a random variable such that E[Ï… | Z, O] = 0, and it satisfies Assumption 4.5, and where T
satisfies Assumption 4.1, Assumption 4.3 and the associated conditional distribution PX|Z,O satisfies
Assumption 2.2. We refer the reader to Section E.1 in the Supplementary Material for a detailed
definition of the NPIR-O model class. We formally prove in Lemma E.1 in the Supplementary
that it suffices to construct a minimax lower bound for the NPIR-O model. To this end, we adapt
Theorem 20 of Fischer and Steinwart [2020], which is itself an adaptation of Proposition 2.3 of
Tsybakov [2008]. Recall that the Kullback-Leibler divergence of two probability measures P1, P2
on some measurable space (â„¦, A) is given by KL(P1, P2) :=
R
â„¦log(dP1
dP2 ) dP1 if P1 is absolutely
continuous with respect to P2, and +âˆotherwise.
20

To apply Theorem 20 of Fischer and Steinwart [2020] on the measurable space â„¦= (Z Ã—OÃ—R)n,
we construct a family of probability measures P0, P1, . . . , PM over (Z Ã— O Ã— R) that share the
same marginal distribution over (Z, O) but different conditional distributions PY |Z,O. Our strategy
follows the construction in Chen and Reiss [2011] and can be explained as follows. We fix a marginal
probability measure PZO over Z Ã—O. We then propose a family of conditional probability measures
Pi;Y |Z,O indexed by fi âˆˆF for 0 â‰¤i â‰¤M. Since fiâ€™s are functions on X Ã— O, we fix a marginal
probability measure PX on X under the constraint that X is independent of O. Then, we define a
smooth copula to parametrize the dependence between PX and PZ, which fully specifies PX|Z,O.
This induces a fixed conditional expectation operator T : L2(PXO) â†’L2(PZO). We then specify
Pi;Y |Z,O for 0 â‰¤i â‰¤M via the following equation:
Y | Z = z, O = o âˆ¼N((Tfi)(z, o), Ïƒ2),
for a fixed Ïƒ > 0, and for any (z, o) âˆˆZ Ã— O. Pi denotes the probability measure on Z Ã— O Ã— R by
coupling Pi;Y |Z,O and PZO. In the above construction, we require PX|Z,O to satisfy Assumption 2.2,
and all fiâ€™s to satisfy Assumption 2.3, namely fi âˆˆS.
Concretely, we fix PZO to be the Lebesgue measure over Z Ã— O. Without loss of generality,
we take X = [âˆ’0.5, 0.5]dx rather than [0, 1]dx. By working with a symmetric domain, we exploit
the fact that even functions have real-valued Fourier transforms, which simplify our subsequent
calculations. We fix PX via the density function
pX(x) âˆQdx
i=1 gi(xi),
gi(xi) := exp

âˆ’
2
1âˆ’4x2
i

1xiâˆˆ[âˆ’0.5,0.5],
(30)
where each gi is a smooth, compactly supported bump function. We fix an arbitrary smooth
copula [Nelsen, 2006], such that PX|Z,O satisfies Assumption 2.2 and T satisfies Assumption 4.3
with parameter Î·1 > 0.
The main technical challenge of the minimax lower bound is the construction of the function
class F whose each element fi induces a conditional distribution PX|Z,O. Our goal is to design F so
that it satisfies three desirable properties: 1) we want F âˆˆBsx,so
2,âˆ(Rdx+do) as per Assumption 2.3. 2)
we want the size of F to be large to enable the application of Fischer and Steinwart [2020, Theorem
20]. 3) we want all elements of F to exhibit high partial Fourier frequency such that after applying
T, the resulting functions become difficult to distinguish. This increases the intrinsic difficulty of
the problem and yields a tighter lower bound. The construction of such a function class satisfying
the first two properties is standard and can be achieved using anisotropic B-splines [Ibragimov
and Khasâ€™ minskii, 1984, Suzuki and Nitanda, 2021, Schmidt-Hieber, 2020]. However, the third
property poses a challenge: the Fourier transform of B-splines has full support. Consequently,
conventional B-splines cannot enforce the desired partial high-frequency restriction. To address
this limitation, we convolve the B-splines with a high frequency bandpass filter in the X-direction.
Since convolution acts as a smoothing operator, the function class constructed with such modified
B-splines remains in the Besov space.
To start with, we introduce the definition of anisotropic B-splines [Leisner, 2003].
Definition 4 (Anisotropic B-spline [Leisner, 2003]). The cardinal B-spline of order m âˆˆN is
defined as the repeated m times convolution Î¹m = Î¹0 âˆ—Â· Â· Â· âˆ—Î¹0 where Î¹0 is the indicator function
1[0,1]. Let d denote the ambient dimension, and let s = (s1, . . . , sd) denote a smoothness vector.
Define the notation s = min(s1, . . . , sd) and sâ€²
i := s
si for 1 â‰¤i â‰¤d. The (anisotropic) B-spline of
order m with resolution K âˆˆN+ and location vector â„“âˆˆQd
i=1{âˆ’m, âˆ’m + 1, . . . , 2âŒŠKsâ€²
iâŒ‹} is defined
as MK,â„“(x) = Qd
i=1 Î¹m(2âŒŠKsâ€²
iâŒ‹xi âˆ’â„“i).
21

3
4
5
6
7
2âˆ’kÏ‰x
Î¶ = 1
3
4
5
6
7
2âˆ’kÏ‰x
Î¶ = 2
Figure 2: This figure illustrates the partial Fourier transform of fv in Eq. (35), where we take dx = 1, K = 2,
m = 4, sx = s â‰¤so. The dashed blue line represents the Fourier transform of the B-spline MK,âˆ’2. The red
line shows the result of applying frequency masks to this B-spline: P
â„“x Î²v(â„“x,â„“o)F[MK,âˆ’2](Ï‰x) Â· 1â„“x(2âˆ’KÏ‰x).
The Fourier transform of fv(Â·, o) is equal to this result up to scaling factors dependent only on o.
Let m > sx âˆ¨so be a non-negative even integer. The resolution K = K(n) is defined later in
Eq. (131), such that K â†’âˆas n â†’âˆ. The B-spline basis in the O-direction follows the standard
construction. Define the set of location vectors LO and the associated B-splines
LO :=

2mN âˆ©

0, . . . , 2
j
K s
so
kdo
,
MK,â„“o(o) = Qdo
j=1 Î¹m

2
j
Ks
so
k
oj âˆ’â„“o,j

.
(31)
Note that here we enforce all location vectors to be a multiple of 2m such that the {MK,â„“o}â„“oâˆˆLO
have disjoint supports, which will simplify the calculations later on. In contrast, the basis functions
in the X-direction are constructed by convolving a standard B-spline M0,âˆ’m
2 on X with the inverse
Fourier transform of the indicator function 1â„“x, for â„“x âˆˆLX:
LX :=
n
0, 1, . . . ,
j
0.8Ï€
Î¶ 2K s
sx
kodx ,
â„¦Kâ„“x(x) :=

M0,âˆ’m
2 âˆ—Fâˆ’1[1â„“x]
 
2
Ks
sx x

.
(32)
where 1â„“x is the indicator function over the following hyper-rectangle:
Iâ„“x :=
dxÃ—
j=1
h
1.1Ï€ + Î¶â„“x,j2âˆ’K s
sx , 1.1Ï€ + (Î¶â„“x,j + 1)2âˆ’K s
sx
i
.
(33)
Î¶ > 0 is a width hyperparameter that determines the spacing of different hyper-rectangles; its
value will be specified later. For sufficiently large n â‰¥1, since â„“x,j â‰¤0.8Ï€
Î¶ 2
Ks
sx , we know that
1.1Ï€ + (Î¶â„“x,j + 1)2âˆ’K s
sx â‰¤1.9Ï€ + 2âˆ’K s
sx â‰¤1.95Ï€. Thus we have Iâ„“x âŠ†[1.1Ï€, 1.95Ï€]dx.
The main consequence of this construction is revealed via the convolution theorem [Rudin, 1987,
Theorem 9.2], which gives
F[â„¦Kâ„“x](Ï‰x) = F[MK,âˆ’m
2 ](Ï‰x) Â· 1â„“x(2âˆ’Ks
sx Ï‰x).
(34)
As a result, we have that supp(F[â„¦Kâ„“x]) = 2
Ks
sx Â· Iâ„“x âŠ†[Ï€2
Ks
sx , 2Ï€2
Ks
sx ]dx. Since K â†’âˆas n â†’âˆ,
the construction guarantees that â„¦Kâ„“x only has high-frequency spectrum, thereby fulfilling the third
desirable property for F.
Define the basis â„¦K(â„“x,â„“o)(x, o) := â„¦Kâ„“x(x) Â· MK,â„“o(o) and the set of all location vectors L =
LX Ã— LO. We have |L| â‰Î¶âˆ’dx2Ks( dx
sx + do
so ). We construct a function class F as follows:
F :=

fv : fv(x, o) = 2âˆ’Ks

1âˆ’dx
2sx
 P
(â„“x,â„“o)âˆˆL Î²v(â„“x,â„“o)â„¦K(â„“x,â„“o)(x, o)
 v âˆˆ{0, 1}|L|

,
(35)
22

where Î²v(â„“x,â„“o) âˆˆ{0, 1} is the value assigned by v âˆˆ{0, 1}|L| to the location vector (â„“x, â„“o) âˆˆL.
Specifically, for each fv âˆˆF and for a fixed o âˆˆO, we can see that fv(Â·, o) is constructed by
applying a sum of frequency masks indexed by â„“x âˆˆLX to the original B-spline MK,âˆ’m
2 ,
F[fv(Â·, o)](Ï‰x) = F
h
MK,âˆ’m
2
i
(Ï‰x) Â·
 X
â„“oâˆˆLO
X
â„“xâˆˆLX
Î²v(â„“x,â„“o)1â„“x

2âˆ’Ks
sx Ï‰x

|
{z
}
frequency masks
MK,â„“o(o)

.
Figure 2 is provided to illustrate this effect of frequency masking.
We prove in Eq. (108) in the Supplementary that F âŠ†U(Bsx,so
2,âˆ(Rdx+do)) âˆ©Lâˆ(Rdx+do) âˆ©
L1(Rdx+do) âˆ©C0(Rdx+do) âŠ†S, satisfying Assumption 2.3. For each fv âˆˆF, we construct the
conditional distribution Pv(Â· | z, o) := N((Tfv)(z, o), Ïƒ2) as the normal distribution on R with mean
(Tfv)(z, o) and variance Ïƒ2, such that Pv satisfies Assumption 4.5. Together with the marginal
distribution PZO over Z Ã— O, we have constructed a family of distributions {Pv, v âˆˆ{0, 1}|L|} over
Z Ã— O Ã— R with each Pv associated to a fv âˆˆF. Next, by the Gilbert-Varshamov Bound [Tsybakov,
2008, Lemma 2.9], we prove in Eq. (123) and Eq. (127) in the Supplementary that there exists a
subset VK âŠ†{0, 1}|L| with |VK| â‰¥2
L
8 such that for any fv, fvâ€² âˆˆFpruned = {fv | v âˆˆVK}, there is
âˆ¥fv âˆ’fvâ€²âˆ¥2
L2(PXO) â‰¥2âˆ’2KsÎ¶âˆ’dx,
KL
 P âŠ—n
v
, P âŠ—n
0

â‰¤n2âˆ’2Ks( dx
sx Î·1+1).
Therefore, we have established that the function class Fpruned satisfies all the three properties as
desired, namely Fpruned âŠ‚G, functions fv âˆˆFpruned are well-separated in L2(PXO) yet statistically
indistinguishable. Finally, we take Î¶ = (log n)2, we apply Fischer and Steinwart [2020, Theorem 20]
and the general reduction scheme in Tsybakov [2008, Section 2.2] to obtain the desired minimax
lower bound.
5
On the gap between the upper bound and minimax lower bound
As explained in Section 4.3, in the general setting where dx
sx > 0 and do
so > 0, a gap arises between
the upper bound given in Theorem 4.1 and the minimax lower bound given in Theorem 4.2. By
setting Î·0 = Î·1 = Î· in Assumption 4.3 and Assumption 4.2, which gives a precise characterization
of the partial smoothing effect of T, we obtain the following upper and lower bounds (ignoring the
logarithmic terms for simplicity):
Lower Bound:
n
âˆ’
sx
dx
1+2( sx
dx +Î·)+ do
so
sx
dx ,
Upper Bound:
n
âˆ’
sx
dx
1+2( sx
dx +Î·)+ do
so
sx
dx + do
so Î·
Note that the denominator in the exponent of the upper bound contains an extra do
so Î· term, which
vanishes when T has no partial smoothing effect (Î· = 0) or when the ratio do
so is zero.
The gap between the upper bound and the minimax lower bound arises due to the existence of
observed covariates O in the analysis of the approximation error term in the upper bound,
âˆ¥[fÎ»] âˆ’fâˆ—âˆ¥L2(PXO),
where fÎ» is defined in Eq. (25). To provide an in-depth discussion, we begin with a warm-up in
Section 5.1 by considering the standard KIV setting without observed covariates, where our upper
and minimax lower bounds match. We then move on to KIV-O in Section 5.2 and demonstrate
how the presence of observed covariates O causes a gap to emerge.
23

5.1
Analysis of the approximation error in KIV
In NPIV, the conditional expectation operator is T : L2(PX) â†’L2(PZ), f 7â†’E[f(X) | Z]. Hence
fÎ» defined in Eq. (25) reduces to the following
fÎ» := arg minfâˆˆHX,Î³x Î»âˆ¥fâˆ¥2
HX,Î³x + âˆ¥T([f] âˆ’fâˆ—)âˆ¥2
L2(PZ).
We first employ the fact that fÎ» is the minimizer, hence for some faux âˆˆHX,Î³x,
âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZ) â‰¤Î»âˆ¥fauxâˆ¥2
HX,Î³x + âˆ¥T([faux] âˆ’fâˆ—)âˆ¥2
L2(PZ).
Here, faux is defined as faux := fâˆ—,low + KÎ³x âˆ—fâˆ—,high , where KÎ³x is defined, following Hang and
Steinwart [2021], Eberts and Steinwart [2013], as
KÎ³x(x) := Pr
j=1
 r
j

(âˆ’1)1âˆ’j
1
(jÎ³x)d
  2
Ï€
 d
2 exp

âˆ’2 Pd
i=1
x2
i
(jÎ³x)2

,
(36)
and fâˆ—,low, fâˆ—,high are defined such that fâˆ—,low (resp. fâˆ—,high) corresponds to the low-frequency (resp.
high-frequency) component of fâˆ—= fâˆ—,low + fâˆ—,high:
F[fâˆ—,low](Ï‰x) = F[fâˆ—](Ï‰x) Â· 1[Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¤Î³âˆ’1
x ]
F[fâˆ—,high](Ï‰x) = F[fâˆ—](Ï‰x) Â· 1[Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¥Î³âˆ’1
x ].
To see why faux âˆˆHX,Î³x, note that âˆ¥fâˆ—,lowâˆ¥2
HX,Î³x â‰²
R
Rdx |F[fâˆ—](Ï‰x)|2 dÏ‰x = âˆ¥fâˆ—âˆ¥2
L2(Rdx) < âˆand
KÎ³x âˆ—fâˆ—,high âˆˆHX,Î³x proved by Eberts and Steinwart [2013]. Then we have
âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZ) â‰¤Î»âˆ¥fauxâˆ¥2
HX,Î³x + âˆ¥T(fâˆ—,high âˆ’KÎ³x âˆ—fâˆ—,high)âˆ¥2
L2(PZ)
(âˆ—)
â‰¤Î»âˆ¥fauxâˆ¥2
HX,Î³x + Î³2dxÎ·
x
âˆ¥fâˆ—,high âˆ’KÎ³x âˆ—fâˆ—,highâˆ¥2
L2(PX)
(37)
(âˆ—âˆ—)
â‰²Î»Î³âˆ’dx
x
+ Î³2dxÎ·+2sx
x
.
where (âˆ—) follows by Assumption 4.3, and (âˆ—âˆ—) follows by the same derivations in Eberts and
Steinwart [2013, Theorem 2.2, Theorem 2.3]. Eq. (37) is the key step which would require significant
modifications in the setting of NPIV-O due to the existence of observed covariates. Finally, we have
âˆ¥[fÎ»] âˆ’fâˆ—âˆ¥2
L2(PX)
â‰¤âˆ¥[fÎ»] âˆ’[faux]âˆ¥2
L2(PX) + âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PX)
(a)
â‰¤Î³âˆ’2dxÎ·
x
âˆ¥T[fÎ»] âˆ’T[faux]âˆ¥2
L2(PZ) + âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PX)
â‰¤Î³âˆ’2dxÎ·
x

âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZ) + âˆ¥T([faux] âˆ’fâˆ—)âˆ¥2
L2(PZ)

+ âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PX)
(b)
â‰¤Î³âˆ’2dxÎ·
x

âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZ) + Î³2dxÎ·
x
âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PX)

+ âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PX)
â‰²Î³âˆ’2dxÎ·
x

Î»Î³âˆ’dx
x
+ Î³2dxÎ·+2sx
x

+ 2âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PX)
(c)
â‰²nâˆ’
2sx
dx+2sx+2Î·dx .
In the above derivations, (a) follows by an application of Assumption 4.2 and Lemma D.9 since
âˆ¥fÎ»âˆ¥2 â‰¤Î»âˆ’1âˆ¥Tfâˆ—âˆ¥2
L2(PZ) â‰²n by the optimality of fÎ» and âˆ¥fauxâˆ¥â‰¤n by the same derivations as in
Eq. (67) in the Supplement. The second term of Eq. (96) in the Supplement is subsumed by the
following choice of Î³x. Furthermore, (b) follows from Assumption 4.3, and (c) follows from using
Eq. (84) in the Supplement and choosing Î» = nâˆ’1, Î³x = nâˆ’
1
dx+2sx+2Î·dx . The upper bound on the
approximation error above is optimal in the sense that it matches the minimax lower bound of
NPIV with Besov targets (see e.g [Chen and Christensen, 2018] and our Theorem 4.2 with do = 0).
24

5.2
Analysis of the approximation error in KIV-O
We now proceed to our NPIV-O setting, and demonstrate how the existence of observed covariates
O fundamentally changes the problem. As above, to upper bound the approximation error âˆ¥[fÎ»] âˆ’
fâˆ—âˆ¥L2(PXO), we first need to upper bound the projected approximation error âˆ¥TfÎ» âˆ’Tfâˆ—âˆ¥L2(PZO).
To this end, we employ the fact that fÎ» is the minimizer in Eq. (25), hence for some faux âˆˆHÎ³x,Î³o,
âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZO) â‰¤Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o + âˆ¥T([faux] âˆ’fâˆ—)âˆ¥2
L2(PZO).
We construct such an auxiliary function faux. For any o âˆˆO, we construct fâˆ—,low(Â·, o) and fâˆ—,high(Â·, o)
such that
F[fâˆ—,low(Â·, o)](Ï‰x) = F[fâˆ—(Â·, o)](Ï‰x) Â· 1[Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¤Î³âˆ’1
x ]
(38)
F[fâˆ—,high(Â·, o)](Ï‰x) = F[fâˆ—(Â·, o)](Ï‰x) Â· 1[Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¥Î³âˆ’1
x ].
(39)
Note that fâˆ—,high = fâˆ—âˆ’fâˆ—,low. Despite fâˆ—,low(Â·, o) âˆˆHX,Î³x for any o âˆˆO, a crucial difference with
NPIV is that, fâˆ—,low /âˆˆHÎ³x,Î³o since fâˆ—,low is only constructed by a cut-off with respect to the partial
Fourier spectrum of on X. Hence, to construct faux we convolve fâˆ—,low again with KÎ³o:
faux = fâˆ—,low âˆ—KÎ³o + fâˆ—,high âˆ—KÎ³x,Î³o âˆˆHÎ³x,Î³o.
(40)
Here, KÎ³x,Î³o(x, o) := KÎ³x(x) Â· KÎ³o(o). Proceeding from the above, we have
âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZO) â‰¤Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o
+ âˆ¥T(fâˆ—,low âˆ’fâˆ—,low âˆ—KÎ³o)âˆ¥2
L2(PZO) + âˆ¥T(fâˆ—,high âˆ’fâˆ—,high âˆ—KÎ³x,Î³o)âˆ¥2
L2(PZO)
(41)
â‰¤Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o + âˆ¥fâˆ—,low âˆ’fâˆ—,low âˆ—KÎ³oâˆ¥2
L2(PXO)
+ Î³2dxÎ·
x
âˆ¥fâˆ—,high âˆ’fâˆ—,high âˆ—KÎ³x,Î³oâˆ¥2
L2(PXO).
The last inequality follows by Jensenâ€™s inequality for the second term and Assumption 4.3 for the
third term. We would like to highlight here that the second term in Eq. (41) is absent in Eq. (37)
for the NPIV setting when there are no observed covariates O. Unlike the high frequency term in
Eq. (41), we cannot employ the Fourier measure of partial contractivity in Assumption 4.3 because
fâˆ—,low âˆ’fâˆ—,low âˆ—KÎ³o only contains low frequency spectrum on X by construction. From the above
derivations, we can also deduce
âˆ¥T([faux] âˆ’fâˆ—)âˆ¥2
L2(PZO) â‰²âˆ¥fâˆ—,low âˆ’fâˆ—,low âˆ—KÎ³oâˆ¥2
L2(PXO)
+ Î³2dxÎ·
x
âˆ¥fâˆ—,high âˆ’fâˆ—,high âˆ—KÎ³x,Î³oâˆ¥2
L2(PXO).
To upper bound the approximation error âˆ¥[fÎ»] âˆ’fâˆ—âˆ¥L2(PXO), we notice that
âˆ¥[fÎ»] âˆ’fâˆ—âˆ¥2
L2(PXO)
â‰¤âˆ¥[fÎ»] âˆ’[faux]âˆ¥2
L2(PXO) + âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PXO)
â‰¤Î³âˆ’2dxÎ·
x
âˆ¥T([fÎ»] âˆ’[faux])âˆ¥2
L2(PZO) + âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PXO)
â‰²Î³âˆ’2dxÎ·
x
âˆ¥T([fÎ»] âˆ’fâˆ—)âˆ¥2
L2(PZO) + Î³âˆ’2dxÎ·
x
âˆ¥T[faux] âˆ’Tfâˆ—âˆ¥2
L2(PZO) + âˆ¥[faux] âˆ’fâˆ—âˆ¥2
L2(PXO)
â‰²Î³âˆ’2dxÎ·
x

Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o + âˆ¥fâˆ—,low âˆ’fâˆ—,low âˆ—KÎ³oâˆ¥2
L2(PXO)
+ Î³2dxÎ·
x
âˆ¥fâˆ—,high âˆ’fâˆ—,high âˆ—KÎ³x,Î³oâˆ¥2
L2(PXO)

.
25

The last inequality holds by plugging into the upper bound on âˆ¥T([fÎ»] âˆ’faux)âˆ¥2
L2(PZO) and
âˆ¥T[faux] âˆ’Tfâˆ—âˆ¥2
L2(PZO) derived above.
Notice that the term âˆ¥fâˆ—,low âˆ’fâˆ—,low âˆ—KÎ³oâˆ¥L2(PXO) is
unnecessarily inflated by the Fourier measure of partial ill-posedness Î³âˆ’dxÎ·
x
â‰«1 (Assumption 4.2)
even if there is no smoothing effect of T acting on O. We obtain
âˆ¥[fÎ»] âˆ’fâˆ—âˆ¥2
L2(PXO) â‰²Î³âˆ’2dxÎ·
x

Î»Î³âˆ’dx
x
Î³âˆ’do
o
+ Î³2so
o
+ Î³2sx+2dxÎ·
x

+ Î³2so
o
+ Î³2sx
x
â‰²n
âˆ’
sx
dx
1+2( sx
dx +Î·)+ do
so ( sx
dx +Î·) .
The last step holds by choosing Î», Î³x, Î³o as in Theorem 4.1. The upper bound on the approximation
error does not match the minimax lower bound in Theorem 4.2.
We conclude this section by offering a more practical perspective on why the gap emerges.
Hyperparameters including the kernel lengthscales are selected via cross-validation in practice.
For the KIV-O estimator Ë†fÎ» computed from Stage II samples D2 = {(zi, oi, yi)}n
i=1, the kernel
lengthscales Î³x, Î³o are selected by minimizing the following cross-validation criterion.
CV(Î³x, Î³o, Î») = 1
n
Pn
i=1(yi âˆ’âŸ¨Ë†fâˆ’i,Î³x,Î³o,Î», Ë†FÎ¾(zi, oi)âŸ©HÎ³x,Î³o)2,
(42)
where Ë†fâˆ’i,Î³x,Î³o,Î» denotes the version of Ë†fÎ³x,Î³o,Î» computed from n âˆ’1 samples from D2 excluding its
i-th sample. This cross-validation criterion CV(Î³x, Î³o, Î») is a consistent estimator for the projected
risk âˆ¥T([ Ë†fÎ»] âˆ’fâˆ—)âˆ¥L2(PZO) and has been used in many 2SLS approaches [Hartford et al., 2017, Xu
et al., 2021b, Mastouri et al., 2021, Xu and Gretton, 2025].
Similarly, the choice of lengthscales Î³x, Î³o, Î» in our upper bound analysis is also obtained by
minimizing the projected risk âˆ¥T([ Ë†fÎ»] âˆ’fâˆ—)âˆ¥L2(PXO) which results Î³sx+dxÎ·
x
= Î³so
o . This can be
contrasted with the choice of kernel lengthscales in anisotropic kernel ridge regression which imposes
a different balance condition Î³sx
x = Î³so
o
[Hang and Steinwart, 2021]. The extra dxÎ· in the exponent
of Î³x arises due to the partial smoothing effect of T, which maps a function that is (sx, so)-smooth
on X Ã— O to a function that is (sx + dxÎ·, so)-smooth on Z Ã— O. On the other hand, our minimax
lower bound requires constructing B-splines in Eq. (98) in the Supplement with resolution K and
Jx := 2âŒŠKs
sx âŒ‹and Jo := 2âŒŠKs
so âŒ‹. Jx, Jo play a role analogous to Î³x, Î³o (see Remark 4.1), but they
satisfy the balance condition Jsx
x â‰Jso
o . This discrepancy between the kernel lengthscales balance
condition and B-spline lengthscales balance condition gives rise to the gap between our upper and
lower bound. Simply enforcing Î³sx
x = Î³so
o
in our upper bound analysis would result in a slower rate.
6
Conclusion
We study nonparametric instrumental variable regression with observed covariates (NPIV-O),
a setting that generalizes NPIV by incorporating observed covariates to enable heterogeneous
treatment effect estimation. The conditional expectation operator T behaves as a partial identity
operator, which makes NPIV-O a hybrid of NPIV and NPR. We prove an upper bound for kernel
2SLS and the first minimax lower bound. Our upper and lower bounds interpolate between the
known optimal rates for NPIV and NPR, and adapt to the anisotropic smoothness of fâˆ—. Our
analysis reveals a gap between the upper and lower bounds in the general setting, and closing this
gap remains an open direction for NPIV-O.
26

References
MÃ©lisande Albert, BÃ©â€™atrice Laurent, Amandine Marrel, and Anouar Meynaoui. Adaptive test of
independence based on hsic measures. The Annals of Statistics, 50(2):858â€“879, 2022.
Donald WK Andrews. Examples of l2-complete and boundedly-complete distributions. Journal of
Econometrics, 199(2):213â€“220, 2017.
Jean-Pierre Aubin. Applied functional analysis. John Wiley & Sons, 2011.
Francis R Bach and Michael I Jordan. Kernel independent component analysis. Journal of Machine
Learning Research, 3:1â€“48, 2002.
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in learning
theory. Journal of Complexity, 23(1):52â€“72, 2007.
Andrew Bennett, Nathan Kallus, and Tobias Schnabel. Deep generalized method of moments for
instrumental variable analysis. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.
Andrew Bennett, Nathan Kallus, Xiaojie Mao, Whitney Newey, Vasilis Syrgkanis, and Masatoshi
Uehara.
Minimax instrumental variable regression and l2 convergence guarantees without
identification or closedness. In The 36th Annual Conference on Learning Theory, pages 2291â€“
2318. PMLR, 2023.
Alain Berlinet and Christine Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and
Statistics. Springer, 2004. ISBN 978-1-4419-9096-9.
Gilles Blanchard and Nicole MÃ¼cke. Optimal rates for regularization of statistical inverse learning
problems. Foundations of Computational Mathematics, 18(4):971â€“1013, 2018.
Richard Blundell, Xiaohong Chen, and Dennis Kristensen. Semi-nonparametric iv estimation of
shape-invariant engel curves. Econometrica, 75(6):1613â€“1669, 2007.
Bariscan Bozkurt, Ben Deaner, Dimitri Meunier, Liyuan Xu, and Arthur Gretton. Density ratio-
based proxy causal learning without density ratios. In Yingzhen Li, Stephan Mandt, Shipra
Agrawal, and Emtiyaz Khan, editors, Proceedings of The 28th International Conference on
Artificial Intelligence and Statistics, volume 258 of Proceedings of Machine Learning Research,
pages 5095â€“5103. PMLR, 03â€“05 May 2025a.
URL https://proceedings.mlr.press/v258/
bozkurt25a.html.
Bariscan Bozkurt, Houssam Zenati, Dimitri Meunier, Liyuan Xu, and Arthur Gretton. Density
ratio-free doubly robust proxy causal learning. arXiv preprint arXiv:2505.19807, 2025b.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331â€“368, 2007.
Claudio Carmeli, Ernesto De Vito, and Alessandro Toigo. Vector valued reproducing kernel hilbert
spaces of integrable functions and mercer theorem. Analysis and Applications, 4(04):377â€“408,
2006.
URL https://www.worldscientific.com/doi/10.1142/S0219530506000838?srsltid=
AfmBOopECjt_9-RIZIsquke2XT4OkUwY1kvYRzJUfbAL5-e9JDTAXR6A.
27

Claudio Carmeli, Ernesto De Vito, Alessandro Toigo, and Veronica UmanitÃ¡.
Vector-valued
reproducing kernel Hilbert spaces and universality. Analysis and Applications, 8(01):19â€“61, 2010.
Marine Carrasco, Jean-Pierre Florens, and Eric Renault. Linear inverse problems in structural
econometrics estimation based on spectral decomposition and regularization.
Handbook of
Econometrics, 6:5633â€“5751, 2007.
Xiaohong Chen.
Large sample sieve estimation of semi-nonparametric models.
Handbook of
econometrics, 6:5549â€“5632, 2007.
Xiaohong Chen and Timothy M. Christensen. Optimal sup-norm rates and uniform inference on
nonlinear functionals of nonparametric iv regression: nonlinear functionals of nonparametric iv.
Quantitative Economics, 9(1):39â€“84, March 2018. ISSN 1759-7323.
Xiaohong Chen and Demian Pouzo. Estimation of nonparametric conditional moment models with
possibly nonsmooth generalized residuals. Econometrica, 80(1):277â€“321, January 2012.
Xiaohong Chen and Markus Reiss. On rate optimality for ill-posed inverse problems in econometrics.
Econometric Theory, 27(3):497â€“521, 2011. ISSN 0266-4666, 1469-4360.
Xiaohong Chen, Victor Chernozhukov, Sokbae Lee, and Whitney K Newey. Local identification of
nonparametric and semiparametric models. Econometrica, 82(2):785â€“809, 2014.
Xiaohong Chen, Timothy Christensen, and Sid Kankanala. Adaptive estimation and uniform
confidence bands for nonparametric structural functions and elasticities, 2024. URL https:
//arxiv.org/abs/2107.11869.
Zonghao Chen, Masha Naslidnyk, and Francois-Xavier Briol. Nested expectations with kernel
quadrature. In Aarti Singh, Maryam Fazel, Daniel Hsu, Simon Lacoste-Julien, Felix Berkenkamp,
Tegan Maharaj, Kiri Wagstaff, and Jerry Zhu, editors, Proceedings of the 42nd International
Conference on Machine Learning, volume 267, pages 8760â€“8793. PMLR, 13â€“19 Jul 2025.
Yifan Cui, Hongming Pu, Xu Shi, Wang Miao, and Eric Tchetgen Tchetgen. Semiparametric
proximal causal inference. Journal of the American Statistical Association, 119(546):1348â€“1359,
2024.
Serge Darolles, Yanqin Fan, Jean-Pierre Florens, and Eric Renault. Nonparametric instrumental
regression. Econometrica, 79(5):1541â€“1565, 2011.
Ernesto De Vito, Lorenzo Rosasco, Andrea Caponnetto, Umberto De Giovannini, Francesca Odone,
and Peter Bartlett. Learning from examples as an inverse problem. Journal of Machine Learning
Research, 6(5), 2005.
Ben Deaner. Proxy controls and panel data. arXiv preprint arXiv:1810.00283, 2018.
Andreas Defant and Carsten Michels. A complex interpolation formula for tensor products of
vector-valued banach function spaces. Archiv der Mathematik, 74:441â€“451, 2000.
Ronald A DeVore and George G Lorentz. Constructive approximation. Springer Science & Business
Media, 1993.
Ronald A. DeVore and Vasil A. Popov. Interpolation of besov spaces. Transactions of the American
Mathematical Society, 305(1):397â€“414, January 1988.
28

Ronald A DeVore and Robert C Sharpley. Besov spaces on domains in Rd. Transactions of the
American Mathematical Society, 335(2):843â€“864, 1993.
Nishanth Dikkala, Greg Lewis, Lester Mackey, and Vasilis Syrgkanis. Minimax estimation of
conditional moment models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 12248â€“12262.
Curran Associates, Inc., 2020.
Xavier Dâ€™Haultfoeuille. On the completeness condition in nonparametric instrumental problems.
Econometric Theory, 27(3):460â€“471, 2011.
Mona Eberts and Ingo Steinwart.
Optimal regression rates for svms using gaussian kernels.
Electronic Journal of Statistics, 7(none):1 â€“ 42, 2013.
David Eric Edmunds and Hans Triebel. Function spaces, entropy numbers, differential operators.
Cambridge University Press, 1996.
Heinz Werner Engl, Martin Hanke, and Andreas Neubauer. Regularization of inverse problems.
Springer Science & Business Media, 1996.
Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares
algorithms. Journal of Machine Learning Research, 21(205):1â€“38, 2020.
Jean-Pierre Florens, Jan Johannes, and SÃ©bastien Van Bellegem. Identification and estimation
by penalization in nonparametric instrumental regression. Econometric Theory, 27(3):472â€“496,
2011.
Friedrich Gerard Friedlander. Introduction to the theory of distributions. Cambridge University
Press, 1998.
Amiremad Ghassami, Andrew Ying, Ilya Shpitser, and Eric Tchetgen Tchetgen. Minimax kernel
machine learning for a class of doubly robust functionals with application to proximal causal
inference. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings
of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of
Proceedings of Machine Learning Research, pages 7210â€“7239. PMLR, 28â€“30 Mar 2022. URL
https://proceedings.mlr.press/v151/ghassami22a.html.
Evarist GinÃ© and Richard Nickl. Mathematical foundations of infinite-dimensional statistical models.
Cambridge University Press, 2021.
A. Gretton. A simpler condition for consistency of a kernel independence test. arXiv preprint
arXiv:1501.06103, 2015.
A. Gretton and L. Gyorfi. Consistent nonparametric tests of independence. Journal of Machine
Learning Research, 11:1391â€“1423, 2010.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard SchÃ¶lkopf. Measuring statistical
dependence with hilbert-schmidt norms. In Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita,
editors, Algorithmic Learning Theory, pages 63â€“77, Berlin, Heidelberg, 2005. Springer Berlin
Heidelberg.
Arthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bernhard SchÃ¶lkopf, and Alex Smola. A
kernel statistical test of independence. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007.
29

Steffen GrÃ¼newÃ¤lder, Guy Lever, Luca Baldassarre, Sam Patterson, Arthur Gretton, and Massimi-
lano Pontil. Conditional mean embeddings as regressors. In Proceedings of the 29th International
Coference on Machine Learning, pages 1803â€”-1810. PMLR, 2012.
Peter Hall and Joel L. Horowitz. Nonparametric methods for inference in the presence of instrumental
variables. The Annals of Statistics, 33(6):2904â€“2929, 2005.
Thomas Hamm and Ingo Steinwart. Adaptive learning rates for support vector machines working
on data with low intrinsic dimension. The Annals of Statistics, 49(6):3153â€“3180, 2021.
Hanyuan Hang and Ingo Steinwart. Optimal learning with anisotropic gaussian svms. Applied and
Computational Harmonic Analysis, 55:337â€“367, 2021.
Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep iv: A flexible approach
for counterfactual prediction. In Doina Precup and Yee Whye Teh, editors, Proceedings of the
34th International Conference on Machine Learning, pages 1414â€“1423. PMLR, 2017.
M Hoffman and Oleg Lepski. Random rates in anisotropic regression. The Annals of Statistics, 30
(2):325â€“396, 2002.
Joel L. Horowitz. Applied nonparametric instrumental variables estimation. Econometrica, 79(2):
347â€“394, Mar 2011.
Tuomas HytÃ¶nen, Jan Van Neerven, Mark Veraar, and Lutz Weis. Analysis in Banach spaces,
volume 12. Springer, 2016.
IA Ibragimov and RZ Khasâ€™ minskii. Asymptotic bounds on the quality of the nonparametric
regression estimation in. Journal of Soviet Mathematics, 24(5):540â€“550, 1984.
Steven G. Johnson.
Saddle-point integration of câˆâ€œbumpâ€ functions.
arXiv preprint
arXiv:1508.04376, 2015.
Nathan Kallus, Xiaojie Mao, and Masatoshi Uehara. Causal inference under unmeasured con-
founding with negative controls: a minimax learning approach. arXiv preprint arXiv:2103.14029,
2021.
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur. Gaussian
processes and kernel methods: A review on connections and equivalences.
arXiv preprint
arXiv:1807.02582, 2018.
Sid Kankanala. Generalized bayes in conditional moment restriction models. arXiv preprint
arXiv:2510.01036, 2025.
Yitzhak Katznelson. An introduction to harmonic analysis. Cambridge University Press, 2004.
Fares El Khoury, Edouard Pauwels, Samuel Vaiter, and Michael Arbel. Learning theory for kernel
bilevel optimization. In Advances in Neural Information Processing Systems, volume 38. Curran
Associates, Inc., 2025.
Juno Kim, Dimitri Meunier, Arthur Gretton, Taiji Suzuki, and Zhu Li. Optimality and adaptivity
of deep neural features for instrumental variable regression. In International Conference on
Learning Representations, 2025.
30

Ilja Klebanov, Ingmar Schuster, and Timothy John Sullivan. A rigorous theory of conditional mean
embeddings. SIAM Journal on Mathematics of Data Science, 2(3):583â€“606, 2020.
Achim Klenke. Probability theory: a comprehensive course. Springer Science & Business Media,
2013.
Yurii Kolomoitsev and Sergey Tikhonov. Properties of moduli of smoothness in lp(Rd). Journal of
Approximation Theory, 257:105423, 2020. ISSN 0021-9045.
David Krieg. Tensor power sequences and the approximation of tensor product operators. Journal
of Complexity, 44:30â€“51, 2018.
J Paul Leigh and Michael Schembri. Instrumental variables technique: cigarette price provided
better estimate of effects of smoking on sf-12. Journal of Clinical Epidemiology, 57(3):284â€“293,
2004.
Christopher Leisner.
Nonlinear wavelet approximation in anisotropic besov spaces.
Indiana
University Mathematics Journal, 52(2):437â€“455, 2003. ISSN 00222518, 19435258. URL https:
//www.math.purdue.edu/~lucier/692/caarticle.pdf.
Yujia Li, Roman Pogodin, Dougal Sutherland, and Arthur Gretton. Self-supervised learning with
kernel dependence maximization. In Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann Dauphin,
Percy S. Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information
Processing Systems, volume 34. Curran Associates, Inc., 2021.
Zhu Li, Dimitri Meunier, Mattes Mollenhauer, and Arthur Gretton. Optimal rates for regularized
conditional mean embedding learning. In Sanmi Koyejo, Shakir Mohamed, Alekh Agarwal,
Danielle Belgrave, Kyunghyun Cho, and Alice Oh, editors, Advances in Neural Information
Processing Systems, volume 35, pages 4433â€“4445. Curran Associates, Inc., 2022.
Zhu Li, Dimitri Meunier, Mattes Mollenhauer, and Arthur Gretton. Towards optimal sobolev norm
rates for the vector-valued regularized least-squares algorithm. Journal of Machine Learning
Research, 25(181):1â€“51, 2024a.
Zihao Li, Hui Lan, Vasilis Syrgkanis, Mengdi Wang, and Masatoshi Uehara. Regularized deep IV
with model selection. arXiv preprint arXiv:2403.04236, 2024b.
Luofeng Liao, You-Lin Chen, Zhuoran Yang, Bo Dai, Mladen Kolar, and Zhaoran Wang. Provably
efficient neural estimation of structural equation models: An adversarial approach. In Hugo
Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 8947â€“8958. Curran
Associates, Inc., 2020.
Junhong Lin and Volkan Cevher. Optimal convergence for distributed learning with stochastic
gradient methods and spectral algorithms. Journal of Machine Learning Research, 21(147):1â€“63,
2020.
Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over Hilbert spaces. Applied and Computational Harmonic
Analysis, 48(3):868â€“890, 2020.
31

Weihao Lu, Yicheng Li, Qian Lin, et al. On the saturation effects of spectral algorithms in large
dimensions. In Amir Globerson, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet,
Jakub Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems,
volume 37, pages 7011â€“7059. Curran Associates, Inc., 2024.
Afsaneh Mastouri, Yuchen Zhu, Limor Gultchin, Anna Korba, Ricardo Silva, Matt Kusner, Arthur
Gretton, and Krikamol Muandet. Proximal causal learning with kernels: Two-stage estimation
and moment restriction. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th
International Conference on Machine Learning, pages 7512â€“7523. PMLR, 2021.
Dimitri Meunier, Zhu Li, Tim Christensen, and Arthur Gretton. Nonparametric instrumental
regression via kernel methods is minimax optimal. arXiv preprint arXiv:2411.19653, 2024a.
Dimitri Meunier, Zikai Shen, Mattes Mollenhauer, Arthur Gretton, and Zhu Li. Optimal rates for
vector-valued spectral regularization learning algorithms. In Amir Globerson, Lester Mackey,
Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub Tomczak, and Cheng Zhang, editors,
Advances in Neural Information Processing Systems. Curran Associates, Inc., 2024b. URL
https://arxiv.org/abs/2405.14778.
Dimitri Meunier, Antoine Moulin, Jakub Wornbard, Vladimir R Kostic, and Arthur Gretton.
Demystifying spectral feature learning for instrumental variable regression.
arXiv preprint
arXiv:2506.10899, 2025.
Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. Identifying causal effects with proxy variables
of an unmeasured confounder. Biometrika, 105(4):987â€“993, 2018.
Roger B Nelsen. An introduction to copulas. Springer, 2006.
Whitney K. Newey and James L. Powell. Instrumental variable estimation of nonparametric models.
Econometrica, 71(5):1565â€“1578, 2003.
Junhyung Park and Krikamol Muandet. A measure-theoretic approach to kernel conditional
mean embeddings. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,
Advances in Neural Information Processing Systems, volume 33, pages 21247â€“21259. Curran As-
sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf.
Ieva PetrulionytË™e, Julien Mairal, and Michael Arbel. Functional bilevel optimization for machine
learning. In Amir Globerson, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet,
Jakub Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems,
volume 37, pages 14016â€“14065. Curran Associates, Inc., 2024.
Michael Reed and Barry Simon. Methods of modern mathematical physics: Functional analysis,
volume 1. Gulf Professional Publishing, 1980.
Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco. Less is more: NystrÃ¶m computa-
tional regularization. In Corinna Cortes, Neil Lawrence, Daniel Lee, Masashi Sugiyama, and
Roman Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran
Associates, Inc., 2015.
Walter Rudin. Real and complex analysis. McGraw-Hill, Inc., 1987.
32

H J Schmeisser. An unconditional basis in periodic spaces with dominating mixed smoothness
properties. Analysis Mathematica, 13(2):153â€“168, 1987.
Hans-JÃ¼rgen Schmeisser. Recent developments in the theory of function spaces with dominating
mixed smoothness. Nonlinear Analysis, Function Spaces and Applications, pages 145â€“204, 2007.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. The Annals of Statistics, 48(4):1875â€“1897, 2020.
Dino Sejdinovic, Arthur Gretton, and Wicher Bergsma.
A kernel test for three-variable
interactions.
In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Wein-
berger, editors, Advances in Neural Information Processing Systems, volume 26. Curran As-
sociates, Inc., 2013. URL https://proceedings.neurips.cc/paper_files/paper/2013/file/
076a0c97d09cf1a0ec3e19c7f2529f2b-Paper.pdf.
H. Shen, S. Jegelka, and A. Gretton. Fast kernel-based independent component analysis. IEEE
Transactions on Signal Processing, 57:3498â€“3511, 2009.
Winfried Sickel and Tino Ullrich. Tensor products of sobolevâ€“besov spaces and applications to
approximation from the hyperbolic cross. Journal of Approximation Theory, 161(2):748â€“786,
2009. ISSN 0021-9045.
Rahul Singh. Kernel methods for unobserved confounding: negative controls, proxies, and instru-
ments. arXiv preprint arXiv:2012.10315, 2020.
Rahul Singh, Maneesh Sahani, and Arthur Gretton. Kernel instrumental variable regression. In
Hanna Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ© Buc, Emily Fox, and
Roman Garnett, editors, Advances in Neural Information Processing Systems, volume 32, 2019.
Rahul Singh, Liyuan Xu, and Arthur Gretton. Kernel methods for causal functions: dose, heteroge-
neous and incremental response curves. Biometrika, 111(2):497â€“516, 2024.
Steve Smale and Ding-Xuan Zhou. Shannon sampling ii: connections to learning theory. Applied
and Computational Harmonic Analysis, 19(3):285â€“302, 2005.
Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their
approximations. Constructive approximation, 26(2):153â€“172, 2007.
L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt. Feature selection via dependence
maximization. Journal of Machine Learning Research, 13:1393â€“1434, 2012.
Le Song, Jonathan Huang, Alex Smola, and Kenji Fukumizu.
Hilbert space embeddings of
conditional distributions with applications to dynamical systems. In Proceedings of the 26th
Annual International Conference on Machine Learning, pages 961â€“968. ACM, 2009.
Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business
Media, 2008.
Ingo Steinwart and Clint Scovel. Mercerâ€™s theorem on general domains: On the interaction between
measures, kernels, and RKHSs. Constructive Approximation, 35:363â€“417, 2012.
Ingo Steinwart, Don R Hush, and Clint Scovel. Optimal rates for regularized least squares regression.
In The 22nd Annual Conference on Learning Theory, pages 79â€“93, 2009.
33

Haotian Sun, Antoine Moulin, Tongzheng Ren, Arthur Gretton, and Bo Dai. Spectral representation
for causal estimation with hidden confounders. In Yingzhen Li, Stephan Mandt, Shipra Agrawal,
and Emtiyaz Khan, editors, Proceedings of the 28th International Conference on Artificial
Intelligence and Statistics, volume 258. PMLR, 2025.
Danica J Sutherland.
Fixing an error in caponnetto and de vito (2007).
arXiv preprint
arXiv:1702.02982, 2017.
Taiji Suzuki and Atsushi Nitanda. Deep learning is adaptive to intrinsic dimensionality of model
smoothness in anisotropic besov space. In Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann
Dauphin, Percy S. Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Informa-
tion Processing Systems, volume 34, pages 3609â€“3621, 2021.
Vasilis Syrgkanis, Victor Lei, Miruna Oprescu, Maggie Hei, Keith Battocchi, and Greg Lewis.
Machine learning estimation of heterogeneous treatment effects with instruments. In Hanna
Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dâ€™AlchÃ© Buc, Emily Fox, and Roman
Garnett, editors, Advances in Neural Information Processing Systems, volume 32, 2019.
ZoltÃ¡n SzabÃ³ and Bharath K Sriperumbudur. Characteristic and universal tensor product kernels.
Journal of Machine Learning Research, 18(233):1â€“29, 2018.
Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, and Wang Miao. An introduction to
proximal causal inference. Statistical Science, 39(3):375â€“390, 2024.
Hans Triebel. Entropy numbers in function spaces with mixed integrability. Revista matemÃ¡tica
complutense, 24:169â€“188, 2011.
Alexandre B. Tsybakov. Introduction to nonparametric estimation. Springer Publishing Company,
Incorporated, 1st edition, 2008. ISBN 0387790519.
Roman Vershynin. High-dimensional probability: an introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Ziyu Wang, Yucen Luo, Yueru Li, Jun Zhu, and Bernhard SchÃ¶lkopf. Spectral representation
learning for conditional moment models. arXiv preprint arXiv:2210.16525, 2022.
Holger Wendland. Scattered Data Approximation, volume 17. Cambridge university press, 2004.
Liyuan Xu and Arthur Gretton. Kernel single proxy control for deterministic confounding. In
Yingzhen Li, Stephan Mandt, Shipra Agrawal, and Emtiyaz Khan, editors, Proceedings of the
International Conference on Artificial Intelligence and Statistics, pages 3736â€“3744. PMLR, 2025.
Liyuan Xu, Yutian Chen, Siddarth Srinivasan, Nando de Freitas, Arnaud Doucet, and Arthur
Gretton. Learning deep features in instrumental variable regression. In International Conference
on Learning Representations, 2021a.
Liyuan Xu, Heishiro Kanagawa, and Arthur Gretton. Deep proxy causal learning and its application
to confounded bandit policy evaluation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,
and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
pages 26264â€“26275. Curran Associates, Inc., 2021b. URL https://proceedings.neurips.cc/
paper_files/paper/2021/file/dcf3219715a7c9cd9286f19db46f2384-Paper.pdf.
34

Haobo Zhang, Yicheng Li, Weihao Lu, and Qian Lin. On the optimality of misspecified kernel
ridge regression. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 41th International Conference
on Machine Learning, pages 41331â€“41353. PMLR, 2023a.
Haobo Zhang, Yicheng Li, and Qian Lin. On the optimality of misspecified spectral algorithms.
Journal of Machine Learning Research, 25(188):1â€“50, 2024.
Q. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. Large-scale kernel methods for independence
testing. Statistics and Computing, 28(1):113â€“130, 2018.
Rui Zhang, Masaaki Imaizumi, Bernhard SchÃ¶lkopf, and Krikamol Muandet. Instrumental variable
regression via kernel maximum moment loss. Journal of Causal Inference, 11(1):20220073, 2023b.
A
Examples for the partial smoothing effect of T
In this section, we extend Assumption 4.2 and Assumption 4.3 from the main textâ€”on the Fourier
measure of partial ill-posedness and on the Fourier measure of partial contractivity of the operator
T, respectivelyâ€”to the space of distributions. This allows us to apply these assumptions to periodic
functions and to construct an explicit example verifying Assumption 4.3. First, we prove the
following Lemma relating Assumption 4.2 and Assumption 4.3.
Lemma A.1. If Assumption 4.2 and 4.3 hold simultaneously, and PXO is absolutely continuous
with respect to the Lebesgue measure on X Ã— O, then Î·0 â‰¥Î·1.
Proof. Let Î³ âˆˆ(0, 1) be arbitrary. Let Ïµ > 0 be arbitrary. We define
fÎ³,Ïµ(x, o) := Fâˆ’1 
1[âˆ¥Ï‰âˆ¥2 âˆˆ[Î³âˆ’1, Î³âˆ’1 + Ïµ]]

(x).
We have fÎ³,Ïµ âˆˆLF
 (Î³âˆ’1 + Ïµ)âˆ’1
âˆ©HF(Î³) âˆ©Lâˆ(PXO). Since PXO admits a density function on
X Ã— O, and fÎ³,Ïµ only vanishes on sets of null Lebesgue measure, we have âˆ¥fÎ³,Ïµâˆ¥L2(PXO) Ì¸= 0. As
imposed by Assumption 4.3 and Assumption 4.2, we thus have
câˆ’1
0
 Î³âˆ’1 + Ïµ
âˆ’dxÎ·0 âˆ¥fÎ³,Ïµâˆ¥L2(PXO) â‰¤âˆ¥TfÎ³,Ïµâˆ¥L2(PZO) â‰¤c1Î³dxÎ·1âˆ¥fÎ³,Ïµâˆ¥L2(PXO).
Since âˆ¥fÎ³,Ïµâˆ¥L2(PXO) Ì¸= 0, we have (âˆ€Î³ âˆˆ(0, 1)) (âˆ€Ïµ > 0) câˆ’1
0
 Î³âˆ’1 + Ïµ
âˆ’dxÎ·0 â‰¤c1Î³dxÎ·1. For a fixed
Î³, taking the limit Ïµ â†’0, we have by continuity
(âˆ€Î³ âˆˆ(0, 1))
1
c0c1
â‰¤Î³dx(Î·1âˆ’Î·0).
(43)
Taking the limit Î³ â†’1 in Eq. (43), we find c0c1 â‰¥1. Then since Eq. (43) holds for all Î³ âˆˆ(0, 1),
we find that Î·1 â‰¤Î·0.
Definition 5 (Distribution and distribution of a function). Let â„¦âŠ†Rd be a domain. A distribution
u âˆˆDâ€²(â„¦) is a continuous linear functional on the space of test functions D(â„¦), where D(â„¦) is the
set Câˆ
c (â„¦) endowed with the canonical limit of FrÃ©chet topology. For a locally integrable function
f âˆˆL1
loc(Rd), for all Ï• âˆˆD(â„¦), we define the distribution Tf by
TfÏ• :=
Z
â„¦
f(x)Ï•(x) dx.
35

Definition 6 (Support of a distribution). A distribution u âˆˆDâ€²(â„¦) is supported in the closed set
K âŠ‚â„¦if u[Ï•] = 0 âˆ€Ï• âˆˆCâˆ
c (â„¦\ K). The support of u, supp u is the set
supp u = âˆ©{K : u is supported in K}.
Definition 7 (Tempered distribution). We define the Schwartz space S via
S =
(
Ï• âˆˆCâˆ(Rd) | âˆ€Î±, âˆ€N âˆˆN, sup
xâˆˆRd
(1 + |x|)NDÎ±Ï•(x)
 < âˆ
)
.
We say that a sequence {Ï•j}âˆ
j=1 âŠ‚S tends to zero iff
sup
xâˆˆRd |(1 + |x|)NDÎ±Ï•j(x)| â†’0
for all N âˆˆN and all multi-indices Î±. This endows S with a topology. We define the space of
tempered distributions to be the continuous dual space of S, denoted as Sâ€².
Definition 8 (Periodic distribution). We define the translation of a distribution u âˆˆDâ€²(Rd) via
Ï„zu[Ï•] = u[Ï„âˆ’zÏ•] for all Ï• âˆˆD(Rd), where we use Ï„zÏ•(x) := Ï•(x âˆ’z). We say that a distribution
u âˆˆDâ€²(Rd) is periodic if for each g âˆˆZd we have Ï„gu = u. Clearly, if f âˆˆL1
loc(Rd) is periodic,
then Tf is a periodic distribution.
Definition 9 (Fourier transform of L1 functions). For f âˆˆL1(Rd), we define the Fourier transform
F[f] = Ë†f : Rd â†’C by
F[f](Î¾) = Ë†f(Î¾) :=
Z
Rd f(x) exp(âˆ’iâŸ¨x, Î¾âŸ©) dx.
Definition 10 (Fourier transform of tempered distributions). For a distribution u âˆˆSâ€², we define
the Fourier transform of u, written Ë†u âˆˆSâ€², to be the distribution satisfying:
Ë†u[Ï•] = u[Ë†Ï•], âˆ€Ï• âˆˆS,
which is well defined since the Fourier transform maps S to S continuously.
We note that, for all Ï• âˆˆS, by the Fourier inversion theorem [Rudin, 1987, 9.11],
T1[Ë†Ï•] =
Z
Rd
Ë†Ï•(x) dx = (2Ï€)dÏ•(0) = (2Ï€)dÎ´0[Ï•].
Hence, âˆ€x âˆˆRd, we define eÎ¾(x) = exp(i2Ï€âŸ¨x, Î¾âŸ©). So âˆ€Ï• âˆˆS, we have
TeÎ¾[Ë†Ï•] =
Z
Rd exp(i2Ï€âŸ¨x, Î¾âŸ©)Ë†Ï•(x) dx = (2Ï€)dÏ•(2Ï€Î¾).
(44)
i.e. F[TeÎ¾] = (2Ï€)dÎ´2Ï€Î¾, the Dirac delta distribution at 2Ï€Î¾. We can now make sense of the Fourier
transform of a periodic function. The following Lemma is from Friedlander [1998, 8.5]:
Lemma A.2 (Periodic distributions are tempered). Let u âˆˆDâ€²(Rd) be a periodic distribution.
Then u is in fact a tempered distribution, i.e. u âˆˆSâ€².
36

Proposition A.1. Suppose u âˆˆDâ€²(Rd) is a periodic distribution. Then there exist constants cÎ¾ âˆˆC
such that u can be represented as a (generalized) Fourier series,
u =
X
Î¾âˆˆZd
cÎ¾ TeÎ¾,
with cÎ¾ satisfying the bound |cÎ¾| â‰¤K(1 + |Î¾|)N for some K > 0 and N âˆˆZ.
We can now make sense of the following definitions: for any scalar Î³ âˆˆ(0, 1), we define the
following two sets of functions which are generalization of the LF(Î³) and HF(Î³) defined in the main
text to distributions:
LF(Î³) := {f âˆˆL1
loc(Rdx+do) | âˆ€o âˆˆO, Tf(Â·,o) âˆˆSâ€²(Rdx),
supp
 F[f(Â·, o)]

âŠ†
n
Ï‰x âˆˆRdx : âˆ¥Ï‰xâˆ¥2 â‰¤Î³âˆ’1o
}.
HF(Î³) := {f âˆˆL1
loc(Rdx+do) | âˆ€o âˆˆO, Tf(Â·,o) âˆˆSâ€²(Rdx),
supp
 F[f(Â·, o)]

âŠ†
n
Ï‰x âˆˆRdx : âˆ¥Ï‰xâˆ¥2 â‰¥Î³âˆ’1o
}.
(45)
We now recall the statements of Assumption 4.2 and 4.3 from the main text with the above
generalization of LF(Î³) and HF(Î³).
Assumption A.1 (Fourier measure of partial ill-posedness of T). There exists a constant c0 > 0
and a parameter Î·0 âˆˆ[0, âˆ) only depending on T, such that for all Î³ âˆˆ(0, 1) and all functions
f âˆˆLF(Î³) âˆ©Lâˆ(PXO), the following inequality is satisfied:
âˆ¥fâˆ¥L2(PXO) â‰¤c0Î³âˆ’dxÎ·0âˆ¥Tfâˆ¥L2(PZO).
In particular, c0 does not depend on Î³.
Assumption A.2 (Fourier measure of partial contractivity of T). There exists a constant c1 > 0
and a parameter Î·1 âˆˆ[0, âˆ) only depending on T, such that for all Î³ âˆˆ(0, 1) and all functions
f âˆˆHF(Î³) âˆ©Lâˆ(PXO), the following inequality is satisfied:
âˆ¥Tfâˆ¥L2(PZO) â‰¤c1Î³dxÎ·1âˆ¥fâˆ¥L2(PXO).
In particular, c1 does not depend on Î³.
Let (S1, +) denote the unit circle (equipped with a group structure via addition), and let dx
denote the Haar measure on S1, which coincides with the pushforward of the Lebesgue measure
under the quotient map S1 âˆ¼= R
Z. We make use of the obvious identification between functions on
(S1)d and 1-periodic functions on Rd, for any d â‰¥1.
Definition 11 (Fourier series [Katznelson, 2004]). Let f âˆˆL1(S1). We define the nth Fourier coef-
ficient of f by F[f][n] =
R
S1 f(x)eâˆ’i2Ï€nx dx. The Fourier series of f âˆˆL1(S1) is the trigonometric
series f(x) = Pâˆ
n=âˆ’âˆF[f][n]ei2Ï€nx.
We make use of the group structure of S1 and the translation invariance of the measure dx on
S1 to define the convolution operation in L1(S1), following [Katznelson, 2004, Section 1.7].
37

Proposition A.2. Let f, g âˆˆL1(S1). For almost all t âˆˆS1, the function f(t âˆ’Ï„)g(Ï„) is L1(S1)-
integrable as a function of Ï„, and if we define the convolution
h(t) =
Z
S1 f(t âˆ’Ï„)g(Ï„) dÏ„
then h âˆˆL1(S1) with âˆ¥hâˆ¥L1(S1) â‰¤âˆ¥fâˆ¥L1(S1)âˆ¥gâˆ¥L1(S1). Moreover, (âˆ€n âˆˆZ), we have
F[h][n] = F[f][n]F[g][n].
For any Î³ > 0, we now exhibit a class of functions f âˆˆHF(Î³) and a distribution p(x, z, o)
satisfying the statement in Assumption A.2, with the help of Proposition A.2. For simplicity, we
assume that dx = dz = do = 1. Fix a scalar Î³ âˆˆ(0, 1). Let g âˆˆL1  S1
be a function whose Fourier
coefficients vanish on low frequencies, in the sense that
F[g][n] = 0
for all n âˆˆZ such that |n| â‰¤(2Ï€Î³)âˆ’1.
(46)
An example of such a function is g(x) = exp(i2Ï€mx), where m âˆˆZ and m > (2Ï€Î³)âˆ’1. Let
h âˆˆL1
loc(Rdo) be such that it does not vanish identically. We then define f : R2 â†’C:
f(x + t, o) := g(x)h(o)
for all x âˆˆ[0, 1), t âˆˆZ, o âˆˆR.
Then it follows that f âˆˆL1
loc(Rdx+do) since g âˆˆL1(S1) and h âˆˆL1
loc(R). It follows from Lemma A.2
that (âˆ€o âˆˆR), Tf(Â·,o) âˆˆSâ€²(R) for any o âˆˆR. Moreover, for every o âˆˆR, we observe that: i)
h(o) = 0, then supp(Fx[Tf(Â·,o)]) = âˆ…, ii) h(o) Ì¸= 0, then
supp(F[Tf(Â·,o)]) = supp(F[Tg])
= supp
 
F
"
âˆ
X
n=âˆ’âˆ
F[g][n] Tei2Ï€nÂ·
#!
(a)
= supp
 
âˆ
X
n=âˆ’âˆ
F[g][n](2Ï€)Î´2Ï€n
!
(b)
= supp
ï£«
ï£­
X
|n|>(2Ï€Î³)âˆ’1
F[g][n](2Ï€)Î´2Ï€n
ï£¶
ï£¸
âŠ†

Ï‰x âˆˆR : |Ï‰x| > 1
Î³

.
Here, step (a) follows from (44) and the use of distributional support as defined in Definition 6,
and step (b) follows from Eq. (46). Thus, from both cases, we conclude that f âˆˆHF(Î³) defined in
Eq. (45).
Fix an integer k â‰¥1. We consider a probability space â„¦= S1 Ã— S1 Ã— S1 Ã— Â· Â· Â· Ã— S1
|
{z
}
k
, where
each copy of S1 is equipped with its Borel Ïƒ-algebra and the normalized Haar measure, and â„¦
is equipped with the product Borel Ïƒ-algebra and the product measure. We define the following
mappings, where 1 â‰¤i â‰¤k:
Ï€Z : â„¦â†’S1,
Ï€Z(z, o, u1, . . . , uk) = z
Ï€O : â„¦â†’S1,
Ï€O(z, o, u1, . . . , uk) = o
38

Ï€Ui : â„¦â†’S1,
Ï€Ui(z, o, u1, . . . , uk) = ui,
Since Ï€Z, Ï€O, Ï€Uiâ€™s are measurable, they are valid random variables, and we also denote them
by Z, O, U1, . . . , Uk. We then write W = 0.1(U1 + Â· Â· Â· + Uk) and X = Z + W, where addition is
understood as a group operation on S1. By construction, X is independent of O. The random
tuple (X, Z, O) is as considered in the NPIV-O set-up in the main text.
We are now going to show that there exists some constant c1 > 0 (which does not depend on
Î³), such that
âˆ¥Tfâˆ¥L2(PZO) â‰¤c1Î³kâˆ¥fâˆ¥L2(PXO).
We observe that
âˆ¥fâˆ¥2
L2(PXO) =
Z
R2 |f(x, o)|2p(x, o) dx do
=
Z
S1Ã—S1 |f(x, o)|2p(x, o) dx do = âˆ¥gâˆ¥2
L2(PX)âˆ¥hâˆ¥2
L2(PO).
(47)
We also observe that
âˆ¥Tfâˆ¥2
L2(PZO) = âŸ¨f, T âˆ—TfâŸ©L2(PXO)
=
Z
S1Ã—S1 f(x, o)(T âˆ—Tf)(x, o)p(x, o) dx do.
(48)
We also observe that for all x âˆˆS1 and o âˆˆS1,
(T âˆ—Tf)(x, o) =
Z
S1
Z
S1 f(xâ€², o)p(xâ€² | z, o) dxâ€² p(z | x, o) dz
(âˆ—)
=
Z
S1 f(xâ€², o)
Z
S1 p(xâ€² | z, o)p(z | x, o) dz

dxâ€²
=
Z
S1 f(xâ€², o)L(x, xâ€², o) dxâ€²,
where (âˆ—) follows by Fubiniâ€™s theorem. L is defined as, for all x, xâ€² âˆˆS1 and o âˆˆS1,
L(x, xâ€², o) :=
Z
S1 p(xâ€² | z, o)p(z | x, o) dz =
Z
S1 p(xâ€² | z)p(z | x) dz.
The last step holds because X is independent of O.
As L is not dependent on o, we write
L(x, xâ€², o) = L(x, xâ€²). Hence (âˆ€x âˆˆS1, o âˆˆS1)
(T âˆ—Tf)(x, o) =
Z
S1 g(xâ€²)L(x, xâ€²) dxâ€²

h(o).
Hence continuing from Eq. (48), we find
âˆ¥Tfâˆ¥2
L2(PZO) =
Z
S1Ã—S1 g(x)h(o)(T âˆ—Tf)(x, o)p(x, o) dx do
=
Z
S1 g(x)
Z
S1 g(xâ€²)L(x, xâ€²)p(x) dxâ€²

dx

âˆ¥hâˆ¥2
L2(PO).
(49)
39

We also notice that (âˆ€x âˆˆS1, xâ€² âˆˆS1), the following hold via Bayesâ€™ rule and the fact that p(z) is
the Haar measure on S1:
L(x, xâ€²)p(x) =
Z
zâˆˆS1 p(xâ€² | z)p(z | x)p(x) dz
=
Z
zâˆˆS1 p(xâ€² | z)p(x | z)p(z) dz
=
Z
zâˆˆS1 p(xâ€² | z)p(x | z) dz
=
Z
zâˆˆS1 pW (xâ€² âˆ’z)pW (x âˆ’z) dz
=
Z
zâˆˆS1 pW (z)pW (z + (x âˆ’xâ€²)) dz
=: L(x âˆ’xâ€²).
where the second last step follows from the change of variable z â†xâ€² âˆ’z, and in the last step we
use the fact that L(x, xâ€²)p(x) is translation-invariant. As throughout the calculations, the difference
x âˆ’xâ€² denotes the group operation on S1 rather than the usual difference on R. We calculate the
Fourier coefficients of L as follows:
F[L][n] =
Z
wâˆˆS1 L(w)eâˆ’i2Ï€nw dw =
Z
wâˆˆS1
Z
zâˆˆS1 pW (z)pW (z + w) dz eâˆ’i2Ï€nw dw
=

Z
zâˆˆS1 pW (z)e2Ï€inz dz

2
= |F[pW ][n]|2.
Since W = 0.1(U1 + Â· Â· Â· + Uk), we have pW is the k-times convolution of the probability density
function of 0.1U1. By the convolution Theorem, we find
F[L][n] =
10
Z 0.1
0
eâˆ’2Ï€inz dz

2k
=

10
2Ï€in
 1 âˆ’eâˆ’0.2Ï€in
2k
=
 10
Ï€n
2k
sin2k(0.1Ï€n).
(50)
Hence
F[L][n] â‰¤
 10
Ï€n
2k
.
(51)
We have
âˆ¥Tfâˆ¥2
L2(PZO)
âˆ¥hâˆ¥2
L2(PO)
=
Z
S1 g(x)
Z
S1 g(xâ€²)L(x âˆ’xâ€²) dxâ€²

dx
(a)
=
âˆ
X
n=âˆ’âˆ
F[g][n]F
Z
S1 g(xâ€²)L(Â· âˆ’xâ€²) dxâ€²

[n]
(b)
=
âˆ
X
n=âˆ’âˆ
F[g][n]F[g][n]F[L][n] =
âˆ
X
n=âˆ’âˆ
|F[g][n]|2 Â· F[L][n]
(52)
(c)
=
X
nâˆˆZ,|n|>(2Ï€Î³)âˆ’1
|F[g][n]|2 Â· F[L][n]
(d)
â‰¤(20Î³)2k
X
nâˆˆZ,|n|>(2Ï€Î³)âˆ’1
|F[g][n]|2
= (20Î³)2k
âˆ
X
n=âˆ’âˆ
|F[g][n]|2 (e)
= (20Î³)2kâˆ¥gâˆ¥2
L2(S1)
40

(f)
= (20Î³)2kâˆ¥gâˆ¥2
L2(PX).
In the above derivations, step (a) follows from Parsevalâ€™s theorem [Katznelson, 2004, 5.4] and the
fact that {ei2Ï€nx}nâˆˆZ forms an orthonormal system in L2(S1), step (b) follows from Proposition A.2,
step (c) follows Eq. (46), step (d) follows from Eq. (51), step (e) follows again from Parsevalâ€™s
Theorem, and finally step (f) follows from the fact that PX is the Haar measure on S1. Continuing
from Eq. (49), we thus have
âˆ¥Tfâˆ¥2
L2(PZO) â‰¤(20Î³)2kâˆ¥gâˆ¥2
L2(PX)âˆ¥hâˆ¥2
L2(PO) = (20Î³)2kâˆ¥fâˆ¥2
L2(PXO),
where the last equality follows from Eq. (47). Therefore, we have proved that Assumption A.2 is
satisfied with Î·1 = k and c1 = 20k. For any Î³ > 0, we now exhibit a class of functions fâ€² âˆˆLF(Î³)
and a distribution p(x, z, o) satisfying the statement in Assumption A.1. We let p(x, z, o) be the
probability distribution constructed above. Let gâ€² âˆˆL1(S1) be a function whose Fourier coefficients
vanish on high frequencies, in the sense that
F[gâ€²][n] = 0
for all n âˆˆZ such that |n| â‰¥(2Ï€Î³)âˆ’1.
We further assume that F[gâ€²][n] Ì¸= 0 only if sin2k(0.1Ï€n) â‰¥c for a fixed positive constant c > 0.
Let h âˆˆL1
loc(Rdo) be such that it does not vanish identically. We then define f â€² : R2 â†’C:
fâ€²(x + t, o) := gâ€²(x)h(o)
for all x âˆˆ[0, 1), t âˆˆZ, o âˆˆR.
We can show that fâ€² âˆˆLF(Î³) defined in Eq. (45) via a similar argument as before. By Eq. (52), we
have
âˆ¥Tfâ€²âˆ¥2
L2(PZO)
âˆ¥hâˆ¥2
L2(PO)
=
âˆ
X
n=âˆ’âˆ
|F[gâ€²][n]|2 Â· F[L][n]
=
X
nâˆˆZ,|n|<(2Ï€Î³)âˆ’1
|F[gâ€²][n]|2 Â· F[L][n]
(a)
=
X
nâˆˆZ,|n|<(2Ï€Î³)âˆ’1
|F[gâ€²][n]|2
 10
Ï€n
2k
sin2k(0.1Ï€n)
(b)
â‰¥
X
nâˆˆZ,|n|<(2Ï€Î³)âˆ’1
|F[gâ€²][n]|2
 10
Ï€n
2k
c
â‰¥c(20Î³)2k
X
nâˆˆZ,|n|<(2Ï€Î³)âˆ’1
|F[gâ€²][n]|2 = c(20Î³)2kâˆ¥gâ€²âˆ¥2
L2(S1),
where step (a) follows from Eq. (50), and step (b) follows from the assumption on the Fourier
coefficient of g. We thus have
âˆ¥Tfâ€²âˆ¥2
L2(PZO) â‰¥c(20Î³)2kâˆ¥fâ€²âˆ¥2
L2(PXO).
B
Explicit Solutions of KIV-O
The following derivation is adapted from Meunier et al. [2024a, Section D], which only covers the
case of no observed covariates. We refer the reader to Singh et al. [2019, Section A.5.1] for the
original derivation of closed-form solution for KIV with no observed covariates, and to Mastouri
41

et al. [2021], Singh [2020], Xu and Gretton [2025] for derivation of closed-form solutions for the
RKHS two-stage proximal causal learning framework, which is mathematically equivalent to KIV
with observed covariates, as discussed in Section 3.1. Whenever an operator or Gram matrix require
distinguishing between Stage I or Stage II kernel on O, we denote this via ; 1 or ; 2 in the subscript.
âŠ™denotes Hadamard product. For a matrix J âˆˆRmÃ—n, JÂ·,j denotes its jth column.
Stage 1
We follow the closed-form solution given in Li et al. [2022]. We define
Î¦ ËœZ ËœO;1 : HZO â†’RËœn,
Î¦ ËœZ ËœO;1 = [Ï•Z (Ëœz1) âŠ—Ï•O,1 (Ëœo1) , . . . , Ï•Z (ËœzËœn) âŠ—Ï•O,1(ËœoËœn)]âˆ—,
Î¦ Ëœ
X : HX â†’RËœn,
Î¦ Ëœ
X = [Ï•X (Ëœx1) , . . . , Ï•X (ËœxËœn)]âˆ—
We obtain the following estimator
Ë†FÎ¾(Â·, Â·) = Ë†CX|Z,O;Î¾Ï•Z(Â·) âŠ—Ï•O,1(Â·),
Ë†CX|Z,O;Î¾ = Î¦âˆ—
Ëœ
X

K ËœZ ËœO;1 + ËœnÎ¾Id
âˆ’1
Î¦ ËœZ ËœO;1,
(53)
where we introduce the Gram matrix
K ËœZ ËœO;1 = Î¦ ËœZ ËœO;1Î¦âˆ—
ËœZ ËœO;1,
[K ËœZ ËœO;1]ij = kZ(Ëœzi, Ëœzj)kO,1(Ëœoi, Ëœoj).
Stage 2
The Stage 2 solution can be written as
Ë†fÎ» =
 1
nÎ¦âˆ—
Ë†FO;2Î¦ Ë†FO;2 + Î»Id
âˆ’1 1
nÎ¦âˆ—
Ë†FO;2Y
where we define
Î¦ Ë†FO;2 : HXO â†’Rn
Î¦ Ë†FO;2 =
h
Ë†FÎ¾(z1, o1) âŠ—Ï•O,2(o1), . . . , Ë†FÎ¾(zn, on) âŠ—Ï•O,2(on)
iâˆ—
.
We then write this in a dual form
Ë†fÎ» = Î¦âˆ—
Ë†FO;2

K Ë†FO;2 + nÎ»Id
âˆ’1
Y.
where we introduce the Gram matrix
K Ë†FO;2 = Î¦ Ë†FO;2Î¦âˆ—
Ë†FO;2,
[K Ë†FO;2]ij = âŸ¨Ë†FÎ¾(zi, oi), Ë†FÎ¾(zj, oj)âŸ©HXkO,2(oi, oj).
By Eq. (53), we obtain, for 1 â‰¤j â‰¤n,
Ë†FÎ¾(zj, oj) = Î¦âˆ—
Ëœ
X

K ËœZ ËœO,1 + ËœnÎ¾Id
âˆ’1 
K ËœZZ âŠ™K ËœOO;1

:j
|
{z
}
=:J:j
=
Ëœn
X
i=1
JijÏ•X(Ëœxi),
(54)
where J as defined column-wise is a Ëœn Ã— n matrix, and we define the (cross) Gram matrices
[K ËœZZ]ij = kZ(Ëœzi, zj),
[K ËœOO;1]ij = kO,1(Ëœoi, oj),
for 1 â‰¤i â‰¤Ëœn, 1 â‰¤j â‰¤n. Consequently, for 1 â‰¤i, j â‰¤n, we have
K Ë†FO;2 =
 JT K Ëœ
X Ëœ
XJ

âŠ™KOO;2,
where we define the Gram matrices
[K Ëœ
X Ëœ
X]ij = kX(Ëœxi, Ëœxj),
[KOO;2]lm = kO,2(ol, om),
42

for 1 â‰¤i, j â‰¤Ëœn, 1 â‰¤l, m â‰¤n. For a new test point (x, o) âˆˆX Ã— O, we have, for 1 â‰¤j â‰¤n,
D
Ï•X(x) âŠ—Ï•O,2(o), Ë†FÎ¾(zj, oj) âŠ—Ï•O,2(oj)
E
HXO
= kO,2(o, oj)
 Ëœn
X
i=1
JijkX(Ëœxi, x)
!
=
 KOo,2 âŠ™(JT K Ëœ
Xx)

j ,
where we define K Ëœ
Xx âˆˆRËœnÃ—1 and KOo,2 âˆˆRnÃ—1 respectively as follows:
[K Ëœ
Xx]i = kX(Ëœxi, x),
[KOo,2]i = kO,2(oi, o).
Thus we have
Ë†fÎ»(x, o) =

Ï•X(x) âŠ—Ï•O,2(o), Î¦âˆ—
Ë†FO;2

K Ë†FO;2 + nÎ»Id
âˆ’1
Y

HXO
=

KT
Oo,2 âŠ™(KT
Ëœ
XxJ)
   JT K Ëœ
X Ëœ
XJ

âŠ™KOO;2 + nÎ»Id
âˆ’1 Y,
where the last line follows by Eq. (54), Thus the derivation is concluded.
C
RKHS HFO
We recall that the NPIV-O problem can be written as
Y = (Tfâˆ—)(Z, O) + Ï…,
E[Ï… | Z, O] = 0,
(55)
where Ï… := fâˆ—(X, O) âˆ’(Tfâˆ—)(Z, O) + Ïµ. As introduced in Meunier et al. [2024a, Appendix E.1.2],
we define an RKHS HFO induced by the statistical inverse problem Eq. (55), following Steinwart
and Christmann [2008, Theorem 4.21]. Our construction can be obtained from that of Meunier
et al. [2024a] with an appropriate feature map construction, as follows.
Recall the definition of
Fâˆ—: Z Ã— O â†’HX,Î³x that for any z âˆˆZ, o âˆˆO, Fâˆ—(z, o) = E[Ï•X,Î³x(X) | Z = z, O = o].
Definition 12. We define a reproducing kernel Hilbert space HFO as
HFO =

f : Z Ã— O â†’R | âˆƒw âˆˆHÎ³x,Î³o, f(z, o) â‰¡âŸ¨w, Fâˆ—(z, o) âŠ—Ï•Î³o(o)âŸ©HÎ³x,Î³o
	
,
equipped with the norm
âˆ¥fâˆ¥HF O := inf

âˆ¥wâˆ¥HÎ³x,Î³o | f(z, o) â‰¡âŸ¨w, Fâˆ—(z, o) âŠ—Ï•Î³o(o)âŸ©HÎ³x,Î³o
	
.
We observe that (z, o) 7â†’Fâˆ—(z, o) âŠ—Ï•Î³o(o) is not the canonical feature map of HFO. To
construct its canonical feature map, we define V : HÎ³x,Î³o â†’HFO such that
(V w)(z, o) â‰¡âŸ¨w, Fâˆ—(z, o) âŠ—Ï•Î³o(o)âŸ©HÎ³x,Î³o,
w âˆˆHÎ³x,Î³o.
From Theorem 4.21 of Steinwart and Christmann [2008], V is a metric surjection. By definition,
for any f âˆˆHÎ³x,Î³o, we have
(V f)(z, o) = âŸ¨f, Fâˆ—(z, o) âŠ—Ï•Î³o(o)âŸ©HÎ³x,Î³o = E[f(X, O) | Z = z, O = o] = (T[f])(z, o).
Furthermore, we know that V is also a surjective partial isometry, i.e. V is surjective and satisfies
(âˆ€f âˆˆker(V )âŠ¥),
âˆ¥fâˆ¥HÎ³x,Î³o = âˆ¥V fâˆ¥HF O.
43

Equivalently, (âˆ€r âˆˆHFO),
âˆ¥râˆ¥HF O = inf{âˆ¥hâˆ¥HÎ³x,Î³o : h âˆˆHÎ³x,Î³o, r = V h},
(56)
Thus, the canonical feature map of HFO is (z, o) 7â†’V (Fâˆ—(z, o) âŠ—Ï•Î³o(o)), a fact which was also
observed on Meunier et al. [2024a, Page 28].
Recall the definition of fÎ» in Eq. (25),
fÎ» := arg min
fâˆˆHÎ³x,Î³o
Î»âˆ¥fâˆ¥2
HÎ³x,Î³o + âˆ¥T([f] âˆ’fâˆ—)âˆ¥2
L2(PZO).
(57)
and Â¯fÎ» in Eq. (23),
Â¯fÎ» := arg min
fâˆˆHÎ³x,Î³o
Î»âˆ¥fâˆ¥2
HÎ³x,Î³o + 1
n
n
X
i=1
(yi âˆ’âŸ¨f, Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi)âŸ©HÎ³x,Î³o)2.
(58)
We define the images of fÎ» and Â¯fÎ» respectively under the metric surjection V as follows:
hÎ» := V fÎ»,
Â¯hÎ» := V Â¯fÎ» âˆˆHFO,
hâˆ—:= Tfâˆ—.
We observe by combining Eq. (56) and Eq. (58) that hÎ» is the solution to a standard kernel ridge
regression problem with the RKHS HFO:
Â¯hÎ» = arg min
hâˆˆHF O
Î»âˆ¥hâˆ¥2
HF O + 1
n
n
X
i=1
(h(zi, oi) âˆ’yi)2.
(59)
Similarly, for hÎ» we have
hÎ» = arg min
hâˆˆHF O
Î»âˆ¥hâˆ¥2
HF O + âˆ¥hâˆ—âˆ’hâˆ¥2
L2(PZO).
(60)
Finally, we have
T
  Â¯fÎ»

âˆ’[fÎ»]

L2(PZO) =

V Â¯fÎ»

âˆ’[V fÎ»]

L2(PZO) =
Â¯hÎ»

âˆ’[hÎ»]

L2(PZO) ,
(61)
which means that the projected error âˆ¥T([ Â¯fÎ»] âˆ’[fÎ»]])âˆ¥L2(PZO) has been translated to the gener-
alization error
Â¯hÎ»

âˆ’[hÎ»]

L2(PZO) of a standard kernel ridge regression (KRR) with this new
hypothesis space HFO, which allows us to apply techniques from the analysis of kernel ridge regres-
sion. To this end, we will analyse the capacity (Section C.1) along with the embedding property
(Section C.2) of HFO, both of which are crucial properties for characterizing the generalization
error of KRR [Fischer and Steinwart, 2020].
C.1
Capacity of HFO
We have
sup
o,oâ€² sup
z,zâ€²

V (Fâˆ—(z, o) âŠ—Ï•Î³o(o)), V (Fâˆ—(zâ€², oâ€²) âŠ—Ï•Î³o(oâ€²))

HF O
= sup
o,oâ€² sup
z,zâ€²

Fâˆ—(z, o) âŠ—Ï•Î³o(o), Fâˆ—(zâ€², oâ€²) âŠ—Ï•Î³o(oâ€²)

HÎ³x,Î³o
= sup
o,oâ€² kO,Î³o(o, oâ€²) sup
z,zâ€²
ZZ
XÃ—X
kX,Î³x(x, xâ€²)p(x | z, o)p(xâ€² | zâ€², oâ€²) dx dxâ€²
44

â‰¤sup
o,oâ€² kO,Î³o(o, oâ€²) sup
x,xâ€² kX,Î³x(x, xâ€²) sup
z,zâ€²
ZZ
XÃ—X
p(x | z, o)p(xâ€² | zâ€², oâ€²) dx dxâ€²
= sup
o,oâ€² kO,Î³o(o, oâ€²) sup
x,xâ€² kX,Î³x(x, xâ€²).
Hence the RKHS HFO has a bounded kernel, therefore by Steinwart and Scovel [2012, Lemma 2.3],
HFO is compactly embedded into L2(PZO). We define the covariance operator CFO : HFO â†’HFO
CFO := V EZO [(Fâˆ—(Z, O) âŠ—Ï•Î³o(O)) âŠ—(Fâˆ—(Z, O) âŠ—Ï•Î³o(O))] V âˆ—
It is a self-adjoint compact operator by Steinwart and Scovel [2012, Lemma 2.2]. The spectral
theorem for self-adjoint compact operators [Reed and Simon, 1980, Theorems VI.16, VI.17] yields,
there exists countable ÂµFO,1 â‰¥ÂµFO,2 â‰¥Â· Â· Â· â‰¥0, ([eFO,i])iâ‰¥1 an orthonormal system of L2(PZO)
and
 âˆšÂµFO,ieFO,i

âŠ†HFO an orthonormal system in HFO, such that
CFO =
X
iâ‰¥1
ÂµFO,iâŸ¨Â·, âˆšÂµFO,ieFO,iâŸ©HF O
âˆšÂµFO,ieFO,i.
(62)
Definition 13 (Effective dimension). The effective dimension of HFO, denoted as NFO : [0, âˆ) â†’
[0, âˆ) is defined as
NFO(Î») = tr
 (CFO + Î»)âˆ’1CFO

=
X
iâ‰¥1
ÂµFO,i
ÂµFO,i + Î».
Proposition C.1. Let n â‰¥10 and Î» = nâˆ’1. Let Î³x, Î³o âˆˆ(0, 1] be the lengthscales for the RKHS
HÎ³x,Î³o. Suppose that the distribution PXO satisfies Assumption 2.2. Then we have,
NFO(Î») â‰¤Câ€²(log n)dx+do+1 
Î³dx
x Î³do
o
âˆ’1
for some constant Câ€² independent of n, Î³x, Î³o.
Proof. From Corollary C.1 and Lemma C.1 and setting p =
1
log n, (âˆ€i â‰¥1), we have
ÂµFO,i â‰¤(C log n)2(dx+do+1) log n(Î³dx
x Î³do
o )âˆ’2 log niâˆ’2 log n.
Next, we use [Caponnetto and De Vito, 2007, Proposition 3] (with error corrected in Sutherland
[2017]) to obtain
NFO(Î») â‰¤
Ï€/(2 log n)
sin(Ï€/(2 log n))

(C log n)2(dx+do+1) log n(Î³dx
x Î³do
o )âˆ’2 log n
1
2 log n Î»âˆ’(2 log n)âˆ’1
=
Ï€/(2 log n)
sin(Ï€/(2 log n))(C log n)(dx+do+1) 
Î³dx
x Î³do
o
âˆ’1
Î»âˆ’(2 log n)âˆ’1
(i)
â‰¤3C(dx+do+1)(log n)(dx+do+1) 
Î³dx
x Î³do
o
âˆ’1
n(2 log n)âˆ’1
(ii)
= 3âˆšeC(dx+do+1)(log n)(dx+do+1) 
Î³dx
x Î³do
o
âˆ’1
= Câ€²(log n)dx+do+1 
Î³dx
x Î³do
o
âˆ’1
,
where Câ€² = âˆše3C(dx+do+1). In the above chain of derivations, (i) holds because
t
sin t â‰¤3 for t â‰¤Ï€/2
and Ï€/(2 log n) â‰¤Ï€/2 and (ii) holds because n
1
2 log n = âˆše for n > 1.
45

In the proof of Proposition C.1, to bound the effective dimension, we need to bound the
eigendecay of the compact self-adjoint operator CFO. We first control the decay of the entropy
numbers of the RKHS HFO, which translates into a bound on the eigendecay of CFO as shown in
Corollary C.1. In Lemma C.2, we show that the ith entropy number of HFO is bounded above
by the ith entropy number of HÎ³x,Î³o. The entropy numbers of HÎ³x,Î³o are well-understood by the
results of Hang and Steinwart [2021] (restated in Lemma C.1), which completes the derivation.
In this section, for real-valued Hilbert spaces E, F and a bounded, linear, compact operator
S : E â†’F, si(S) denotes the ith singular value of S, as defined in Steinwart and Christmann
[2008, Eq. (A.25) Page 505]; ei(S) denotes the ith entropy number of S, as defined in Steinwart
and Christmann [2008, Definition A.5.26 Page 516]; ai(S) denotes the ith approximation number
of S, as defined in Steinwart and Christmann [2008, Eq. (A.29) Page 506].
Lemma C.1. Suppose that PXO satisfies Assumption 2.2 in the main text. Then, (âˆ€i â‰¥1),
(Î³x, Î³o âˆˆ(0, 1]), there exists a constant C > 0 such that for any p > 0,
ei(id : HÎ³x,Î³o ,â†’L2(PXO)) â‰¤(3C)
1
p
dx + do + 1
ep
 dx+do+1
p

Î³dx
x Î³do
o
âˆ’1
p iâˆ’1
p .
Proof. The corollary follows immediately from Hang and Steinwart [2021, Proposition 1] by setting
Î³ = [Î³x, . . . , Î³x
|
{z
}
dx
, Î³o, . . . , Î³o
|
{z
}
do
]âŠ¤âˆˆ(0, 1]dx+do, and noting that Lâˆ(X Ã— O) continuously embeds
into L2(PXO) with âˆ¥Lâˆ(X Ã— O) ,â†’L2(PXO)âˆ¥â‰¤1 and Steinwart and Christmann [2008, Eq.
(A.38)].
Lemma C.2. We have, (âˆ€i â‰¥1),
ei(id : HFO ,â†’L2(PZO)) â‰¤ei(id : HÎ³x,Î³o ,â†’L2(PXO)).
Proof. Fix i â‰¥1. For a Hilbert space H, BH denotes the unit ball in H, and B(x, r, âˆ¥Â· âˆ¥H) denotes
the ball in H centred at x âˆˆH with radius r. Fix Ïµ > 0 such that âˆƒg1, . . . , g2iâˆ’1 âˆˆBHÎ³x,Î³o such
that
BHÎ³x,Î³o âŠ†
2iâˆ’1
[
i=1
B
 gi, Ïµ, âˆ¥Â· âˆ¥L2(PXO)

.
Fix f âˆˆBHF O and an arbitrary ËœÏµ > 0. By Steinwart and Christmann [2008, Eq. (4.11)], there
exists g âˆˆHÎ³x,Î³o such that
f = âŸ¨g, Fâˆ—(Â·, Â·) âŠ—Ï•Î³o(Â·)âŸ©HÎ³x,Î³o = V g,
and âˆ¥gâˆ¥HÎ³x,Î³o â‰¤1 + ËœÏµ. By the preceding statement, (âˆƒi âˆˆ{1, . . . , 2iâˆ’1}) such that
âˆ¥g âˆ’(1 + ËœÏµ)giâˆ¥L2(PXO) â‰¤Ïµ(1 + ËœÏµ).
For i âˆˆ{1, . . . , 2iâˆ’1}, define fi = (1 + ËœÏµ)V gi = (1 + ËœÏµ)âŸ¨gi, Fâˆ—(Â·, Â·) âŠ—Ï•Î³o(Â·)âŸ©HÎ³x,Î³o. Then we have
âˆ¥f âˆ’fiâˆ¥2
L2(PZO) =
Z
ZÃ—O
âŸ¨g âˆ’(1 + ËœÏµ)gi, Fâˆ—(z, o) âŠ—Ï•Î³o(o)âŸ©2
HÎ³x,Î³op(z, o) dz do
=
Z
ZÃ—O
E[(g âˆ’(1 + ËœÏµ)gi)(X, O) | Z = z, O = o]2p(z, o) dz do
46

â‰¤âˆ¥g âˆ’(1 + ËœÏµ)giâˆ¥2
L2(PXO),
where the inequality is deduced by Jensenâ€™s inequality. We find thus that
HBF O âŠ†
2iâˆ’1
[
i=1
B(fi, Ïµ(1 + ËœÏµ), âˆ¥Â· âˆ¥L2(PZO)).
Hence ei(id : HFO ,â†’L2(PZO)) â‰¤Ïµ(1 + ËœÏµ). Since Ïµ > ei(id : HÎ³x,Î³o ,â†’L2(PXO)) and ËœÏµ > 0 are
arbitrary, it follows that ei(id : HFO ,â†’L2(PZO)) â‰¤ei(id : HÎ³x,Î³o ,â†’L2(PXO)).
Corollary C.1. Suppose that PXO satisfies Assumption 2.2 in the main text. We have that
ÂµFO,i â‰¤4ei(id : HÎ³x,Î³o ,â†’L2(PXO))2
Proof. Let Î¹FO : HFO â†’L2(PZO) denote the embedding HFO ,â†’L2(PZO). We have, (âˆ€i â‰¥1),
the following chain of derivations
ÂµFO,i = Î»i(Î¹âˆ—
FOÎ¹FO)
(a)
= si(Î¹FO)2 (b)
= ai(Î¹FO)2
(c)
â‰¤4ei(Î¹FO)2 (d)
â‰¤4ei(id : HÎ³x,Î³o ,â†’L2(PXO))2.
In the above derivations, (a) follows from Steinwart and Christmann [2008, Eq. (A.25)], (b) follows
from the paragraph after Steinwart and Christmann [2008, Eq. (A.29)], (c) follows from Steinwart
and Christmann [2008, Eq. (A.44)], and (d) follows from Lemma C.2.
C.2
Embedding property of HFO
Definition 14 (Continuous embedding). A Hilbert space (X, âˆ¥Â· âˆ¥) is said to continuously embed
into Hilbert space (Y, âˆ¥Â· âˆ¥) if X âŠ‚Y and there exists a constant C such that âˆ¥xâˆ¥Y â‰¤Câˆ¥xâˆ¥X for
all x âˆˆX. We denote this as X ,â†’Y . The embedding norm âˆ¥X ,â†’Y âˆ¥is defined as the smallest
constant C for which the above inequality holds.
Definition 15 (Interpolation space). Assume that X0 and X1 are Hilbert spaces and X1 âŠ†X0.
For Î¸ âˆˆ(0, 1) and x âˆˆX0, we define the K-functional K(t, x; X0, X1) as follows
K(t, x; X0, X1) := inf
yâˆˆX1 {âˆ¥x âˆ’yâˆ¥X0 + tâˆ¥yâˆ¥X1} .
(63)
For Î¸ âˆˆ(0, 1), we define interpolation space HytÃ¶nen et al. [2016][Definition C.3.1]
(X0, X1)Î¸,2 := {x âˆˆX0 | âˆ¥xâˆ¥Î¸,2 < âˆ},
âˆ¥xâˆ¥Î¸,2 :=
Z âˆ
0

tâˆ’Î¸K(t, x; X0, X1)
2 dt
t
 1
2
.
We have proved in Section C.1 that HFO can be compactly embedded into L2(PZO). Since
PZO is equivalent to the Lebesgue measure over Z Ã— O as per Assumption 4.4, HFO can also
be compactly embedded into L2(Z Ã— O). So we can define the Î¸-power space of HFO following
[Steinwart and Scovel, 2012, Eq. (36)]

[HFO]Î¸
PZO, âˆ¥Â· âˆ¥[HF O]Î¸
PZO

:=
ï£±
ï£²
ï£³
X
iâ‰¥1
aiÂµÎ¸/2
FO,i[eFO,i] : (ai) âˆˆâ„“2(N)
ï£¼
ï£½
ï£¾âŠ†L2(PZO).
For 0 < Î¸ < 1, the Î¸-power space coincides with the interpolation space [HFO]Î¸
L2(PZO) âˆ¼=
[L2(PZO), [HFO]PZO]Î¸,2 [Steinwart and Scovel, 2012, Theorem 4.6]. We write [HFO]L2(PZO) :=
[HFO]1
L2(PZO). Similarly, we write [HFO]L2(ZÃ—O) := [HFO]1
L2(ZÃ—O).
47

Proposition C.2. Suppose Assumption 2.2 holds, and let mo, mz, Ï be as defined in the statement
of Assumption 2.2. Suppose Î¸ âˆˆ(0, 1) satisfies
do
2moÎ¸ < 1 and
dz
2mzÎ¸ < 1. Then there exists a constant
CÎ¸ > 0 independent of n such that
kÎ¸
FO

âˆ:=
[HFO]Î¸
PZO ,â†’Lâˆ(PZO)
 â‰²CÎ¸ÏÎ¸Î³âˆ’Î¸mo
o
.
Proof. By Lemma C.3, we have [HFO]L2(ZÃ—O) ,â†’MW mz,mo(Z Ã— O) and
âˆ¥[HFO]L2(ZÃ—O) ,â†’MW mz,mo(Z Ã— O)âˆ¥â‰²ÏÎ³âˆ’mo
o
.
By Lemma C.6, since
do
2so < 1 and
dz
2sz < 1, we have MW Î¸mz,Î¸mo(Z Ã— O) ,â†’Lâˆ(Z Ã— O),
with embedding norm a universal constant independent of n. By Lemma C.5, we thus have
(L2(Z Ã— O), [HFO]L2(ZÃ—O))Î¸,2 ,â†’Lâˆ(Z Ã— O) and
âˆ¥(L2(Z Ã— O), [HFO]L2(ZÃ—O))Î¸,2 ,â†’Lâˆ(Z Ã— O)âˆ¥â‰²ÏÎ¸Î³âˆ’Î¸mo
o
,
Next, since we know by Assumption 2.2 that PZO is equivalent to Lebesgue measure on Z Ã— O,
Lemma C.4 shows that we have
 L2(PZO), [HFO]L2(PZO)

Î¸,2 ,â†’
 L2(Z Ã— O), [HFO]L2(ZÃ—O)

Î¸,2 ,â†’Lâˆ(PZO),
and we have

 L2(PZO), [HFO]L2(PZO)

Î¸,2 ,â†’Lâˆ(PZO)
 â‰²ÏÎ¸Î³âˆ’Î¸mo
o
(64)
Finally, by Steinwart and Scovel [2012][Theorem 4.6], (L2(PZO), [HFO]L2(PZO))Î¸,2 âˆ¼= [HFO]Î¸
L2(PZO)
with the constant of equivalence depends only on Î¸. Putting it back to Eq. (64) completes the
proof of the proposition.
Lemma C.3. Suppose Assumption 2.2 holds, and let mz, mo, Ï be as defined in the statement of
Assumption 2.2. We have [HFO]L2(ZÃ—O) ,â†’MW mz,mo
2
(Z Ã— O) with
[HFO]L2(ZÃ—O) ,â†’MW mz,mo
2
(Z Ã— O)
 â‰²ÏÎ³âˆ’mo
o
.
Proof. Let id : HFO ,â†’L2(Z Ã— O) denote the canonical inclusion map. Since we have (ker id)âŠ¥âˆ¼=
[HFO]L2(ZÃ—O), we may represent an arbitrary element of [HFO]L2(ZÃ—O) as [f], where f âˆˆ(ker id)âŠ¥.
Moreover, we have âˆ¥[f]âˆ¥L2(ZÃ—O) = âˆ¥fâˆ¥HF O. We fix f for the remainder of the proof.
Let Î± âˆˆNdz with |Î±| â‰¤mz and Î² âˆˆNdo with |Î²| â‰¤mo, we have for any f âˆˆHFO,
âˆ‚Î±
z âˆ‚Î²
o f
2
L2(ZÃ—O;R) =
Z
Z
Z
O
 âˆ‚Î±
z âˆ‚Î²
o f(z, o)
2 dz do
(a)
=
Z
Z
Z
O

âˆ‚Î±
z âˆ‚Î²
o
Z
X
w(x, o)p(x | z, o) dx
2
dz do
(b)
=
Z
Z
Z
O
ï£«
ï£­
X
Î²1+Î²2=Î²
 Î²
Î²1
 Z
X
âˆ‚Î²1
o w(x, o)âˆ‚Î±
z âˆ‚Î²2
o p(x | z, o) dx
ï£¶
ï£¸
2
dz do
(c)
â‰¤
Z
Z
Z
O
ï£«
ï£­
X
Î²1+Î²2=Î²
 Î²
Î²1
 Z
X
 âˆ‚Î²1
o w(x, o)
2 dx
 1
2 Z
X
 âˆ‚Î±
z âˆ‚Î²2
o p(x | z, o)
2 dx
 1
2
ï£¶
ï£¸
2
dz do
48

â‰¤
ï£«
ï£­
X
Î²1+Î²2=Î²
 Î²
Î²1
ï£¶
ï£¸
2 
max
Î²2â‰¤Î² sup
x,z,o
âˆ‚Î±
z âˆ‚Î²2
o p(x | z, o)
2 Z
Z
Z
O
max
Î²1â‰¤Î²
Z
X
 âˆ‚Î²1
o w(x, o)
2 dx dz do
=
ï£«
ï£­
X
Î²1+Î²2=Î²
 Î²
Î²1
ï£¶
ï£¸
2 
max
Î²2â‰¤Î² sup
x,z,o
âˆ‚Î±
z âˆ‚Î²2
o p(x | z, o)
2
max
Î²1â‰¤Î²
Z
O
Z
X
 âˆ‚Î²1
o w(x, o)
2 dx do
(d)
=cmo,do

max
Î²2â‰¤Î² sup
x,z,o
âˆ‚Î±
z âˆ‚Î²2
o p(x | z, o)
2
Î³âˆ’2|Î²|
o
âˆ¥wâˆ¥2
HÎ³x,Î³o
â‰¤cmo,do

max
Î±â‰¤mz max
Î²â‰¤mo sup
x,z,o
âˆ‚Î±
z âˆ‚Î²
o p(x | z, o)
2
Î³âˆ’2mo
o
âˆ¥wâˆ¥2
HÎ³x,Î³o,
where (a) follows from the fact that, for any f âˆˆHFO, there exists w âˆˆHÎ³x,Î³o such that f(z, o) =
âŸ¨w, Fâˆ—(z, o) âŠ—Ï•Î³o(o)âŸ©HÎ³x,Î³o =
R
X w(x, o)p(x | z, o) dx and âˆ¥fâˆ¥HF O = âˆ¥wâˆ¥HÎ³x,Î³o. (b) follows from
generalized Leibnizâ€™s rule and differentiation under the integral sign Klenke [2013][Theorem 6.28],
(c) follows from a Cauchy-Schwarz inequality. (d) follows from the following arguments.
From Steinwart and Christmann [2008, Theorem 4.21], we know that for any w âˆˆHÎ³x,Î³o, there
exists g âˆˆL2(Rdx+do) such that
w(x, o) = âŸ¨g, Î¦Î³x(x)Î¦Î³o(o)âŸ©L2(Rdx+do),
âˆ¥wâˆ¥HÎ³x,Î³o = âˆ¥gâˆ¥L2(Rdx+do),
with Î¦Î³x : X â†’L2(Rdx), Î¦Î³o : O â†’L2(Rdo) defined in Steinwart and Christmann [2008, Lemma
4.45]. We have
Z
O
Z
X

âˆ‚Î²1
o w(x, o)
2
dx do
=
Z
O
Z
X

âˆ‚Î²1
o
Z
Rdx+do g(xâ€², oâ€²)Î¦Î³x(x)(xâ€²)Î¦Î³o(o)(oâ€²) dxâ€² doâ€²
2
dx do
(i)
=
Z
O
Z
X
Z
Rdx+do g(xâ€², oâ€²)Î¦Î³x(x)(xâ€²)âˆ‚Î²1
o (Î¦Î³o(o)(oâ€²)) dxâ€² doâ€²
2
dx do
â‰¤âˆ¥gâˆ¥2
L2(Rdx+do)
Z
O
Z
X
Z
Rdx+do

Î¦Î³x(x)(xâ€²)âˆ‚Î²1
o (Î¦Î³o(o)(oâ€²))
2
dxâ€² doâ€² dx do
=âˆ¥gâˆ¥2
L2(Rdx+do)
Z
O
Z
X
Z
Rdx(Î¦Î³x(x)(xâ€²))2 dxâ€²
 Z
Rdo

âˆ‚Î²1
o (Î¦Î³o(o)(oâ€²))
2
doâ€²

dx do
(ii)
= âˆ¥gâˆ¥2
L2(Rdx+do)
Z
O
Z
Rdo

âˆ‚Î²1
o (Î¦Î³o(o)(oâ€²))
2
doâ€² do
(iii)
â‰¤âˆ¥gâˆ¥2
L2(Rdx+do)cÎ²1,doÎ³âˆ’2|Î²1|
o
=âˆ¥wâˆ¥2
HÎ³x,Î³ocÎ²1,doÎ³âˆ’2|Î²1|
o
,
where weâ€™re allowed to exchange differentiation and integration in (i) using the differentiation
lemma Klenke [2013][Theorem 6.28], (ii) follows from the fact that
Z
Rdx(Î¦Î³x(x)(xâ€²))2 dxâ€² =
Z
Rdx
 
2dx/2
Ï€dx/4Î³dx/2
x
exp

âˆ’2âˆ¥x âˆ’xâ€²âˆ¥2
2
Î³2x
!2
dxâ€² = 1,
and (iii) follows from the proof of Steinwart and Christmann [2008][Theorem 4.48]. Here cÎ²1,do is a
constant only depending on Î²1, do. Hence weâ€™ve shown
âˆ¥fâˆ¥2
MW mz,mo
2
(ZÃ—O;R)
49

=
X
Î±â‰¤mz,Î²â‰¤mo
âˆ‚Î±
z âˆ‚Î²
o f

2
L2(ZÃ—O;R)
â‰²
X
Î±â‰¤mz,Î²â‰¤mo

max
Î±â‰¤mz max
Î²â‰¤mo sup
x,z,o
âˆ‚Î±
z âˆ‚Î²
o p(x | z, o)

2
Î³âˆ’2mo
o
âˆ¥wâˆ¥2
HÎ³x,Î³o
=
ï£«
ï£­
X
Î±â‰¤mz,Î²â‰¤mo
1
ï£¶
ï£¸

max
Î±â‰¤mz max
Î²â‰¤mo sup
x,z,o
âˆ‚Î±
z âˆ‚Î²
o p(x | z, o)

2
Î³âˆ’2mo
o
âˆ¥fâˆ¥2
HF O
â‰²Ï2Î³âˆ’2mo
o
âˆ¥fâˆ¥2
HF O
=Ï2Î³âˆ’2mo
o
âˆ¥[f]âˆ¥2
[HF O]L2(ZÃ—O).
The second last inequality holds by Assumption 2.2. This concludes the proof.
C.3
Auxiliary results for Section C
Lemma C.4. Let Î½1, Î½2 be two measures on Z which are equivalent, i.e 0 < câ€² â‰¤dÎ½2
dÎ½1 â‰¤c < âˆ.
Let H âŠ†L2(Î½1) be a Hilbert space. Then (L2(Î½1), H)Î¸,2 ,â†’(L2(Î½2), H)Î¸,2 with embedding norm
âˆ¥(L2(Î½1), H)Î¸,2 ,â†’(L2(Î½2), H)Î¸,2âˆ¥â‰¤c1âˆ’Î¸.
Proof. For any f âˆˆ(L2(Î½1), H)Î¸,2, we have
âˆ¥fâˆ¥(L2(Î½2),H)Î¸,2 =
Z âˆ
0

tâˆ’Î¸K(t, x; L2(Î½2), H)
2 dt
t
 1
2
=
 Z âˆ
0

tâˆ’Î¸ inf
yâˆˆH

âˆ¥x âˆ’yâˆ¥L2(Î½2) + tâˆ¥yâˆ¥H
	2 dt
t
! 1
2
(a)
â‰¤
 Z âˆ
0

ctâˆ’Î¸ inf
yâˆˆH

âˆ¥x âˆ’yâˆ¥L2(Î½1) + t
câˆ¥yâˆ¥H
2 dt
t
! 1
2
(b)
= c1âˆ’Î¸
 Z âˆ
0

tâˆ’Î¸ inf
yâˆˆH

âˆ¥x âˆ’yâˆ¥L2(Î½1) + tâˆ¥yâˆ¥H
	2 dt
t
! 1
2
= c1âˆ’Î¸âˆ¥fâˆ¥(L2(Î½1),H)Î¸,2
In the above derivations, we use âˆ¥x âˆ’yâˆ¥L2(Î½2) â‰¤câˆ¥x âˆ’yâˆ¥L2(Î½1) in (a), and the change of variables
t 7â†’ct in (b).
Lemma C.5. Let Z = [0, 1]d. Let H, W âŠ†L2(Z) be two Hilbert spaces. Suppose that H ,â†’W with
embedding norm âˆ¥H ,â†’Wâˆ¥â‰¤c. Suppose that Î¸ âˆˆ(0, 1) is chosen so that
 L2(Z), W

Î¸,2 ,â†’Lâˆ(Z)
holds. Then we have
 L2(Z), H

Î¸,2 ,â†’Lâˆ(Z) with embedding norm â‰¤cÎ¸c0, where c0 only depends
on Î¸, m and dz.
Proof. We adapt the proof of Kanagawa et al. [2018][Lemma A.2, Corollary 4.13]. We have, for any
x âˆˆ(L2(Z), W)Î¸,2,
âˆ¥xâˆ¥(L2(Z),W)Î¸,2 =
Z âˆ
0

tâˆ’Î¸K
 t, x; L2(Z), W
2 dt
t
 1
2
50

=
 Z âˆ
0

tâˆ’Î¸ inf
yâˆˆW

âˆ¥x âˆ’yâˆ¥L2(Z) + tâˆ¥yâˆ¥W
	2 dt
t
! 1
2
â‰¤
 Z âˆ
0

tâˆ’Î¸ inf
yâˆˆH

âˆ¥x âˆ’yâˆ¥L2(Z) + tâˆ¥yâˆ¥W
	2 dt
t
! 1
2
â‰¤
 Z âˆ
0

tâˆ’Î¸ inf
yâˆˆH

âˆ¥x âˆ’yâˆ¥L2(Z) + tcâˆ¥yâˆ¥H
	2 dt
t
! 1
2
= cÎ¸
 Z âˆ
0

(tc)âˆ’Î¸ inf
yâˆˆH

âˆ¥x âˆ’yâˆ¥L2(Z) + tcâˆ¥yâˆ¥H
	2 dt
t
! 1
2
= cÎ¸
Z âˆ
0

tâˆ’Î¸K
 t, x; L2(Z), H
2 dt
t
 1
2
= cÎ¸âˆ¥xâˆ¥(L2(Z),H)Î¸,2.
Notice that we correct an error in Kanagawa et al. [2018] which obtained the exponent âˆ’Î¸
instead of Î¸ due to an error in the change of variables argument in (a). Now we have proved
that (L2(Z), H)Î¸,2 ,â†’(L2(Z), W)Î¸,2 and
(L2(Z), H)Î¸,2 ,â†’(L2(Z), W)Î¸,2
 â‰¤cÎ¸. On the other
hand, by assumption, we have (L2(Z), W)Î¸,2 ,â†’Lâˆ(Z). Consequently, we have (L2(Z), H)Î¸,2 ,â†’
W(L2(Z), W)Î¸,2 ,â†’Lâˆ(Z). The embedding norm is bounded by cÎ¸c0, where c0 depends only on
Î¸, m, and dz.
Lemma C.6. If
do
2so < 1 and
dz
2sz < 1, then the dominating mixed smoothness Sobolev space
MW so,sz
2
([0, 1]do+dz) continuously embeds into Lâˆ([0, 1]do+dz).
Proof. See [Schmeisser, 2007, Equation 1.13] for the result where the domain is Rdz+do. By DeVore
and Sharpley [1993], there exists a continuous extension operator E : MW so,sz
2
([0, 1]do+dz) â†’
MW so,sz
2
(Rdo+dz) such that âˆ¥E[f]âˆ¥MW so,sz
2
(Rdo+dz ) â‰¤Câ€²âˆ¥fâˆ¥MW so,sz
2
([0,1]do+dz ) holds for some uni-
versal constant Câ€².
Hence, for f âˆˆMW so,sz
2
([0, 1]do+dz), we thus have âˆ¥fâˆ¥Lâˆ([0,1]do+dz ) â‰¤
âˆ¥E[f]âˆ¥Lâˆ(Rdo+dz) â‰¤Câˆ¥E[f]âˆ¥MW so,sz
2
(Rdo+dz ) â‰¤Câ€²âˆ¥fâˆ¥MW so,sz
2
([0,1]do+dz ), for some universal con-
stants C, Câ€² > 0.
Lemma C.7. Let Z = [0, 1]dz and O = [0, 1]do. The interpolation space [L2(Z Ã— O), W sz
2 (Z) âŠ—
W so
2 (O)]Î¸,2 âˆ¼= W szÎ¸
2
(Z) âŠ—W soÎ¸
2
(O).
Proof. The proof follows immediately from Defant and Michels [2000].
Lemma C.8. Let G be the vector-valued RKHS of the operator-valued kernel defined in Eq. (4)
with kZ, kO,1 being Sobolev reproducing kernels of smoothness tz, to. Then,
G
(a)
âˆ¼= HX âŠ—HZ âŠ—HO,1
(b)
â‰ƒHX âŠ—W tz
2 (Z; R) âŠ—W to
2 (O; R)
(c)
âˆ¼= HX âŠ—MW tz,to
2
(Z Ã— O; R)
(d)
âˆ¼= MW tz,to
2
(Z Ã— O; HX),
(65)
Proof. (a) is proved in Li et al. [2024a, Theorem 1], (b) holds by definition of Sobolev reproducing
kernels, (c) is proved in Aubin [2011, Theorem 12.7.2] and (d) is an extension of Aubin [2011,
Theorem 12.7.1, Theorem 12.4.1].
51

Roadmap for Proof of Theorem 4.1
The following figures illustrate how Theorem 4.1 follows from supporting propositions and lemmas
(with arrows indicate that one result is used in the proof of another).
Proposition D.1
(projected stage 1 error w/ CME rate)
Proposition D.6
(Bound on âˆ¥Â¯fÎ»âˆ¥HXO)
Proposition D.7
(projected stage 2 approximation error)
Proposition D.3
(projected stage 1 error)
Lemma D.3
Lemma D.2
Proposition D.4
(CME learning rate)
Proposition D.5
(CME source condition)
Lemma D.4
(tensor RKHS eigenvalue)
Figure 3: Roadmap for proof of Proposition D.1.
Proposition D.2
(projected stage 2 error)
Proposition D.7
(projected stage 2 approximation error)
Lemma D.5
(Gaussian convolution)
Proposition D.8
(projected stage 2 estimation error)
Lemma D.10
(lower bound on âˆ¥CFOâˆ¥)
Proposition C.1
(effective dimension of HFO)
Proposition C.2
(embedding property of HFO)
Figure 4: Roadmap for proof of Proposition D.2.
Theorem 4.1
(L2-upper rate)
Eq. (67)
(Bound on âˆ¥fauxâˆ¥HXO)
Lemma D.6
Corollary D.1
(Gaussian convolution HXO-norm)
Lemma D.7
Lemma D.9
(projected rate to L2 rate)
Lemma D.8
(âˆ¥Â· âˆ¥HXO w/ Fourier transform)
Proposition D.1
(projected stage one error)
Proposition D.2
(projected stage two error)
Figure 5: Roadmap for proof of Theorem 4.1.
52

D
Proof of Theorem 4.1 in the main text
We first construct the auxiliary RKHS function faux defined in Eq. (40) in the main text, and give
an upper bound on âˆ¥fauxâˆ¥HÎ³x,Î³o. Let r = max{âŒŠsxâŒ‹, âŒŠsoâŒ‹} + 1. For Î³ = (Î³1, . . . , Î³d), we define an
approximate identity [GinÃ© and Nickl, 2021, Section 4.1.2] KÎ³ : Rd â†’R via
K1(x) :=
r
X
j=1
r
j

(âˆ’1)1âˆ’j 1
jd
 2
Ï€
 d
2
exp
 
âˆ’2
d
X
i=1
x2
i
j2
!
,
KÎ³(x) := K1
x
Î³

d
Y
i=1
1
Î³i
(66)
Note that Eq. (66) reduces to Eberts and Steinwart [2013][Eq. (8)] when Î³1 = Â· Â· Â· = Î³d, s1 = . . . , sd.
We define KÎ³x : Rdx â†’R to be KÎ³x with Î³x = [Î³x, . . . , Î³x] âˆˆRdx (note this agrees with Eq. (36)
in the main) and KÎ³o : Rdo â†’R to be KÎ³o with Î³o = [Î³o, . . . , Î³o] âˆˆRdo. Define Î¹x,Î³âˆ’1
x
: Rdx â†’R
as an indicator function of {Ï‰x : âˆ¥Ï‰xâˆ¥â‰¤Î³âˆ’1
x }.
By the Youngâ€™s Convolution Inequality and the fact that F is unitary, we have
fâˆ—(Â·, o) âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ]

L2(Rdx) â‰¤âˆ¥fâˆ—(Â·, o)âˆ¥L1(Rdx)âˆ¥Î¹x,Î³âˆ’1
x âˆ¥L2(Rdx)
The right hand side is finite by Assumption 2.3 in the main text. Hence we apply the Fourier
operator F to fâˆ—(Â·, o) âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ] to find that, (âˆ€o âˆˆO),
F[fâˆ—(Â·, o) âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ]] = F[fâˆ—(Â·, o)] Â· Î¹x,Î³âˆ’1
x .
Therefore, fâˆ—âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ] = fâˆ—,low where fâˆ—,low is defined in Eq. (38). We define fâˆ—,high = fâˆ—âˆ’fâˆ—,low.
Then we have, (âˆ€o âˆˆO),
F[fâˆ—,high(Â·, o)] = F[fâˆ—(Â·, o)] âˆ’F[fâˆ—,low(Â·, o)]
satisfies Eq. (39) in the main text. We let faux be as defined in Eq. (40) in the main text. We have
âˆ¥fauxâˆ¥HÎ³x,Î³o
(67)
â‰¤
fâˆ—âˆ—KÎ³o âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ]

HÎ³x,Î³o
+ âˆ¥KÎ³x âˆ—KÎ³o âˆ—(fâˆ—âˆ’fâˆ—,low)âˆ¥HÎ³x,Î³o
(a)
â‰¤n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ¥L2(Rdx+do) + n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ’fâˆ—,lowâˆ¥L2(Rdx+do)
(b)
â‰¤3n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
(68)
In the above chain of derivations, (a) holds by applying Lemma D.7 to the first term and ap-
plying Corollary D.1 to the second term, (b) holds by âˆ¥fâˆ—,lowâˆ¥L2(Rdx+do) â‰¤âˆ¥fâˆ—âˆ¥L2(Rdx+do) â‰¤1 by
Plancherelâ€™s Theorem and Assumption 2.3. Notice that,
(âˆ—) :=

h
Ë†fÎ»
i
âˆ’fâˆ—

L2(PXO) â‰¤

h
Ë†fÎ»
i
âˆ’faux

L2(PXO) + âˆ¥fâˆ—âˆ’fauxâˆ¥L2(PXO) .
(69)
By definition of Ë†fÎ» in Eq. (7) in the main text, it satisfies
Î»âˆ¥Ë†fÎ»âˆ¥2
HÎ³x,Î³o + 1
n
n
X
i=1

yi âˆ’
D
f, Ë†FÎ¾(zi, oi) âŠ—Ï•Î³o(oi)
E
HÎ³x,Î³o
2
â‰¤1
n
n
X
i=1
y2
i .
(70)
53

Notice that
1
n
n
X
i=1
y2
i = 1
n
n
X
i=1
((Tfâˆ—)(zi, oi) + Ï…i)2 â‰²2 + 2
n
n
X
i=1
Ï…2
i ,
where the inequality follows from
âˆ¥Tfâˆ—âˆ¥Lâˆ(ZÃ—O) â‰¤âˆ¥fâˆ—âˆ¥Lâˆ(XÃ—O) â‰¤âˆ¥fâˆ—âˆ¥Lâˆ(Rdx+do) â‰²1,
by Assumption 2.3. By Assumption 4.5, Ï…1, . . . , Ï…n are n i.i.d. mean zero Ïƒ-sub-Gaussian random
variables so Ï…2
1, . . . , Ï…2
n are n i.i.d. Ïƒ2-sub-exponential random variables [Vershynin, 2018, Lemma
2.7.6] with mean E[Ï…2
1] < âˆ. By Exercise 2.7.10 (Centering) of Vershynin [2018] we know that
Ï…2
1 âˆ’E[Ï…2
1], . . . , Ï…2
n âˆ’E[Ï…2
n] are n i.i.d. CÏƒ2-sub-exponential random variables for some universal
constant C. By Vershynin [2018, Theorem 2.8.1 (Bernsteinâ€™s inequality)], we have
(âˆ€t â‰¥0), P
 
n
X
i=1
Ï…2
i
 â‰¥n
 1 + E

Ï…2
i

!
â‰¤2 exp

âˆ’c min

n
C2Ïƒ4 ,
n
CÏƒ2

,
where c is a universal constant. For a fixed Ï„ â‰¥1, for sufficiently large n â‰¥1, the above right hand
side â‰¤2 exp(âˆ’Ï„). Thus for a fixed Ï„ â‰¥1, with P n-probability â‰¥1 âˆ’2eâˆ’Ï„, for sufficiently large
n â‰¥1, we have
1
n
n
X
i=1
y2
i â‰²2 + 2
n
n
X
i=1
Ï…2
i â‰¤4 + 2E

Ï…2
i

.
Under the same high probability event, from Eq. (70), using Î» = 1
n, we have
âˆ¥Ë†fÎ»âˆ¥HÎ³x,Î³o â‰²
q
4 + 2E

Ï…2
i
âˆšn.
(71)
Also, we have
âˆ¥fauxâˆ¥HÎ³x,Î³o â‰¤2âˆšn
(72)
proved above in Eq. (67). Continuing from Eq. (69), we apply Lemma D.9 to âˆ¥[ Ë†fÎ»] âˆ’fauxâˆ¥L2(PXO),
where we notice that Ë†fÎ», faux âˆˆHÎ³x,Î³o, and we use Eq. (71) and Eq. (72) to verify the assumption
of that lemma. We find, with P n-probability â‰¥1 âˆ’2eâˆ’Ï„,
(âˆ—) â‰¤Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2
T
h
Ë†fÎ»
i
âˆ’Tfaux

L2(PZO) + n
âˆ’
sx
dx
1+(2+ do
so )( sx
dx +Î·1) + âˆ¥fâˆ—âˆ’fauxâˆ¥L2(PXO) .
From Eq. (84), âˆ¥fâˆ—âˆ’fauxâˆ¥L2(PXO) can be upper bounded (up to a constant) by the second last
term above and hence subsumed. Through a triangular inequality, we have with P n-probability
â‰¥1 âˆ’2eâˆ’Ï„,
(âˆ—) â‰¤Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2
T
h
Ë†fÎ»
i
âˆ’Tfâˆ—

L2(PZO) + âˆ¥Tfâˆ—âˆ’Tfauxâˆ¥L2(PZO)

+ n
âˆ’
sx
dx
1+(2+ do
so )( sx
dx +Î·1) .
(73)
54

We prove in Eq. (83) in Section D.2 that âˆ¥Tfâˆ—âˆ’Tfauxâˆ¥L2(PZO) â‰¤n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) , so we continue
from above to obtain, with P n-probability â‰¥1 âˆ’2eâˆ’Ï„,
(âˆ—) â‰²Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2
T
h
Ë†fÎ»
i
âˆ’Tfâˆ—

L2(PZO) + n
âˆ’
sx
dx +Î·1âˆ’Î·0
1+(2+ do
so )( sx
dx +Î·1) (log n)
dxÎ·0
2 .
(74)
Note the last term of Eq. (73) is subsumed by the second term above since Î·0 â‰¥Î·1. Recall the
definition of Â¯fÎ» in Eq. (23) in the main text. Then, we have, through a triangular inequality, with
P n-probability â‰¥1 âˆ’2eâˆ’Ï„, the following holds
(âˆ—) â‰¤Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2
ï£«
ï£¬
ï£¬
ï£¬
ï£­
T
h
Ë†fÎ»
i
âˆ’T
 Â¯fÎ»

L2(PZO)
|
{z
}
Projected Stage I Error
+
T
 Â¯fÎ»

âˆ’Tfâˆ—

L2(PZO)
|
{z
}
Projected Stage II Error
ï£¶
ï£·
ï£·
ï£·
ï£¸
+ n
âˆ’
sx
dx +Î·1âˆ’Î·0
1+(2+ do
so )( sx
dx +Î·1) (log n)
dxÎ·0
2 .
The projected stage I error can be upper bounded by Proposition D.1 as n > AÎ»,Ï„ is satisfied for
sufficiently large n â‰¥1 proved in Eq. (89).
Projected Stage I Error =
T
h
Ë†fÎ»
i
âˆ’T
 Â¯fÎ»

L2(PZO) â‰¤Ï„n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1)
holds with P n+Ëœn-probability â‰¥1 âˆ’34eâˆ’Ï„. The projected stage II error can be upper bounded by
Proposition D.2.
Projected Stage II Error =
T
 Â¯fÎ»

âˆ’Tfâˆ—

L2(PZO) â‰¤Ï„n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) Â· (log n)
dx+do+1
2
holds with P n-probability â‰¥1 âˆ’4eâˆ’Ï„. Combine the above two upper bounds, and we have
(âˆ—) =

h
Ë†fÎ»
i
âˆ’fâˆ—

L2(PXO) â‰¤Ï„n
âˆ’
sx
dx +Î·1âˆ’Î·0
1+(2+ do
so )( sx
dx +Î·1) (log n)
dx+do+1+dxÎ·0
2
(75)
holds with P n+Ëœn-probability 1 âˆ’40eâˆ’Ï„. So the proof is concluded.
Proposition D.1 (Projected stage-I error). Suppose that the assumptions of Proposition D.4 hold.
Suppose that n > AÎ»,Ï„ with AÎ»,Ï„ defined in Eq. (87). Suppose Assumptions 2.2 hold. Suppose that
Ëœn â‰¥1 satisfies Eq. (21). Let Î» â‰nâˆ’1 and
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Then, with P n+Ëœn-probability â‰¥1 âˆ’34eâˆ’Ï„, we have
T
 Â¯fÎ»

âˆ’
h
Ë†fÎ»
i
L2(PZO) â‰²Ï„n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) .
55

Proof. From Proposition D.3, with P n+Ëœn-probability â‰¥1 âˆ’28eâˆ’Ï„, we have
T
 Â¯fÎ»

âˆ’
h
Ë†fÎ»
i
L2(PZO)
â‰²Ï„Î»âˆ’1
2
ï£«
ï£¬
ï£­
 Ë†FÎ¾ âˆ’Fâˆ—

G
âˆšn
+
Fâˆ—âˆ’[ Ë†FÎ¾]

L2(ZÃ—O;HX,Î³x)
ï£¶
ï£·
ï£¸
|
{z
}
(â€¡)

1 + âˆ¥Â¯fÎ»âˆ¥HXO,Î³x,Î³0

.
By Proposition D.6, we have with P n-probability â‰¥1 âˆ’2eâˆ’Ï„
âˆ¥Â¯fÎ»âˆ¥HÎ³x,Î³o â‰²Ï„n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Under the probabilistic event that Proposition D.3 holds, the bounds in Proposition D.4 also hold,
so we have

h
Ë†FÎ¾
i
âˆ’Fâˆ—

L2(ZÃ—O;HX,Î³x) â‰¤JÏ„ Ëœn
âˆ’1
2
mâ€ 
mâ€ +dâ€ /2+Î¶ ,
 Ë†FÎ¾ âˆ’Fâˆ—

G â‰¤JÏ„ Ëœn
âˆ’1
2
mâ€ âˆ’1
mâ€ +dâ€ /2+Î¶ ,
hold for any Î¶ > 0. J is some constant independent of Ëœn, n. Thus under this probabilistic event, a
sufficient condition so that (â€¡) â‰²1
n is given by
Ëœn â‰¥(JÏ„n)2 mâ€ +dâ€ /2+Î¶
mâ€ 
âˆ¨(J2Ï„ 2n)
mâ€ +dâ€ /2+Î¶
mâ€ âˆ’1
.
This is satisfied since Ëœn satisfies Eq. (21). Hence,

h
Ë†FÎ¾
i
âˆ’Fâˆ—

L2(ZÃ—O;HX,Î³x) â‰¤nâˆ’1,
 Ë†FÎ¾ âˆ’Fâˆ—

G â‰¤nâˆ’1
2 .
(76)
By the union bound, we have that with P n+Ëœn-probability â‰¥1 âˆ’30eâˆ’Ï„, the following bound holds
T
 Â¯fÎ»

âˆ’
h
Ë†fÎ»
i
L2(PZO) â‰²Ï„n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) .
Hence the proof is concluded.
Proposition D.2 (Projected stage-II error). Suppose Assumptions 4.3, 4.2, 4.1, 2.3, 2.2 hold. Let
Î» â‰nâˆ’1 and
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Then, with P n-probability â‰¥1 âˆ’4eâˆ’Ï„, for sufficiently large n â‰¥1, we have
T
 fâˆ—âˆ’
 Â¯fÎ»

L2(PZO) â‰¤2CÏ„(log n)
dx+do+1
2
n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1)
for some constant C > 0 independent of n.
56

Proof. The proposition is proved through the following triangular inequality
T
 fâˆ—âˆ’
 Â¯fÎ»

L2(PZO) â‰¤âˆ¥T (fâˆ—âˆ’[fÎ»])âˆ¥L2(PZO) +
T
  Â¯fÎ»

âˆ’[fÎ»]

L2(PZO)
â‰¤n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) + n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) (log n)
dx+do+1
2
â‰¤2n
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) (log n)
dx+do+1
2
.
The second last inequality holds by using Proposition D.7 for the first term and Proposition D.8
for the second term, which holds with P n-probability â‰¥1 âˆ’4eâˆ’Ï„.
D.1
Projected Stage I Error
With the introduction of HFO and the partial isometry V : HXO â†’HFO in Section C, we define
Ë†hÎ» := arg min
hâˆˆHF O
1
n
n
X
i=1

yi âˆ’
D
h, V

Ë†FÎ¾(zi, oi) âŠ—Ï•Î³o(oi)
E
HF O
2
+ Î»âˆ¥hâˆ¥2
HF O
Â¯hÎ» = arg min
hâˆˆHF O
1
n
n
X
i=1

yi âˆ’
D
h, V

Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi)
E
HF O
2
+ Î»âˆ¥hâˆ¥2
HF O.
As shown in Section C, we have Ë†hÎ» = V Ë†fÎ» and Â¯hÎ» = V Â¯fÎ», where V : HXO â†’HFO is the metric
surjection map introduced in Section C. Ë†fÎ» âˆˆHÎ³x,Î³o is defined in Eq. (7) and Â¯fÎ» âˆˆHÎ³x,Î³o is defined
in Eq. (23) in the main text.
We further define
Î¦ Ë†FO : HFO â†’Rn =
h
V

Ë†FÎ¾(z1, o1) âŠ—Ï•Î³o(o1)

, . . . , V

Ë†FÎ¾(zn, on) âŠ—Ï•Î³o(on)
iâˆ—
Î¦FO : HFO â†’Rn = [V (Fâˆ—(z1, o1) âŠ—Ï•Î³o(o1)) , . . . , V (Fâˆ—(zn, on) âŠ—Ï•Î³o(on))]âˆ—
Y âˆˆRn,
Y = [y1, . . . , yn]âŠ¤,
(77)
and the following operators on HFO
C Ë†FO := E
h
V

Ë†FÎ¾(Z, O) âŠ—Ï•Î³o(O)

âŠ—V

Ë†FÎ¾(Z, O) âŠ—Ï•Î³o(O)
i
,
Ë†C Ë†FO := 1
nÎ¦âˆ—
Ë†FOÎ¦ Ë†FO
CFO := E [V (Fâˆ—(Z, O) âŠ—Ï•Î³o(O)) âŠ—V (Fâˆ—(Z, O) âŠ—Ï•Î³o(O))] ,
Ë†CFO := 1
nÎ¦âˆ—
FOÎ¦FO.
Hence, we have closed form expression for
Ë†hÎ» = 1
n

Ë†C Ë†FO + Î»
âˆ’1
Î¦âˆ—
Ë†FOY,
Â¯hÎ» = 1
n

Ë†CFO + Î»
âˆ’1
Î¦âˆ—
FOY.
(78)
Proposition D.3. Fix Ï„ â‰¥1. Suppose that the assumptions of Proposition D.4 hold. Suppose that
n > AÎ»,Ï„ with AÎ»,Ï„ defined in Eq. (87), and Ëœn â‰¥1 satisfies Eq. (21) in the main text. We have
with P n+Ëœn-probability â‰¥1 âˆ’28eâˆ’Ï„, the following inequality holds
T[ Ë†fÎ»] âˆ’T[ Â¯fÎ»]

L2(PZO)
â‰²Ï„Î»âˆ’1
2
ï£«
ï£¬
ï£­
 Ë†FÎ¾ âˆ’Fâˆ—

G
âˆšn
+

h
Fâˆ—âˆ’Ë†FÎ¾
i
L2(ZÃ—O;HX,Î³x)
ï£¶
ï£·
ï£¸
 1 + âˆ¥Â¯fÎ»âˆ¥HÎ³x,Î³o

.
Under the same probabilistic event, the bounds in Proposition D.4 also hold.
57

Proof. We find
T[ Ë†fÎ»] âˆ’T[ Â¯fÎ»]

L2(PZO) =
[Ë†hÎ»] âˆ’[Â¯hÎ»]

L2(PZO) â‰¤
C
1
2
FO

Ë†hÎ» âˆ’Â¯hÎ»

HF O
.
The last step follows from Lemma 12 of Fischer and Steinwart [2020]. We can proceed by plugging
in the closed form expressions from Eq. (78) to have
C
1
2
F O

Ë†hÎ» âˆ’Â¯hÎ»

HF O
=
C
1
2
F O

Ë†C Ë†
F O + Î»
âˆ’1 1
nÎ¦âˆ—
Ë†
F OY âˆ’

Ë†CF O + Î»
âˆ’1 1
nÎ¦âˆ—
F OY

HF O
â‰¤
C
1
2
F O(CF O + Î»)âˆ’1
2
 Â·
(CF O + Î»)
1
2

Ë†C Ë†
F O + Î»
âˆ’1 1
nÎ¦âˆ—
Ë†
F OY âˆ’

Ë†CF O + Î»
âˆ’1 1
nÎ¦âˆ—
F OY

HF O
â‰¤
(CF O + Î»)
1
2

Ë†C Ë†
F O + Î»
âˆ’1 1
nÎ¦âˆ—
Ë†
F OY âˆ’

Ë†CF O + Î»
âˆ’1 1
nÎ¦âˆ—
F OY

HF O
â‰¤Sâˆ’1 + S0,
where we define
Sâˆ’1 :=
(CFO + Î»)
1
2

Ë†C Ë†FO + Î»
âˆ’1 1
n
 Î¦ Ë†FO âˆ’Î¦FO
âˆ—Y

HF O
S0 =
(CFO + Î»)
1
2

Ë†C Ë†FO + Î»
âˆ’1 
Ë†CFO âˆ’Ë†C Ë†FO
 
Ë†CFO + Î»
âˆ’1 1
nÎ¦âˆ—
FOY

HF O
We bound Sâˆ’1 with P n+Ëœn-high probability by Lemma D.3 and S0 in P n+Ëœn-high probability by
Lemma D.2. We thus have, with P n+Ëœn-probability â‰¥1 âˆ’28eâˆ’Ï„, the following bound
C
1
2
FO

Ë†hÎ» âˆ’Â¯hÎ»

HF O
â‰¤cÏ„âˆšn
ï£«
ï£¬
ï£­
 Ë†FÎ¾ âˆ’Fâˆ—

G
âˆšn
+

h
Ë†FÎ¾ âˆ’Fâˆ—
i
L2(ZÃ—O;HX,Î³x)
ï£¶
ï£·
ï£¸
 âˆ¥Â¯hÎ»âˆ¥HF O + 1

.
âˆ¥Â¯hÎ»âˆ¥HF O = âˆ¥Â¯fÎ»âˆ¥HÎ³x,Î³o since V is a metric surjection.
Proposition D.4 (CME rate). Suppose Assumption 2.2 in the main text holds. Suppose kO :
O Ã— O â†’R and kZ : Z Ã— Z â†’R are MatÃ©rn reproducing kernels whose RKHSs HO and HZ are
norm equivalent to W to
2 (O) and W tz
2 (Z) with mo > to > do/2, mz > tz > dz/2 and mz
tz âˆ§mo
to â‰¤2.
Define mâ€  = (mztâˆ’1
z ) âˆ§(motâˆ’1
o ) and dâ€  = (dztâˆ’1
z ) âˆ¨(dotâˆ’1
o ). For Î¶ > 0 arbitrarily small, we take
Î¾ = Ëœn
âˆ’
1
mâ€ +dâ€ /2+Î¶ , then with P Ëœn-probability at least 1 âˆ’4eâˆ’Ï„, we have

h
Ë†FÎ¾ âˆ’Fâˆ—
i
L2(ZÃ—O;HX,Î³x) â‰¤JÏ„ Ëœn
âˆ’1
2
mâ€ 
mâ€ +dâ€ /2+Î¶ ,
 Ë†FÎ¾ âˆ’Fâˆ—

G â‰¤JÏ„ Ëœn
âˆ’1
2
mâ€ âˆ’1
mâ€ +dâ€ /2+Î¶ .
J is a constant independent of Ëœn.
Proof. We are going to apply Li et al. [2024a, Theorem 3] to obtain the desired result. To this
end, we need to verify the assumptions (EVD) and (SRC) made in Li et al. [2024a]. Note that
we let the assumption (EMB) of Li et al. [2024a] be trivially verified with Î± = 1, as we prove in
58

Proposition D.5 that Fâˆ—âˆˆ[G]Î² with Î² > 1, therefore Î² + p > 1 + p > 1 = Î± falls in Case 2 of
Theorem 3 of Li et al. [2024a].
Verification of (EVD) Let Ï•Z (resp.
Ï•O) denote the feature map of HZ (resp.
HO). Define
CZO : HZ âŠ—HO â†’HZ âŠ—HO as the covariance operator.
CZO =
ZZ
ZÃ—O
(Ï•Z(z) âŠ—Ï•O(o)) âŠ—(Ï•Z(z) âŠ—Ï•O(o))pZO(z, o) dz do.
We also define another two auxiliary covariance operators Â¯CZ : HZ â†’HZ and Â¯CO : HO â†’HO.
Â¯CZ =
Z
Z
Ï•Z(z) âŠ—Ï•Z(z)pZ(z) dz,
Â¯CO =
Z
O
Ï•O(o) âŠ—Ï•O(o)pO(o) do.
Since kZ and kO are bounded, CZO, Â¯CZ, Â¯CO are all self-adjoint compact operators. From Edmunds
and Triebel [1996] and Fischer and Steinwart [2020][Section 4], we have Î»i( Â¯CZ) â‰iâˆ’2tz/dz and
Î»i( Â¯CO) â‰iâˆ’2to/do. We know from Assumption 2.2 that PZO is equivalent to the Lebesgue measure,
so by Jensenâ€™s inequality, we have CZO â‰¤Â¯CZ âŠ—Â¯CO, where âŠ—here denotes an operator tensor
product. Hence, from Lemma 17 of Meunier et al. [2024a] we have Î»i(CZO) â‰¤Î»i( Â¯CZ âŠ—Â¯CO). Finally,
from Lemma D.4 we have
Î»i(CZO) â‰¤Î»i( Â¯CZ âŠ—Â¯CO) â‰²iâˆ’2tz/dzâˆ§2to/do+Î¶
for any Î¶ > 0. Therefore, we have proved that (EVD) hold with 1/p = 2tz/dz âˆ§2to/do âˆ’Î¶. Hence,
p = dâ€ /2 + Î¶ for any Î¶ > 0.
Verification of (SRC) Let Î² = mâ€ . By the definition of vector-valued interpolation space in Li et al.
[2024a][Definition 2] and the Assumption that PZO is equivalent to the Lebesgue measure, we have
that
[G]Î² âˆ¼= HX,Î³x âŠ—[L2(Z Ã— O), HZ âŠ—HO]Î²,2
(a)
âˆ¼= HX,Î³x âŠ—[L2(Z Ã— O), W tz
2 (Z) âŠ—W to
2 (O)]Î²,2
(b)
âˆ¼= HX,Î³x âŠ—W tzÎ²
2
(Z) âŠ—W toÎ²
2
(O)
(âˆ—)
âŠ‡HX,Î³x âŠ—W mz
2
(Z) âŠ—W mo
2
(O)
(c)
âˆ¼= HX,Î³x âŠ—MW mz,mo
2
(Z Ã— O; R)
(d)
âˆ¼= MW mz,mo
2
(Z Ã— O; HX,Î³x).
where in (a) we use Corollary 10.13 and Theorem 10.46 of Wendland [2004], in (b) we use Lemma C.7,
in (c) we use proposition 3.1 of Sickel and Ullrich [2009], which shows MW mz,mo
2
(Z Ã— O) âˆ¼=
W mz
2
(Z)âŠ—W mo
2
(O), and in (d) we use Lemma D.1. Furthermore, the inclusion in (âˆ—) is a continuous
embedding. We show in Proposition D.5 that under Assumption 2.2, âˆ¥Fâˆ—âˆ¥MW mz,mo
2
(ZÃ—O;HX,Î³x) is
bounded by a constant. Hence we deduce from the above embedding that
Fâˆ—âˆˆ[G]Î²,
Î² = mâ€  = mz
tz
âˆ§mo
to
.
(79)
Hence âˆ¥Fâˆ—âˆ¥[G]Î² â‰¤C for some constant C > 0. Hence Fâˆ—satisfies (SRC) in Li et al. [2024a] with
Î² = mâ€ .
59

Now we use Case 2 in Theorem 3 of Li et al. [2024a] to obtain the following result: if we take
Î¾ = Ëœn
âˆ’
1
mâ€ +dâ€ /2+Î¶ , then for any 0 â‰¤Î³ < mâ€ , the following bound on the Î³-norm

h
Ë†FÎ¾ âˆ’Fâˆ—
i
2
Î³ â‰¤Ï„ 2Ëœn
âˆ’
mâ€ âˆ’Î³
mâ€ +dâ€ /2+Î¶ .
holds with P Ëœn-probability at least 1 âˆ’4eâˆ’Ï„. Therefore, noting that mâ€  > 1, by taking Î³ = 0 and
Î³ = 1, we obtain with P Ëœn-probability at least 1 âˆ’4eâˆ’Ï„,

h
Ë†FÎ¾ âˆ’Fâˆ—
i
L2(ZÃ—O;HX,Î³x) â‰¤Ï„ Ëœn
âˆ’1
2
mâ€ 
mâ€ +dâ€ /2+Î¶ ,
 Ë†FÎ¾ âˆ’Fâˆ—

G â‰¤Ï„ Ëœn
âˆ’1
2
mâ€ âˆ’1
mâ€ +dâ€ /2+Î¶ .
Proposition D.5. Suppose that Assumption 2.2 in the main text is satisfied. Then,
âˆ¥Fâˆ—âˆ¥MW mz,mo
2
(ZÃ—O;HX,Î³x) â‰¤
smz + dz âˆ’1
dz âˆ’1
smo + do âˆ’1
do âˆ’1

Ï.
Proof. Notice that, by definition of Fâˆ—,
âˆ¥Fâˆ—âˆ¥MW mz,mo
2
(ZÃ—O;HX,Î³x) =

Z
X
Ï•Î³x(x)p(x|Â·, Â·) dx

MW mz,mo
2
(ZÃ—O;HX,Î³x)
â‰¤
Z
X
âˆ¥Ï•Î³x(x)p(x|Â·, Â·)âˆ¥MW mz,mo
2
(ZÃ—O;HX,Î³x) dx.
In the last inequality above, we use HytÃ¶nen et al. [2016][Proposition 1.2.11]. Next, notice that
âˆ¥Ï•Î³x(x)p(x | Â·, Â·)âˆ¥2
MW mz,mo
2
(ZÃ—O;HX,Î³x)
=
X
|Î±|â‰¤mz
X
|Î²|â‰¤mo
Z
ZÃ—O
âˆ‚Î±
z âˆ‚Î²
o (Ï•Î³x(x)p(x | z, o))

2
HX,Î³x
dz do
=
X
|Î±|â‰¤mz
X
|Î²|â‰¤mo
Z
ZÃ—O
Ï•Î³x(x)âˆ‚Î±
z âˆ‚Î²
o p(x | z, o)

2
HX,Î³x
dz do
â‰¤
X
|Î±|â‰¤mz
X
|Î²|â‰¤mo
Z
ZÃ—O
âˆ‚Î±
z âˆ‚Î²
o p(x|z, o)

2
dz do
â‰¤
mz + dz âˆ’1
dz âˆ’1
mo + do âˆ’1
do âˆ’1

Ï2.
In the last step,
 mz+dzâˆ’1
dzâˆ’1

shows up as the evaluation of P
|Î±|â‰¤mz 1.
D.1.1
RKHS norm of Â¯fÎ»
We now provide a refined control of the RKHS norm of Â¯fÎ» by invoking an oracle inequality for
the RKHS HFO. We remind the reader that V Â¯fÎ» âˆˆHFO, i.e. the image of Â¯fÎ» under the metric
surjection V , is the solution to a KRR problem with respect to the RKHS HFO (see Eq. (59)).
In Section C.1 we characterized the rate of decay of the entropy numbers of HFO. The only
remaining technical hurdle is that the noise Ï… = Y âˆ’(Tfâˆ—)(Z, O) is unbounded; however, under
Assumption 4.5 it is Ïƒ-subgaussian. Concretely, we introduce a logarithmically growing sequence
60

of clipping values Mn, which facilitates a key step in the derivation of the oracle inequality and
ensures that the conclusions of Steinwart and Christmann [2008, Theorem 7.23] are applicable.
Firstly, by Vershynin [2018, Eq. (2.14)], P(|Ï…| â‰¥t) â‰¤2 exp(âˆ’ct2
Ïƒ2 ) for some universal constant c.
Define Ï…i = yi âˆ’(Tfâˆ—)(zi, oi) for stage II samples {zi, oi, yi}n
i=1. Hence
P n

{zi, oi, yi}n
i=1 âˆˆ(Z Ã— O Ã— Y )n : max
i
|Ï…i| â‰¤t}

â‰¥1 âˆ’
n
X
i=1
P(|Ï…i| â‰¥t)
â‰¥1 âˆ’2 exp

ln n âˆ’ct2
Ïƒ2

.
Hence, with P n-probability â‰¥1 âˆ’2 exp(âˆ’Ë†Ï), we have
max
1â‰¤iâ‰¤n |Ï…i| â‰¤Ïƒ
r
ln n + Ë†Ï
c
.
By Assumption 2.3, we have âˆ¥Tfâˆ—âˆ¥âˆâ‰¤âˆ¥fâˆ—âˆ¥âˆâ‰¤1. For n â‰¥1, we define
Mn = 1 + Ïƒ
r
ln n + Ë†Ï
c
.
Hence yi âˆˆ[âˆ’Mn, Mn] for all 1 â‰¤i â‰¤n with P n-probability â‰¥1 âˆ’2 exp(âˆ’Ë†Ï).
Secondly, we verify the assumptions of Steinwart and Christmann [2008, Theorem 7.23]. By
Steinwart and Christmann [2008, Example 7.3], the supremum bound is satisfied for B = 4M2
n and
the variance bound is satisfied for V = 16M2
n and Î½ = 1. Define DZO = {zi, oi}n
i=1 and L2(DZO)
as the L2 space with respect to the empirical data measure DZO. We are about to show that
(âˆ€n â‰¥1)(âˆƒp âˆˆ(0, 1))(âˆƒa â‰¥B = 4M2
n) such that
(âˆ€i â‰¥1) EDZOâˆ¼P n
ZOei(id : HFO â†’L2(DZO)) â‰¤aiâˆ’1
2p .
(80)
To this end, we invoke Steinwart and Christmann [2008, Corollary 7.31], Lemma C.1 and Lemma C.2.
We conclude that, there exists a constant cp > 0 only depending on p, such that (âˆ€i â‰¥1)(âˆ€n â‰¥1),
we have
EDZOâˆ¼P n
ZOei(id : HFO ,â†’L2(DZO))
â‰¤cp(3C)
1
2p
dx + do + 1
2ep
 dx+do+1
2p

Î³dx
x Î³do
o
âˆ’1
2p (min{i, n})
1
2p iâˆ’1
p
â‰¤cp(3C)
1
2p
dx + do + 1
2ep
 dx+do+1
2p

Î³dx
x Î³do
o
âˆ’1
2p iâˆ’1
2p .
We define a new constant Ëœcp := cp(3C)
1
2p

dx+do+1
2ep
 dx+do+1
2p
. Since for all n â‰¥1,
 Î³dx
x Î³do
o
âˆ’1
2p â‰¥1,
we set
a = max{4M2
n, Ëœcp}

Î³dx
x Î³do
o
âˆ’1
2p
in Eq. (80), which satisfies a â‰¥B = 4M 2
n for all n â‰¥1.
61

We thus apply Steinwart and Christmann [2008, Theorem 7.23] restricted to the probabilistic
event where yi âˆˆ[âˆ’2Mn, 2Mn] for all 1 â‰¤i â‰¤n. We find, with P n-probability â‰¥1 âˆ’5 exp(âˆ’Ë†Ï),
Î»âˆ¥Â¯fÎ»âˆ¥2
HXO â‰¤9

Î»âˆ¥f0âˆ¥2
HXO + âˆ¥T(f0 âˆ’fâˆ—)âˆ¥2
L2(PZO)

+ K
max{4M2
n, Ëœcp}2pÎ³âˆ’dx
x
Î³âˆ’do
o
Î»pn

+ 216M2
n Ë†Ï
n
+ 15B0Ë†Ï
n
where K â‰¥1 is a constant depending on p, Mn, f0 âˆˆHÎ³x,Î³o. B0 â‰¥B is a constant that satisfies
âˆ¥L â—¦(Tf0)âˆ¥âˆ:=
sup
(z,o,y)âˆˆZÃ—OÃ—[âˆ’2Mn,2Mn]
(y âˆ’(Tf0)(z, o))2 â‰¤B0,
By checking the dependence of K on p, Mn in the proof of Steinwart and Christmann [2008,
Theorem 7.23], we find that K â‰¤c(p)M2
n for some constant c(p) depending only on p. We choose
f0 = fÎ», where fÎ» âˆˆHÎ³x,Î³o is defined in Eq. (57). We have, since Y = [âˆ’2Mn, 2Mn],
âˆ¥L â—¦(TfÎ»)âˆ¥âˆ=
sup
(z,o,y)âˆˆZÃ—OÃ—[âˆ’2Mn,2Mn]
(y âˆ’(TfÎ»)(z, o))2 â‰¤(2Mn + âˆ¥TfÎ»âˆ¥âˆ)2
â‰¤(2Mn + âˆ¥fÎ»âˆ¥âˆ)2 â‰¤(2Mn + âˆ¥fÎ»âˆ¥HÎ³x,Î³o)2.
We choose thus
B0 = (2Mn + âˆ¥fÎ»âˆ¥HÎ³x,Î³o)2 â‰¥4M2
n = B.
Hence we deduce from the above high probability inequality that
Î»âˆ¥Â¯fÎ»âˆ¥2
HXO â‰¤C

Î»âˆ¥fÎ»âˆ¥2
HXO + âˆ¥T(fÎ» âˆ’fâˆ—)âˆ¥2
L2(PZO)

+ C

M2+2p
n
max{4, Ëœcp}2pÎ³âˆ’dx
x
Î³âˆ’do
o
Î»pn

+ (Mn + âˆ¥fÎ»âˆ¥HXO)2Ë†Ï
n

.
for some constant C that is independent of Î», Mn, n, Î³x, Î³o, B, V, Ë†Ï.
We now state the main
Proposition in this subsection.
Proposition D.6. Suppose Assumptions 2.2, 2.3, 4.1, 4.2 and 4.3 in the main text hold. Let
Î» = nâˆ’1 and
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Fix an arbitrary p > 0. Then with probability â‰¥1 âˆ’5eâˆ’Ï„, for sufficiently large n â‰¥1, the following
bound holds
âˆ¥Â¯fÎ»âˆ¥2
HÎ³x,Î³o â‰²Ï„n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)+p
.
Proof. Fix p > 0. We combine the results of Proposition D.7 with the inequality immediately
preceding this proposition to find, for Ë†Ï â‰¥1, there exists a constant C1, C2, C3 independent of
Î», Mn, n, Î³x, Î³o, B, V, Ë†Ï such that , with probability â‰¥1 âˆ’5eâˆ’Ë†Ï the following bound holds
Î»âˆ¥Â¯fÎ»âˆ¥2
HXO
62

(a)
â‰¤C1Ë†Ï

Î»âˆ¥fÎ»âˆ¥2
HXO + âˆ¥T(fÎ» âˆ’fâˆ—)âˆ¥2
L2(PZO) + M2+2p
n
max{4, Ëœcp}2pÎ³âˆ’dx
x
Î³âˆ’do
o
Î»pn

+ M2
n
n

(b)
â‰¤C2Ë†Ï
ï£«
ï£­n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) + M2+2p
n
max{4, Ëœcp}2pÎ³âˆ’dx
x
Î³âˆ’do
o
Î»pn

+ M2
n
n
ï£¶
ï£¸
(c)
= C2Ë†Ï
ï£«
ï£­n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)

1 + M2+2p
n
max{4, Ëœcp}2p
Î»p

+ M2
n
n
ï£¶
ï£¸
(d)
â‰¤C3Ë†Ïn
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) n2p.
In the above derivations, (a) holds by Î» = nâˆ’1, (b) holds by the conclusion of Proposition D.7, (c)
holds by the choice of Î³x, Î³o as functions of n, (d) holds for sufficiently large n â‰¥1, since p is fixed
and np > M2+2p
n
=

1 + Ïƒ
q
ln n+Ë†Ï
c
2+2p
for sufficiently large n â‰¥1, and n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) â‰¥M2
n
n
for sufficiently large n â‰¥1. In particular, the constant C3 depends on p.
D.2
Projected approximation error in Stage II
Proposition D.7. Suppose Assumptions 2.2, 2.3, 4.1, 4.2 and 4.3 in the main text hold. Let
Î» = nâˆ’1 and
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Then we have
Î»âˆ¥fÎ»âˆ¥2
HÎ³x,Î³o + âˆ¥T (fâˆ—âˆ’[fÎ»])âˆ¥2
L2(PZO) â‰¤Cn
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) ,
for some constant C > 0 independent of n.
Proof. Recall faux âˆˆHÎ³x,Î³o defined in Eq. (40). We write
faux = fâˆ—âˆ—KÎ³o âˆ’(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o + (fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ—KÎ³x.
By definition of fÎ», we have
Î»âˆ¥fÎ»âˆ¥2
HÎ³x,Î³o + âˆ¥T (fâˆ—âˆ’[fÎ»])âˆ¥2
L2(PZO)
=
inf
fâˆˆHÎ³x,Î³o
Î»âˆ¥fâˆ¥2
HÎ³x,Î³o + âˆ¥Tfâˆ—âˆ’T[f]âˆ¥2
L2(PZO)
â‰¤Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o + âˆ¥Tfâˆ—âˆ’T[faux]âˆ¥2
L2(PZO)
â‰¤Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o + âˆ¥Tfâˆ—âˆ’T (fâˆ—âˆ—KÎ³o)âˆ¥2
L2(PZO)
+ âˆ¥T ((fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ’(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ—KÎ³x)âˆ¥2
L2(PZO) .
(81)
For the first term, we know from Eq. (67) and Assumption 2.3 in the main text that
Î»âˆ¥fauxâˆ¥2
HÎ³x,Î³o â‰¤nâˆ’1 Â· 2n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ¥L2(Rdx+do) â‰¤2n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
63

For the second term, we have
âˆ¥Tfâˆ—âˆ’T (fâˆ—âˆ—KÎ³o)âˆ¥2
L2(PZO) â‰¤âˆ¥fâˆ—âˆ’fâˆ—âˆ—KÎ³oâˆ¥2
L2(PXO)
â‰²âˆ¥fâˆ—âˆ’fâˆ—âˆ—KÎ³oâˆ¥2
L2(XÃ—O)
â‰²|fâˆ—|2
Bsx,so
2,q
(Rdx+do) Â· max{0, Î³2so
o
}
= |fâˆ—|2
Bsx,so
2,q
(Rdx+do)n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
The first inequality above holds because T is a bounded operator; the second inequality holds by
Assumption 2.2 that PXO admits a bounded density. The second last inequality above holds by
using Lemma D.5 for Î³ = (0, . . . , 0
| {z }
dx
, Î³o, . . . , Î³o
|
{z
}
do
).
For the third term, we note that (âˆ€o âˆˆRdo),
F[((fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ’(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ—KÎ³x) (Â·, o)]
is supported on the complement of {x : âˆ¥xâˆ¥â‰¤Î³âˆ’1
x } (see Eq.
(38)).
Thus it follows from
Assumption 4.3 that
âˆ¥T ((fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ’(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ—KÎ³x)âˆ¥2
L2(PZO)
â‰¤Î³2dxÎ·1
x
âˆ¥(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ’(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o âˆ—KÎ³xâˆ¥2
L2(PXO)
(i)
â‰²Î³2dxÎ·1+2sx
x
Â· |(fâˆ—âˆ’fâˆ—,low) âˆ—KÎ³o|2
Bsx,so
2,q
(Rdx+do)
(ii)
â‰¤Î³2dxÎ·1+2sx
x
Â· |fâˆ—âˆ—KÎ³o|2
Bsx,so
2,q
(Rdx+do)
(iii)
â‰¤Î³2dxÎ·1+2sx
x
Â· |fâˆ—|2
Bsx,so
2,q
(Rdx+do) âˆ¥KÎ³oâˆ¥2
L1(Rdo)
(iv)
â‰²n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
In the above derivations, (i) follows by using Lemma D.5 for Î³ = (Î³x, . . . , Î³x
|
{z
}
dx
, 0, . . . , 0
| {z }
do
) and the
Assumption that PXO admits a bounded density, (ii) follows from the proof of Lemma E.3, (iii)
follows from Lemma E.2, and (iv) follows by the fact that âˆ¥KÎ³oâˆ¥L1(R) = 1 [GinÃ© and Nickl, 2021,
Section 4.1.2] and Assumption 2.3 in the main text.
Combine the upper bound on the three terms in Eq. (81) and we obtain
Î»âˆ¥fÎ»âˆ¥2
HÎ³x,Î³o + âˆ¥T (fâˆ—âˆ’[fÎ»])âˆ¥2
L2(PZO) â‰¤n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
(82)
The proof concludes here. In addition to that, from Eq. (81), we also have
âˆ¥Tfâˆ—âˆ’T[faux]âˆ¥L2(PZO) â‰¤Cn
âˆ’
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) .
(83)
Also, if we follow the same derivations as above, we obtain
âˆ¥fâˆ—âˆ’[faux]âˆ¥L2(PXO) â‰¤Cn
âˆ’
sx
dx
1+(2+ do
so )( sx
dx +Î·1) .
(84)
64

D.3
Projected estimation error in Stage II
Proposition D.8. Suppose Assumptions 2.2, 2.3, 4.1, 4.2 and 4.3 in the main text hold. Let
Î» â‰nâˆ’1 and
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
With P n-probability â‰¥1 âˆ’4eâˆ’Ï„, for sufficiently large n â‰¥1, we have
T
  Â¯fÎ»

âˆ’[fÎ»]]
2
L2(PZO) â‰¤CÏ„ 2n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) (log n)dx+do+1,
for some constant C independent of n.
Proof. Recall the definition of Â¯hÎ» defined in Eq. (60) and hÎ» defined in Eq. (59).
Â¯hÎ» = arg min
hâˆˆHF O
Î»âˆ¥hâˆ¥2
HF O + 1
n
n
X
i=1
(h(zi, oi) âˆ’yi)2.
hÎ» = arg min
hâˆˆHF O
Î»âˆ¥hâˆ¥2
HF O + âˆ¥hâˆ—âˆ’hâˆ¥2
L2(PZO).
Recall that we have proved that âˆ¥T([ Â¯fÎ»] âˆ’[fÎ»])âˆ¥L2(PZO) = âˆ¥[Â¯hÎ»] âˆ’[hÎ»]âˆ¥L2(PZO) in Eq. (61), so the
proof of Proposition D.8 is translated to the estimation error of a standard kernel ridge regression
with hypothesis space HFO and the target function hâˆ—:= Tfâˆ—âˆˆL2(PZO). Next, we are going to
apply existing results, mainly Theorem 16 of Fischer and Steinwart [2020], to our setting.
From Eq. (61) and Fischer and Steinwart [2020][Lemma 12], we have
[Â¯hÎ»] âˆ’[hÎ»]

L2(PZO) â‰¤
C
1
2
FO
 Â¯hÎ» âˆ’hÎ»

HF O
=: (âˆ—)
(85)
Next, we will upper bound (âˆ—) using Theorem 16 from Fischer and Steinwart [2020]. To apply this
result, we must verify the underlying assumptions and control the auxiliary quantities specified in
Theorem 16 from Fischer and Steinwart [2020].
First, we are going to verify the (MOM) condition. By Assumption 4.5, we know that
Z
R
|y âˆ’(Tfâˆ—)(z, o)|m p(y | z, o)dy = E [|Ï…|m | Z = z, O = o]
â‰¤(
âˆš
2CÏƒ)m(m/2)
m
2 â‰¤1
2(2CÏƒ)mm!
by Vershynin [2018, Eq. (2.15)], for some universal constant C > 0. Hence the (MOM) condition
of Fischer and Steinwart [2020] is satisfied.
Next, we control the auxiliary quantities in Fischer and Steinwart [2020][Theorem 16]. Recall
mz, mo as defined in Eq. (19). By definition, they satisfy
do
2mo
<
1 + 2

sx
dx + Î·1

1 + 2

sx
dx + Î·1

+ do
so

sx
dx + Î·1
 < 1,
dz
2mz
â‰¤
do
2mo
< 1.
Rearranging, we deduce that
do
2mo
<
 
2mo
1
so ( sx
dx + Î·1)
1 + (2 + do
so )( sx
dx + Î·1) + 1
!âˆ’1
65

do
2mo
<
1 + (2 + do
so )( sx
dx + Î·1)
1 + 2( sx
dx + Î·1) + 2mo+do
so
( sx
dx + Î·1).
Hence there exists Î¸ âˆˆ(0, 1) such that the following inequalities hold simultaneously
do
2moÎ¸ < 1,
dz
2mzÎ¸ < 1
Î¸ â‰¤
1 + (2 + do
so )( sx
dx + Î·1)
1 + 2( sx
dx + Î·1) + 2mo+do
so
( sx
dx + Î·1)
Î¸
 
2mo
1
so ( sx
dx + Î·1)
1 + (2 + do
so )( sx
dx + Î·1) + 1
!
< 1
(86)
Next, we define
gÎ» := log

2eNFO(Î»)âˆ¥CFOâˆ¥+ Î»
âˆ¥CFOâˆ¥

AÎ»,Ï„ := 8
kÎ¸
FO

2
âˆÏ„gÎ»Î»âˆ’Î¸
LÎ» := max
n
L, âˆ¥hâˆ—âˆ’[hÎ»]âˆ¥Lâˆ(PZO)
o
(87)
Controlling gÎ» and AÎ»,Ï„:
From Proposition C.1, we know that
NFO(Î») â‰²(log n)dx+do+1 
Î³dx
x Î³do
o
âˆ’1
= (log n)dx+do+1n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
From Lemma D.10, we know that
âˆ¥CFOâˆ¥â‰¥aâˆ’1
f
âˆšÏ€
4
 dx+do
2
n
âˆ’1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Therefore, since Î» â‰nâˆ’1, we have
gÎ» = log
 2eNFO(Î»)
 1 + Î»âˆ¥CFOâˆ¥âˆ’1
â‰²log
ï£«
ï£¬
ï£­(log n)dx+do+1n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)
ï£«
ï£¬
ï£­1 + nâˆ’1 Â· n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)
ï£¶
ï£·
ï£¸
ï£¶
ï£·
ï£¸
â‰¤log
ï£«
ï£¬
ï£­2(log n)dx+do+1n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)
ï£¶
ï£·
ï£¸
= (dx + do + 1) log(log n) +
1 + do
so ( sx
dx + Î·1)
1 + (2 + do
so )( sx
dx + Î·1) log(n)
â‰¤2
1 + do
so ( sx
dx + Î·1)
1 + (2 + do
so )( sx
dx + Î·1) log(n).
66

The last step holds because log(n) dominates a constant term and log(log n) for sufficiently large n.
Since
do
2moÎ¸ < 1,
dz
2mzÎ¸ < 1, by Proposition C.2, we have
kÎ¸
FO

âˆâ‰²ÏÎ¸Î³âˆ’Î¸mo
o
.
(88)
Therefore,
AÎ»,Ï„ â‰²
kÎ¸
FO

2
âˆÏ„ log(n)nÎ¸ â‰²Î³âˆ’2moÎ¸
o
Ï„ log(n)nÎ¸ = Ï„ log(n)n
Î¸
 
2mo
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)+1
!
.
(89)
Since Î¸(2mo
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) + 1) < 1 from Eq. (86), n > AÎ»,Ï„ holds for sufficiently large n â‰¥1.
Controlling LÎ»:
By Eq. (82), we have
Î»âˆ¥fÎ»âˆ¥2
HÎ³x,Î³o â‰¤n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ¥2
Bsx,so
2,q
(Rdx+do).
Thus
âˆ¥fÎ»âˆ¥2
HÎ³x,Î³o â‰¤n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ¥2
Bsx,so
2,q
(Rdx+do).
(90)
Thus we have
LÎ» = max

2CÏƒ, âˆ¥T(fâˆ—âˆ’[fÎ»])âˆ¥Lâˆ(ZÃ—O)
	
â‰¤max

2CÏƒ, âˆ¥fâˆ—âˆ’[fÎ»]âˆ¥Lâˆ(XÃ—O)
	
â‰¤max

2CÏƒ, âˆ¥fâˆ—âˆ¥Lâˆ(XÃ—O) + âˆ¥[fÎ»]âˆ¥Lâˆ(XÃ—O)
	
(i)
â‰¤max

2CÏƒ, âˆ¥fâˆ—âˆ¥Lâˆ(XÃ—O) + âˆ¥fÎ»âˆ¥HÎ³x,Î³o
	
(ii)
â‰¤max
ï£±
ï£´
ï£²
ï£´
ï£³
2CÏƒ, âˆ¥fâˆ—âˆ¥Lâˆ(XÃ—O) + n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ¥Bsx,so
2,q
(Rdx+do)
ï£¼
ï£´
ï£½
ï£´
ï£¾
(iii)
â‰¤2n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ—âˆ¥Bsx,so
2,q
(Rdx+do),
(91)
where (i) holds by the reproducing property, (ii) holds by Eq. (90), and (iii) holds for sufficiently
large n.
Bounding (âˆ—):
Now we are ready to apply Fischer and Steinwart [2020][Theorem 16] to upper
bound (âˆ—). By Fischer and Steinwart [2020][Theorem 16], with P n-probability â‰¥1 âˆ’4eâˆ’Ï„, for
n â‰¥AÎ»,Ï„,
(âˆ—) =
C
1
2
F O
 Â¯hÎ» âˆ’hÎ»

2
HF O
â‰¤576Ï„ 2
n
 
Ïƒ2NF O(Î») +
kÎ¸
F O
2
âˆ
âˆ¥hâˆ—âˆ’[hÎ»]âˆ¥2
L2(PZO)
Î»Î¸
+ 2
kÎ¸
F O
2
âˆ
L2
Î»
nÎ»Î¸
!
(a)
â‰²Ï„ 2
n
ï£«
ï£­(log n)dx+do+1n
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) +
kÎ¸
F O
2
âˆnÎ¸

âˆ¥hâˆ—âˆ’[hÎ»]âˆ¥2
L2(PZO) + L2
Î»
n
ï£¶
ï£¸
67

(b)
â‰²Ï„ 2
ï£«
ï£­n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) (log n)dx+do+1 + Î³âˆ’2moÎ¸
o
nÎ¸âˆ’1 Â· n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)
ï£¶
ï£¸
(c)
â‰²Ï„ 2
ï£«
ï£­n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) (log n)dx+do+1 + n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1)
ï£¶
ï£¸
â‰²Ï„ 2n
âˆ’
2( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) (log n)dx+do+1.
We use Proposition C.1 to upper bound NFO(Î») in (a), we use Eq. (91) to upper bound LÎ»,
Eq. (88) to upper bound âˆ¥kÎ¸
FOâˆ¥âˆand Proposition D.7 to upper bound âˆ¥hâˆ—âˆ’[hÎ»]âˆ¥L2(PZO) =
âˆ¥Tfâˆ—âˆ’T[fÎ»]âˆ¥L2(PZO in (b), and we use the following fact in (c): by Eq. (86), we have
Î¸ â‰¤
1 + (2 + do
so )( sx
dx + Î·1)
1 + 2( sx
dx + Î·1) + 2mo+do
so
( sx
dx + Î·1)
â‡â‡’n
2moÎ¸
so
( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) nÎ¸âˆ’1 â‰¤1
â‡â‡’Î³âˆ’2moÎ¸
o
nÎ¸âˆ’1 â‰¤1.
The proof is concluded.
D.4
Auxiliary Lemmas for Section D
Lemma D.1. Let â„¦1, â„¦2 be two open sets in Rn1 and Rn2 respectively. Let H be a Hilbert space.
Then we have
MW m,p(â„¦1 Ã— â„¦2, H) âˆ¼= W m(â„¦1, W p(â„¦2, H)) âˆ¼= W m(â„¦1) âŠ—W p(â„¦2) âŠ—H
Proof. We adapt the proof of Aubin [2011][Theorem 12.7.2]. Let f âˆˆW m(â„¦1, W p(â„¦2, H)). Then
f âˆˆL2(â„¦1, W p(â„¦2, H)) such that âˆ‚Î±
1 f âˆˆL2(â„¦1, W p(â„¦2, H)) for |Î±| â‰¤m. As explained in HytÃ¶nen
et al. [2016][Definition 2.5.1], to say that âˆ‚Î±
1 f âˆˆL2(â„¦1, W p(â„¦2, H)) is equivalent to saying that for
every Ï• âˆˆCâˆ
c (â„¦1), we have
Z
â„¦1
f(x, Â·)âˆ‚Î±
1 Ï•(x) dx = (âˆ’1)|Î±|
Z
â„¦1
(âˆ‚Î±
1 f)(x, Â·)Ï•(x) dx
(92)
belongs to W p(â„¦2, H). But this exactly says that f âˆˆMW m,p(â„¦1 Ã—â„¦2, H). Thus the first isometric
isomorphism is proved. The second isomorphism is proved by two successive applications of Aubin
[2011, Theorem 12.7.1], namely the fact that the Hilbert-space valued Sobolev space W m(â„¦, H) is
isometrically isomorphic to the Hilbertian tensor product W m(â„¦) âŠ—H.
Lemma D.2. Fix Ï„ â‰¥1. Suppose that the assumptions of Proposition D.4 hold. Let Î» = 1
n. We
assume that n â‰¥1 satisfies n > AÎ»,Ï„ with AÎ»,Ï„ defined in Eq. (87) and Ëœn â‰¥1 satisfies Eq. (21).
Then, with P n+Ëœn-probability â‰¥1 âˆ’10eâˆ’Ï„
S0 â‰¤câ€²Ï„âˆšn
ï£«
ï£¬
ï£­
 Ë†FÎ¾ âˆ’Fâˆ—

G
âˆšn
+

h
Ë†FÎ¾
i
âˆ’Fâˆ—

L2(ZÃ—O;HX,Î³x)
ï£¶
ï£·
ï£¸
Â¯hÎ»

HF O ,
where câ€² is a constant independent of n, Ëœn. Under the same probabilistic event, the bounds in
Proposition D.4 hold.
68

Proof. We adapt the proof of Meunier et al. [2024a][Theorem 12].
S0 =
(CFO + Î»)1/2 
Ë†C Ë†FO + Î»
âˆ’1 
Ë†C Ë†FO âˆ’Ë†CFO
 
Ë†CFO + Î»
âˆ’1 1
nÎ¦âˆ—
FOY

HF O
â‰¤Î»âˆ’1/2
(CFO + Î»)1/2 
Ë†CFO + Î»
âˆ’1/2
|
{z
}
(A)
Â·


Ë†CFO + Î»
1/2 
Ë†C Ë†FO + Î»
âˆ’1/2
|
{z
}
(B)
Â·
 Ë†C Ë†FO âˆ’Ë†CFO

|
{z
}
(C)
Â·
Â¯hÎ»

HF O
Upper bound for term (A). Notice that all assumptions from Fischer and Steinwart [2020, Lemma
17] have been checked already in the analysis of projected estimation error in Section D.3. Since
n â‰¥AÎ»,Ï„ with AÎ»,Ï„ defined in Eq. (87), with P n-probability â‰¥1 âˆ’2eâˆ’Ï„,
(CFO + Î»)âˆ’1/2 
CFO âˆ’Ë†CFO

(CFO + Î»)âˆ’1/2 â‰¤4
kÎ¸
FO
2
âˆÏ„gÎ»
3nÎ»Î¸
+
s
2
kÎ¸
FO
2
âˆÏ„gÎ»
nÎ»Î¸
â‰¤2
3.
By Rudi et al. [2015, Proposition 7], we have
(A) â‰¤

1 âˆ’
(CFO + Î»)âˆ’1/2 
CFO âˆ’Ë†CFO

(CFO + Î»)âˆ’1/2
âˆ’1
2 â‰¤(1/3)âˆ’1
2 â‰¤2.
Upper bound for term (C). Define
ËœÎ¦ Ë†FO : HÎ³x,Î³o â†’Rn =
h
Ë†FÎ¾(z1, o1) âŠ—Ï•Î³o(o1)

, . . . ,

Ë†FÎ¾(zn, on) âŠ—Ï•Î³o(on)
iâˆ—
ËœÎ¦FO : HÎ³x,Î³o â†’Rn = [(Fâˆ—(z1, o1) âŠ—Ï•Î³o(o1)) , . . . , (Fâˆ—(zn, on) âŠ—Ï•Î³o(o1))]âˆ—.
An immediate consequence is that V ËœÎ¦âˆ—
Ë†FO = Î¦âˆ—
Ë†FO and V ËœÎ¦âˆ—
FO = Î¦âˆ—
FO for V defined following
Definition 12 and Î¦ Ë†FO, Î¦FO defined in Eq. (77). Hence, we have
(C) =
 Ë†C Ë†FO âˆ’Ë†CFO

=

1
nÎ¦âˆ—
FOÎ¦FO âˆ’1
nÎ¦âˆ—
Ë†FOÎ¦ Ë†FO

=

1
nV ËœÎ¦âˆ—
FO ËœÎ¦FOV âˆ—âˆ’1
nV ËœÎ¦âˆ—
Ë†FO ËœÎ¦ Ë†FOV âˆ—

=

1
n
ËœÎ¦âˆ—
FO ËœÎ¦FO âˆ’1
n
ËœÎ¦âˆ—
Ë†FO ËœÎ¦ Ë†FO

=

1
n
n
X
i=1

Ë†FÎ¾(zi, oi) âŠ—Ï•Î³o(oi)

âŠ—

Ë†FÎ¾(zi, oi) âŠ—Ï•Î³o(oi)

âˆ’1
n
n
X
i=1
(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi)) âŠ—(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi))
 .
69

The second last equality holds because V : HXO â†’HFO is an isometry. We start with the following
decomposition

Ë†FÎ¾(zi, oi) âŠ—Ï•Î³o(oi)

âŠ—

Ë†FÎ¾(zi, oi) âŠ—Ï•Î³o(oi)

âˆ’(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi)) âŠ—(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi))
=

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

âŠ—

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

+

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

âŠ—(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi))
+ (Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi)) âŠ—

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

.
Thus we have
(C) â‰¤

1
n
n
X
i=1

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

âŠ—

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

+ 2

1
n
n
X
i=1

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

âŠ—(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi))

â‰¤1
n
n
X
i=1


Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

âŠ—

Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

+ 2 1
n
n
X
i=1


Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

âŠ—Ï•Î³o(oi)

âŠ—(Fâˆ—(zi, oi) âŠ—Ï•Î³o(oi))

â‰¤1
n
n
X
i=1
 Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

2
HX,Î³x
+ 2
n
n
X
i=1
 Ë†FÎ¾(zi, oi) âˆ’Fâˆ—(zi, oi)

HX,Î³x
,
where in the last step we use âˆ¥a âŠ—bâˆ¥H1âŠ—H2 = âˆ¥aâˆ¥H1âˆ¥bâˆ¥H2 for a âˆˆH1, b âˆˆH2 for Hilbert spaces
H1, H2 [Gretton et al., 2005, Eq. (3)], âˆ¥Ï•Î³o(oi) âŠ—Ï•Î³o(oi)âˆ¥â‰¤1 and âˆ¥Fâˆ—(zi, oi)âˆ¥HX,Î³x â‰¤1. Note
that the last line is exactly analyzed in the proof of Meunier et al. [2024a, Lemma 7]. Their analysis
employs a Hoeffdingâ€™s concentration bounds with respect to Stage 2 samples D2, and conditioned
on Stage 1 samples D1 to show that, conditioned on D1,
(C) â‰¤J0
rÏ„
n
Fâˆ—âˆ’Ë†FÎ¾

G +
Fâˆ—âˆ’[ Ë†FÎ¾]

L2(ZÃ—O;HX,Î³x)

(93)
with P n-probability â‰¥1 âˆ’4eâˆ’Ï„, under the assumptions that
Fâˆ—âˆ’[ Ë†FÎ¾]

L2(ZÃ—O;HX,Î³x) âˆ¨
Fâˆ—âˆ’Ë†FÎ¾

G â‰¤1.
(94)
We note that the independence of D1 and D2 is implicitly used in this step to ensure that D2
remains i.i.d.
after conditioning on D1.
J0 is a constant independent of n, Ëœn.
Denote as D
the P Ëœn-probabilistic event that the bounds in Proposition D.4 hold (which has P Ëœn-probability
â‰¥1 âˆ’4eâˆ’Ï„). Under this event D along with the fact that Ëœn satisfies Eq. (21), Eq. (94) is satisfied
so Eq. (93) holds. Additionally, from Eq. (93), a sufficient condition for (C) â‰¤
1
6n is given by
Ëœn â‰¥(6JJ0Ï„
3
2 n)2 mâ€ +dâ€ /2+Î¶
mâ€ 
âˆ¨(36J2J2
0Ï„ 2n)
mâ€ +dâ€ /2+Î¶
mâ€ âˆ’1
, which is satisfied since Ëœn satisfies Eq. (21).
Upper bound for term (B). By Rudi et al. [2015, Proposition 7], we have (B) â‰¤(1 âˆ’t)âˆ’1
2 ,
where
t :=


Ë†CF O + Î»
âˆ’1
2 
Ë†CF O âˆ’Ë†C Ë†
F O
 
Ë†CF O + Î»
âˆ’1
2  â‰¤Î»âˆ’1  Ë†CF O âˆ’Ë†C Ë†
F O
 = n Â· (C) â‰¤1
6,
70

from where we have (B) â‰¤6
5. Under the event D, the upper bounds (A) â‰¤2, (B) â‰¤6/5,
(C) â‰¤1
6 hold simultaneously with P n-probability â‰¥1 âˆ’6eâˆ’Ï„. Since D holds with P Ëœn-probability
â‰¥1 âˆ’4eâˆ’Ï„, by independence of Stage 1 and Stage 2 samples (which is a consequence of the sample
splitting strategy), we have that the above upper bounds hold simultaneously with P n+Ëœn-probability
â‰¥(1 âˆ’6eâˆ’Ï„) (1 âˆ’4eâˆ’Ï„) â‰¥1 âˆ’10eâˆ’Ï„.
Lemma D.3. Fix Ï„ â‰¥1. Suppose that the assumptions of Proposition D.4 hold. Let Î» = 1
n. We
assume that n â‰¥1 satisfies n â‰¥AÎ»,Ï„ with AÎ»,Ï„ defined in Eq. (87) and Ëœn â‰¥1 satisfies Eq. (21).
Then, with P n+Ëœn-probability â‰¥1 âˆ’18eâˆ’Ï„
Sâˆ’1 â‰¤cÏ„âˆšn
ï£«
ï£¬
ï£­
 Ë†FÎ¾ âˆ’Fâˆ—

G
âˆšn
+

h
Ë†FÎ¾
i
âˆ’Fâˆ—

L2(ZÃ—O;HX,Î³x)
ï£¶
ï£·
ï£¸
Â¯hÎ»

HF O .
where c is a constant independent of Ëœn, n. Under the same probabilistic event, the bounds in
Proposition D.4 hold.
Proof. We omit the proof since it is similar to Meunier et al. [2024a, Theorem 11], with adaptations
similar to those in the proof of Lemma D.2.
Lemma D.4. Suppose the eigenvalues of the operator Î£1 satisfy Âµ1,i â‰iâˆ’1/p1 and the eigenvalues
of the operator Î£2 satisfy Âµ2,i â‰iâˆ’1/p2. Then the eigenvalues of their tensor product Î£1 âŠ—Î£2 satisfy,
for any Î¶ > 0,
Î»i(Î£1 âŠ—Î£2) â‰¤iâˆ’1/p1âˆ§1/p2+Î¶.
Proof. Suppose (Âµ1,i)iâˆˆN+ (resp. (Âµ2,j)jâˆˆN+) are the eigenvalues of Î£1 (resp. Î£2) with corresponding
eigenfunctions (q1,i)iâˆˆN+ (resp. (q2,j)jâˆˆN+). By definition of operator tensor product, we have
(Î£1 âŠ—Î£2) (q1,i âŠ—q2,j) = (Î£1q1,i) âŠ—(Î£2q2,j) = Âµ1,iÂµ2,j(q1,i âŠ—q2,j).
Therefore, for any i âˆˆN+, j âˆˆN+ we obtain that Âµ1,i Â· Âµ2,j is the eigenvalue of Î£1 âŠ—op Î£2 with
corresponding eigenfunction q1,i âŠ—q2,j. Therefore the eigenvalues of Î£1 âŠ—op Î£2 equal the tensor
product of two sequences (Âµ1,i)iâˆˆN+ and (Âµ2,j)jâˆˆN+.
Without loss of generality, we assume p1 â‰¤p2 so that Âµ1,i â‰¤Âµ2,i for i large enough. Denote
(Ïƒn)nâˆˆN+ as the tensor product of two sequences (Âµ2,i)iâˆˆN+ and (Âµ2,j)jâˆˆN+ rearranged in a non-
increasing order. Hence, we have Î»n(Î£1 âŠ—Î£2) â‰¤Ïƒn. From Krieg [2018], we have
Ïƒn â‰¤nâˆ’1/p2(log n)1/p2 â‰¤nâˆ’1/p2+Î¶,
for any Î¶ > 0. It concludes the proof.
Lemma D.5. Let X = [0, 1]d. Let f : Rd â†’R and f âˆˆBs
2,q(X) âˆ©L1(Rd) with s = [s1, . . . , sd]âŠ¤.
Recall KÎ³ defined in Eq. (66) with Î³ = [Î³1, . . . , Î³d]âŠ¤âˆˆ(0, 1]d. Then, we have
âˆ¥[KÎ³ âˆ—f] âˆ’fâˆ¥L2(X) â‰¤C0|f|Bs
2,q(X) Â· max{Î³s1
1 , . . . , Î³sd
d }
for some constant C0 that only depends on d, s.
71

Proof. Notice that
KÎ³ âˆ—f(x) =
Z
Rd
r
X
j=1
r
j

(âˆ’1)1âˆ’j 1
jd
 2
Ï€
 d
2
 d
Y
i=1
1
Î³i
!
exp
 
âˆ’2
d
X
i=1
(xi âˆ’ti)2
j2Î³2
i
!
f(t) dt
=
r
X
j=1
r
j

(âˆ’1)1âˆ’j 1
jd
 2
Ï€
 d
2
 d
Y
i=1
1
Î³i
! Z
Rd exp
 
âˆ’2
d
X
i=1
(xi âˆ’ti)2
j2Î³2
i
!
f(t) dt
=
r
X
j=1
r
j

(âˆ’1)1âˆ’j 1
jd
 2
Ï€
 d
2
 d
Y
i=1
1
Î³i
! Z
Rd exp
 
âˆ’2
d
X
i=1
h2
i
Î³2
i
!
f(x + jh)jd dh
=
r
X
j=1
r
j

(âˆ’1)1âˆ’j
 2
Ï€
 d
2 Z
Rd exp
 âˆ’2âˆ¥hâˆ¥2
f(x + jh âŠ™Î³) dh
=
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
ï£«
ï£­
r
X
j=1
r
j

(âˆ’1)1âˆ’jf(x + jh âŠ™Î³)
ï£¶
ï£¸dh.
Next, since
R
Rd
  2
Ï€
 d
2 exp
 âˆ’2âˆ¥hâˆ¥2
dh = 1, we have that
âˆ¥[KÎ³ âˆ—f] âˆ’fâˆ¥2
L2(X)
=
Z
X
ï£«
ï£­
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
ï£«
ï£­
r
X
j=0
r
j

(âˆ’1)1âˆ’jf(x + jh âŠ™Î³)
ï£¶
ï£¸dh
ï£¶
ï£¸
2
dx
=
Z
X
ï£«
ï£­
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
ï£«
ï£­
r
X
j=0
r
j

(âˆ’1)2r+1âˆ’jf(x + jh âŠ™Î³)
ï£¶
ï£¸dh
ï£¶
ï£¸
2
dx
=
Z
X
 Z
Rd(âˆ’1)r+1
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
âˆ†r
hâŠ™Î³f(x) dh
!2
dx.
Where the last step follows from the definition of modulus of smoothness in Eq. (9). Then, from
Cauchy-Schwarz inequality, we have
â‰¤
Z
X
 Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
dh
!  Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2 âˆ†r
hâŠ™Î³f(x)
2 dh
!
dx
=
Z
X
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2 âˆ†r
hâŠ™Î³f(x)
2 dh dx
=
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2 âˆ†r
hâŠ™Î³f
2
L2(X) dh.
(95)
Define N := min{Î³âˆ’s1
1
, . . . , Î³âˆ’sd
d
}. Since f âˆˆBs
2,q(X), we have
|f|Bs
2,q(X) â‰¥|f|Bs
2,âˆ(X) := sup
t

tâˆ’1Ï‰r,2

f, t
1
s1 , . . . , t
1
sd , X

â‰¥
  d
X
i=1
hsi
i
!
Nâˆ’1
!âˆ’1
Ï‰r,2
ï£«
ï£­f,
 d
X
i=1
hsi
i
! 1
s1
Nâˆ’1
s1 , . . . ,
 d
X
i=1
hsi
i
! 1
sd
N
âˆ’1
sd , X
ï£¶
ï£¸
72

â‰¥N
 d
X
i=1
hsi
i
!âˆ’1
Ï‰r,2 (f, h1Î³1, . . . , hdÎ³d, X)
â‰¥N
 d
X
i=1
hsi
i
!âˆ’1 âˆ†r
hâŠ™Î³f

L2(X) .
As a result, we have âˆ¥âˆ†r
hâŠ™Î³fâˆ¥L2(X) â‰¤|f|Bs
2,q(X)Nâˆ’1(Pd
i=1 hsi
i ).
Plugging the above back to
Eq. (95), we obtain
âˆ¥KÎ³ âˆ—f âˆ’fâˆ¥2
L2(X) â‰¤
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
 
|f|Bs
2,q(X)Nâˆ’1
 d
X
i=1
hsi
i
!!2
dh
â‰¤|f|2
Bs
2,q(X)Nâˆ’2d
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
 d
X
i=1
h2si
i
!
dh.
Notice that
C2
0 := d
Z
Rd
 2
Ï€
 d
2
exp
 âˆ’2âˆ¥hâˆ¥2
 d
X
i=1
h2si
i
!
dh = d
d
X
i=1
Z
R
 2
Ï€
 1
2
exp
 âˆ’2h2
i

h2si
i
dhi,
which is the sum of si-th moment of a one dimensional Gaussian distribution N(0, 1/4) from i = 1
to i = d. Finally, we obtain
âˆ¥KÎ³ âˆ—f âˆ’fâˆ¥2
L2(X) â‰¤C2
0|f|2
Bs
2,q(X)Nâˆ’2 = C2
0|f|2
Bs
2,q(X)
 max{Î³s1
1 , . . . , Î³sd
d }
2 .
Lemma D.6. Given a function f : Rd â†’R, we have
âˆ¥KÎ³ âˆ—fâˆ¥HX,Î³ â‰¤
 d
Y
i=1
Î³
âˆ’1
2
i
!
Ï€âˆ’d
4 (2r âˆ’1)âˆ¥fâˆ¥L2(Rd).
Proof. Following the same arguments in the proof of proposition 4 in Hang and Steinwart [2021],
we have KÎ³ âˆ—f âˆˆHX,Î³. For a function f : Rd â†’R, we define Ï„Î³f(x) = f(Î³ âŠ™x). We have, for all
x âˆˆRd,
Ï„Î³ (KÎ³ âˆ—f) (x) =
Z
Rd KÎ³ (Î³ âŠ™x âˆ’t) f(t) dt
=
Z
Rd K1
Î³ âŠ™x âˆ’t
Î³
  d
Y
i=1
1
Î³i
!
f(t) dt
=
Z
Rd K1

x âˆ’t
Î³
  d
Y
i=1
1
Î³i
!
f (t) dt
=
Z
Rd K1 (x âˆ’t) f (t âŠ™Î³) dt
= (K1 âˆ—(Ï„Î³f))(x).
73

Proposition 4.37 of Steinwart and Christmann [2008] can be generalized to anisotropic case which
shows that for any f âˆˆHX,Î³, Ï„Î³f âˆˆHX,1 and Ï„Î³ : HX,Î³ â†’HX,1 is an isometric isomorphism.
Hence we have
âˆ¥KÎ³ âˆ—fâˆ¥HX,Î³ = âˆ¥Ï„Î³ (KÎ³ âˆ—f)âˆ¥HX,1
= âˆ¥K1 âˆ—(Ï„Î³f)âˆ¥HX,1
â‰¤Ï€âˆ’d
4 (2r âˆ’1)âˆ¥Ï„Î³fâˆ¥L2(Rd)
=
 d
Y
i=1
Î³
âˆ’1
2
i
!
Ï€âˆ’d
4 (2r âˆ’1)âˆ¥fâˆ¥L2(Rd),
where the second last inequality follows from Eberts and Steinwart [2013][Theorem 2.3] setting
Î³ = 1.
Corollary D.1. Let f : Rdx+do â†’R and
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Recall KÎ³x : Rdx â†’R and KÎ³o : Rdo â†’R defined in Eq. (66), then we have
âˆ¥KÎ³x âˆ—KÎ³o âˆ—fâˆ¥HÎ³x,Î³o â‰¤C1n
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ¥L2(Rdx+do),
for some constant C1 that only depends on dx, do, sx, so.
Proof. The proof is a direct application of Lemma D.6.
Lemma D.7. Let Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) , Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) . Let Î¹x,Î³âˆ’1
x
: Rdx â†’R be an
indicator function over {Ï‰x : âˆ¥Ï‰xâˆ¥2 â‰¤Î³âˆ’1
x } and KÎ³o : Rdx â†’R be as defined in Eq. (66). Then we
have
âˆ¥Fâˆ’1[Î¹x,Î³âˆ’1
x ] âˆ—KÎ³o âˆ—fâˆ¥HÎ³x,Î³o â‰²2rn
1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) âˆ¥fâˆ¥L2(Rdx+do).
Proof. In contrast to the notation used in the rest of the document (with the exception of
Lemma D.9), in this proof, we use F to denote the unitary Fourier operator on L2(Rdx+do),
we use Fx to denote the unitary Fourier operator on L2(Rdx), and we use Fo to denote the unitary
Fourier operator on L2(Rdo). We let âŠ—: L2(Rdx) Ã— L2(Rdo) â†’L2(Rdx+do) denotes the tensor prod-
uct mapping, where (f âŠ—g)(x, o) = f(x)g(o) for all (x, o) âˆˆRdx+do. By considering a convergent
sequence to f1 in L1(Rdx) âˆ©L2(Rdx), and similarly for f2, we can show that
F[f âŠ—g] = Fx[f] âŠ—Fo[g]
for f âˆˆL2(Rdx) and g âˆˆL2(Rdo). At the start of Section D, we proved that for f âˆˆL1(Rdx+do),
f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ] âˆˆL2(Rdx+do). Since âˆ¥KÎ³oâˆ¥L1(Rdo) = 1 [GinÃ© and Nickl, 2021, Section 4.1.2], we have
âˆ¥(f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ]) âˆ—KÎ³oâˆ¥2
L2(Rdx+do)
(a)
=
Z
Rdx âˆ¥(f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ])(x, Â·) âˆ—KÎ³oâˆ¥2
L2(Rdo) dx
74

(b)
â‰¤
Z
Rdx âˆ¥(f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ])(x, Â·)âˆ¥2
L2(Rdo)âˆ¥KÎ³oâˆ¥2
L1(Rdo)
= âˆ¥f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ]âˆ¥2
L2(Rdx+do).
where in (a), (f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ])(x, Â·) is a slice function of any representative of f âˆ—Fâˆ’1[Î¹x,Î³âˆ’1
x ] âˆˆ
L2(Rdx), and (b) follows from Youngâ€™s Convolution Inequality. Hence we can apply F to f âˆ—
Fâˆ’1[Î¹x,Î³âˆ’1
x ] âˆˆL2(Rdx+do) and we find
F
h
Fâˆ’1
x [Î¹x,Î³âˆ’1
x ] âˆ—KÎ³o âˆ—f
i
= (Î¹x,Î³âˆ’1
x
âŠ—Fo[KÎ³o]) Â· F[f].
For Ï•(x, o) = exp(âˆ’Î³âˆ’2
x âˆ¥xâˆ¥2
2) Â· exp(âˆ’Î³âˆ’2
o âˆ¥oâˆ¥2
2), its Fourier transform is
F[Ï•](Ï‰x, Ï‰o) = Ï€dx/2+do/2Î³dx
x exp

âˆ’1
4Î³2
xâˆ¥Ï‰xâˆ¥2
2

Â· Î³do
o exp

âˆ’1
4Î³2
oâˆ¥Ï‰oâˆ¥2
2

.
Hence, by definition of Gaussian RKHS norm [Wendland, 2004, Theorem 10.12],
Fâˆ’1
x [Î¹x,Î³âˆ’1
x ] âˆ—KÎ³o âˆ—f

2
HÎ³x,Î³o
=
Z
Rdx+do

Î¹x,Î³âˆ’1
x (Ï‰x) Â· F[f](Ï‰x, Ï‰o) Â· Fo[KÎ³o](Ï‰o)
2
Ï€dx/2+do/2Î³dx
x exp
 âˆ’1
4Î³2xâˆ¥Ï‰xâˆ¥2
2

Â· Î³do
o exp(âˆ’1
4Î³2oâˆ¥Ï‰oâˆ¥2
2)
dÏ‰x dÏ‰o
â‰²Î³âˆ’dx
x
Î³âˆ’do
o
Z
Rdo
Z
{Ï‰x:âˆ¥Ï‰xâˆ¥2â‰¤Î³âˆ’1
x }
(F[f](Ï‰x, Ï‰o) Â· Fo[KÎ³o](Ï‰o))2
exp
 âˆ’1
4Î³2xâˆ¥Ï‰xâˆ¥2
2

Â· exp(âˆ’1
4Î³2oâˆ¥Ï‰oâˆ¥2
2) dÏ‰x dÏ‰o
â‰²Î³âˆ’dx
x
Î³âˆ’do
o
Z
Rdx+do exp
1
4Î³2
oâˆ¥Ï‰oâˆ¥2
2

(F[f](Ï‰x, Ï‰o) Â· Fo[KÎ³o](Ï‰o))2 dÏ‰x dÏ‰o.
Since KÎ³o(o) := Pr
j=1
 r
j

(âˆ’1)1âˆ’j
1
jdoÎ³do
o
  2
Ï€
 d
2 exp(âˆ’2jâˆ’2Î³âˆ’2
o âˆ¥oâˆ¥2
2), so we have
Fo[KÎ³o](Ï‰o) =
r
X
j=1
r
j

(âˆ’1)1âˆ’j exp

âˆ’1
8j2Î³2
oâˆ¥Ï‰oâˆ¥2
2

.
Consequently, we have
Fâˆ’1[Î¹x,Î³âˆ’1
x ] âˆ—KÎ³o âˆ—f

2
HÎ³x,Î³o
â‰²Î³âˆ’dx
x
Î³âˆ’do
o
Z
Rdx+do

F[f](Ï‰x, Ï‰o) Â· Pr
j=1
 r
j

(âˆ’1)1âˆ’j exp(âˆ’1
8j2Î³2
oâˆ¥Ï‰oâˆ¥2
2)
2
exp(âˆ’1
4Î³2oâˆ¥Ï‰oâˆ¥2
2)
dÏ‰xdÏ‰o
â‰¤Î³âˆ’dx
x
Î³âˆ’do
o
22r
Z
Rdx+do (F[f](Ï‰x, Ï‰o))2 dÏ‰xdÏ‰o
= Î³âˆ’dx
x
Î³âˆ’do
o
22râˆ¥fâˆ¥2
L2(Rdx+do).
where the last equality follows by Plancherelâ€™s Theorem.
Lemma D.8. Let Î³ = (Î³1, . . . , Î³d) âˆˆ(0, 1)d. Let kÎ³ be the anisotropic Gaussian kernel on Rd and
let HX,Î³ be the RKHS associated with kÎ³. Then
Z
Rd |F[f](Ï‰)|2 exp
âˆ¥Ï‰ âŠ™Î³âˆ¥2
2
4

dÏ‰ = 2dÏ€d/2
 d
Y
i=1
Î³i
!
âˆ¥fâˆ¥2
HX,Î³
75

Proof. We define Ï•X,Î³(x) = exp(âˆ’Pd
j=1 Î³âˆ’2
j x2
j). Then
F[Ï•X,Î³](Ï‰) =
Z
Rd Ï•X,Î³(x) exp(âˆ’iâŸ¨x, Ï‰âŸ©)dx
=
 d
Y
i=1
Î³i
! Z
Rd Ï•X,1(x) exp (âˆ’i âŸ¨x, Ï‰ âŠ™Î³âŸ©) dx
= Ï€d/2
 d
Y
i=1
Î³i
!
exp

âˆ’âˆ¥Ï‰ âŠ™Î³âˆ¥2
2
4

where the last step follows from Wendland [2004][Theorem 5.20]. By Kanagawa et al. [2018][Theorem
2.4], we have
âˆ¥fâˆ¥2
HX,Î³ = (2Ï€)âˆ’d
Z
Rd
|F[f](Ï‰)|2
F[Ï•X,Î³](Ï‰)dÏ‰
= (2Ï€)âˆ’dÏ€d/2
Z
Rd |F[f](Ï‰)|2
 d
Y
i=1
Î³i
!âˆ’1
exp
âˆ¥Ï‰ âŠ™Î³âˆ¥2
2
4

dÏ‰
= 2âˆ’dÏ€âˆ’d/2
 d
Y
i=1
Î³i
!âˆ’1 Z
|F[f](Ï‰)|2 exp
âˆ¥Ï‰ âŠ™Î³âˆ¥2
2
4

dÏ‰
whence the Lemma follows.
Lemma D.9. Suppose that Assumption 4.2 in the main text holds, and suppose Î³Î´x
x = Î³Î´o
o for some
positive constant Î´x, Î´o. Suppose f âˆˆHÎ³x,Î³o with âˆ¥fâˆ¥2
HÎ³x,Î³o â‰¤Cn for some constant C > 0. Then,
âˆ¥fâˆ¥L2(PXO) â‰²Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO)
+ Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 Î³
1
8 Î´x
x
(nÎ³dx
x Î³do
o )
1
2 .
(96)
In particular, for
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
and 1
8Î´x = 2sx + dxÎ·1 + dxÎ·0, Î´x = Î´o dx
so ( sx
dx + Î·1). Then, we have
âˆ¥fâˆ¥L2(PXO) â‰²Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO) + n
âˆ’
sx
dx
1+(2+ do
so )( sx
dx +Î·1) .
Proof. Define the following sets
Ix :=
n
Ï‰x âˆˆRdx : âˆ¥Ï‰xâˆ¥2
2Î³2
x(log(Î³âˆ’Î´x
x
))âˆ’1 â‰¤1
o
Io :=
n
Ï‰o âˆˆRdo : âˆ¥Ï‰oâˆ¥2
2Î³2
o(log(Î³âˆ’Î´o
o
))âˆ’1 â‰¤1
o
I := Ix Ã— Io
ËœI :=
n
(Ï‰x, Ï‰o) âˆˆRdx+do : âˆ¥Ï‰xâˆ¥2
2Î³2
x(log(Î³âˆ’Î´x
x
))âˆ’1 + âˆ¥Ï‰oâˆ¥2Î³2
o(log(Î³âˆ’Î´o
o
))âˆ’1 â‰¤1
o
.
76

Note that ËœI âŠ‚I. Let Î¹x : Rdx â†’{0, 1} denote the indicator function on Ix, let Î¹o : Rdo â†’{0, 1}
denote the indicator function on Io, let Î¹ : Rdx+do â†’{0, 1} denote the indicator function on I,
which satisfies Î¹ = Î¹x Â· Î¹o. Thus we have
âˆ¥fâˆ¥L2(PXO)
â‰¤
f âˆ—Fâˆ’1(Î¹)

L2(PXO) +
f âˆ—Fâˆ’1(Î¹) âˆ’f

L2(PXO)
(i)
â‰¤Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 âˆ¥T(f âˆ—Fâˆ’1(Î¹))âˆ¥L2(PZO) +
f âˆ—Fâˆ’1(Î¹) âˆ’f

L2(PXO)
(ii)
â‰¤Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO)
+

1 + Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2
 f âˆ—Fâˆ’1(Î¹) âˆ’f

L2(PXO)
â‰²Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO) + Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2
f âˆ—Fâˆ’1(Î¹) âˆ’f

L2(XÃ—O) .
(97)
In the above derivations,
â€¢ (i) follows from Assumption 4.2, and for Fx, Fo defined in the proof of Lemma D.7,
(âˆ€o âˆˆO), supp(Fx[(f âˆ—Fâˆ’1(Î¹))(Â·, o)]) = supp(Fx[(f âˆ—Fâˆ’1
o [Î¹o])(Â·, o)] Â· Î¹x) âŠ†Ix,
where we note that the relevant Fourier transforms exist since f âˆˆHÎ³x,Î³o âŠ‚L2(Rdx+do).
â€¢ (ii) follows from a triangular inequality and a Jensenâ€™s inequality.
Next, we bound (âˆ—) = âˆ¥f âˆ—Fâˆ’1(Î¹) âˆ’fâˆ¥L2([0,1]dx+do). We have
(âˆ—) =
f âˆ—Fâˆ’1(Î¹) âˆ’f

L2([0,1]dx+do) â‰¤
f âˆ—Fâˆ’1(Î¹) âˆ’f

L2(Rdx+do)
(a)
=
F

f âˆ—Fâˆ’1(Î¹) âˆ’f

L2(Rdx+do) = âˆ¥F[f]Î¹ âˆ’F[f]âˆ¥L2(Rdx+do)
=
Z
Ic |F[f](Ï‰)|2 dÏ‰
 1
2 (b)
â‰¤
Z
ËœIc |F[f](Ï‰)|2 dÏ‰
 1
2
,
where (a) holds by Plancherelâ€™s Theorem, (b) holds by ËœI âŠ†I. Recall that for a, b âˆˆRd, a âŠ™b :=
(a1b1, . . . , adbd) âˆˆRd. For Ï‰ = (Ï‰x, Ï‰o), we proceed from above to have,
â‰¤
Z
ËœIc |F[f](Ï‰)|2 exp
âˆ¥(Ï‰x âŠ™Î³x, Ï‰o âŠ™Î³o)âˆ¥2
2
4

exp

âˆ’âˆ¥(Ï‰x âŠ™Î³x, Ï‰o âŠ™Î³o)âˆ¥2
2
4

dÏ‰
 1
2
â‰¤sup
Ï‰âˆˆËœIc
exp

âˆ’âˆ¥(Ï‰x âŠ™Î³x, Ï‰o âŠ™Î³o)âˆ¥2
2
8
 Z
Rdx+do
|F[f](Ï‰)|2 exp
âˆ¥(Ï‰x âŠ™Î³x, Ï‰o âŠ™Î³o)âˆ¥2
2
4

dÏ‰
 1
2
(c)
â‰¤sup
Ï‰âˆˆËœIc
exp

âˆ’âˆ¥(Ï‰x âŠ™Î³x, Ï‰o âŠ™Î³o)âˆ¥2
2
8
 
âˆ¥fâˆ¥2
HXO,Î³x,Î³o2âˆ’dx+do
2
Î³dx
x Î³do
o
 1
2
â‰²
sup
âˆ¥Ï‰xâˆ¥2
2Î³2x(log(Î³âˆ’Î´x
x
))âˆ’1+âˆ¥Ï‰oâˆ¥2Î³2o(log(Î³âˆ’Î´o
o
))âˆ’1â‰¥1
exp

âˆ’Î³2
xâˆ¥Ï‰xâˆ¥2
2 + Î³2
oâˆ¥Ï‰oâˆ¥2
2
8
  nÎ³dx
x Î³do
o
 1
2
â‰¤exp

âˆ’1
8 log(Î³âˆ’Î´x
x
)
  nÎ³dx
x Î³do
o
 1
2
â‰Î³
1
8 Î´x
x
 nÎ³dx
x Î³do
o
1/2 ,
where (c) holds by Lemma D.8. Now we have proved the first claim of this lemma. Next, we
proceed to prove the second claim by plugging in the specific values of Î´x, Î´o and Î³x, Î³o. Recall
77

that 1
8Î´x = 2sx + dxÎ·1 + dxÎ·0 + dx, we obtain
(âˆ—) â‰²n
âˆ’
Î´x
8
1
dx
1+(2+ do
so )( sx
dx +Î·1) Â· n
sx
dx +Î·1
1+(2+ do
so )( sx
dx +Î·1) = n
âˆ’sx
dx âˆ’Î·0âˆ’1
1+(2+ do
so )( sx
dx +Î·1) .
We plug it back to Eq. (97) and obtain
âˆ¥fâˆ¥L2(PXO) â‰²Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO)
+ Î³âˆ’dxÎ·0
x
(log(Î³âˆ’Î´x
x
))
dxÎ·0
2 n
âˆ’sx
dx âˆ’Î·0âˆ’1
1+(2+ do
so )( sx
dx +Î·1)
â‰²Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO) + n
âˆ’
sx
dx +1
1+(2+ do
so )( sx
dx +Î·1) (log n)
dxÎ·0
2
â‰²Î³âˆ’dxÎ·0
x
(log n)
dxÎ·0
2 âˆ¥Tfâˆ¥L2(PZO) + n
âˆ’
sx
dx
1+2( sx
dx +Î·1)+ do
so ( sx
dx +Î·1) .
So the proof is concluded.
Lemma D.10. Suppose Assumption 2.2 hold, and let
Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Then, for sufficiently large n â‰¥1, we have
âˆ¥CFOâˆ¥â‰¥a
âˆšÏ€
4
 dx+do
2
n
âˆ’1
2
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) .
Proof. By definition of the operator norm, we have
âˆ¥CFOâˆ¥2
=
sup
âˆ¥fâˆ¥HÎ³x,Î³o =1
âŸ¨f, E[(Fâˆ—(Z, O) âŠ—Ï•Î³o(O)) âŠ—(Fâˆ—(Z, O) âŠ—Ï•Î³o(O))]fâŸ©HÎ³x,Î³o
=
sup
âˆ¥fâˆ¥HÎ³x,Î³o =1
E
h
(E[f(X, O)|Z, O])2i
â‰¥
sup
âˆ¥fâˆ¥HÎ³x,Î³o =1
(EZOâˆ¼PZO [E[f(X, O)|Z, O]])2
=
sup
âˆ¥fâˆ¥HÎ³x,Î³o =1
E[f(X, O)]2
=âˆ¥ÂµXOâˆ¥2
HÎ³x,Î³o,
where ÂµXO := EPXO[Ï•Î³x(X) âŠ—Ï•Î³o(O)]. Notice that
âˆ¥ÂµXOâˆ¥2
HÎ³x,Î³o
=
ZZ
OÃ—X
Ï•Î³x(x) âŠ—Ï•Î³o(o)pXO(x, o) dx do,
ZZ
OÃ—X
Ï•Î³x(xâ€²) âŠ—Ï•Î³o(oâ€²)pXO(xâ€², oâ€²) dxâ€² doâ€²

HÎ³x,Î³o
78

=
ZZ
OÃ—X
ZZ
OÃ—X
KÎ³o(o, oâ€²)KÎ³x(x, xâ€²)pXO(x, o)pXO(xâ€², oâ€²) dx dxâ€² do doâ€²
(a)
â‰¥a2
Z
[1/4,3/4]do
Z
[1/4,3/4]do KÎ³o(o, oâ€²) do doâ€²
Z
[1/4,3/4]dx
Z
[1/4,3/4]dx KÎ³x(x, xâ€²) dx dxâ€²
(b)
= a2
ï£«
ï£­Î³x
âˆšÏ€
ï£®
ï£°
erf

1
2Î³x

2
+ Î³x
exp

âˆ’1
4Î³2x

âˆ’1
âˆšÏ€
ï£¹
ï£»
ï£¶
ï£¸
dx
Â·
ï£«
ï£­Î³o
âˆšÏ€
ï£®
ï£°
erf

1
2Î³o

2
+ Î³o
exp

âˆ’1
4Î³2o

âˆ’1
âˆšÏ€
ï£¹
ï£»
ï£¶
ï£¸
do
,
where erf(x) :=
1
âˆšÏ€
R x
âˆ’x exp
 âˆ’t2
dt is the standard error function of normal distribution. Step (a)
holds by Assumption 2.2 and step (b) holds by using Lemma D.11. We plug in Î³x = n
âˆ’
1
dx
1+(2+ do
so )( sx
dx +Î·1) ,
Î³o = n
âˆ’
1
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) to obtain
âˆ¥ÂµXOâˆ¥2
HÎ³x,Î³o
(i)
â‰¥a2  âˆšÏ€Î³x/4
dx  âˆšÏ€Î³o/4
do = a2(âˆšÏ€/4)dx+don
âˆ’
1+ do
so ( sx
dx +Î·1)
1+(2+ do
so )( sx
dx +Î·1) ,
where (i) holds for sufficiently large n â‰¥1 because erf(x) â‰¥1
2 when x â‰¥1.
Lemma D.11. Let kÎ³ : Rd Ã— Rd â†’R be the Gaussian kernel with length scale Î³ âˆˆ(0, 1]. Then we
have,
Z
[1/4,3/4]d
Z
[1/4,3/4]d kÎ³(x, xâ€²) dxdxâ€² =
ï£«
ï£­Î³âˆšÏ€
ï£®
ï£°
erf

1
2Î³

2
+ Î³
exp

âˆ’1
4Î³2

âˆ’1
âˆšÏ€
ï£¹
ï£»
ï£¶
ï£¸
dx
.
Here, erf(x) :=
1
âˆšÏ€
R x
âˆ’x exp
 âˆ’t2
dt is the standard error function.
Proof. Since kÎ³(x, xâ€²) = Qd
i=1 exp

âˆ’(xiâˆ’xâ€²
i)2
Î³2

, it suffices to prove the following result
Z 3/4
1/4
Z 3/4
1/4
kÎ³(xi, xâ€²
i) dxidxâ€²
i = Î³2âˆšÏ€
ï£®
ï£°
erf

1
2Î³

2Î³
+
exp

âˆ’1
4Î³2

âˆ’1
âˆšÏ€
ï£¹
ï£».
Notice that
Z 3/4
1/4
exp

âˆ’(xi âˆ’xâ€²
i)2
Î³2

dxi
= Î³
Z (3/4âˆ’xâ€²
i)/Î³
(1/4âˆ’xâ€²
i)/Î³
exp(âˆ’x2) dx
= Î³
Z (3/4âˆ’xâ€²
i)/Î³
âˆ’âˆ
exp(âˆ’x2) dx âˆ’Î³
Z (1/4âˆ’xâ€²
i)/Î³
âˆ’âˆ
exp(âˆ’x2) dx
= Î³
âˆšÏ€
2

1 + erf
3/4 âˆ’xâ€²
i
Î³

âˆ’Î³
âˆšÏ€
2

1 âˆ’erf
xâ€²
i âˆ’1/4
Î³

79

= Î³
âˆšÏ€
2

erf
3/4 âˆ’xâ€²
i
Î³

+ erf
xâ€²
i âˆ’1/4
Î³

.
Therefore, we have
Z 3/4
1/4
Z 3/4
1/4
exp

âˆ’(xi âˆ’xâ€²
i)2
Î³2

dxi dxâ€²
i
= Î³
âˆšÏ€
2
Z 3/4
1/4
erf
3/4 âˆ’xâ€²
i
Î³

+ erf
xâ€²
i âˆ’1/4
Î³

dxâ€²
i
= Î³2âˆšÏ€
Z
1
2Î³
0
erf(y) dy
= Î³2âˆšÏ€
ï£®
ï£°
erf

1
2Î³

2Î³
+
exp

âˆ’1
4Î³2

âˆ’1
âˆšÏ€
ï£¹
ï£».
where the last inequality holds by using the identity
Z
erf(x)dx = x Â· erf(x) + eâˆ’x2/âˆšÏ€ + C.
E
Proof of Theorem 4.2 in the main text
E.1
Relationship Between the NPIR-O Model and the NPIV-O Model
Following Chen and Reiss [2011, Section 3], we first establish that the NPIV-O model is no more
informative than the reduced form nonparametric indirect regression with observed confounders
(NPIR-O) model.
Definition 16 (Restricted NPIV-O model). Let Ïƒ0 > 0 be a finite constant. Recall S as defined
in Assumption 2.3 in the main text. Let C be a set of elements (PÏµZXO, f) such that the following
property holds: (âˆ€f âˆˆS) (âˆƒPÏµZXO, f) âˆˆC such that PZY is determined by PÏµZX and f, and that
Yi âˆ’E[Yi | Z = zi, O = oi] = f(zi, oi) + Ïµi âˆ’(Tf)(zi, oi)
given Zi = zi, Oi = oi is N(0, Ïƒ2(zi, oi))-distributed with Ïƒ2(zi, oi) â‰¥Ïƒ2
0.
For an NPIV-O model as defined in Definition 16, we specify the reduced form NPIR-O model
as
Yi = (Tf)(Zi, Oi) + Ï…i,
i = 1, . . . , n
with (Zi, Oi, Ï…i) i.i.d., PÏ…i|Zi=zi,Oi=oi = N(0, Ïƒ2(zi, oi)), f âˆˆS the unknown structural function,
and T : L2(PXO) â†’L2(PZO) a known operator satisfying Assumption 4.1. The observations
corresponding to the NPIR are {(Yi, zi, oi)}n
i=1.
Definition 17 (NPIR-O model class). Let C be as defined in Definition 16. The NPIR-O model
class C0 consists of all model parameters (PZâ€²Oâ€², Ïƒ(Â·, Â·), f) such that (âˆƒ(PÏµZX, f) âˆˆC) with the
following properties: PZO = PZâ€²Oâ€², Ïƒ2(z, o) â‰¥Ïƒ2
0 > 0, the conditional law PX|Z,O is prescribed
according to T, and PÏµ|ZOX is arbitrary among the conditions imposed in C.
80

The following Lemma is Chen [2007, Lemma 1], by augmenting relevant variables to include
observed confounders O.
Lemma E.1. The NPIR-O model is more informative than the NPIV-O model in the sense that
for each estimator Ë†fn for the NPIV-O model, there is an estimator Ëœfn for the NPIR-O model with
sup
(PZO,Ïƒ(Â·,Â·),f)âˆˆC0
E(PZO,Ïƒ(Â·,Â·),f)[âˆ¥Ëœfn âˆ’fâˆ¥2
L2(PXO)] â‰¤
sup
(PÏµZXO,f)âˆˆC
E(PÏµZXO,f)[âˆ¥Ë†fn âˆ’fâˆ¥2
L2(PXO)].
In this section, we provide a lower bound for the NPIR-O model class defined in Definition 17,
which by the above discussion implies a lower bound for the (restricted) NPIV-O model class
defined in Definition 16.
E.2
The Lower Bound for NPIR-O Model
Step One. We take m to be the smallest even integer such that m > sx âˆ¨so. To help us construct
fv, we need to introduce several functions. We define
MK,âˆ’m
2 (x) :=
dx
Y
i=1
Î¹m

2
j
K s
sx
k
xi + m
2

,
MK,â„“o(o) :=
do
Y
j=1
Î¹m

2
j
K s
so
k
oi âˆ’â„“o,j

.
(98)
Define
L :=
(
(â„“x, â„“o) : â„“x âˆˆ

0, 1, . . . ,
0.8Ï€
Î¶
2
Ks
sx
dx
, â„“o âˆˆ

mZ âˆ©

1, . . . , 2
j
K s
so
kdo)
,
(99)
then we can compute the size of L as
|L| â‰Î¶âˆ’dx2Ks

dx
sx + do
so

(100)
For (â„“x, â„“o) âˆˆL, define 1â„“x as an indicator function of the set
Iâ„“x :=
dxÃ—
j=1
h
1.1Ï€ + Î¶â„“x,j2âˆ’K s
sx , 1.1Ï€ + (Î¶â„“x,j + 1)2âˆ’K s
sx
i
,
(101)
Next, we define
â„¦K,â„“x(x) :=

M0,âˆ’m
2 âˆ—Fâˆ’1[1â„“x]
 
2
Ks
sx x

,
â„¦K(â„“x,â„“o)(x, o) := â„¦K,â„“x(x)MK,â„“o(o).
(102)
Now we are ready to define fv. Note that a vector v âˆˆ{0, 1}|L| canonically associates to each point
(â„“x, â„“o) a value Î²v(â„“x,â„“o) âˆˆ{0, 1}. We define
fv := Ïµ02âˆ’Ks

1âˆ’dx
2sx
 X
â„“x,â„“o
Î²v(â„“x,â„“o)â„¦K(â„“x,â„“o),
(103)
where Ïµ0 > 0 is a fixed scaler to be chosen later, to ensure that âˆ¥fvâˆ¥Bsx,so
2,âˆ(Rdx+do) â‰¤1, thus ensuring
that fv satisfies Assumption 2.3. The function class F := {fv, v âˆˆ{0, 1}|L|}. For each fv, consider
the joint data generating distribution PZXOY specified as follows:
1. The marginal distribution PZO is the product of independent distributions PZ and PO, where
both are uniform distributions on [0, 1]dz and [0, 1]do.
81

2. The marginal distribution PXO is the product of independent distributions PX and PO, where
PO is the uniform distribution on [0, 1]do, and PX is supported on [âˆ’1/2, 1/2]dx and admits
the following density function:
pX(x) âˆ
dx
Y
i=1
g(xi),
g(xi) := exp

âˆ’
2
1 âˆ’4x2
i

1xiâˆˆ[âˆ’1/2,1/2].
(104)
3. The conditional distribution PX|Z,O satisfies Assumption 2.2 and it induces an operator
T : L2 (PXO) â†’L2 (PZO) that satisfies Assumption 4.3.
4. The conditional distribution PY |Z=z,O=o is a Gaussian distribution N((Tfv)(z, o), Ïƒ2) for
any z âˆˆZ and o âˆˆO.
Step Two. Now we are going to present some properties of the basis â„¦K(â„“x,â„“o), which will be used
later on to prove some properties of fv. By Youngâ€™s Convolution Theorem, we have
M0,âˆ’m
2 âˆ—Fâˆ’1[1â„“x]

L2(Rdx) â‰¤âˆ¥M0,âˆ’m
2 âˆ¥L1(Rdx)âˆ¥1â„“xâˆ¥L2(Rdx) < âˆ,
hence its Fourier transform is well-defined. We have
F[â„¦K,â„“x](Ï‰x) = 2âˆ’K sdx
sx F
h
M0,âˆ’m
2 âˆ—Fâˆ’1[1â„“x]
i 
2âˆ’Ks
sx Ï‰x

= 2âˆ’K sdx
sx Â· F
h
M0,âˆ’m
2
i 
2âˆ’Ks
sx Ï‰x

Â· 1â„“x

2âˆ’Ks
sx Ï‰x

= F
h
MK,âˆ’m
2
i
(Ï‰x) Â· 1â„“x

2âˆ’Ks
sx Ï‰x

.
(105)
Also, since F[Î¹m](Ï‰) = exp (âˆ’miÏ‰/2) Â· sin(Ï‰/2)m Â· (Ï‰/2)âˆ’m, we have
F
h
M0,âˆ’m
2
i
(Ï‰x) =
dx
Y
i=1
sin(Ï‰i,x/2)m
(Ï‰i,x/2)m .
(106)
Now we can see that we pick the location vector to be âˆ’m
2 such that the Fourier transform of
M0,âˆ’m
2 is a real valued function. Note that since Î¶ â‰¥1, we have (Î¶â„“x,j +1)2âˆ’K s
sx â‰¤Î¶(â„“x,j +1)2âˆ’K s
sx
for j âˆˆ{1, . . . , dx}, hence Iâ„“x âˆ©Iâ„“â€²x = âˆ…if â„“x Ì¸= â„“â€²
x which means that the support of F[â„¦K,â„“x] is
disjoint for â„“x Ì¸= â„“â€²
x. Also, note that
supp(MK,â„“o) =
doÃ—
j=1
[â„“o,j2âˆ’K s
so , (â„“o,j + m)2âˆ’K s
so ]
and â„“o,j are multiples of m by definition of L in Eq. (99), we have supp(MK,â„“o) âˆ©supp(Mk,â„“â€²o) = âˆ…
for any â„“o Ì¸= â„“â€²
o.
Step Three. In this step, we are going to prove that fv âˆˆBsx,so
2,âˆ(Rdx+do) and its Besov norm is
bounded by Ïµ0. We have, (âˆ€o âˆˆO),
F[fv(Â·, o)](Ï‰x) = Ïµ02âˆ’Ks

1âˆ’dx
2sx
 X
â„“x,â„“o
Î²v(â„“x,â„“o)F[â„¦K(â„“x,â„“o)(Â·, o)](Ï‰x)
= Ïµ02âˆ’Ks

1âˆ’dx
2sx
 X
â„“x,â„“o
Î²v(â„“x,â„“o)F[â„¦K,â„“x](Ï‰x) Â· MK,â„“o(o)
82

= Ïµ02âˆ’Ks

1âˆ’dx
2sx
 X
â„“x,â„“o
Î²v(â„“x,â„“o)2âˆ’K sdx
sx F[M0,âˆ’m
2 ]

2âˆ’K s
sx Ï‰x

Â· 1â„“x

2âˆ’K s
sx Ï‰x

Â· MK,â„“o(o)
= Ïµ02âˆ’Ks

1âˆ’dx
2sx
 X
â„“o
F[MK,âˆ’m
2 ](Ï‰x) Â· MK,â„“o(o) Â·
ï£«
ï£­X
â„“x
Î²v(â„“x,â„“o)1â„“x

2âˆ’K s
sx Ï‰x

ï£¶
ï£¸.
(107)
Note that, for fixed â„“o, Ï‰x 7â†’P
â„“x Î²v(â„“x,â„“o)1â„“x(2âˆ’K s
sx Ï‰x) is the indicator function on
[
â„“x:Î²v(â„“x,â„“o)>0
dxÃ—
j=1
h
1.1Ï€2K s
sx + Î¶â„“x,j, 1.1Ï€2K s
sx + (Î¶â„“x,j + 1)
i
,
The above observation as applied to the right hand side of Eq. (107) means that we can apply
Lemma E.3 to obtain
âˆ¥fvâˆ¥Bsx,so
2,q
(Rdx+do) â‰²Ïµ02âˆ’Ks

1âˆ’dx
2sx

Â· 2Ks

1âˆ’dx
2sx

= Ïµ0.
(108)
Hence we can choose Ïµ0 to be a small scalar such that âˆ¥fvâˆ¥Bsx,so
2,q
(Rdx+do) â‰¤1. The rest of the
conditions that fv âˆˆLâˆ(Rdx+do) âˆ©L1(Rdx+do) âˆ©C0(Rdx+do) are all trivial to verify. Therefore, fv
satisfies Assumption 2.3.
Step Four. By the Gilbert-Varshamov Bound Tsybakov [2008][Lemma 2.9], for |L| â‰¥8, there exists
a subset VK of {0, 1}|L| such that 0 âˆˆVK and
X
â„“x,â„“o
Î²v(â„“x,â„“o) âˆ’Î²vâ€²(â„“x,â„“o)
2 â‰¥|L|
8
(109)
for v Ì¸= vâ€² âˆˆVK and |VK| â‰¥2
|L|
8 . Note that since âˆšpX is an even function over [âˆ’1/2, 1/2]dx, its
Fourier transform q is a real valued function. Define
q(Ï‰x) := F[âˆšpX](Ï‰x),
Ëœq(Ï‰x) := F[âˆšpX](Ï‰x)1[âˆ’Î¶/3,Î¶/3]dx(Ï‰x).
(110)
For coefficients Î±v(â„“x,â„“o) âˆˆR, we have

X
â„“x,â„“o
Î±v(â„“x,â„“o)â„¦K(â„“x,â„“o)

2
L2(PXO)
=
Z
[0,1]do
Z
Rdx
X
â„“x,â„“o
Ëœâ„“x, Ëœ
â„“o
Î±v(â„“x,â„“o)Î±v( Ëœ
â„“x,Ëœâ„“o)â„¦K,â„“x(x)â„¦kËœâ„“x(x)MK,â„“o(o)MkËœâ„“o(o)pX(x) dx do
=
Z
Rdx
X
â„“x,â„“o
Ëœâ„“x, Ëœ
â„“o
Î±v(â„“x,â„“o)Î±v( Ëœ
â„“x,Ëœâ„“o)â„¦K,â„“x(x)â„¦kËœâ„“x(x)pX(x)
 Z
[0,1]do
MK,â„“o(o)MkËœâ„“o(o) do
!
dx
(a)
=
X
â„“o
âˆ¥MK,â„“oâˆ¥2
L2(Rdo)
Z
Rdx
 X
â„“x
Î±v(â„“x,â„“o)â„¦K,â„“x(x)
p
p(x)
!2
dx
(b)
=
X
â„“o
âˆ¥MK,â„“oâˆ¥2
L2(Rdo)
Z
Rdx
 X
â„“x
Î±v(â„“x,â„“o)F [â„¦K,â„“x
âˆšp] (Ï‰x)
!2
dÏ‰x
83

=2âˆ’Ksdo
so âˆ¥M00âˆ¥2
L2(Rdo)
Z
Rdx
X
â„“o
 X
â„“x
Î±v(â„“x,â„“o)F [â„¦K,â„“x
âˆšp] (Ï‰x)
!2
dÏ‰x.
(111)
where (a) follows from the fact that MK,â„“o have pairwise disjoint support verified in Step Two
above; and (b) follows from the Plancherelâ€™s Theorem. By Eq. (111), we have
âˆ¥fv âˆ’fvâ€²âˆ¥2
L2(PXO)
= Ïµ02âˆ’2Ks

1âˆ’dx
2sx

2âˆ’Ksdo
so âˆ¥M00âˆ¥2
L2(Rdo)
Â·
Z
Rdx
X
â„“o
ï£«
ï£­X
â„“x
 Î²v(â„“x,â„“o) âˆ’Î²vâ€²(â„“x,â„“o)

F [â„¦K,â„“x
âˆšpX] (Ï‰x)
ï£¶
ï£¸
2
dÏ‰x
â‰¥Ïµ02âˆ’2Ks

1âˆ’dx
2sx

2âˆ’Ksdo
so âˆ¥M00âˆ¥2
L2(Rdo)
 (A)2/2 âˆ’(B)2
,
(112)
where the last step follows from the reverse triangular inequality (a + b)2 â‰¥a2
2 âˆ’b2, and we define
(A)2 :=
X
â„“o

X
â„“x
 Î²v(â„“x,â„“o) âˆ’Î²vâ€²(â„“x,â„“o)

F[â„¦K,â„“x] âˆ—Ëœq

2
L2(Rdx)
(B)2 :=
X
â„“o

X
â„“x
 Î²v(â„“x,â„“o) âˆ’Î²vâ€²(â„“x,â„“o)

F[â„¦K,â„“x] âˆ—(q âˆ’Ëœq)

2
L2(Rdx)
.
In order to lower bound Eq. (112), we need to lower bound (A)2 and upper bound (B)2. First, we
are going to lower bound (A)2. From Eq. (105), we know that the support of F[â„¦K,â„“x] is
Ëœ
Iâ„“x :=
dxÃ—
j=1
h
1.1Ï€2K s
sx + Î¶â„“x,j, 1.1Ï€2K s
sx + (Î¶â„“x,j + 1)
i
.
(113)
Since the support of Ëœq is [âˆ’Î¶/3, Î¶/3]dx, by the standard fact that supp(f âˆ—g) âŠ†supp(f) + supp(g),
we have
supp (F[â„¦K,â„“x] âˆ—Ëœq) âŠ†
dxÃ—
j=1
h
1.1Ï€2K s
sx + Î¶(â„“x,j âˆ’1/3), 1.1Ï€2K s
sx + (Î¶(â„“x,j + 1/3) + 1)
i
=: Î›K
(114)
Note that for Î¶ â‰¥3, we have Î¶(â„“j + 1/3) + 1 â‰¤Î¶(â„“j + 1 âˆ’1/3), hence supp (F[â„¦K,â„“x] âˆ—Ëœq) is pairwise
disjoint with respect to different â„“x. Hence we obtain that
(A)2 =
X
â„“o
X
â„“x
(Î²v(â„“x,â„“o) âˆ’Î²vâ€²(â„“x,â„“o))2 âˆ¥F[â„¦K,â„“x] âˆ—Ëœqâˆ¥2
L2(Rdx) .
Notice that
âˆ¥F[â„¦K,â„“x] âˆ—Ëœqâˆ¥2
L2(Rdx)
=
Z
Î›K

Z
Rdx F[â„¦K,â„“x](Ï‰â€²
x)Ëœq(Ï‰x âˆ’Ï‰â€²
x) dÏ‰â€²
x

2
dÏ‰x
84

= 2âˆ’2Ksdx
sx
Z
Î›K

Z
Rdx F[M0,âˆ’m
2 ]

2âˆ’Ks
sx Ï‰â€²
x

Â· 1â„“x

2âˆ’Ks
sx Ï‰â€²
x

Â· Ëœq(Ï‰x âˆ’Ï‰â€²
x) dÏ‰â€²
x

2
dÏ‰x
(i)
â‰¥2âˆ’2Ksdx
sx
Z
Ëœ
Iâ„“x

Z
Rdx F[M0,âˆ’m
2 ]

2âˆ’Ks
sx Ï‰â€²
x

Â· 1â„“x

2âˆ’Ks
sx Ï‰â€²
x

Â· Ëœq(Ï‰x âˆ’Ï‰â€²
x) dÏ‰â€²
x

2
dÏ‰x
(ii)
= 2âˆ’2Ksdx
sx
Z
Ëœ
Iâ„“x

Z
Ëœ
Iâ„“x
F[M0,âˆ’m
2 ]

2âˆ’Ks
sx Ï‰â€²
x

Â· Ëœq(Ï‰x âˆ’Ï‰â€²
x) dÏ‰â€²
x

2
dÏ‰x.
(115)
(i) above holds because Ëœ
Iâ„“x âŠ‚Î›K; and (ii) holds because of the indicator function. Notice, for
any Ï‰x, Ï‰â€²
x âˆˆËœ
Iâ„“x, we have for j âˆˆ{1, . . . , dx}, 1 â‰¥Ï‰x,j âˆ’Ï‰â€²
x,j â‰¥âˆ’1. Since for any âˆ’1 â‰¤t â‰¤1,
i âˆˆ{1, . . . , dx} and g defined in Eq. (104), there is F[âˆšg](t) =
R 1/2
âˆ’1/2
âˆšg(xi) exp(âˆ’xit)dxi =
R 1/2
âˆ’1/2
âˆšg(xi) cos(xit)dxi > 0. So we have, for Î¶ > 3,
Ëœq(Ï‰x âˆ’Ï‰â€²
x) = q(Ï‰x âˆ’Ï‰â€²
x) = F[âˆšpX](Ï‰x âˆ’Ï‰â€²
x) =
dx
Y
i=1
F[âˆšg](Ï‰x,i âˆ’Ï‰â€²
x,i) > 0.
Since both F[M0,âˆ’m
2 ](2âˆ’Ks
sx Ï‰â€²
x) and Ëœq(Ï‰x âˆ’Ï‰â€²
x) are positive and real, we continue from Eq. (115)
to have
â‰¥2âˆ’2Ksdx
sx

inf
Ï‰xâˆˆIâ„“x
F[M0,âˆ’m
2 ](Ï‰x)
2
Â·
Z
Ëœ
Iâ„“x
 Z
Ëœ
Iâ„“x
Ëœq
 Ï‰x âˆ’Ï‰â€²
x

dÏ‰â€²
x
!2
dÏ‰x
|
{z
}
(âˆ—)
.
Notice that in the integration in the term (âˆ—) above, only the difference of Ï‰x âˆ’Ï‰â€²
x show up. So we
can obtain
(âˆ—) =
Z
[0,1]dx
 Z
[0,1]dx Ëœq
 Ï‰x âˆ’Ï‰â€²
x

dÏ‰â€²
x
!2
dÏ‰x
=
dx
Y
i=1
Z 1
0
Z 1
0
Ëœqi
 Ï‰x,i âˆ’Ï‰â€²
x,i

dÏ‰â€²
x,i
2
dÏ‰x,i
=
dx
Y
i=1
Z 1
0
 Z Ï‰x,i
Ï‰x,iâˆ’1
Ëœqi
 Ï‰â€²â€²
x,i

dÏ‰â€²â€²
x,i
!2
dÏ‰x,i
=
dx
Y
i=1
Z 1
0
 Z Ï‰x,i
Ï‰x,iâˆ’1
qi
 Ï‰â€²â€²
x,i

dÏ‰â€²â€²
x,i
!2
dÏ‰x,i.
The second last equality holds by change of variables, and the last equality holds because Ëœqi(Ï‰) =
qi(Ï‰) for Ï‰ âˆˆ[âˆ’Î¶/3, Î¶/3]. So (âˆ—) is a strictly positive constant independent of Î¶, k, n. Plugging it
back to above, we obtain
âˆ¥F[â„¦K,â„“x] âˆ—Ëœqâˆ¥2
L2(Rdx) â‰¥2âˆ’2Ksdx
sx

inf
Ï‰xâˆˆIâ„“x
F[M0,âˆ’m
2 ](Ï‰x)
2
Â· (âˆ—).
Notice that, since Iâ„“x âŠ†[1.1Ï€, 1.95Ï€]dx and we use Eq. (106) to obtain, since M0,âˆ’m
2 is an even
function and thus has real-valued Fourier transform
inf
Ï‰xâˆˆIâ„“x
F[M0,âˆ’m
2 ](Ï‰x)

2
=
inf
Ï‰xâˆˆIâ„“x

dx
Y
i=1
sin(Ï‰x,i/2)m
(Ï‰x,i/2)m

2
â‰¥
inf
Ï‰âˆˆ[1.1Ï€,1.95Ï€]
sin(Ï‰/2)
Ï‰/2
2mdx
> 0.
85

Define the following positive constant, independent of Î¶, k or n,
CÏ‡ := (âˆ—) Â·
inf
Ï‰âˆˆ[1.1Ï€,1.95Ï€]
sin(Ï‰/2)
Ï‰/2
2mdx
.
(116)
We thus have
âˆ¥F[â„¦K,â„“x] âˆ—Ëœqâˆ¥2
L2(Rdx) â‰¥CÏ‡2âˆ’2Ks dx
sx .
(117)
Therefore,
(A)2 â‰¥
X
â„“o,â„“x
(Î²v(â„“x,â„“o) âˆ’Î²vâ€²(â„“â€²x,â„“â€²o))2CÏ‡2âˆ’2Ksdx
sx
(a)
â‰¥CÏ‡
8 |L|2âˆ’2Ksdx
sx
(b)
â‰³Î¶âˆ’dx2âˆ’Ks

dx
sx âˆ’do
so

.
where (a) follows from Eq. (109) and (b) follows from Eq. (100).
Next, we are going to upper bound (B). We have
(B)2 â‰¤
X
â„“o
ï£«
ï£­X
â„“x
âˆ¥F[â„¦K,â„“x] âˆ—(q âˆ’Ëœq)âˆ¥L2(Rdx)
ï£¶
ï£¸
2
(a)
â‰¤âˆ¥q âˆ’Ëœqâˆ¥2
L1(Rdx)
X
â„“o
ï£«
ï£­X
â„“x
âˆ¥F[â„¦K,â„“x]âˆ¥L2(Rdx)
ï£¶
ï£¸
2
(b)
â‰¤âˆ¥q âˆ’Ëœqâˆ¥2
L1(Rdx)
X
â„“o
ï£«
ï£­X
â„“x
âˆ¥MK,âˆ’m
2 âˆ¥L2(Rdx)
ï£¶
ï£¸
2
(c)
= âˆ¥q âˆ’Ëœqâˆ¥2
L1(Rdx)
X
â„“o
ï£«
ï£­X
â„“x
2âˆ’K sdx
2sx âˆ¥M0,âˆ’m
2 âˆ¥L2(Rdx)
ï£¶
ï£¸
2
= âˆ¥q âˆ’Ëœqâˆ¥2
L1(Rdx)
X
â„“o
ï£«
ï£­X
â„“x
1
ï£¶
ï£¸
2
2âˆ’K sdx
sx âˆ¥M0,âˆ’m
2 âˆ¥2
L2(Rdx)
â‰²âˆ¥q âˆ’Ëœqâˆ¥2
L1(Rdx)Î¶âˆ’2dx2Ks

2dx
sx + do
so

2âˆ’K sdx
sx âˆ¥M0,âˆ’m
2 âˆ¥2
L2(Rdx)
(d)
â‰¤âˆ¥q âˆ’Ëœqâˆ¥2
L1(Rdx)Î¶âˆ’1
2 dx2
Ksdo
so 2
Ksdx
sx âˆ¥M0,âˆ’m
2 âˆ¥2
L2(Rdx)
(118)
In the above derivations, (a) holds by Youngâ€™s convolution inequality, (b) holds by Plancherelâ€™s
Theorem and Eq. (105), (c) holds by the change of variables x â†2âˆ’Ks
sx x, and (d) holds for Î¶ â‰¥1.
We have
âˆ¥q âˆ’Ëœqâˆ¥L1(Rdx) =
Z
Rdx |q(Ï‰x) âˆ’Ëœq(Ï‰x)| dÏ‰x
(i)
= 2dx
dx
Y
i=1
Z âˆ
Î¶/3
|F[âˆšg](Ï‰x,i)| dÏ‰x,i
(ii)
â‰
dx
Y
i=1
Z âˆ
Î¶/3
Ï‰âˆ’3/4
x,i
exp(âˆ’âˆšÏ‰x,i) dÏ‰x
86

â‰¤Î¶âˆ’3
4 dx
dx
Y
i=1
Z âˆ
Î¶
exp(âˆ’âˆšÏ‰x,i) dÏ‰x,i
= Î¶âˆ’3
4 dx2dx 
1 + Î¶1/2dx exp

âˆ’dxÎ¶
1
2

â‰¤22dxÎ¶âˆ’1
4 dx exp

âˆ’dxÎ¶
1
2

,
(119)
where in (i) we use the definition of Ëœq in Eq. (110); and in (ii) we use the asymptotic decay of the
Fourier transform of the bump function [Johnson, 2015, Section 2].
|F[âˆšg](Ï‰)| â‰Ï‰âˆ’3/4 exp(âˆ’âˆšÏ‰),
Ï‰ â‰«1.
(120)
Hence, we plug Eq. (119) back to Eq. (118) and we find that
(B)2 â‰²Î¶âˆ’1
2 dx2
Ksdo
so + Ksdx
sx

22dxÎ¶âˆ’1
4 dx exp

âˆ’dxÎ¶
1
2
2
âˆ¥M0,âˆ’m
2 âˆ¥2
L2(Rdx)
â‰²2
Ksdo
so + Ksdx
sx Î¶âˆ’dx exp

âˆ’2dxÎ¶
1
2

.
(121)
Hence in order for (B)2 â‰¤(A)2/4 to hold, a sufficient condition on Î¶ is given by the following
inequality, up to some constants,
2
Ksdo
so + Ksdx
sx Î¶âˆ’dx exp

âˆ’2dxÎ¶
1
2

â‰²1
4Î¶âˆ’dx2âˆ’Ksdx
sx 2
Ksdo
so
â‡â‡’
exp

âˆ’2dxÎ¶
1
2

â‰²2âˆ’2Ksdx
sx .
(122)
Hence for sufficiently large Î¶ = Î¶(n), such that Eq. (122) is satisfied, we have (A) â‰¥(B)/2, thus
âˆ¥fv âˆ’fvâ€²âˆ¥2
L2(PXO) â‰¥Ïµ2
02âˆ’2Ks

1âˆ’dx
2sx

2âˆ’Ksdo
so âˆ¥M0,âˆ’m
2 âˆ¥2
L2(Rdo)
1
4(A)2
â‰³Ïµ2
02âˆ’2Ks

1âˆ’dx
2sx

2âˆ’Ksdo
so Î¶âˆ’dx2âˆ’Ksdx
sx 2
Ksdo
so
= Ïµ2
02âˆ’2KsÎ¶âˆ’dx.
(123)
Step Five Recall the distributions Pfv defined above, and recall that 0 âˆˆVK for VK defined in
Eq. (109). In this step, we are going to show that, for P âŠ—n
fv := Pfv âŠ—Â· Â· Â· âŠ—Pfv which is a probability
distribution over (Z Ã— O Ã— R)n, it satisfies
1
|VK|
X
vâˆˆVK
KL

P âŠ—n
fv , P âŠ—n
f0

â‰²Ïµ2
0n2âˆ’2K sdx
sx Î·1âˆ’2Ks.
Notice that KL divergence tensorizes over independent copies at each dimension, we have that
KL(P âŠ—n
f0 âˆ¥P âŠ—n
fv ) = n KL(Pf0âˆ¥Pfv), so we are going to study KL(Pf0âˆ¥Pfv) as follows.
KL (Pfv, Pf0) = E(z,o)âˆ¼PZO [KL (Pfv (Â· | z, o)) , Pf0 (Â· | z, o)]
(a)
=
âˆ¥Tfvâˆ¥2
L2(PZO)
2Ïƒ2
(b)
â‰¤2âˆ’2K sdx
sx Î·1 âˆ¥fvâˆ¥2
L2(PXO)
2Ïƒ2
(124)
87

In the above chain of derivations, we use Blanchard and MÃ¼cke [2018][Proposition 6.2] in (a); and
in (b) we use the Assumption 4.3 along with the fact from Eq. (107) that the support of the Fourier
transform of fv is indeed in high frequency.
supp(F[fv]) = âˆªâ„“x,â„“oâˆˆL ËœIâ„“x âŠ†
h
1.1Ï€ Â· 2K s
sx , 1.9Ï€ Â· 2K s
sx
idx .
Next notice that, by Eq. (111), we have
âˆ¥fvâˆ¥2
L2(PXO)
(a)
â‰¤Ïµ2
02âˆ’2Ks(1âˆ’dx
2sx )2âˆ’Ksdo
so âˆ¥M0,0âˆ¥2
L2(Rdo)
Z
Rdx
X
â„“o
 X
â„“x
Î²v(â„“x,â„“o)F [â„¦K,â„“x
âˆšp] (Ï‰x)
!2
dÏ‰x
â‰¤Ïµ2
02âˆ’2Ks(1âˆ’dx
2sx )2âˆ’Ksdo
so âˆ¥M0,0âˆ¥2
L2(Rdo)
Z
Rdx
X
â„“o
 X
â„“x
|F [â„¦K,â„“x
âˆšp] (Ï‰x)|
!2
dÏ‰x
(b)
â‰²Ïµ2
02âˆ’2Ks(1âˆ’dx
2sx )
Z
Rdx
 X
â„“x
|F [â„¦K,â„“x
âˆšp] (Ï‰x)|
!2
dÏ‰x
â‰¤Ïµ2
02âˆ’2Ks(1âˆ’dx
2sx )

X
â„“x
|F[â„¦K,â„“x] âˆ—(q âˆ’Ëœq + Ëœq)|

2
L2(Rdx)
(c)
â‰¤Ïµ2
02âˆ’2Ks(1âˆ’dx
2sx )
ï£«
ï£­2

X
â„“x
|F[â„¦K,â„“x] âˆ—(q âˆ’Ëœq)|

2
L2(Rdx)
+ 2

X
â„“x
|F[â„¦K,â„“x] âˆ—Ëœq|

2
L2(Rdx)
ï£¶
ï£¸.
(125)
In the above derivations, (a) holds by Eq. (111), (b) holds by P
â„“o 1 â‰²2
Ksdo
so , (c) holds by triangular
inequality. The first term in Eq. (125) has already been upper bounded in Eq. (121). Hence for
sufficiently large Î¶ that satisfies Eq. (122),

X
â„“x
|F[â„¦K,â„“x] âˆ—(q âˆ’Ëœq)|

2
L2(Rdx)
â‰²2
Ksdx
sx Î¶âˆ’dx
2 exp

âˆ’2dxÎ¶
1
2

â‰²2
Ksdx
sx 2âˆ’2Ksdx
sx
= 2âˆ’Ksdx
sx .
The second term in Eq. (125) can be upper bounded by

X
â„“x
|F[â„¦K,â„“x] âˆ—Ëœq|

2
L2(Rdx)
(a)
â‰²
X
â„“x
âˆ¥F[â„¦K,â„“x] âˆ—Ëœqâˆ¥2
L2(Rdx)
(b)
â‰¤
X
â„“x
âˆ¥F[â„¦K,â„“x]âˆ¥2
L2(Rdx) Â· âˆ¥Ëœqâˆ¥2
L1(Rdx)
(c)
â‰¤
X
â„“x
F[MK,âˆ’m
2 ]

2
L2(ËœIâ„“x) Â· âˆ¥Ëœqâˆ¥2
L1(Rdx)
â‰¤
F[MK,âˆ’m
2 ]

2
L2(Rdx) Â· âˆ¥Ëœqâˆ¥2
L1(Rdx)
(d)
=
MK,âˆ’m
2

2
L2(Rdx) Â· âˆ¥Ëœqâˆ¥2
L1(Rdx)
88

(e)
= 2âˆ’Ksdx
sx
M0,âˆ’m
2

2
L2(Rdx) âˆ¥Ëœqâˆ¥2
L1(Rdx)
â‰²2âˆ’Ksdx
sx .
In the above chain of derivations, (a) holds because F[â„¦K,â„“x] âˆ—Ëœq have disjoint support for different
â„“x proved in Eq. (114); (b) holds by Youngâ€™s convolution inequality; (c) holds by Eq. (105); (d)
holds by the fact that Iâ„“x are pairwise disjoint and Plancherelâ€™s Theorem and (e) holds by change
of variables x â†2âˆ’Ks
sx x. Therefore, combining the upper bound on the above two terms, we obtain
âˆ¥fvâˆ¥2
L2(PXO) â‰²Ïµ2
02âˆ’2Ks

1âˆ’dx
2sx

2âˆ’Ksdx
sx
= Ïµ2
02âˆ’2Ks.
(126)
We plug the upper bound on âˆ¥fvâˆ¥2
L2(PXO) back to Eq. (124) to obtain
KL (Pfv, Pf0) â‰²Ïµ2
0Ïƒâˆ’22âˆ’2K sdx
sx Î·1âˆ’2Ks.
(127)
And thus,
1
|VK|
X
vâˆˆVK
KL

P âŠ—n
fv , P âŠ—n
f0

â‰¤Ïµ2
0nÏƒâˆ’22âˆ’2K sdx
sx Î·1âˆ’2Ks.
(128)
Step Six In this step, we are going to show that, for any measurable learning method (zi, oi, yi)n
i=1 =:
D 7â†’Ë†fD, there is a distribution P among Pfv with v âˆˆVK which is difficult to learn for the considered
learning method. We define a measurable mapping
Î¨ :

[0, 1]dz Ã— [0, 1]do Ã— R
n
â†’VK,
Î¨(D) := argmin
vâˆˆVK
 Ë†fD âˆ’fv

L2(PXO) .
(129)
For v âˆˆVK and D âˆˆ
 [0, 1]dz Ã— [0, 1]do Ã— R
n with Î¨(D) Ì¸= v, we start from (123) to have
Ïµ02âˆ’KsÎ¶âˆ’dx
2 â‰²
fÎ¨(D) âˆ’fv

L2(PXO)
â‰¤
fÎ¨(D) âˆ’Ë†fD

L2(PXO) +
 Ë†fD âˆ’fv

L2(PXO)
â‰¤2
 Ë†fD âˆ’fv

L2(PXO) .
Consequently, for all v âˆˆVK we find
P âŠ—n
fv (D : Î¨(D) Ì¸= v) â‰¤P âŠ—n
fv

D :
 Ë†fD âˆ’fv

L2(PXO) â‰³Ïµ02âˆ’KsÎ¶âˆ’dx
2

.
Therefore, we have
max
vâˆˆVK P âŠ—n
fv

D :
 Ë†fD âˆ’fv

L2(PXO) â‰³Ïµ02âˆ’KsÎ¶âˆ’dx
2

â‰¥max
vâˆˆVK P âŠ—n
fv (D : Î¨(D) Ì¸= v)
(a)
â‰³
p
|VK|
p
|VK| + 1
 
1 âˆ’Ïµ2
0n2âˆ’2K sdx
sx Î·12âˆ’2Ks
log |VK|
âˆ’
1
2 log |VK|
!
89

(b)
â‰¥
p
|VK|
p
|VK| + 1
 
1 âˆ’Ïµ2
0n2âˆ’2K sdx
sx Î·12âˆ’2Ks
Î¶âˆ’dx2K sdx
sx +K sdo
so
âˆ’
1
2 log |VK|
!
â‰¥
p
|VK|
p
|VK| + 1

1 âˆ’Ïµ2
0Î¶dxn2âˆ’K s
sx

2dxÎ·1+2sx+dx+ do
so sx

âˆ’
1
2 log |VK|

.
(130)
(a) holds by Fischer and Steinwart [2020, Theorem 20] and Eq. (128). (b) holds by the construction
of VK in Eq. (109) with |VK| â‰¥2
L
8 so log |VK| â‰³Î¶âˆ’dx2Ks( dx
sx + do
so ). Next, we choose Î¶ and k as the
following function of n:
Î¶ =
 
1
2
1
2dxÎ·1 + 2sx + dx + do
so sx
log n
!2
,
2âˆ’Ks = n
âˆ’
sx
2dxÎ·1+2sx+dx+ do
so sx Î¶âˆ’sx.
(131)
The choice of Î¶ ensures that Eq. (122) is satisfied for sufficiently large n â‰¥1. The choice of k as a
function of n ensures that we can proceed from Eq. (130) to obtain
max
vâˆˆVK P âŠ—n
fv

D :
 Ë†fD âˆ’fv

L2(PXO) â‰¥Ïµ0n
âˆ’
sx
2sx+2dxÎ·1+dx+ do
so sx (log n)âˆ’2sxâˆ’dx

â‰³
p
|VK|
p
|VK| + 1

1 âˆ’Ïµ2
0 âˆ’
1
2 log |VK|

â‰¥1 âˆ’2Ïµ2
0.
The last inequality holds for sufficiently large n because |VK| â†’âˆas n â†’âˆ. We set 2Ïµ2
0 = Ï„ 2 and
relabelling the constants, to obtain
inf
D7â†’Ë†fD
sup
fâˆˆBsx,so
2,q
(Rdx+do)
 Ë†fD âˆ’f

L2(PXO) â‰¥
inf
D7â†’Ë†fD
max
vâˆˆVK
 Ë†fD âˆ’fv

L2(PXO)
â‰¥Ï„n
âˆ’
sx
2sx+2dxÎ·1+dx+ do
so sx (log n)âˆ’2sxâˆ’dx
= Ï„n
âˆ’
sx
dx
1+2( sx
dx +Î·1)+ do
so
sx
dx (log n)âˆ’2sxâˆ’dx,
holds for sufficiently large n with probability â‰¥1 âˆ’CÏ„ 2.
E.3
Auxiliary Results
Lemma E.2. Let f : Rd â†’R, f âˆˆBs
2,q(Rd) and K âˆˆL1(Rd), then
âˆ¥K âˆ—fâˆ¥Bs
2,q(Rd) â‰¤âˆ¥Kâˆ¥L1(Rd)âˆ¥fâˆ¥Bs
2,q(Rd)
Proof. Define (Ï„hf)(x) := f(x + h). We have
Ï„h(K âˆ—f)(x) = (K âˆ—f)(x + h) = (K âˆ—(Ï„hf))(x).
Since âˆ†r
h = (Ï„h âˆ’id)r, we have âˆ†r
h(K âˆ—f) = K âˆ—(âˆ†r
hf). Thus for any r > max{s1, . . . , sd},
Ï‰r,2

K âˆ—f, t
1
s1 , . . . , t
1
sd , Rd
=
sup
0<|hi|<t
1
si
âˆ¥âˆ†r
h(K âˆ—f)âˆ¥L2(Rd)
90

=
sup
0<|hi|<t
1
si
âˆ¥K âˆ—âˆ†r
h(f)âˆ¥L2(Rd)
(i)
â‰¤
sup
0<|hi|<t
1
si
âˆ¥Kâˆ¥L1(Rd) âˆ¥âˆ†r
hfâˆ¥L2(Rd)
(132)
= âˆ¥Kâˆ¥L1(Rd)Ï‰r,2

f, t
1
s1 , . . . , t
1
sd , Rd
,
where we use Youngâ€™s convolution inequality in (i). Hence
|K âˆ—f|Bs
2,q(Rd) =
Z 1
0
h
tâˆ’1Ï‰r,2

K âˆ—f, t
1
s1 , . . . , t
1
sd , Rdiq dt
t
 1
q
â‰¤âˆ¥Kâˆ¥L1(Rd)
Z 1
0
h
tâˆ’1Ï‰r,2

f, t
1
s1 , . . . , t
1
sd , Rdiq dt
t
 1
q
= âˆ¥Kâˆ¥L1(Rd)|f|Bs
2,q(Rd).
Hence
âˆ¥K âˆ—fâˆ¥Bs
2,q(Rd) = âˆ¥K âˆ—fâˆ¥L2(Rd) + |K âˆ—f|Bs
2,q(Rd)
(i)
â‰¤âˆ¥Kâˆ¥L1(Rd)âˆ¥fâˆ¥L2(Rd) + âˆ¥Kâˆ¥L1(Rd)|f|Bs
2,q(Rd) = âˆ¥Kâˆ¥L1(Rd)âˆ¥fâˆ¥Bs
2,q(Rd),
where (i) is due to Youngâ€™s convolution inequality.
Lemma E.3. Let Mk,âˆ’m
2 : Rdx â†’R, MK,â„“o : Rdo â†’R be defined in Eq. (98). Let 1S(â„“o) : Rdx â†’R
be an indicator function over a measurable set S(â„“o) âŠ‚Rdx for each â„“o in Eq. (99).
Define
f(x, o) := P
â„“o(Mk,âˆ’m
2 âˆ—Fâˆ’1[1S(â„“o)])(x) Â· MK,â„“o(o). Then we have âˆ¥fâˆ¥Bsx,so
2,âˆ(Rdx+do) â‰²2Ksâˆ’K s
sx
dx
2 .
Proof. By definition of Besov norm in Leisner [2003][Eq. (2.2)], for any r > max{sx, so}, we have
âˆ¥fâˆ¥Bsx,so
2,âˆ
(Rdx+do) â‰sup
0<t<1
h
tâˆ’1Ï‰x
r,2(f, t1/sx, . . . , t1/sx)
i
|
{z
}
(I)
+ sup
0<t<1
h
tâˆ’1Ï‰o
r,2(f, t1/so, . . . , t1/so)
i
|
{z
}
(II)
,
where Ï‰x
r,2 (respectively Ï‰o
r,2) denotes the partial modulus of smoothness in the x (respectively
o)-direction, and we restrict the supremum from t âˆˆ(0, âˆ) to t âˆˆ(0, 1) by DeVore and Lorentz
[1993, Theorem 10.1]. Specifically, define (Ï„hg)(x) := g(x + h) and âˆ†r
h := (Ï„h âˆ’id)r, we write
Ï‰x
r,2(f, t1/sx, . . . , t1/sx) :=
sup
|hi|â‰¤t1/sx
Z
Rdo
Z
Rdx |âˆ†r
h(f(Â·, o))(x)|2 dx do
 1
2
,
and Ï‰o
r,2 is defined similarly. We first bound (I). We have
Ï„h(K âˆ—g)(x) = (K âˆ—g)(x + h) = (K âˆ—(Ï„hg))(x).
We thus have âˆ†r
h(K âˆ—g) = K âˆ—(âˆ†r
hg). Thus we have
Ï‰x
r,2(f, t1/sx, . . . , t1/sx)
91

=
sup
|hi|â‰¤t1/sx
ï£«
ï£­
Z
Rdo
Z
Rdx

X
â„“o
((âˆ†r
hMk,âˆ’m
2 ) âˆ—Fâˆ’1[1S(â„“o)])(x)MK,â„“o(o)

2
dx do
ï£¶
ï£¸
1
2
=
sup
|hi|â‰¤t1/sx
ï£«
ï£­X
â„“o,â„“â€²o
Z
Rdx((âˆ†r
hMk,âˆ’m
2 ) âˆ—Fâˆ’1[1S(â„“o)])(x)((âˆ†r
hMk,âˆ’m
2 ) âˆ—Fâˆ’1[1S(â„“â€²o)])(x) dx
Â·
Z
Rdo MK,â„“o(o)Mk,â„“â€²o(o) do
 1
2
(a)
=
sup
|hi|â‰¤t1/sx
ï£«
ï£­X
â„“o
âˆ¥Fâˆ’1[1S(â„“o)] âˆ—(âˆ†r
hMk,âˆ’m
2 )âˆ¥2
L2(Rdx)âˆ¥MK,â„“oâˆ¥2
L2(Rdo)
ï£¶
ï£¸
1
2
(b)
â‰¤
sup
|hi|â‰¤t1/sx
ï£«
ï£­X
â„“o
âˆ¥âˆ†r
hMk,âˆ’m
2 âˆ¥2
L2(Rdx)âˆ¥MK,â„“oâˆ¥2
L2(Rdo)
ï£¶
ï£¸
1
2
=
 
sup
|hi|â‰¤t1/sx
âˆ¥âˆ†r
hMk,âˆ’m
2 âˆ¥L2(Rdx)
! ï£«
ï£­X
â„“o
âˆ¥MK,â„“oâˆ¥2
L2(Rdo)
ï£¶
ï£¸
1
2
(c)
â‰¤
sup
|hi|â‰¤t1/sx
âˆ¥âˆ†r
hMk,âˆ’m
2 âˆ¥L2(Rdx)
=Ï‰r,2(Mk,âˆ’m
2 , t1/sx, . . . , t1/sx).
In the above derivations,
â€¢ we use supp(MK,â„“o) âˆ©supp(MK,â„“o) = âˆ…if â„“o Ì¸= â„“â€²
o in (a),
â€¢ we use in (b) the inequality
âˆ¥Fâˆ’1[1S(â„“o)] âˆ—(âˆ†r
hMk,âˆ’m
2 )âˆ¥2
L2(Rdx) = âˆ¥1S(â„“o) Â· F[âˆ†r
hMk,âˆ’m
2 ]âˆ¥2
L2(Rdx)
â‰¤âˆ¥F[âˆ†r
hMk,âˆ’m
2 ]âˆ¥2
L2(Rdx) = âˆ¥âˆ†r
hMk,âˆ’m
2 âˆ¥2
L2(Rdx),
(133)
â€¢ we use P
â„“o âˆ¥MK,â„“oâˆ¥2
L2(Rdo) â‰¤2
Ksdo
so 2âˆ’Ksdo
so âˆ¥M00âˆ¥2
L2(Rdo) â‰¤1 in (c).
Indeed, note that
âˆ¥M00âˆ¥2
L2(Rdo) = âˆ¥Î¹mâˆ¥2do
L2(R) â‰¤âˆ¥Î¹mâˆ¥2do
L1(R) = 1.
Thus we have
(I) â‰¤sup
t>0
h
tâˆ’1Ï‰r,2

Mk,âˆ’m
2 , t
1
sx , . . . , t
1
sx
i
â‰|Mk,âˆ’m
2 |Bsx
2,âˆ(Rdx) â‰2Ks

1âˆ’dx
2sx

,
where we bound (I) by an isotropic Besov norm, and we use the sequential Besov norm equivalence
[DeVore and Popov, 1988][Theorem 5.1] (see also Leisner [2003, Theorem 3.4] applied to Bsx
2,âˆ(Rdx))
in the last equality. Now we bound (II). Notice that
Ï‰o
r,2(f, t1/so, . . . , t1/so)
(a)
â‰²Ï‰o
so,2(f, t1/so, . . . , t1/so)
92

(b)
â‰²2KsÏ‰o
so,2

f, 2âˆ’Ks
so t1/so, . . . , 2âˆ’Ks
so t1/so
=2Ks
sup
|hi|â‰¤2âˆ’Ks
so t1/so
ï£«
ï£­
Z
Rdx+do

X
â„“o
(Fâˆ’1[1S(â„“o)] âˆ—Mk,âˆ’m
2 )(x) Â· (âˆ†so
h MK,â„“o)(o)

2
dx do
ï£¶
ï£¸
1
2
=2Ks
sup
|hi|â‰¤2âˆ’Ks
so t1/so
ï£«
ï£­X
â„“o,â„“â€²o
Z
Rdx(Fâˆ’1[1S(â„“o)] âˆ—Mk,âˆ’m
2 )(x) Â· Fâˆ’1[1S(â„“â€²o)] âˆ—Mk,âˆ’m
2 (x) dx
Â·
Z
Rdo(âˆ†so
h MK,â„“o)(o) Â· (âˆ†so
h Mk,â„“â€²o)(o) do
 1
2
(c)
=2Ks
sup
|hi|â‰¤2âˆ’Ks
so t1/so
ï£«
ï£­X
â„“o
âˆ¥Fâˆ’1[1S(â„“o)] âˆ—Mk,â„“xâˆ¥2
L2(Rdx) Â· âˆ¥âˆ†so
h MK,â„“oâˆ¥2
L2(Rdo)
ï£¶
ï£¸
1
2
(d)
â‰¤2Ks
sup
|hi|â‰¤2âˆ’Ks
so t1/so
ï£«
ï£­X
â„“o
âˆ¥Mk,âˆ’m
2 âˆ¥2
L2(Rdx) Â· âˆ¥âˆ†so
h MK,â„“oâˆ¥2
L2(Rdo)
ï£¶
ï£¸
1
2
=2Ksâˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx)
ï£«
ï£­
sup
|hi|â‰¤2âˆ’Ks
so t1/so
X
â„“o
âˆ¥âˆ†so
h MK,â„“oâˆ¥2
L2(Rdo)
ï£¶
ï£¸
1
2
(e)
=2Ksâˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx)
ï£«
ï£¬
ï£­
sup
|hi|â‰¤2âˆ’Ks
so t1/so

X
â„“o
 âˆ†so
h MK,â„“o


2
L2(Rdo)
ï£¶
ï£·
ï£¸
1
2
(f)
=2Ksâˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx)
ï£«
ï£¬
ï£­
sup
|hi|â‰¤2âˆ’Ks
so t1/so

âˆ†so
h
ï£«
ï£­X
â„“o
MK,â„“o
ï£¶
ï£¸

2
L2(Rdo)
ï£¶
ï£·
ï£¸
1
2
=2Ksâˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx)
ï£«
ï£­Ï‰so,2
ï£«
ï£­X
â„“o
MK,â„“o, 2âˆ’Ks
so t
1
so , . . . , 2âˆ’Ks
so t
1
so
ï£¶
ï£¸
2ï£¶
ï£¸
1
2
.
In the above derivations,
â€¢ we use r = sx âˆ¨so â‰¥so and Minkowskiâ€™s inequality in (a) (this is also sometimes referred to
as the reverse Marchaud inequality, see Kolomoitsev and Tikhonov [2020][Property 8]),
â€¢ we use Leisner [2003][Theorem 2.1.1] in (b), which we can apply since 2
Ks
so â‰¥1,
â€¢ we deduce from Eq. (98) that
supp(Mkâ„“o) =
doÃ—
j=1

2âˆ’
j
Ks
so
k
â„“o,j, 2âˆ’
j
Ks
so
k
(m + â„“o,j)

.
We also see that
supp(âˆ†so
h Mkâ„“o) =
doÃ—
j=1

2âˆ’
j
Ks
so
k
â„“o,j âˆ’sohj, 2âˆ’
j
Ks
so
k
(m + â„“o,j)

93

We deduce that a sufficient condition to guarantee that
supp
 âˆ†so
h MK,â„“o

âˆ©supp
 âˆ†so
h Mk,â„“â€²o

= âˆ…
if â„“o Ì¸= â„“â€²
o is given by
(âˆ€j = 1, . . . , do), |hj| â‰¤2âˆ’
j
Ks
so
k m âˆ’1
so
.
Since m âˆ’1 â‰¥so by choice, and we have for all i = 1, . . . , do, |hi| â‰¤2âˆ’Ks
so t1/so â‰¤2âˆ’Ks
so , this
sufficient condition is satisfied. Hence

âˆ†so
h MK,â„“o, âˆ†so
h Mk,â„“â€²o

L2(Rdo) =
âˆ†so
h MK,â„“o
2
L2(Rdo) Î´â„“o,â„“â€²o.
(134)
â€¢ we use Plancherelâ€™s Theorem in (d), in a similar way as in Eq. (133),
â€¢ we use Eq. (134) again for step (e),
â€¢ we use linearity of âˆ†so
h for step (f).
Thus we have shown that, for q = âˆ,
(II) â‰²2Ksâˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx) sup
t>0
ï£®
ï£°tâˆ’1Ï‰so,2
ï£«
ï£­X
â„“o
MK,â„“o, 2âˆ’Ks
so t
1
so , . . . , 2âˆ’Ks
so t
1
so
ï£¶
ï£¸
ï£¹
ï£»
â‰²âˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx) sup
t>0
ï£®
ï£°

t2âˆ’Ksâˆ’1
Ï‰so,2
ï£«
ï£­X
â„“o
MK,â„“o, 2âˆ’Ks
so t
1
so , . . . , 2âˆ’Ks
so t
1
so
ï£¶
ï£¸
ï£¹
ï£»
(a)
â‰²âˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx) sup
t>0
ï£®
ï£°
Z âˆ
t
wâˆ’soÏ‰so+1
ï£«
ï£­X
â„“o
MK,â„“o, w, . . . , w
ï£¶
ï£¸dw
w
ï£¹
ï£»
â‰¤âˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx)
Z âˆ
0
wâˆ’soÏ‰so+1
ï£«
ï£­X
â„“o
MK,â„“o, w, . . . , w
ï£¶
ï£¸dw
w
â‰âˆ¥Mk,âˆ’m
2 âˆ¥L2(Rdx)

X
â„“o
MK,â„“o

Bso
2,1(Rdo)
(b)
â‰²2âˆ’Ksdx
2sx 2
Ks
so (soâˆ’do/2)
ï£«
ï£­X
â„“o
1
ï£¶
ï£¸
1
2
â‰²2Ks

1âˆ’dx
2sx

,
where we use the Marchaud-type estimate DeVore and Lorentz [1993, Chapter 2, Eq. (10.3)] in
(a), we use the sequential Besov norm equivalence DeVore and Popov [1988][Theorem 5.1] (see also
[Leisner, 2003, Theorem 3.3.3]) in (b).
94
