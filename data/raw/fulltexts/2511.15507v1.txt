Sample-Adaptivity Tradeoff in On-Demand Sampling
Nika Haghtalab1, Omar Montasser2, and Mingda Qiao3
1University of California, Berkeley
2Yale University
3University of Massachusetts Amherst
Abstract
We study the tradeoff between sample complexity and round complexity in on-demand sam-
pling, where the learning algorithm adaptively samples from k distributions over a limited num-
ber of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the
optimal sample complexity of an r-round algorithm scales approximately as dkΘ(1/r)/ε. For the
general agnostic case, we present an algorithm that achieves near-optimal sample complexity
of eO((d + k)/ε2) within eO(
√
k) rounds. Of independent interest, we introduce a new frame-
work, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity
tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the
round complexity in the OODS setting. The upper bounds directly yield the eO(
√
k)-round al-
gorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round
complexity would require fundamentally new techniques that bypass the inherent hardness of
OODS.
1
Introduction
Modern machine learning pipelines increasingly treat the training set as a mutable resource—
adapting the data collection process in response to intermediate learning signals in order to focus
effort where it matters most. This adaptivity arises in a range of settings. In multi-distribution
learning (see e.g. [BHPQ17, HJZ22]), the on-demand sampling framework allows algorithms to
adaptively select domains from which to sample to minimize the worst-case loss.
Similarly in
multi-armed bandit problems, adaptively selecting which arm to pull next is an important aspect
of algorithm design. In practice, pipelines for training models adaptively decide how to reweigh or
augment their datasets to improve downstream accuracy [XPD+23, SRC24]. While these paradigms
demonstrate—both theoretically and empirically—that adaptive data collection can significantly
improve performance and sample efficiency, adaptivity is often an undesirable feature. It requires
the ML practitioner to collect data sequentially, slowing down the end-to-end training process and
limiting opportunities for parallelism and scalability. This tension raises a central question:
to what extent is adaptive sample collection necessary to achieve the observed gains in
learning performance? And what are the quantitative tradeoffs between the number of
adaptive rounds and sample complexity?
1
arXiv:2511.15507v1  [cs.LG]  19 Nov 2025

We study this question in the context of on-demand sampling within the framework of multi-
distribution learning [HJZ22]. Multi-distribution learning extends the classical agnostic learning
setting by giving the learner sampling access to k distributions D1, . . . , Dk, with the goal of learning
a single predictor that minimizes the worst-case error across all distributions. This framework has
emerged as a central model for studying algorithmic dataset selection, offering both a method of
allocating a fixed sampling budget across heterogeneous data sources and a unifying perspective
on several recent advances in federated learning, multi-task learning, domain adaptation, and fair
and robust machine learning [KNRW18, MSS19, SKHL20, RY21, TH22, HJZ23, ZZC+24].
Prior work on multi-distribution learning has established optimal on-demand sample complex-
ities of eO((d + k)/ε) in the realizable case [BHPQ17, CZZ18, NZ18] and eO((log(|H|) + k)/ε2) in
the agnostic case [HJZ22], where d is the VC dimension of hypothesis class H. The latter was
recently extended to eO((d + k)/ε2) for infinite hypothesis classes [ZZC+24, Pen24]. However, these
algorithms rely on a large number of adaptive rounds—often polynomial in 1/ε and the complexity
of H and with a mild sublinear dependence on k (see Table 1 for details). In some cases, these
algorithms collect a single sample per round, resulting in a number of rounds that is as large as the
sample complexity itself! In contrast, the best known fully non-adaptive algorithms incur signifi-
cantly higher sample complexities of eO(dk/ε) and eO(dk/ε2) in the realizable and agnostic settings,
respectively.
Despite this gap, the complexity landscape between the two extremes—full adaptivity and
full non-adaptivity—remains largely unexplored. In particular, it is unknown whether a constant
number of adaptive rounds, or even one that is merely independent of the accuracy level ε, could
suffice to recover the optimal sample complexities achieved in the fully adaptive setting.
Our Contributions and Results
In this work, we formalize the problem of studying the trade-
offs between adaptivity and sample complexity of on-demand sampling algorithms. Specifically,
we aim for achieving the optimal sample complexity with a number of adaptive rounds that is
nearly independent of ε and d and with only sublinear dependence on k. We refer to the number
of adaptive rounds as the round complexity of an algorithm.
In the realizable case, we provide a tight characterization of the sample-adaptivity tradeoff. In
particular, we prove that a round complexity r allows for a sample complexity of dkΘ(1/r)/ε, yielding
a smooth tradeoff between round complexity and sample complexity. In addition to confirming that
log k rounds are necessary for achieving the optimal sample complexity in the realizable setting,
this also indicates that a small constant number of rounds—say, 3 rounds (!)—are sufficient to
achieve an eO(d
√
k/ε) sample complexity, which is a significant improvement over fully non-adaptive
approaches. In the agnostic case, we show that eO(
√
k) rounds of adaptivity is sufficient to achieve
the optimal sample complexity of eO((d + k)/ε2).
From a technical perspective, we establish the tradeoff between adaptivity and sample complex-
ity through two approaches. In the realizable case, our algorithms are based on a novel application
of a variant of the AdaBoost algorithm with a particular notion of margin. In the agnostic setting,
we introduce a general and abstract optimization problem called Optimization via On-Demand
Sampling (OODS). In this framework, the goal is to optimize a concave function f over [0, 1]k,
representing weights over k distributions. There is no notion of sample complexity in this setting;
instead, the algorithm can only access value and gradient information about f within a restricted
trust region, which the algorithm can expand in every round. At a high level, the extent of the trust
region serves as a proxy for sample complexity: the more a distribution is sampled, the better we
2

Setting
Sample Complexity
Round Complexity
Reference
Realizable
eO((d + k)/ε)
O(log k)
[BHPQ17, CZZ18, NZ18]
Agnostic
eO((log(|H|) + k)/ε2)
eO((log(|H|) + k)/ε2)
[HJZ22]
Agnostic
eO(d/ε4 + k/ε2)
O(log(k)/ε2)
[AHZ23]
Agnostic
eO((d + k)/ε2) · (log k)O(log(1/ε))
(log k)O(log(1/ε))
[Pen24]
Agnostic
eO((d + k)/ε2)
O(log(k)/ε2)
[ZZC+24]
Realizable
eO((k2/r · d + k)/ε)
r
Theorem 1
Realizable
eΩ(k1/r · d/r)
r
Theorem 2
Agnostic
eO((d + k)/ε2)
min{ eO(
√
k), O(k log k)}
Propositions 2 and 3
Table 1: An overview of sample-adaptivity tradeoff in multi-distribution learning. k is the number
of distributions. H is the hypothesis class and d is its VC dimension. r denotes a tunable round
complexity between 1 and O(log k). eO(·) and eΩ(·) suppress polylog(d, k, 1/ε, 1/δ) factors.
can estimate the performance of predictors on it. The number of times the trust region is expanded
before finding the optimum of f corresponds to the round complexity. We establish both upper
and lower bounds on the round complexity in the OODS setting.
A strength of the OODS framework is that algorithms developed for OODS naturally transfer to
the agnostic multi-distribution learning problem, forming the foundation for our performance guar-
antees. Additionally, the optimization formulation gives rise to more natural algorithm-independent
lower bounds on adaptivity. In particular, we prove poly(k) lower bounds on the round complexity
of the OODS problem. These lower bounds shed light on the challenges of achieving the optimal
sample complexity in agnostic multi-distribution learning using a sub-polynomial number of rounds.
1.1
Related Work
Multi-Distribution Learning
Blum, Haghtalab, Procaccia and Qiao [BHPQ17] introduced the
realizable setting of multi-distribution learning, for which several O(log k)-round algorithms with
near-optimal sample complexity of eO((d+k)/ε) were given [BHPQ17, CZZ18, NZ18]. On the other
hand, it is folklore that the sample complexity is Ω(dk/ε) without adaptive sampling. For the
more challenging agnostic setting where a “perfect” predictor may not exist, the optimal sample
complexity was shown to be eO((d + k)/ε2) in a series of recent work [HJZ22, AHZ23, ZZC+24,
Pen24].
Interestingly, all these algorithms have a round complexity of at least poly(1/ε) (see
Table 1 for details). Other variants of the problem, where some data sources might be adversarial
or differently labeled, have also been studied [Qia18, DQ24].
Power of Adaptivity in Learning and Beyond
Agarwal, Agarwal, Assadi and Khanna [AAAK17]
systematically formulated the tradeoff between adaptivity and sample complexity in several learn-
ing problems, including a batched setting of multi-armed bandits that was previously introduced by
3

Jun, Jamieson, Nowak and Zhu [JJNZ16] and subsequently studied in [GHRZ19, JYT+24, JZZ25].
Chen, Papadimitriou and Peng [CPP22] proposed a PAC learning framework of continual learn-
ing, and quantified the tradeoff between the number of sequential passes and the memory usage
of the learning algorithm. Another recent line of work focused on the adaptivity-query tradeoff in
submodular optimization [BS18, FMZ19, EN19, CQ19b, BRS19, CQ19a, ENV19, LLV20].
2
Preliminaries
Multi-Distribution Learning (MDL)
We follow the formulation of MDL in [HJZ22]. Let X
be the instance space and Y = {0, 1} be the binary label space. Let H ⊆YX be a hypothesis class
and d be its VC dimension. There are k unknown data distributions D1, D2, . . . , Dk over X × Y.
In each round, the algorithm draws samples from the k distributions. The number of samples may
differ on the k distributions, and may be chosen adaptively based on samples drawn in previous
rounds. The sample complexity is the total number of samples drawn from all distributions in all
rounds. An r-round algorithm draws r rounds of samples and has a round complexity of r.
The goal is to learn a predictor ˆh : X →Y that performs well on all k distributions D1, . . . , Dk.
Formally, letting err(ˆh, D) := Pr(x,y)∼D
h
ˆh(x) ̸= y
i
denote the population error of predictor ˆh on
distribution D, an MDL algorithm is (ε, δ)-PAC (Probably Approximately Correct) if
max
i∈[k] err(ˆh, Di) ≤OPT + ε where OPT := min
h∈H max
i∈[k] err(h, Di)
holds with probability at least 1 −δ over the randomness in both the algorithm and the samples.
In the realizable setting, the data distributions are promised to satisfy OPT = 0, i.e., there
exists a perfect predictor h⋆∈H such that err(h⋆, Di) = 0 for every i ∈[k]. We also refer to the
general MDL setting—where OPT can be non-zero—as the agnostic setting.
MDL via Game Dynamics
Most previous agnostic MDL algorithms (e.g., [HJZ22, ZZC+24,
Pen24]) view the learning problem as a zero-sum game, in which the “min player” chooses a
hypothesis (or a mixture of multiple hypotheses) and the “max player” chooses a mixture of the k
data distributions. These algorithms solve MDL by simulating the game dynamics when the two
players follow certain strategies, e.g., best response or a no-regret online learning algorithm. The
analysis then boils down to finding the sample size that suffices for simulating the game dynamics
accurately.
For instance, simulating a “min player” that best-responds to the “max player” is
equivalent to finding a hypothesis that approximately minimizes the error on a given mixture of
the k distributions.
3
Sample-Adaptivity Tradeoff for Realizable MDL
3.1
Overview of Upper Bound
For the realizable setting of MDL, we present an algorithm that establishes a tradeoff between
sample complexity and round complexity.
Theorem 1 (Informal version of Theorem 10). Algorithm 1 is an r-round (ε, δ)-PAC algorithm
for realizable MDL with sample complexity O(k2/r log k · d
ε + k log(k) log(k/δ)
ε
).
4

We highlight two special cases of the general tradeoff above. First, when r = log k, Algorithm 1
is most sample-efficient and recovers the near-optimal sample complexity bound of eO((d + k)/ε)
for realizable MDL [BHPQ17, CZZ18, NZ18]. Second, with a small constant number of adaptive
rounds (e.g., r = 4), Algorithm 1 has a sample complexity of eO((d
√
k + k)/ε). Thus, our result
demonstrates that even in the limited-adaptivity regime of r = O(1), we can improve on the Ω(dk/ε)
sample complexity required in the fully non-adaptive case of r = 1.
Remark 1. Using more sophisticated variants of boosting such as boost-by-majority [SF12, Chap-
ter 13] or recursive boosting [Sch90], it is possible to achieve a sample complexity of eO((d
√
k+k)/ε)
using exactly 3 rounds. Formal claims are deferred to Appendix A.
Additional Notations
To state our algorithm succinctly, we introduce a few more notations.
For a distribution D, we write D⊗m as its m-fold product distribution, and S ∼D⊗m is a shorthand
for drawing a size-m sample S from D. At each iteration t of the algorithm, we maintain weights
qt(1), qt(2), . . . , qt(k) ≥0 that sum up to 1.
We also abuse the notation and write qt as the
mixture distribution Pk
i=1 qt(i)·Di. While distributions D1, D2, . . . , Dk are unknown, the algorithm
may still sample from the mixture qt easily: it suffices to first draw a random index i such that
Pr [i = j] = qt(j) for each j ∈[k] and then sample from Di.
Algorithm 1: Trade-off Multi-Distribution Learning
Input: Sample access to k unknown distributions D1, . . . , Dk, an optimal PAC learner A
for class H, number of rounds r, target error ε, and failure probability δ.
1 Set margin θ =
r
2 log(k), p = 1
2 ·
 4k
2/r−1/(1−θ), τ =
ε
1+1/θ, and α = 1
2 ln

1−p
p

.
2 Set εA = τp
4 , δA = δ
2r, and m = O

d+log(1/δA)
εA

.
3 Initialize q1(j) = 1
k for each j ∈{1, . . . , k}, and let q1 = Pk
j=1 q1(j)Dj.
4 for t = 1, 2, . . . , r do
5
Call learner A on a sample eSt ∼q⊗m
t
, and let ht be the returned predictor.
6
for j = 1, 2, . . . , k do
7
Draw a sample Sj,t ∼D⊗n
j
, where n = 12
τ log(2rk/δ).
8
Update:
qt+1(j) = qt(j)
Zt
×
 e−α
if err(ht, Sj,t) ≤τ
2
eα
if err(ht, Sj,t) > τ
2
where Zt is a normalization constant that ensures Pk
j=1 qt+1(j) = 1.
Output: Majority-Vote Predictor F : x 7→1
1
r
Pr
t=1 ht(x) ≥1/2

.
Technical Overview
We explain here the main ideas behind Algorithm 1; the full proof and
analysis is deferred to Appendix A. We run a variant of the classical AdaBoost algorithm [SF12] to
maximize a particular notion of “margin” that is defined on distributions D1, . . . , Dk. Specifically,
in each round 1 ≤t ≤r, Algorithm 1 calls learner A to learn a predictor ht that has a low error on
5

qt:
k
X
j=1
qt(j) · err(ht, Dj) = err(ht, qt) ≤τp.
Then, by Markov’s inequality, ht also minimizes the fraction of distributions (as weighted by qt)
that have error more than τ:
k
X
j=1
qt(j)1 [err(ht, Dj) > τ] ≤p.
Subsequently, Algorithm 1 updates the weighted mixture qt over the k distributions based on the
thresholded loss function 1[err(ht, Dj) > τ] (as is done in AdaBoost). After r rounds, the margin-
maximization property of AdaBoost guarantees that
1
k
k
X
j=1
1
"
1
r
r
X
t=1
1[err(ht, Dj) > τ] > 1
2 −θ
2
#
≤
r
Y
t=1
2
q
(1 −p)1+θp1−θ.
By choosing the margin parameter θ and p such that Qr
t=1 2
p
(1 −p)1+θp1−θ < 1/k, we are guar-
anteed that, on each of the k distributions, at least 1/2+θ/2 fraction of the r predictors have error
at most τ. Formally, it holds for every j ∈[k] that
1
r
r
X
t=1
1[err(ht, Dj) > τ] ≤1
2 −θ
2.
Finally, with this margin property, invoking Lemma 13 implies that the majority-vote predictor
will have error at most (1 + 1/θ)τ = ε on all k distributions.
3.2
Overview of Lower Bound
The following theorem complements Theorem 1 by showing that a kΩ(1/r) overhead is unavoidable.
Theorem 2 (Informal version of Theorem 16). For every r = O(log k) and sufficiently large d,
every r-round algorithm for realizable MDL has an Ω

dk1/r
r log2 k

sample complexity.
This sample complexity lower bound nearly matches the dk2/r log k term in Theorem 1, up to
a poly(r, log k) factor and a factor of 2 in the exponent of kΘ(1/r).
We briefly sketch the proof of the r = 2 case, i.e., an Ω(d
√
k) lower bound against two-round
algorithms. We consider the class of linear functions over X = Fd
2, which has a VC dimension of d.
The ground truth classifier h⋆is drawn uniformly at random from all the 2d linear functions. To
construct the k data distributions, we choose k difficulty levels diff1, diff2, . . . , diffk as a uniformly
random permutation of: (1) 1 copy of Θ(d); (2)
√
k copies of Θ(d/
√
k); (3) k −
√
k −1 copies of
Θ(d/k). Note that Pk
i=1 diffi ≤d. Each data distribution Di is the uniform distribution over a
randomly chosen diffi-dimensional subspace Vi ⊆Fd
2. Furthermore, the subspaces V1, V2, . . . , Vk are
chosen such that they are linearly independent, i.e., dim(Span(V1 ∪V2 ∪· · · ∪Vk)) = Pk
i=1 diffi.
Intuitively, diffi measures the “effective sample complexity” for learning Di: Θ(diffi) samples
are sufficient and necessary to learn an accurate classifier for Di. In addition, since h⋆is randomly
chosen and the subspaces V1, V2, . . . , Vk are independent, samples collected from one distribution
6

Di provide no information about the value of h⋆on Vj (except for the zero vector) for every j ̸= i.
Furthermore, if diffi ∈{Θ(d/
√
k), Θ(d)} and m ≪d/
√
k samples have been drawn from Di, it
holds with high probability that the m vectors in these samples are linearly independent. Then,
the learner gains no information for distinguishing whether diffi = Θ(d/
√
k) or diffi = Θ(d).
A three-round learner has a simple strategy: (1) In Round 1, draw Θ(d/k) samples from each
distribution, thereby identifying the distributions with diffi = Θ(d/k) as well as learning the value
of h⋆on each Vi; (2) In Round 2, draw Θ(d/
√
k) samples from each of the
√
k + 1 remaining dis-
tributions, which is sufficient for all distributions except the one with diffi = Θ(d); (3) In Round 3,
learn the only remaining distribution using Θ(d) samples. The resulting sample complexity is O(d).
In contrast, a two-round learner must “skip” one of the three steps. For example, in Round 2
where there are still
√
k + 1 “suspects” among which one distribution has difficulty level Θ(d),
the learner could draw Θ(d) samples from each of them. Alternatively, the learner could draw
Θ(d/
√
k) samples from each distribution in Round 1, so that the distribution with diffi = Θ(d) can
be identified and then learned in Round 2. However, both strategies would have an Ω(d
√
k) sample
complexity.
The formal proof (in Appendix B) extends the hard instance construction to all r = O(log k)
by using r + 1 different difficulty levels separated by a k1/r factor. We then formalize the intuition
that every r-round MDL algorithm must “skip” a step and thus incur a k1/r overhead in the sample
complexity.
4
Sample-Adaptivity Tradeoff for Agnostic MDL
For the agnostic setting, we show that the near-optimal sample complexity of eO((d + k)/ε2) can be
achieved by a poly(k)-round algorithm.
Proposition 1 (Corollaries 21 and 22). There is a min{ eO(
√
k), O(k log k)}-round MDL algorithm
with sample complexity eO((d + k)/ε2).
The MDL Algorithm of [ZZC+24]
Our starting point is the approach of [ZZC+24, Al-
gorithm 1], which we briefly describe below.
For brevity, we use eO(·) and eΘ(·) to suppress
polylog(k, d, 1/ε, 1/δ) factors, and let err(h, S) :=
1
|S|
P
(x,y)∈S 1 [h(x) ̸= y] denote the empirical
error of hypothesis h : X →Y on dataset S ⊆X × Y.
The algorithm maintains k datasets S1, S2, . . . , Sk, where each Si contains training examples
drawn from Di. The algorithm runs the Hedge algorithm for T = Θ((log k)/ε2) iterations starting
at w(1) = (1/k, 1/k, . . . , 1/k). Each iteration t ∈[T] consists of the following two steps:
• ERM step: For each i ∈[k], draw additional samples from Di and add them to Si until
|Si| ≥w(t)
i
· eΘ((d + k)/ε2). Then, find a hypothesis h(t) ∈H that minimizes the empirical error
ˆL(h) := Pk
i=1 w(t)
i
· err(h, Si), which is an estimate of the error of h on Pk
i=1 w(t)
i Di.
• Hedge update step: For each i ∈[k], draw w(t)
i
· Θ(k) fresh samples from Di to obtain an
estimate r(t)
i
≈err(h(t), Di). Compute w(t+1) from w(t) and r(t) via a Hedge update.
The crux of the analysis of [ZZC+24] is to show that the dataset sizes in the two steps above
are sufficiently large, so that h(t) approximately minimizes the error on mixture Pk
i=1 w(t)
i Di, and
the reward vector r(t) is accurate enough for the Hedge update.
7

A straightforward implementation of the algorithm needs T = Θ((log k)/ε2) rounds of sampling.
In comparison, the eO(
√
k) round complexity in Proposition 1 is lower when ε ≪1/k1/4.
Hedge with Lazy Updates
We prove Proposition 1 by modifying the algorithm of [ZZC+24] so
that it draws samples more lazily. The resulting algorithm is termed LazyHedge and formally defined
in Algorithm 2. There are two versions of the algorithm—the “box” version and the “ellipsoid”
version—that give the O(k log k) and eO(
√
k) round complexity bounds, respectively.
Algorithm 2: LazyHedge: Hedge with Lazy Updates
Input: Number of distributions k, number of iterations T = Θ((log k)/ε2), step size
η = Θ(ε), margin parameter C > 1.
1 Box version: Define O(w) := {w ∈∆k−1 : wi ≤wi, ∀i ∈[k]}.
2 Ellipsoid version: Define O(w) := {w ∈∆k−1 : Pk
i=1 w2
i /wi ≤1}.
3 Set w(1) = (1/k, 1/k, . . . , 1/k) and w(0) = (0, 0, . . . , 0).
4 Set Si = ∅for i ∈[k] and Si,t = ∅for i ∈[k] and t ∈[T].
5 for t = 1, 2, . . . , T do
6
if w(t) ∈O(w(t−1)) then
7
Set w(t) = w(t−1).
8
else
9
Set w(t)
i
= C · max{w(1)
i , w(2)
i , . . . , w(t)
i } for every i ∈[k].
10
Add samples from Di to Si until |Si| ≥w(t)
i
· eΘ((d + k)/ε2) for every i ∈[k].
11
Add samples from Di to Si,t′ until |Si,t′| ≥w(t)
i
· Θ(k) for every i ∈[k] and
t ≤t′ ≤T.
12
ERM step: Set ˆh(t) ∈argminh∈H
Pk
i=1 w(t)
i
· err(h, Si).
13
Hedge update step: Set r(t)
i
= err(ˆh(t), Si,t). Compute w(t+1) ∈∆k−1 such that
w(t+1)
i
=
w(t)
i
·eηr(t)
i
Pk
j=1 w(t)
j ·e
ηr(t)
j
for every i ∈[k].
Output: Randomized classifier uniformly distributed over {ˆh(1), ˆh(2), . . . , ˆh(T)}.
Similar to the Hedge algorithm, LazyHedge maintains a weight vector w(t) at each iteration
t. In addition, it maintains a cap vector w(t)
i
as a proxy for the size of dataset Si at time t. At
the start of iteration t, it checks whether w(t) is “observable” under cap w(t−1) in the sense that
w(t) ∈O(w(t−1)). If the condition holds, no additional samples are drawn and the cap vector is left
unchanged. Otherwise, the cap w(t) is updated to C times the entrywise maximum of all weight
vectors so far, and additional samples are drawn so that both |Si| and |Si,t| match w(t)
i . Finally,
LazyHedge computes the next weight vector w(t+1) from w(t) using the Hedge update rule.
The correctness and sample complexity of LazyHedge follow from the analysis of [ZZC+24]. At
a high level, either version of LazyHedge ensures that using Si in the ERM step and using Si,t in
the Hedge update step lead to low-variance estimates, which allow the analysis of [ZZC+24] to go
through. We provide a more detailed analysis in Appendix C.4.
It remains to upper bound the round complexity of LazyHedge, namely, the number of times
the cap vector is updated in Line 9. For the box version, we have an O(k log k) upper bound.
8

Proposition 2. The box version of LazyHedge takes at most O(k log k) rounds.
Proof sketch. If the cap vector is updated in the t-th iteration, there exists i ∈[k] such that
w(t)
i
> w(t−1)
i
. We call such index i the culprit of this cap update. For index i to be the culprit,
the historical high of w(t)
i
must have increased by a factor of C since the last cap update. As this
historical high is non-decreasing and in [1/k, 1], each index i can be the culprit at most O(logC k)
times. Thus, the round complexity is at most k·O(logC k) = O(k log k) for any constant C > 1.
For the ellipsoid version, a more involved analysis gives an eO(
√
k) round complexity bound.
Proposition 3. The ellipsoid version of LazyHedge takes at most eO(
√
k) rounds.
The analysis applies the following technical lemma shown by [ZZC+24].
Lemma 3 (Lemma 3 of
[ZZC+24]). For some choice of T = Θ((log k)/ε2) and η = Θ(ε) in
LazyHedge, it holds with probability 1 −δ that Pk
i=1 max1≤t≤T w(t)
i
≤O(log8(k/(εδ))) = eO(1).
Proof sketch of Proposition 3. We classify the cap updates into two types: A “Type I” update is
when some coordinate wi reaches a historical high of > 1/
√
k, and a “Type II” update is one
without a significant increase in any coordinate. We show that either type of cap updates happen
eO(
√
k) times.
The upper bound for Type I updates follows from Lemma 3, which implies that there are at
most eO(1) ·
√
k Type I updates where the coordinate reaches ≈1/
√
k, at most eO(1) ·
√
k/2 Type I
updates where the coordinate reaches ≈2/
√
k, and so on. These upper bounds sum up to eO(
√
k).
The analysis for Type II updates is more involved. Roughly speaking, we say that a coordinate
i ∈[k] gains a potential of a2/b when the historical high of wi increases from b to a through the
Hedge dynamics. The eO(
√
k) bound follows from two technical claims: (1) Each Type II update
may happen only if a total potential of Ω(1) is accrued over all k coordinates; (2) The total potential
that the k coordinates may contribute to Type II updates is at most eO(
√
k).
5
A General Framework: Optimization via On-Demand Sampling
5.1
Problem Setup
In Optimization via On-Demand Sampling (OODS), the goal is to maximize a concave function f
over the probability simplex ∆k−1 := {w ∈Rk : Pk
i=1 wi = 1, wi ≥0 ∀i ∈[k]} by “sampling” from
the k coordinates. The algorithm does not have full access to f; instead, it maintains a cap vector
w ∈[0, 1]k that specifies the observable region of the simplex. We focus on two concrete settings of
the problem, where the observable region is either a box or an ellipsoid defined by w.
Definition 1 (Optimization via On-Demand Sampling). f : ∆k−1 →[0, 1] is an unknown concave
function. In each round t = 1, 2, . . . , r, the algorithm chooses cap w(t) ∈[0, 1]k that is lower bounded
by w(t−1) entry-wise (if t > 1). Then, the algorithm makes arbitrarily many queries to a first-order
oracle of f—which returns the value and a supergradient—at any w ∈O(w(t)), where O(w) :=
{w ∈∆k−1 : wi ≤wi, ∀i ∈[k]} in the box setting, and O(w) := {w ∈∆k−1 : Pk
i=1 w2
i /wi ≤1}
in the ellipsoid setting. The goal is to find ˆw ∈∆k−1 such that f( ˆw) ≥maxw∈∆k−1 f(w) −ε while
minimizing the sample overhead Pk
i=1 w(r)
i
and the round complexity r.
9

To see how OODS connects to MDL and on-demand sampling in general, we view the k coordi-
nates as distributions D1, D2, . . . , Dk from which the algorithm may sample. Maximizing f(w) can
then be viewed as optimizing the mixing weights in mixture Pk
i=1 wiDi. The cap wi is a proxy for
and proportional to the number of samples that have already been drawn from Di. In light of this
analogy, the sample overhead Pk
i=1 w(r)
i
is simply a proxy for the total number of samples that the
algorithm draws, while the round complexity r is the number of rounds of on-demand sampling.
The observable region O(w) represents the mixing weights w ∈∆k−1 on which f(w) can be
accurately estimated using the current dataset specified by w. In the box setting, the algorithm
is only allowed to query f(w) if wi ≤wi holds for every i ∈[k], which can be viewed as a
sufficient condition for the algorithm to obtain an accurate estimate for mixture Pk
i=1 wiDi using
the Θ(wi) samples collected from each Di. In the ellipsoid setting, we use the more refined condition
Pk
i=1 w2
i /wi ≤1, where the summation is a proxy for the variance in estimating the mixture
Pk
i=1 wiDi using the datasets. More details on how the box and ellipsoid settings connect to MDL
can be found in Appendix C.4.
5.2
Overview of Upper Bounds
For the OODS problem, we give a simple algorithm with a poly(k) round complexity. The algorithm
is also termed LazyHedge, as it is almost identical to the agnostic MDL algorithm in Section 4. We
formally define the algorithm (Algorithm 3) in Appendix C for completeness. To guarantee a sample
overhead of eO(s), the algorithm takes eO(k/s) rounds in the box setting and eO(
p
k/s) rounds in
the ellipsoid setting. Here, the eO(·) notation hides polylog(k/ε) factors, where ε is the accuracy
parameter in OODS.
Theorem 4 (Informal version of Theorems 8 and 20). There is an OODS algorithm with sample
overhead eO(s) that takes eO(k/s) rounds in box setting and eO(
p
k/s) rounds in ellipsoid setting.
Hedge with Lazy Updates
We apply the same LazyHedge strategy as in Algorithm 2 for MDL.
LazyHedge maintains a weight vector w(t) at each iteration t. At the start of iteration t, it checks
whether w(t) is still in the observable region O(w(t−1)) specified by the previous cap w(t−1). If so,
the cap is left unchanged; otherwise, the cap w(t) is set to C times the entrywise maximum of all
weight vectors so far. Then, LazyHedge queries the first-order oracle to obtain a supergradient r(t)
at w(t), and computes the next weight vector w(t+1) using the Hedge update. While LazyHedge
takes T = Θ((log k)/ε2) iterations, its round complexity is the number of times the cap is updated,
which can be much lower than T.
Analysis for the Box Setting
We sketch the analysis for the box setting, and defer the ellipsoid
setting to Appendix C. The standard regret analysis of Hedge shows that LazyHedge finds an O(ε)-
approximate maximum. We prove the following lemma in Appendix C.1.
Lemma 5. LazyHedge outputs ˆw ∈∆k−1 such that f( ˆw) ≥maxw∈∆k−1 f(w) −O(ε).
Next, we show that the sample overhead of LazyHedge is low. Recall that C > 1 is the margin
parameter used in LazyHedge for the cap updates.
Lemma 6. LazyHedge has an O(C log8(k/ε)) sample overhead.
10

Proof. LazyHedge guarantees that w(T)
i
≤C · max1≤t≤T w(t)
i
for every i ∈[k]. By Lemma 3, the
sample overhead is Pk
i=1 w(T)
i
≤C · Pk
i=1 max1≤t≤T w(t)
i
= O(C · log8(k/ε)).
It remains to upper bound the round complexity of LazyHedge in the box setting. The proof of
the following lemma resembles and extends that of Proposition 2 in the MDL setting.
Lemma 7. LazyHedge takes min

k, O((k/C) · log8(k/ε)

} · O(logC k) rounds in the box setting.
Proof sketch. If the cap is updated in the t-th iteration of LazyHedge, there exists i ∈[k] such that
w(t)
i
> w(t−1)
i
. We call such index i the culprit of this cap update. By the same argument as in
Proposition 2, each index i can be the culprit of at most O(logC k) cap updates. It remains to
bound the number of indices that become the culprit of at least one cap update. This number
is trivially at most k. Furthermore, since LazyHedge sets w(1) = (C/k, C/k, . . . , C/k) in the first
iteration, for index i to become the culprit of a later cap update, w(t)
i
must reach C/k for some t.
By Lemma 3, at most eO(1)/(C/k) = eO(k/C) indices can satisfy this. Thus, the round complexity
is at most min{k, eO(k/C)} · O(logC k).
Combining Lemmas 5, 6 and 7 immediately gives the first part of Theorem 4.
Theorem 8. For any C ∈[2, k], in the box setting, LazyHedge finds an O(ε)-approximate maximum
with an O(C log8(k/ε)) sample overhead in min{O(k log k), O((k/C) · log9(k/ε))} rounds.
5.3
Overview of Lower Bounds
The following theorem shows that the poly(k/s) round complexity in Theorem 4 cannot be avoided
when ε ≤1/ poly(k). For exponentially small ε, the exponents on k/s also match Theorem 4.
Theorem 9 (Informal version of Theorems 25 and 28). If ε ≤O(1/k), every OODS algorithm
with sample overhead s must take Ω(
p
k/s) rounds in the box setting and Ω((k/s)1/4) rounds in the
ellipsoid setting. If ε ≤e−Ω(k), every OODS algorithm with sample overhead s must take Ω(k/s)
rounds in the box setting and Ω(
p
k/s) rounds in the ellipsoid setting.
As a corollary, if ε ≤O(1/k), every OODS algorithm has either a poly(k) round complexity or
a sample overhead that is almost linear in k. While these lower bounds do not directly imply lower
bounds for agnostic MDL, they show that further improving the round complexity in Proposition 1
requires a substantially different approach. Roughly speaking, the MDL algorithm of [ZZC+24] fits
into the OODS framework because: (1) It uses samples in a restricted way: finding an ERM ˆh on
mixture Pk
i=1 wiDi for some weight vector w in an “observable region”, and estimating the error of
ˆh on every Di; (2) By solving the MDL instance, it finds a “hard” mixture Pk
i=1 ˆwiDi on which the
best hypothesis in H has an error close to the minimax value. The first property ensures that the
MDL algorithm requires no more information than what the first-order oracle provides in OODS.
The second ensures that the MDL algorithm implicitly solves the OODS problem. Theorem 9 then
suggests that every algorithm with the two properties faces an inherent obstacle in achieving a
sub-polynomial round complexity.
We sketch the proof of Theorem 9 in the box setting and the ε ≤O(1/k) regime; the formal
proofs are deferred to Appendix D. We consider the objective function f(w) := minj∈[m]{wi⋆
j +
j/m2}, where i⋆
1, i⋆
2, . . . , i⋆
m ∈[k] are m ≤k different critical indices sampled uniformly at random.
11

The lower bound builds on two observations on f: (1) (Lemma 23) If ε ≤O(1/k), every ε-
approximate maximum must put an Ω(1/m) weight on at least half of the critical indices i⋆
1, . . . , i⋆
m;
(2) (Lemma 24) Unless we put a weight of > 1/m2 on each of i⋆
1, i⋆
2, . . . , i⋆
j, the value of f(w) is
determined by the first j terms in the minimum. Intuitively, unless we already “know” the first j
critical indices, we cannot learn the values of i⋆
j+1, . . . , i⋆
m from the first-order oracle.
There is a natural m-round algorithm that solves the instance above. In the first round, we
query the uniform weight vector w = (1/k, 1/k, . . . , 1/k) to learn the value of i⋆
1. In the second
round, we query f on some w with wi⋆
1 ≫1/m2, thereby learning the value of i⋆
2. Repeating this
m times recovers all the critical indices. One might hope to be “more clever” and learn many
critical indices in a round. For example, the algorithm might put a cap of ≫1/m2 on several
coordinates in the first round, in the hope of hitting more than one indices in i⋆
1, i⋆
2, . . .. However,
if the algorithm has a sample overhead of s, only O(m2s) such guesses can be made. In particular,
assuming m ≪
p
k/s, the O(m2s) ≪k guesses only cover a tiny fraction of the indices. Thus, over
the uniform randomness in i⋆, the algorithm learns only O(1) critical indices within each round in
expectation.
6
Discussion
In this work, we formalized the trade-off between sample and round complexities in multi-distribution
learning (MDL). For the realizable case, we obtained a nearly tight characterization: when the
learner is allowed r rounds of sampling, the optimal sample complexity is proportional to kΘ(1/r).
In particular, a constant number of rounds suffice to achieve a sublinear dependence on k, whereas
nearly log k rounds are necessary to reach near-optimal sample complexity.
For the more general agnostic setting, we introduced the optimization via on-demand sampling
(OODS) problem as an abstraction of the common approach shared by many recent MDL algo-
rithms. We then leveraged the intuition behind the OODS algorithms to obtain an improved round
complexity of eO(
√
k). On the negative side, any MDL algorithm based on the OODS approach
must take poly(k) rounds to match the near-optimal sample complexity of eO((d + k)/ε2).
To further understand the landscape of sample-adaptivity trade-off in agnostic MDL, we high-
light the following concrete open question:
Open Question 1. Does there exist an agnostic MDL algorithm that simultaneously achieves
sample complexity eO
  d+k
ε2

and round complexity polylog(k/ε)?
Our results on OODS may shed light on both directions. The lower bounds for OODS suggest
that any such algorithm must leverage the data in a more sophisticated manner—beyond invoking
an ERM oracle or merely evaluating empirical risks. Notably, a recent algorithm of Peng [Pen24] is
one such candidate: despite having a high round complexity (cf. Table 1), it uses the collected data
to construct a “refined” hypothesis class and might therefore circumvent the OODS lower bounds.
Conversely, to establish a round lower bound for sample-optimal MDL algorithms, a natural
approach would be to recast our hard instances for OODS as MDL instances. The key challenge,
however, lies in ensuring that every MDL algorithm gains no more information from the samples
than OODS-based learners do. Developing such a reduction would establish a deeper connection
between MDL and OODS, and represent a major step toward a complete understanding of the
sample-adaptivity trade-off.
12

Acknowledgments
This work was supported in part by the National Science Foundation under grant CCF-2145898,
by the Office of Naval Research under grant N00014-24-1-2159, an Alfred P. Sloan fellowship, and
a Schmidt Science AI2050 fellowship. Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and do not necessarily reflect the views of
sponsoring agencies.
References
[AAAK17] Arpit Agarwal, Shivani Agarwal, Sepehr Assadi, and Sanjeev Khanna. Learning with
limited rounds of adaptivity: Coin tossing, multi-armed bandits, and ranking from
pairwise comparisons. In Satyen Kale and Ohad Shamir, editors, Proceedings of the
30th Conference on Learning Theory, COLT 2017, Amsterdam, The Netherlands, 7-
10 July 2017, volume 65 of Proceedings of Machine Learning Research, pages 39–75.
PMLR, 2017. 1.1
[AHZ23] Pranjal Awasthi, Nika Haghtalab, and Eric Zhao. Open problem: The sample complex-
ity of multi-distribution learning for VC classes. In Gergely Neu and Lorenzo Rosasco,
editors, The Thirty Sixth Annual Conference on Learning Theory, COLT 2023, 12-15
July 2023, Bangalore, India, volume 195 of Proceedings of Machine Learning Research,
pages 5943–5949. PMLR, 2023. 1, 1.1
[BHPQ17] Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, and Mingda Qiao. Collaborative PAC
learning. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Pro-
cessing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 2392–2401,
2017. 1, 1, 1.1, 3.1
[BRS19] Eric Balkanski, Aviad Rubinstein, and Yaron Singer.
An optimal approximation
for submodular maximization under a matroid constraint in the adaptive complex-
ity model. In Moses Charikar and Edith Cohen, editors, Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA,
June 23-26, 2019, pages 66–77. ACM, 2019. 1.1
[BS18] Eric Balkanski and Yaron Singer. The adaptive complexity of maximizing a submod-
ular function. In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors,
Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2018, Los Angeles, CA, USA, June 25-29, 2018, pages 1138–1151. ACM, 2018.
1.1
[CPP22] Xi Chen, Christos H. Papadimitriou, and Binghui Peng. Memory bounds for continual
learning. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS
2022, Denver, CO, USA, October 31 - November 3, 2022, pages 519–530. IEEE, 2022.
1.1
13

[CQ19a] Chandra Chekuri and Kent Quanrud. Parallelizing greedy for submodular set function
maximization in matroids and beyond. In Moses Charikar and Edith Cohen, editors,
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 78–89. ACM, 2019. 1.1
[CQ19b] Chandra Chekuri and Kent Quanrud. Submodular function maximization in parallel
via the multilinear relaxation. In Timothy M. Chan, editor, Proceedings of the Thirti-
eth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego,
California, USA, January 6-9, 2019, pages 303–322. SIAM, 2019. 1.1
[CZZ18] Jiecao Chen, Qin Zhang, and Yuan Zhou. Tight bounds for collaborative PAC learning
via multiplicative weights. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kris-
ten Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pages
3602–3611, 2018. 1, 1, 1.1, 3.1
[DQ24] Yuyang Deng and Mingda Qiao. Collaborative learning with different labeling functions.
In Forty-first International Conference on Machine Learning, ICML 2024, Vienna,
Austria, July 21-27, 2024, pages 10530–10552. OpenReview.net, 2024. 1.1
[EN19] Alina Ene and Huy L. Nguyen. Submodular maximization with nearly-optimal approx-
imation and adaptivity in nearly-linear time. In Timothy M. Chan, editor, Proceedings
of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2019,
San Diego, California, USA, January 6-9, 2019, pages 274–282. SIAM, 2019. 1.1
[ENV19] Alina Ene, Huy L. Nguyen, and Adrian Vladu. Submodular maximization with matroid
and packing constraints in parallel.
In Moses Charikar and Edith Cohen, editors,
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages 90–101. ACM, 2019. 1.1
[FMZ19] Matthew Fahrbach, Vahab S. Mirrokni, and Morteza Zadimoghaddam. Submodular
maximization with nearly optimal approximation, adaptivity and query complexity. In
Timothy M. Chan, editor, Proceedings of the Thirtieth Annual ACM-SIAM Symposium
on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019,
pages 255–273. SIAM, 2019. 1.1
[GHRZ19] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed ban-
dits problem. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alch´e-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Infor-
mation Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages
501–511, 2019. 1.1
[Han16] Steve Hanneke. Refined error bounds for several learning algorithms. J. Mach. Learn.
Res., 17:135:1–135:55, 2016. 11
[HJZ22] Nika Haghtalab, Michael I. Jordan, and Eric Zhao. On-demand sampling: Learning
optimally from multiple distributions. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
14

Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Pro-
cessing Systems 35: Annual Conference on Neural Information Processing Systems
2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.
1, 1, 1.1, 2, 2
[HJZ23] Nika Haghtalab, Michael I. Jordan, and Eric Zhao. A unifying perspective on multi-
calibration: Game dynamics for multi-objective learning. In Alice Oh, Tristan Nau-
mann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Ad-
vances in Neural Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, Decem-
ber 10 - 16, 2023, 2023. 1
[JJNZ16] Kwang-Sung Jun, Kevin G. Jamieson, Robert D. Nowak, and Xiaojin Zhu. Top arm
identification in multi-armed bandits with batch arm pulls. In Arthur Gretton and
Christian C. Robert, editors, Proceedings of the 19th International Conference on Ar-
tificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11, 2016, vol-
ume 51 of JMLR Workshop and Conference Proceedings, pages 139–148. JMLR.org,
2016. 1.1
[JYT+24] Tianyuan Jin, Yu Yang, Jing Tang, Xiaokui Xiao, and Pan Xu.
Optimal batched
best arm identification. In Amir Globersons, Lester Mackey, Danielle Belgrave, An-
gela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in
Neural Information Processing Systems 38: Annual Conference on Neural Information
Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15,
2024, 2024. 1.1
[JZZ25] Tianyuan Jin, Qin Zhang, and Dongruo Zhou. Breaking the log(1/∆2) barrier: Better
batched best arm identification with adaptive grids. In International Conference on
Learning Representations (ICLR), 2025. 1.1
[KNRW18] Michael J. Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness
gerrymandering: Auditing and learning for subgroup fairness. In Jennifer G. Dy and
Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, vol-
ume 80 of Proceedings of Machine Learning Research, pages 2569–2577. PMLR, 2018.
1
[Lar23] Kasper Green Larsen. Bagging is an optimal PAC learner. In Gergely Neu and Lorenzo
Rosasco, editors, The Thirty Sixth Annual Conference on Learning Theory, COLT
2023, 12-15 July 2023, Bangalore, India, volume 195 of Proceedings of Machine Learn-
ing Research, pages 450–468. PMLR, 2023. 11
[LLV20] Wenzheng Li, Paul Liu, and Jan Vondr´ak. A polynomial lower bound on adaptive com-
plexity of submodular maximization. In Konstantin Makarychev, Yury Makarychev,
Madhur Tulsiani, Gautam Kamath, and Julia Chuzhoy, editors, Proceedings of the 52nd
Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago, IL,
USA, June 22-26, 2020, pages 140–152. ACM, 2020. 1.1
15

[MRT18] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning,
second edition. Adaptive Computation and Machine Learning series. MIT Press, 2018.
C.1
[MSS19] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages
4615–4625. PMLR, 2019. 1
[NZ18] Huy L. Nguyen and Lydia Zakynthinou. Improved algorithms for collaborative PAC
learning. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,
Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada, pages 7642–7650, 2018.
1, 1, 1.1, 3.1
[Pen24] Binghui Peng. The sample complexity of multi-distribution learning. In Shipra Agrawal
and Aaron Roth, editors, The Thirty Seventh Annual Conference on Learning Theory,
June 30 - July 3, 2023, Edmonton, Canada, volume 247 of Proceedings of Machine
Learning Research, pages 4185–4204. PMLR, 2024. 1, 1, 1.1, 2, 6
[Qia18] Mingda Qiao. Do outliers ruin collaboration? In Jennifer G. Dy and Andreas Krause,
editors, Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceed-
ings of Machine Learning Research, pages 4177–4184. PMLR, 2018. 1.1
[RY21] Guy N. Rothblum and Gal Yona. Multi-group agnostic PAC learnability. In Marina
Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on
Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Pro-
ceedings of Machine Learning Research, pages 9107–9115. PMLR, 2021. 1
[Sch90] Robert E. Schapire. The strength of weak learnability. Mach. Learn., 5:197–227, 1990.
1, A
[SF12] Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The
MIT Press, 2012. 1, 8, A, A, 15, A
[SKHL20] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distribu-
tionally robust neural networks. In 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020. 1
[SRC24] Judy Hanwen Shen, Inioluwa Deborah Raji, and Irene Y. Chen. The data addition
dilemma. In Machine Learning for Healthcare Conference, 2024. 1
[TH22] Christopher J. Tosh and Daniel Hsu.
Simple and near-optimal algorithms for hid-
den stratification and multi-group learning. In Kamalika Chaudhuri, Stefanie Jegelka,
16

Le Song, Csaba Szepesv´ari, Gang Niu, and Sivan Sabato, editors, International Confer-
ence on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,
volume 162 of Proceedings of Machine Learning Research, pages 21633–21657. PMLR,
2022. 1
[XPD+23] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S
Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures
speeds up language model pretraining. In Advances in Neural Information Processing
Systems, pages 69798–69818, 2023. 1
[ZZC+24] Zihan Zhang, Wenhao Zhan, Yuxin Chen, Simon S. Du, and Jason D. Lee. Optimal
multi-distribution learning. In Shipra Agrawal and Aaron Roth, editors, The Thirty
Seventh Annual Conference on Learning Theory, June 30 - July 3, 2023, Edmonton,
Canada, volume 247 of Proceedings of Machine Learning Research, pages 5220–5223.
PMLR, 2024. 1, 1, 1.1, 2, 4, 4, 13, 13, 3, 5.3, C.3, C.4, C.4, C.4, C.4
17

A
Upper Bound for Realizable MDL
In this section, we prove the following upper bound on sample-adaptivity tradeoff for realizable
multi-distribution learning:
Theorem 10. For any k unknown distributions D1, . . . , Dk, any class H such that OPT = 0, and
any target error ε and number of rounds r ≤log(k), the output predictor F of Algorithm 1 satisfies
with probability at least 1 −δ,
∀1 ≤j ≤k : err(F, Dj) ≤ε.
Furthermore, Algorithm 1 uses r adaptive rounds and has a sample complexity of
O

k2/r log(k)d
ε + k log(k)
ε
log
k
δ

.
Before proceeding with the proof of Theorem 10, we state a few lemmas that will be useful in the
proof. First, instead of using plain ERM which incurs a sample complexity of O(d log(1/ε)+log(1/δ)
ε
),
it will be particularly advantageous for us to use an optimal PAC learner that avoids the log(1/ε)
factor in sample complexity.
Lemma 11 (Optimal Sample Complexity of PAC Learning, [Han16, Lar23]). For any class H, there
exists an improper learner A, such that for any distribution D where infh∈H err(h, D) = 0, with
probability at least 1 −δ over S ∼Dm where m = O(d+log(1/δ)
ε
), err(A(S), D) ≤ε. In particular,
as shown by [Lar23], bagging combined with ERM yields an optimal PAC learner A.
Next, the lemma below essentially allows us to argue that we can perform weight updates in
Algorithm 1 based on empirical error instead of population error.
Lemma 12 (Empirical Samples and Population Error). For any fixed predictor h : X →Y, any
distribution D over X × Y, any δ ∈(0, 1), any τ ∈(0, 1/2), with probability at least 1 −δ over
S ∼Dn where n = 12
τ log(1/δ), the followings holds:
1. If err(h, D) > τ then err(h, S) > τ/2, and
2. if err(h, S) > τ/2 then err(h, D) > τ/4.
Proof. We apply a standard Chernoff bound.
Finally, the lemma below shows that, when at least a 1/2 + θ/2 fraction of predictors achieve
error at most τ on a distribution D, the majority-vote achieves error at most (1 + 1/θ)τ on D.
Lemma 13 (Weighted-Majority Error Bound). Let D be an arbitrary distribution over X × Y,
τ ∈(0, 1), θ ∈(0, 1).
For any predictors h1, . . . , hr with corresponding (non-negative) weights
α1, . . . , αr that satisfy
r
X
t=1
αt1[err(ht, D) ≤τ] −
r
X
t=1
αt1[err(ht, D) > τ] ≥θ
r
X
t=1
αt,
it holds that
Pr
(x,y)∼D
"
1
" r
X
t=1
αtht(x) ≥1
2
r
X
t=1
αt
#
̸= y
#
≤

1 + 1
θ

τ.
18

Proof. Let Gτ = {1 ≤t ≤r : err(ht, D) ≤τ}, and Bτ = [r] \ Gτ. Without loss of generality, we
will consider the normalized weights, i.e., we assume Pr
t=1 αt = 1.
Let WG = P
t∈Gτ αt and
WB = P
t∈Bτ αt. Observe that
Pr(x,y)∼D
"
1
" r
X
t=1
αtht(x) ≥1
2
#
̸= y
#
≤Pr(x,y)∼D
" r
X
t=1
αt1[ht(x) ̸= y] ≥1
2
#
≤Pr(x,y)∼D
" X
t∈Gτ
αt1[ht(x) ̸= y] ≥1
2 −WB
#
≤
P
t∈Gτ αterr(ht, D)
1
2 −WB
≤
τWG
1
2 −WB
= τ ·
2WG
1 −2WB
= τ ·
2WG
WG −WB
= τ · (WG + WB) + (WG −WB)
WG −WB
= τ

1
WG −WB
+ 1

≤τ

1 + 1
θ

,
where we have used Markov’s inequality and the fact that WG −WB ≥θ.
We are now ready to proceed with the proof of Theorem 10.
Proof of Theorem 10. Before analyzing Algorithm 1, with probability at least 1−δ/2 over sampling
in Line 5, we have the following PAC learning guarantee
∀1 ≤t ≤r : err(ht, qt) =
k
X
j=1
qt(j)err(ht, Dj) ≤εA = τp
4 .
(1)
Ideally, we would compute weight updates in a round t ∈[r] in Line 8 in Algorithm 1 based on
evaluating the population error err(ht, Dj) for each j ∈{1, . . . , k}. However, we only have access to
samples from each Dj. In the analysis below, we show that we can effectively carry out the weight
updates based on samples.
This hinges on the following property which follows from invoking
Lemma 12: with probability at least 1 −δ/2 over sampling in Line 7, we are guaranteed
∀1 ≤t ≤r, ∀1 ≤j ≤k :
err(ht, Dj) > τ ⇒err(ht, Sj,t) > τ/2,
err(ht, Sj,t) > τ/2 ⇒err(ht, Dj) > τ/4.
(2)
We now proceed with analysis of Algorithm 1 assuming Equations (1) and (2) hold simul-
taneously, which happens with probability at least 1 −δ. The essence of the proof lies in ana-
lyzing an appropriate “margin-type” loss function defined based on the majority-vote predictor
F(x) = 1
1
r
Pr
t=1 ht(x) ≥1/2

and the distributions D1, . . . , Dk. Specifically, for each distribution
Dj, we will analyze the following 0-1 loss function:
Lτ,θ(F, Dj) = 1
" r
X
t=1
1 [err(ht, Dj) ≤τ] −
r
X
t=1
1 [err(ht, Dj) > τ] ≤θ · r
#
.
19

This loss function considers the “margin” which in this context is the fraction of good predictors
achieving error below threshold τ minus the fraction of bad predictors which have error above τ:
1
r
 r
X
t=1
1 [err(ht, Dj) ≤τ] −
r
X
t=1
1 [err(ht, Dj) > τ]
!
.
The loss function Lτ,θ(F, Dj) evaluates to 0 if and only if the margin is greater than θ, i.e., the
fraction of predictors ht achieving population error err(ht, Dj) below the threshold τ is greater than
1
2 + θ
2.
Observe now that when Lτ,θ(F, Dj) = 1, by definition,
r
X
t=1
α1 [err(ht, Dj) ≤τ] −
r
X
t=1
α1 [err(ht, Dj) > τ] ≤θ
r
X
t=1
α
⇐⇒
r
X
t=1
α (1 [err(ht, Dj) > τ] −1 [err(ht, Dj) ≤τ]) + θ
r
X
t=1
α ≥0
⇐⇒exp
 r
X
t=1
α (1 [err(ht, Dj) > τ] −1 [err(ht, Dj) ≤τ]) + θ
r
X
t=1
α
!
≥1.
Thus, we can bound from above the fraction of distributions Dj for which Lτ,θ(F, Dj) = 1 as follows
1
k
k
X
j=1
Lτ,θ(F, Dj)
≤1
k
k
X
j=1
exp
 r
X
t=1
α (1 [err(ht, Dj) > τ] −1 [err(ht, Dj) ≤τ]) + θ
r
X
t=1
α
!
= exp
 
θ
r
X
t=1
α
!
k
X
j=1
1
k exp
 r
X
t=1
α (1 [err(ht, Dj) > τ] −1 [err(ht, Dj) ≤τ])
!
≤exp
 
θ
r
X
t=1
α
!
k
X
j=1
1
k exp
 r
X
t=1
α (1 [err(ht, Sj,t) > τ/2] −1 [err(ht, Sj,t) ≤τ/2])
!
,
where the last inequality follows from Equation (2). Observe now that by the update rule in Line
9 in Algorithm 1,
qr+1(j) = q1(j) exp (Pr
t=1 α (1 [err(ht, Sj,t) > τ/2] −1 [err(ht, Sj,t) ≤τ/2]))
Qr
t=1 Zt
.
Thus, by combining the above, it follows that
1
k
k
X
j=1
Lτ,θ(F, Dj) ≤exp
 
θ
r
X
t=1
α
!
r
Y
t=1
Zt =
r
Y
t=1
eθαZt,
(3)
where the normalization constant
Zt =
k
X
j=1
qt(j) exp (α (1 [err(ht, Sj,t) > τ/2] −1 [err(ht, Sj,t) ≤τ/2])) .
20

Observe that, by Equation (2), if err(ht, Sj,t) > τ/2 then err(ht, Dj) > τ/4, thus we can bound Zt
from above as follows
Zt ≤
k
X
j=1
qt(j) exp (α (1 [err(ht, Dj) > τ/4] −1 [err(ht, Dj) ≤τ/4])) = e−α(1 −pt) + eαpt,
where pt = Prj∼qt [err(ht, Dj) > τ/4]. Next, we bound pt from above by the parameter p. To do
this, we invoke Markov’s inequality and the PAC learning guarantee in Equation (1),
pt = Prj∼qt [err(ht, Dj) > τ/4] ≤Ej∼qt [err(ht, Dj)]
τ/4
≤εA
τ/4 = p.
(4)
Since α = 1
2 ln

1−p
p

and 1 −p > p, combined with the above, we get
Zt ≤e−α(1 −p) + eαp =
r
p
1 −p(1 −p) +
r1 −p
p
p = 2
p
p(1 −p).
Combining Equation (3) with the above bound on Zt, we get
1
k
k
X
j=1
Lτ,θ(F, Dj) ≤
r
Y
t=1

eθα · 2
p
p(1 −p)

=
r
Y
t=1
2
q
(1 −p)1+θp1−θ.
(5)
Observe that by the bound established in Equation (5), what remains is to solve for suitable values
of θ and p such that
r
Y
t=1
2
q
(1 −p)1+θp1−θ < 1
k,
because that would imply 1
k
Pk
j=1 Lτ,θ(F, Dj) < 1/k, and hence the majority-vote predictor F has
“margin” θ on all k distributions:
∀1 ≤j ≤k :
r
X
t=1
α1 [err(ht, Dj) ≤τ] −
r
X
t=1
α1 [err(ht, Dj) > τ] > θ
r
X
t=1
α.
Combining this margin guarantee and Lemma 13 implies that F achieves error at most ε on all k
distributions, i.e., for every j ∈[k],
err(F, Dj) =
Pr
(x,y)∼Dj
"
1
"
1
r
r
X
t=1
ht(x) ≥1
2
#
̸= y
#
≤(1 + 1/θ) τ = (1 + 1/θ)
ε
1 + 1/θ = ε.
We now turn to choosing p and θ to guarantee the above.
r
Y
t=1
2
q
(1 −p)1+θp1−θ < 1
k ⇐⇒

4(1 −p)1+θp1−θr/2
< 1
k
⇐⇒4(1 −p)1+θp1−θ <
1
k2/r .
21

Observe that (1 −p)(1+θ) ≤1 for all θ, p ∈(0, 1). Thus, it suffices to choose p, θ so that
1
p
1−θ
> 4k
2/r,
(6)
which can be satisfied by setting
1
p = 2 ·

4k
2/r1/(1−θ)
.
Plugging-in these parameters, the total sample complexity is
r ·
"
O
 
(1 + 1/θ)
 4k
2/r1/(1−θ)
ε
·

d + log
2r
δ
!
+ k · O
1 + 1/θ
ε
log
2rk
δ
#
.
To conclude, we optimize the bound above by choosing θ =
r
2 log k. This choice ensures that 1+1/θ =
O(log(k)/r). Since r ≤log k, we also have θ ≤1/2, which implies 1/(1−θ) = 1+θ/(1−θ) ≤1+2θ.
Hence,
 4k
2/r1/(1−θ) ≤
 4k
2/r  4k
2/r2θ = O(k2/r). This yields the following sample complexity
bound:
O

k2/r log(k)d
ε + k log(k)
ε
log
k
δ

.
We sketch below alternative bounds that can be achieved by employing slightly more sophis-
ticated variants of boosting such as boost-by-majority [SF12, Chapter 13] or recursive boosting
[Sch90]. The general idea remains the same as in our application of AdaBoost, where we optimize
a particular “margin” loss function defined on the k distributions, but the bounds below can be
favorable in the regime where r is small (e.g., constant).
Claim 14 (Recursive Boosting–Base Case). For any p > 0 and any mixture q = (w1, . . . , wk),
suppose that predictors h1, h2, h3 satisfy:
1. Prj∼q [err(h1, Dj) > τ] ≤p.
2. Prj∼q2 [err(h2, Dj) > τ] ≤p, where q2 = 1
2qC + 1
2qI, qC is q conditioned on
{j ∈[k] : err(h1, Dj) ≤τ} ,
and qI is q conditioned on
{j ∈[k] : err(h1, Dj) > τ} .
3. Prj∼q3 [err(h3, Dj) > τ] ≤p, where q3 is q conditioned on
{j ∈[k] : 1[err(h1, Dj) > τ] ̸= 1[err(h2, Dj) > τ]} .
Then, the majority vote predictor F obtained from h1, h2, and h3 satisfies
Prj∼q [err(F, Dj) > 2τ] ≤3p2 −2p3.
22

It follows from the claim above as a corollary, by choosing p = 1/
√
4k, that only 3 rounds of
adaptive sampling suffice to achieve a sample complexity of eO(d
√
k/ε) for realizable MDL.
More generally, by employing a variant of Boost-by-Majority [SF12, Chapter 13, Exercise 13.5],
we get the following guarantee:
Claim 15 (Boost-by-Majority). For any target ε, for any number of rounds r ≥1, for any p ∈
(0, 1
2), and a suitably chosen θ ∈(0, 1), running a modified version of Boost-by-Majority [SF12,
Chapter 13, Exercise 13.5] for r rounds making calls to a

τ :=
εp
1+1/θ, δ

-PAC-learner A, outputs
a predictor F : x 7→1
1
r
Pr
t=1 ht(x) ≥1
2

that satisfies
1
k
k
X
j=1
1 [err (F, Dj) > τ] ≤Binom

r,
1 + θ
2

r, 1 −p

.
To invoke the claim above, one would need to solve for p and θ such that Binom
 r,
  1+θ
2

r, 1 −p

<
1/k. See [SF12, Chapter 13, Figure 13.3] for an illustrative comparison between AdaBoost and
Boost-by-Majority in terms of performance based on number of rounds r.
B
Lower Bound for Realizable MDL
In this section, we prove the following sample complexity lower bound against r-round MDL algo-
rithms. The lower bound nearly matches the upper bound in Theorem 10, and holds even for MDL
algorithms that only work in the realizable setting. For brevity, we treat the PAC parameters ε
and δ as sufficiently small constants (below 1/100).
Theorem 16 (Formal version of Theorem 2). For k ≥1, r = O(log k) and
d ≥max{Ω(k log k), Ω(k1−1/r log(k) log(r))},
there exists a hypothesis class of VC dimension d such that every r-round, (0.01, 0.01)-PAC algo-
rithm for realizable MDL on k distributions has a sample complexity of
Ω
 
dk1/r
r log2 k
!
.
In particular, to achieve a near-optimal sample complexity of O((d+k) polylog(k)), the learning
algorithm must have a round complexity of Ω

log k
log log k

.
Remark 2. While the lower bound in Theorem 16 is stated for a constant accuracy parameter
(i.e., ϵ = 0.01) for brevity, it is easy to derive an Ω(1
ϵ ·
dk1/r
r log2 k) lower bound for general ϵ via a
standard argument. We start with the construction for ϵ0 = 0.01. Then, we obtain a new MDL
instance by “diluting” each data distribution by a factor of 100ϵ: we scale the probability mass on
each example by a factor of 100ϵ, and put the remaining mass of 1 −100ϵ on a “trivial” example
(⊥, 0), where ⊥is a dummy instance that is labeled with 0 by every hypothesis. Learning this new
instance up to error ϵ is equivalent to learning the original instance (before diluting) up to error
ϵ/(100ϵ) = 0.01 = ϵ0. Intuitively, the example (⊥, 0) provides no information, and the learning
algorithm has to draw Ω(1/ϵ) samples in expectation to see an informative example. This leads to
a lower bound with an additional 1/ϵ factor.
23

B.1
Intuition
We start by explaining the intuition behind the construction of the hard MDL instance.
For
simplicity, we start with the r = 2 case and sketch the proof of an Ω(d
√
k) lower bound.
Hard Instance against Two-Round Algorithms
We will consider the class of linear functions
over X = Fd
2, namely, the hypothesis class
Hd :=
n
hw : w ∈Fd
2
o
, where hw : x ∈Fd
2 7→w⊤x ∈F2.
The ground truth h⋆is drawn uniformly at random from Hd.
To construct the k data distributions, we will randomly choose difficulty levels diff1, diff2, . . . , diffk
such that Pk
i=1 diffi ≤d. (We will specify the distribution of (diffi)k
i=1 later.) Then, we randomly
choose subspaces V1, V2, . . . , Vk ⊆Fd
2 such that: (1) dim(Vi) = diffi; (2) dim(Span(V1 ∪V2 ∪· · · ∪
Vk)) = Pk
i=1 diffi; (3) The k-tuple (V1, V2, . . . , Vk) is uniformly distributed over all possible choices
that satisfy Constraints (1) and (2). Finally, each Di is set to the uniform distribution over Vi.
It remains to specify the choice of diff1 through diffk. Let d0 and k0 be integers to be chosen
later. We will choose diff1, . . . , diffk to be a uniform permutation of: (1) 1 copy of d0; (2) √k0
copies of d0/√k0; (3) ≥k0 copies of d0/k0. For this to be valid, we need
1 +
p
k0 + k0 ≤k
and
1 · d0 +
p
k0 · d0/
p
k0 + k · d0/k0 ≤d,
both of which can be satisfied for some d0 = Θ(d) and k0 = Θ(k). In the following, we will drop
the subscripts in d0 and k0 for brevity.
Two-Round vs. Three-Round Algorithms
Given labeled examples {(xi, yi)}i∈[m], we can
determine the value of h⋆(x) for every x ∈Span({x1, x2, . . . , xm}). Since the prior distribution
of h⋆is uniform over Hd, for every x ∈Fd
2 \ Span({x1, x2, . . . , xm}), the conditional distribution
of h⋆(x) is uniform. Intuitively, this means that, for every such instance x, we cannot predict its
label better than random guessing. Therefore, to learn a classifier with error ≤0.01 on Di, we
must observe diffi linearly independent instances from Vi. In particular, we must draw at least diffi
samples from Di.
Suppose that the learner were allowed three rounds of sampling. The following is a natural
algorithm:
• Draw ≈d/k samples from each distribution. This allows us to identify the distributions with
diffi = d/k as well as to find an accurate classifier for each such distribution.
• From each of the
√
k + 1 remaining distributions, draw ≈d/
√
k samples.
This satisfies all
distributions except the one with diffi = d.
• Finally, draw ≈d samples from the only remaining distribution.
Note that each step draws O(d) samples, so the total sample complexity is O(d).
If only two rounds are allowed, the learner must “skip” one of the three steps above, and use
either of the following two strategies, both of which lead to an Ω(d
√
k) sample complexity:
24

• Strategy 1: In the first round, draw ≈d/
√
k samples from each of the k distribution to identify
the only distribution with diffi = d. In the second round, draw ≈d samples from the remaining
distribution.
• Strategy 2: As before, draw ≈d/k samples from each distribution to identify the distributions
with diffi ≥d/
√
k. Then, there are still
√
k + 1 “suspects” among which one distribution has
difficulty level diffi = d. The learner must draw ≈d samples from each of them in the second
round.
Hard Instance against r-Round Algorithms
For the general case, we set α := k1/r. We
extend the construction above such that diff1 through diffk is a random permutation of
• n0 = k copies of d0 = d/(rk).
• n1 = k/α copies of d1 = αd/(rk).
• n2 = k/α2 copies of d2 = α2d/(rk).
• · · ·
• nr = k/αr = 1 copy of dr = αrd/(rk) = d/r.
As long as r ≤log2 k, we have α = k1/r ≥2, which implies that n0, . . . , nr decreases geometrically.
It follows that
n0 + n1 + · · · + nr ≤2k
and
r
X
i=0
ni · di = (r + 1) · (d/r) ≤2d.
Thus, the above would give a valid instance after scaling every ni down by a factor of 2.
Again, an (r + 1)-round learner would spend O(ni−1 · di−1) = O(d/r) samples in the i-th
round to learn the distributions with difficulty levels ≤di−1. Then, there are only O(ni) remaining
distributions, each of which has a difficulty level ≥di. The sample complexity is thus (d/r)·(r+1) =
O(d).
When the learner is only allowed r rounds of adaptive sampling, intuitively, the learner must
“skip” one of the r +1 rounds outlined above. Suppose that, for some i ∈[r], the learner decides to
draw Θ(di) samples from each of the Θ(ni−1) remaining distributions, in the hope of learning the
distributions with difficulty levels both di−1 and di. Note that the parameters ni and di are chosen
such that ni−1 · di = dk1/r/r, so the learner must incur a k1/r blowup in the sample complexity.
B.2
Towards a Formal Proof
To formalize the intuition outlined above, we slightly modify the construction into a k-fold direct
sum of a single-distribution version.
Random MDL Instance I
Let α := k1/r.
For some d0 ≥k, let Ddiff be the probability
distribution over [d0] such that Ddiff(d0) = 1
k and
Ddiff
d0
αi

= αi −αi−1
k
, ∀i ∈[r].
25

Here and in the rest of this section, we abuse the notation Ddiff for its probability mass function.
We also assume for brevity that d0/αi is an integer; if not, the same proof would go through after
proper rounding. Note that
E
diff∼Ddiff [diff] ≤
r
X
i=0
d0
αi · αi
k = (r + 1) · d0
k .
Now we define a distribution over MDL instances with k distributions.
Definition 2 (Multi-distribution instance). Given d0, k, and a sufficiently large d := Θ(d0 log k),
we construct an MDL instance I as follows:
• Independently sample diff1, diff2, . . . , diffk ∼Ddiff. Repeat until Pk
i=1 diffi ≤d.
• Sample subspaces V1, V2, . . . , Vk ⊆Fd
2 uniformly at random, subject to dim(Vi) = diffi and
dim(Span(V1 ∪V2 ∪· · · ∪Vk)) = Pk
i=1 diffi.
• Let I be the MDL instance on the class Hd of linear functions over Fd
2, where the i-th data
distribution Di is the uniform distribution over Vi, and the ground truth classifier h⋆∈Hd is
chosen uniformly at random.
Random Single-Distribution Instance I′
We also consider a closely-related single-distribution
version of the problem, where only one difficulty level diff is sampled from Ddiff.
Definition 3 (Single-distribution instance). Given d ≥d0 ≥1 and distribution Ddiff over [d0], we
construct the single-distribution learning instance I′ as follows:
• Sample diff ∼Ddiff and a uniformly random diff-dimensional subspace V ⊆Fd
2.
• Let I′ be the realizable PAC learning instance on the class Hd of linear functions over Fd
2, where
the data distribution D is the uniform distribution over V , and the ground truth classifier h⋆∈Hd
is chosen uniformly at random.
Roadmap
We prove Theorem 16 by combining the following two technical lemmas:
• Lemma 17: An r-round, (0.01, 0.01)-PAC MDL algorithm with sample complexity M can be
transformed into an r-round (0.02, O(1/k))-PAC algorithm for I′ with sample complexity bound
O((M/k) log k).
• Lemma 18: An r-round, (0.02, O(1/k))-PAC algorithm for I′ must draw Ω(d0k1/r/(rk)) samples
in expectation.
Lemma 17. For any d0 ≥k ≥1 and 1 ≤r ≤O(log k), if there is an r-round (0.01, 0.01)-PAC MDL
algorithm A with sample complexity M on k distributions and hypothesis classes of VC-dimension
d = Θ(d0 log k), there is another r-round algorithm A′ such that:
• On a random single-distribution instance I′ (Definition 3), A′ learns an 0.02-accurate classifier
with probability at least 1 −
1
100k.
• On a random single-distribution instance I′, A′ takes O((M/k) log k) samples in expectation.
26

The probability and expectation above are over the randomness in instance I′, the learning algorithm
A′, as well as the drawing of samples.
Lemma 18. Suppose that k ≥1, r ≤O(log k) and d0 ≥max{k, Ω(k1−1/r log r)}. If an r-round
learning algorithm A outputs a 0.02-accurate classifier with probability ≥1 −
1
100k on a random
instance I′ with parameters (d0, k, r), A must take Ω(d0k1/r/(rk)) samples in expectation.
Proof of Theorem 16 assuming Lemmas 17 and 18. Let d0 = Θ(d/ log k).
The assumption that
d ≥max{Ω(k log k), Ω(k1−1/r log(k) log(r))} ensures that d0 ≥k and d0 ≥Ω(k1−1/r log r). Suppose
that A is an r-round, (0.01, 0.01)-PAC MDL algorithm with sample complexity M for k distributions
and hypothesis classes of VC dimension ≤d. By Lemma 17, there is another algorithm A′ that, on
a random instance I′ from Definition 3, returns a 0.02-accurate classifier with probability at least
1 −
1
100k and takes O((M/k) log k) samples in expectation. By Lemma 18, we have
O
M
k · log k

≥Ω
 
d0k1/r
rk
!
= Ω
 
dk1/r
rk log k
!
.
It follows that M ≥Ω

dk1/r
r log2 k

.
B.3
Proof of Lemma 17
To prove Lemma 17, it suffices to show that we can solve a random single-distribution instance I′
(from Definition 3) by running an MDL algorithm A in a black-box way. Naturally, we plant I′
into an MDL instance I with k distributions (from Definition 2), solve instance I using A, and use
the output of A to solve the actual instance I′. While the idea is simple, some care needs to be
taken to carry out this plan correctly.
First, we draw i⋆uniformly at random from [k], and let the i⋆-th data distribution in I corre-
spond to the distribution in I′. For each i ∈[k] \ {i⋆}, we draw diffi from Ddiff independently. Let
Vi⋆denote the subspace of Fd
2 corresponding to the task I′, where d := Θ(d0 log k) is the ambient
dimension. Intuitively, for each i ̸= i⋆, we want to sample a diffi-dimensional subspace Vi to con-
struct the MDL instance. This ensures that (V1, V2, . . . , Vk) form a random MDL instance (from
Definition 2), in which the k distributions are symmetric, so that the identity of i⋆would not be
revealed. Here, an obstacle is that we cannot construct Vi for i ̸= i⋆without knowing the subspace
Vi⋆. In particular, knowing Vi⋆or just the value of dim(Vi⋆) would make the single-distribution
version too easy.
Our workaround is to slightly modify the definition of the single-distribution task. We allow the
algorithm for the single-distribution instance to specify an integer m0 ≥0 at the beginning. Then,
the algorithm receives m0 vectors in Fd
2 chosen uniformly at random subject to that, along with
any basis of Vi⋆, these m0 + dim(Vi⋆) vectors are linearly independent. Intuitively, the algorithm is
provided with information about the “complement” of Vi⋆in Fd
2, which allows it to construct the
k−1 fictitious subspaces {Vi}i̸=i⋆and thus an MDL instance on k distributions. If m0 > d−diffi⋆=
d −dim(Vi⋆), the learner fails the learning task immediately. Note that we cannot simply give all
the d −diffi⋆vectors to the learner directly—otherwise the learner would know diffi⋆, which makes
the single-distribution instance easy.
Proof of Lemma 17. Let I′ be a single-distribution instance generated randomly with parameters
k and d0 according to Definition 3. Recall that the hypothesis class is Hd, where d = Θ(d0 log k)
27

is the ambient dimension. The data distribution D is uniform over an unknown subspace V ⊆Fd
2
with an unknown dimension diff ∼Ddiff.
We consider the following procedure for solving I′ using the given algorithm A:
• Draw i⋆from [k] uniformly at random. For each i ∈[k] \ {i⋆}, draw diffi independently from
Ddiff.
• Set m0 := P
i∈[k]\{i⋆} diffi. Request m0 random vectors in Fd
2 that are linearly independent of the
unknown subspace V . In this step, the algorithm might fail due to m0 > d −diffi⋆, where diffi⋆
is the unknown difficulty level of the single-distribution instance I′.
• Use these vectors to construct a diffi-dimensional subspace Vi for each i ∈[k] \ {i⋆}. Let Di be
the uniform distribution over Vi.
• Simulate the MDL algorithm A on distributions D1, D2, . . . , Dk, where Di⋆is the alias of the
data distribution D in instance I′. In addition, when A draws the first round of samples, we
draw Θ(log k) additional samples from Di⋆to form a validation dataset.
• When A terminates and outputs a classifier ˆh, check whether ˆh has an error ≤0.01+0.02
2
on the
validation dataset. If so, output ˆh as the answer; otherwise, report “failure”.
Let Asim denote the algorithm defined above. In the following, we will show that: (1) Asim can
be implemented using A as a black box; (2) Asim solves instance I′ with a good probability; (3)
Asim does not draw too many samples from D.
Details of the Simulation
When Asim simulates algorithm A, it maintains a set Si ⊆Fd
2 × F2
for each i ̸= i⋆, which is empty at the beginning. Whenever A requests samples from Di⋆, Asim
draws from D and forwards the labeled examples to A. When A requests a sample from Di for
some i ̸= i⋆, Asim first draws x ∼Di. If x lies in Span({x ∈Fd
2 : (x, y) ∈Si, ∃y ∈F2}), Asim
computes the unique label y ∈F2 such that (x, y) remains consistent with the labeled examples in
Si; otherwise, Asim draws a random y ∼Bernoulli(1/2). The labeled example (x, y) is forwarded to
A, and then added to the dataset Si. By doing so, we defer the randomness in the ground truth
classifier h⋆, namely, we realize one bit of information of h⋆whenever this information is needed to
determine the label of an example.
Equivalence
To analyze the performance of Asim, the key observation is the following: Condi-
tioning on that Asim does not fail when requesting the m0 vectors, from the perspective of the
simulated copy of A, it is running on a random MDL instance I from Definition 2. This is because
conditioning on that Asim does not fail has the same effect as the conditioning on Pk
i=1 diffi ≤d
in Definition 2. Furthermore, both the marginals of D1 through Dk on Fd
2 and the choice of the
labels are identical to those in the definition of I.
Boost the Success Probability
Since A is (0.01, 0.01)-PAC, it holds with probability ≥0.99
that its output ˆh is a 0.01-accurate classifier for each Di.
In particular, ˆh is 0.01-accurate for
distribution D = Di⋆, and is thus a valid answer for instance I′. However, this only guarantees a
success probability of 0.99, falling short of the desired 1 −1/(100k).
Fortunately, it suffices to run l = Θ(log k) independent copies of algorithm A′ to boost the
success probability. Note that these l copies must be simulated in parallel, so that the resulting
28

algorithm still takes r rounds of samples.
Furthermore, we share the vectors requested at the
beginning of different copies of A′, so that the number of vectors that we actually request, m0, is
the maximum realization of P
i̸=i⋆diffi over the l simulations. Whenever one of the l simulations
outputs a classifier ˆh, we test it on the size-Θ(log k) validation dataset and verify whether its
empirical error is below 0.01+0.02
2
. We output ˆh as the answer to I′ only if it passes the test. Let
A′ denote the above algorithm.
Correctness of A′
Now, we analyze the probability that A′ fails to output a 0.01-accurate
classifier for I′. There are three possible reasons:
• Reason 1: m0 > d −diffi⋆. This happens when Pk
i=1 diffi > d holds in one of the l copies of
Asim. Note that in each fixed copy of Asim, diff1, diff2, . . . , diffk independently follow Ddiff. Recall
that Ediff∼Ddiff [diff] = O(rd0/k) and d = Θ(d0 log k). It follows from a multiplicative Chernoff
bound that Pk
i=1 diffi > d happens with probability 1/ poly(k), which is still ≪1/k after a union
bound over the l = Θ(log k) copies.
In more detail, let random variable Xi denote the value of diffi/d0. Since Ddiff is supported
over [d0], Xi ∈[0, 1]. Furthermore, E [Xi] = Ediff∼Ddiff [diff/d0] = O(rd0/k)/d0 = O(r/k). Then,
Pk
i=1 Xi is the sum of k random variables in [0, 1] and has an expectation of µ := k·O(r/k) = O(r).
Assuming that r = O(log k) holds with a sufficiently small constant factor, we have µ = O(r) ≤
ln k. Let δ = (5 ln k)/µ−1 ≥4. Also recall that d = Θ(d0 log k), so we may assume d/d0 ≥5 ln k.
Then, we have
Pr
" k
X
i=1
diffi > d
#
= Pr
" k
X
i=1
Xi > d/d0
#
≤Pr
" k
X
i=1
Xi > 5 ln k
#
= Pr
" k
X
i=1
Xi > (1 + δ)µ
#
.
A multiplicative Chernoff bound gives
Pr
" k
X
i=1
Xi > (1 + δ)µ
#
≤exp

−δ2µ
2 + δ

≤exp

−2δµ
3

≤exp

−8 ln k
3

≪1
k,
where the second step applies δ ≥4 and the third step applies δµ = 5 ln k −µ ≥4 ln k.
• Reason 2: None of the l simulations succeeds. Note that conditioning on the realization of
instance I′, the l copies of the simulation are independent. Furthermore, since A is (0.01, 0.01)-
PAC, each copy fails with probability ≤0.01. The probability for all l = Θ(log k) copies to fail
is thus at most 0.01l ≪1/k.
• Reason 3:
The validation procedure fails.
We need to validate at most l = O(log k)
classifiers that are generated independently of the validation set. For each classifier ˆh, we need
to distinguish the two cases err(ˆh, D) ≤0.01 and err(ˆh, D) > 0.02. By a Chernoff bound and
the union bound, the validation set of size Θ(log k) is sufficient for upper bounding the failure
probability by l · e−Ω(log k) ≤1/ poly(k) ≪1/k.
Combining the three cases above, for all sufficiently large k, the total failure probability is
smaller than
1
100k. In other words, algorithm A′ outputs a 0.01-accurate classifier with probability
1 −
1
100k on a random single-distribution instance I′.
29

Sample Complexity
Each of the l = O(log k) simulated copies of A draws at most M samples
from D1 through Dk in total, as each simulated copy of A effectively runs on a valid MDL instance.
Furthermore, from the perspective of each simulated copy of A, the k distributions D1, . . . , Dk are
generated symmetrically, so the expected number of actual samples drawn from Di⋆= D is at
most M/k. Therefore, the l copies of Asim together draw (M/k) · l = O((M/k) log k) samples in
expectation. Note that this dominates the O(log k) samples drawn for the purpose of validation,
so the overall sample complexity of A′ is O((M/k) log k).
B.4
Proof of Lemma 18
It remains to prove a sample complexity lower bound for solving the single-distribution instance I′
from Definition 3 within r rounds. Recall that I′ is defined as the task of learning linear functions
over the hypercube of dimension d = Θ(d0 log k), and the data distribution is uniform over a
random subspace of dimension diff ∼Ddiff. Also recall that α = k1/r ≥2, Ddiff(d0) =
1
k and
Ddiff(d0/αi) = (αi −αi−1)/k for every i ∈[r].
Proof of Lemma 18. Suppose towards a contradiction that there exists an r-round learning algo-
rithm A for the single-distribution instance I′ defined in Definition 3, such that it outputs a
0.02-accurate classifier with probability 1 −
1
100k and takes at most
αd0
100rk samples in expectation.
Typical Events
Consider the execution of A on the random instance I′. We introduce 2r + 1
“typical events”, which will be shown to happen simultaneously with high probability:
• Event Enum
0
: Let random variable M0 denote the value of m0, i.e., the number of linearly inde-
pendent vectors requested by A at the beginning. Enum
0
is defined as the event that M0 ≤d −d0.
• Events Enum
1
, Enum
2
, . . . , Enum
r
: For each i ∈[r], let random variable Mi denote the number of
samples that A draws in the i-th round. Enum
i
is defined as the event that Mi ≤αid0
4k .
• Events Eind
1 , Eind
2 , . . . , Eind
r : For each i ∈[r], Eind
i
is defined as the event that, among the labeled
examples that A draws in the first i rounds, all the instances (namely, the vectors in Fd
2) are
linearly independent.
For i ∈{0, 1, . . . , r}, we use the shorthands
Enum
≤i
:= Enum
0
∩Enum
1
∩· · · Enum
i
and
Eind
≤i := Eind
1
∩Eind
2
∩· · · Eind
i
.
Moreover, we shorthand Ediff
i
for the event diff = αid0
k .
Induction Hypothesis
We will prove the following statement by induction on i: For every
i ∈{0, 1, . . . , r} and every i⋆∈{i, i + 1, . . . , r}, it holds that
Pr
h
Enum
≤i
∩Eind
≤i | Ediff
i⋆
i
≥0.99 −4i
25r ≥1
2.
(7)
We first show that the above leads to a contradiction and thus proves the lemma.
When
i = i⋆= r, Equation (7) reduces to
Pr
h
Enum
≤r
∩Eind
≤r | diff = d0
i
≥1
2.
30

Recall that α = k1/r ≥2. Thus, with probability ≥1/2 conditioning on diff = d0, A draws
r
X
i=1
Mi ≤αd0
4k + α2d0
4k
+ · · · + αrd0
4k
≤2 · αrd0
4k
= d0
2
samples in total. Then, over the remaining randomness in the ground truth classifier h⋆∈Hd,
the correct label of every instance outside the span of the observed instances is still uniformly
distributed over {0, 1}. In particular, regardless of how A outputs a classifier, the error of the
classifier is at least (1 −2⌊d0/2⌋−d0)/2 in expectation.
By Markov’s inequality, the conditional
probability of outputting a classifier with error ≤0.02 is at most
1 −(1 −2⌊d0/2⌋−d0)/2
1 −0.02
< 4
5.
Therefore, the probability that A fails to output a classifier with error ≤0.02 on a random single-
distribution instance I′ is at least
Prdiff∼Ddiff [diff = d0] · Pr
h
Enum
≤r
∩Eind
≤r | diff = d0
i
· 1
5 ≥1
k · 1
2 · 1
5 >
1
100k,
contradicting the assumption on A.
The remainder of the proof will establish Equation (7) by induction.
Base Case
We start by verifying the base case that i = 0, namely,
Pr
h
M0 ≤d −d0 | Ediff
i⋆
i
≥0.99, ∀i⋆∈{0, 1, . . . , r}.
Note that A chooses M0 before drawing any samples, so M0 is independent of diff. Suppose towards
a contradiction that Pr [M0 > d −d0] > 0.01. Then, we have
Pr [M0 > d −d0 ∧diff = d0] = Pr [M0 > d −d0] · Prdiff∼Ddiff [diff = d0] > 0.01 · 1
k =
1
100k.
Moreover, when both M0 > d −d0 and diff = d0 hold, A would fail due to M0 > d −diff. This
shows that the failure probability of A is strictly higher than
1
100k, a contradiction.
Inductive Step: Event Enum
i
Fix i ∈[r] and i⋆∈{i, i + 1, . . . , r}. Suppose that the induction
hypothesis (Equation (7)) holds for i −1: shorthanding Egood := Enum
≤i−1 ∩Eind
≤i−1, it holds for every
j ∈{i −1, i, . . . , r} that
Pr
h
Egood | Ediff
j
i
≥0.99 −4(i −1)
25r
≥1
2.
In the following, we condition on Egood and Ediff
i⋆and aim to lower bound the conditional probabilities
of events Enum
i
and Eind
i
.
We observe that, conditioning on Egood, the labeled examples that A draws in the first i −1
rounds are conditionally independent of diff. In particular, the conditional distribution of Mi | Egood
is identical to that of Mi | (Egood ∩Ediff
i⋆). Then, we note that E

Mi | Egood
≤αid0
50rk must hold;
otherwise, we have
E [Mi] ≥E
h
Mi | Egoodi
· Pr
h
Egoodi
> αid0
50rk · Pr
h
Egoodi
.
31

Furthermore, the induction hypothesis implies
Pr
h
Egoodi
≥
r
X
j=i−1
Pr
h
Ediff
j
i
· Pr
h
Egood | Ediff
j
i
≥
r
X
j=i−1
Pr

diff = αjd0
k

· 1
2
= 1
2Pr

diff ≥αi−1d0
k

=
1
2αi−1 .
It would then follow that
E [Mi] > αid0
50rk ·
1
2αi−1 =
αd0
100rk,
contradicting the assumption that A has an expected sample complexity of at most
αd0
100rk.
Thus, we have E

Mi | Egood ∩Ediff
i⋆

= E

Mi | Egood
≤αid0
50rk. Markov’s inequality then gives
Pr

Mi > αid0
4k
| Egood ∩Ediff
i⋆

≤E

Mi | Egood ∩Ediff
i⋆

αid0/(4k)
≤αid0/(50rk)
αid0/(4k)
=
2
25r.
Therefore, we have
Pr
h
Enum
i
∩Egood | Ediff
i⋆
i
≥Pr
h
Egood | Ediff
i⋆
i
· Pr
h
Enum
i
| Egood ∩Ediff
i⋆
i
≥

0.99 −4(i −1)
25r

·

1 −
2
25r

≥0.99 −4i −2
25r .
Inductive Step: Event Eind
i
Now, we condition on event Enum
i
∩Egood ∩Ediff
i⋆
and aim to lower
bound the conditional probability of Eind
i
. After the conditioning, it holds that
M1 + M2 + · · · + Mi ≤αd0
4k + α2d0
4k
+ · · · + αid0
4k
≤2 · αid0
4k
≤αi⋆d0
2k
= diff/2.
Regardless of the N := M1 + M2 + · · · + Mi−1 samples that A draws in the first i −1 rounds,
the probability that the next instance falls into the subspace spanned by those N instances is at
most 2N−diff. By the same argument, for every i ∈{0, 1, . . . , Mi −1}, after i samples have been
drawn in the i-th round, the next instance is outside the span of the first N + i instances except
with probability 2(N+i)−diff. By the union bound, the Mi additional instances in the i-th round are
linearly independent together with the previous N instances, except with probability
2N−diff + 2(N+1)−diff + · · · + 2(N+Mi−1)−diff ≤2N+Mi−diff ≤2diff/2−diff = 2−diff/2.
Recall that diff = αi⋆d0
k
≥αd0
k
=
d0
k1−1/r . Therefore, for all sufficiently large d0 = Ω(k1−1/r log r),
it holds that 2−diff/2 ≤
2
25r. It then follows that
Pr
h
Eind
i
| Enum
i
∩Egood ∩Ediff
i⋆
i
≥1 −
2
25r,
32

which further implies
Pr
h
Enum
≤i
∩Eind
≤i | Ediff
i⋆
i
≥Pr
h
Enum
i
∩Egood | Ediff
i⋆
i
· Pr
h
Eind
i
| Enum
i
∩Egood ∩Ediff
i⋆
i
≥

0.99 −4i −2
25r

·

1 −
2
25r

≥0.99 −4i
25r.
This completes the inductive step and concludes the proof.
C
Upper Bounds for OODS
We analyze the correctness and the round complexity of the LazyHedge algorithm (Algorithm 3).
We then show how the ideas in both the algorithm and its analysis can be easily transferred to the
agnostic MDL setting, resulting in an algorithm with eO(
√
k) round complexity and near-optimal
sample complexity.
Algorithm 3: LazyHedge: Hedge with Lazy Updates
Input: Number of dimensions k, number of iterations T = Θ

log k
ε2

, step size η = Θ(ε),
margin parameter C > 1, and access to a first-order oracle of f.
1 Set w(1) = (1/k, 1/k, . . . , 1/k) and w(0) = (0, 0, . . . , 0).
2 for t = 1, 2, . . . , T do
3
if w(t) ∈O(w(t−1)) then
4
Set w(t) = w(t−1).
5
else
6
Set w(t)
i
= C · max{w(1)
i , w(2)
i , . . . , w(t)
i } for every i ∈[k].
7
Start a new OODS round with cap w(t).
8
Query the first-order oracle to obtain supergradient r(t) ∈∇f(w(t)).
9
Compute w(t+1) ∈∆k−1 such that for every i ∈[k],
w(t+1)
i
=
w(t)
i
· eηr(t)
i
Pk
j=1 w(t)
j
· eηr(t)
j
.
Output: Average weight vector 1
T
PT
t=1 w(t).
C.1
Proof of Lemma 5
Recall that Lemma 5 states that LazyHedge finds an O(ε)-approximate maximizer of f on ∆k−1.
Proof of Lemma 5. The standard regret analysis of the Hedge algorithm (e.g., [MRT18, Theorem
8.6]) gives
T
X
t=1
⟨r(t), w(t)⟩≥max
i∈[k]
T
X
t=1
⟨r(t), ei⟩−R,
33

where R = ln k
η + ηT
8 = O

log k
ε

. Let w∗∈∆k−1 be a maximizer of f over ∆k−1. Then,
T
X
t=1
⟨r(t), w∗⟩≤max
i∈[k]
T
X
t=1
⟨r(t), ei⟩≤
T
X
t=1
⟨r(t), w(t)⟩+ R.
Rearranging gives PT
t=1⟨r(t), w∗−w(t)⟩≤R. Since f is concave and r(t) ∈∇f(w(t)) for every
t ∈[T], we have f(w∗) ≤f(w(t)) + ⟨r(t), w∗−w(t)⟩. It follows that
T
X
t=1
[f(w∗) −f(w(t))] ≤
T
X
t=1
⟨r(t), w∗−w(t)⟩≤R.
Dividing both sides by T gives
f(w∗) −1
T
T
X
t=1
f(w(t)) ≤R
T = O(ε).
Finally, applying the concavity of f again gives
f( ˆw) = f
 
1
T
T
X
t=1
w(t)
!
≥1
T
T
X
t=1
f(w(t)) ≥f(w∗) −O(ε) =
max
w∈∆k−1 f(w) −O(ε).
C.2
Upper Bound for the Box Setting
We formally prove Lemma 7, which states an upper bound of min

k, O((k/C) · log8(k/ε)

} ·
O(logC k) on the round complexity of LazyHedge in the box setting. The proof proceeds in the
following three steps. First, whenever the cap is updated in LazyHedge, we find an index i that
leads to this update and call it the culprit of this cap update. Then, we show that every an index
can become the culprit of at most O(logC k) cap updates. Finally, we control the number of indices
that become the culprit at least once by min{k, O((k/C) · log8(k/ε))}, so the lemma immediately
follows.
Proof of Lemma 7. If the cap is updated in the t-th iteration of LazyHedge, i.e., w(t) ̸= w(t−1), by
definition of Algorithm 3, it must hold that w(t) /∈O(w(t−1)). By definition of the box setting,
there exists i ∈[k] such that w(t)
i
> w(t−1)
i
. We call the smallest such index i the culprit of the cap
update in iteration t. For each i ∈[k], we define
Ti := {t ∈[T] : i is the culprit of the cap update in iteration t}.
Then, the round complexity of LazyHedge is exactly Pk
i=1 |Ti|.
Every Ti is Small
We show that |Ti| ≤O(logC k) holds for every i ∈[k].
Fix i ∈[k] and
let t1 < t2 < · · · < tm be the m = |Ti| elements of Ti in increasing order. For each j ∈[m], let
aj := max1≤t≤tj w(t)
i
denote the maximum weight on the i-th coordinate among all weight vectors up
34

to time tj. Fix j ∈{2, 3, . . . , m}. Since i is the culprit at time tj, it holds that aj ≥w(tj)
i
> w(tj−1)
i
.
Recall that, at iteration tj−1, the cap w(tj−1) is updated such that
w(tj−1)
i
= C · max
n
w(1)
i , . . . , w(tj−1)
i
o
= C · aj−1.
Moreover, LazyHedge ensures that w(t)
i
is non-decreasing in t. Therefore, we have
C · aj−1 = w(tj−1)
i
≤w(tj−1)
i
≤aj.
It follows that am ≥Cm−1a1. Since a1 ≥w(1)
i
= 1/k and am ≤1, we have m −1 ≤logC(am/a1) ≤
logC k, which gives |Ti| = m = O(logC k).
Only a Few Tis are Non-empty
The number of indices i ∈[k] such that Ti ̸= ∅is trivially
upper bounded by k. Next, we give another upper bound of O((k/C) · log8(k/ε)). Fix i ∈[k].
Suppose that Ti is non-empty and let t1 be the smallest element in Ti. Either one of the following
must be true:
• t1 = 1. This can only hold for i = 1.
• t1 > 1. By definition of Algorithm 3, the cap is set to w(1) = (C/k, C/k, . . . , C/k) in the first
iteration. For index i to be the culprit at time t1 > 1, we must have
w(t1)
i
> w(t1−1)
i
≥w(1)
i
= C/k,
which implies max1≤t≤T w(t)
i
≥C/k. By Lemma 3, at most
Pk
i=1 max1≤t≤T w(t)
i
C/k
≤k
C · O(log8(k/ε))
different indices i can satisfy this condition.
Thus, the number of non-empty sets Ti is at most O((k/C) · log8(k/ε)).
Putting everything together, the round complexity of LazyHedge is given by
k
X
i=1
|Ti| ≤
X
i∈[k]:Ti̸=∅
O(logC k) ≤min

k, O((k/C) · log8(k/ε)

} · O(logC k).
C.3
Upper Bound for the Ellipsoid Setting
In the ellipsoid setting, we have an improved upper bound eO(
√
k) on the round complexity of
LazyHedge.
Lemma 19. For any C ∈[4, k], LazyHedge takes O(
p
k/C · log8(k/ε)) rounds in the ellipsoid
setting.
Again, combining Lemmas 5, 6 and 19 immediately shows that the ellipsoid setting of OODS
can be solved with an eO(C) sample overhead in eO(
p
k/C) rounds.
35

Theorem 20. For any C ∈[4, k], in the ellipsoid setting, LazyHedge finds an O(ε)-approximate
maximum with an O(C log8(k/ε)) sample overhead in O(
p
k/C · log8(k/ε)) rounds.
The proof of Lemma 19 is significantly more technical than that of Lemma 7. We classify the
cap updates (except the one in the first iteration) into two types: A “Type I” update is when some
coordinate wi exceeds 1/
p
k/C, and a “Type II” update is one without a significant increase in
any of the k coordinates. We bound the number of Type I and Type II updates by eO(
p
k/C)
separately.
The upper bound for Type I updates is a simple consequence of the polylog(k/ε) upper bound
on the Hedge trajectory shown by [ZZC+24] (Lemma 3). This bound implies that there are at most
eO(1) ·
p
k/C Type I updates where the coordinate reaches ≈1/
p
k/C, at most eO(1) ·
p
k/C/2
Type I updates where the coordinate reaches ≈2/
p
k/C, and so on. These upper bounds sum up
to eO(
p
k/C).
The analysis for Type II updates is more involved. Roughly speaking, we say that a coordinate
i ∈[k] gains a potential of a2/b when it increases from b to a through the Hedge dynamics.
Then, we prove the following two claims: (1) Each Type II update may happen only if an Ω(1)
potential is accrued over all k coordinates; (2) The total amount of potential that the k coordinates
may contribute to Type II updates is at most eO(
p
k/C). Combining these two claims proves the
eO(
p
k/C) upper bound on the number of Type II updates.
Proof of Lemma 19. Consider an iteration t ∈{2, 3, . . . , T} of LazyHedge in which the cap is up-
dated, i.e., w(t) /∈O(w(t−1)). By definition of the ellipsoid setting (Definition 1), we have
k
X
i=1
[w(t)
i ]2
w(t−1)
i
> 1.
Let Isig
t
:= {i ∈[k] : w(t)
i
≥w(t−1)
i
/2} denote the set of significant indices on which the weight w(t)
i
exceeds half of the cap w(t−1)
i
. Let Iinsig
t
:= {i ∈[k] : w(t)
i
< w(t−1)
i
/2} = [k] \ Isig be the set of
insignificant indices. Then, the contribution from insignificant indices to the left-hand side can be
upper bounded:
X
i∈Iinsig
t
[w(t)
i ]2
w(t−1)
i
≤
X
i∈Iinsig
t
[w(t)
i ]2
2w(t)
i
= 1
2
X
i∈Iinsig
t
w(t)
i
≤1
2,
where the first step applies i ∈Iinsig
t
=⇒w(t−1)
i
> 2w(t)
i , and the last step applies w(t) ∈∆k−1.
Therefore, the contribution from the significant indices is at least
X
i∈Isig
t
[w(t)
i ]2
w(t−1)
i
=
k
X
i=1
[w(t)
i ]2
w(t−1)
i
−
X
i∈Iinsig
t
[w(t)
i ]2
w(t−1)
i
≥1 −1
2 = 1
2.
We say that the cap update in iteration t is Type I if there exists an index i ∈Isig
t
such that
max
n
w(1)
i , w(2)
i
, . . . , w(t)
i
o
≥
1
p
k/C
;
otherwise, the update is Type II. Let T1 ⊆[T] and T2 ⊆[T] denote the iterations in which a Type I
and a Type II cap update happens, respectively. Then, the round complexity is simply |T1|+|T2|+1,
where the “+1” accounts for the cap update at t = 1, which is neither Type I nor Type II.
36

Number of Type I Updates
We further classify the Type I cap updates (in T1) into O(log k)
sub-types. For each integer j ∈[0, log2
p
k/C], let T1,j denote the set of pairs (t, i) ∈T1 × [k] such
that: (1) A Type I cap update happens in iteration t; (2) i ∈Isig
t
is the smallest index such that
max
n
w(1)
i , w(2)
i
, . . . , w(t)
i
o
≥1/
p
k/C;
(3) It holds that
max
n
w(1)
i , w(2)
i , . . . , w(t)
i
o
∈
"
2j
p
k/C
, 2j+1
p
k/C
!
.
Then, it remains to control the size of each T1,j.
We first argue that only a few indices i ∈[k] may appear as the second coordinate of a (t, i)-pair
in T1,j. Indeed, if (t0, i) ∈T1,j, we must have max1≤t≤T w(t)
i
≥max1≤t≤t0 w(t)
i
≥
2j
√
k/C . Recall from
Lemma 3 that Pk
i=1 max1≤t≤T w(t)
i
≤O(log8(k/ε)), so the aforementioned condition hold for at
most 2−j · O(
p
k/C log8(k/ε)) different values of i ∈[k].
Then, we argue that no T1,j may contain two pairs (t1, i) and (t2, i) for t1 ̸= t2. In other words,
no index i can contribute to the same sub-type j twice. Suppose towards a contradiction that for
some t1 < t2 and i ∈[k], (t1, i), (t2, i) ∈T1,j. By definition of T1,j, max1≤t≤t1 w(t)
i
≥2j/
p
k/C.
Then, by the cap update in LazyHedge, we have
w(t2−1)
i
≥w(t1)
i
= C · max
1≤t≤t1 w(t)
i
≥C ·
2j
p
k/C
.
On the other hand, since (t2, i) ∈T1,j, we have i ∈Isig
t2 , which further implies
w(t2)
i
≥1
2w(t2−1)
i
≥C
2 ·
2j
p
k/C
≥
2j+1
p
k/C
,
where the last step applies C ≥4. We then have max1≤t≤t2 w(t)
i
≥w(t2)
i
≥2j+1/
p
k/C, which
contradicts (t2, i) ∈T1,j.
Therefore, the number of Type I updates is at most
|T1| =
⌊log2
√
k/C⌋
X
j=0
|T1,j| ≤
⌊log2
√
k/C⌋
X
j=0
2−j · O(
p
k/C log8(k/ε)) = O(
p
k/C log8(k/ε)).
Number of Type II Updates: Overview
Recall that if a cap update happens in iteration
t ≥2, we have P
i∈Isig
t
[w(t)
i
]2
w(t−1)
i
≥1
2. Summing over t ∈T2 gives
X
t∈T2
X
i∈Isig
t
[w(t)
i ]2
w(t−1)
i
≥|T2|
2 .
(8)
Next, we upper bound the double summation on the left-hand side of (8) by changing the order of
summation. We will show that, for every i ∈[k], it holds that
X
t∈T2:i∈Isig
t
[w(t)
i ]2
w(t−1)
i
≤min
(
k
C

max
1≤t≤T w(t)
i
2
, 1
)
.
(9)
37

Furthermore, we will upper bound the sum of the right-hand side of (9) as follows:
k
X
i=1
min
(
k
C

max
1≤t≤T w(t)
i
2
, 1
)
≤O(
p
k/C log8(k/ε)).
(10)
Then, combining Equations (8) through (10) immediately gives |T2| ≤O(
p
k/C log8(k/ε)) and
completes the proof. We prove Equations (9) and (10) in the remainder of the proof.
Proof of Equation (9)
Towards proving Equation (9), we fix i ∈[k] and list the elements in
T2 that satisfies i ∈Isig
t
in increasing order: 2 ≤t1 < t2 < · · · < tm ≤T. We write t0 = 1 and
aj := max1≤t≤tj w(t)
i
for every j ∈{0, 1, . . . , m}.
We first upper bound the left-hand side of (9) in terms of (aj)m
j=0. For each j ∈[m], we have
w(tj)
i
≤max
1≤t≤tj w(t)
i
= aj
and
w(tj−1)
i
≥w(tj−1)
i
= C ·
max
1≤t≤tj−1 w(t)
i
= C · aj−1.
Then, each j ∈[m] contributes a term of
[w(tj)
i
]2
w(tj−1)
i
≤
a2
j
C · aj−1
= 1
C ·
a2
j
aj−1
to the left-hand side of (9). Thus, it suffices to upper bound Pm
j=1
a2
j
aj−1 .
Next, we examine the sequence (aj)m
j=0. Fix j ∈[m]. Since i ∈Isig
tj , we have
aj ≥w(tj)
i
≥1
2w(tj−1)
i
≥1
2w(tj−1)
i
.
The cap update at time tj−1 ensures that
w(tj−1)
i
= C ·
max
1≤t≤tj−1 w(t)
i
= C · aj−1.
Combining the above and applying the assumption that C ≥4 gives aj ≥C
2 aj−1 ≥2aj−1.
The sequence (aj)m
j=0 starts with a0 = w(1)
i
= 1/k and ends with am = max1≤t≤tm w(t)
i . More-
over, since the cap update in iteration tm is Type II and i ∈Isig
tm, it holds that
max
1≤t≤tm w(t)
i
<
1
p
k/C
.
Therefore, we have the upper bound am ≤min

max1≤t≤T w(t)
i ,
1
√
k/C

. In summary, (a0, a1, . . . , am)
is a sequence of positive numbers such that:
• aj ≥2aj−1.
• a0 = 1/k and am ≤min

max1≤t≤T w(t)
i ,
1
√
k/C

.
38

For any positive numbers a, b, c that satisfy a/b ≥2 and b/c ≥2, we have
a2
b ≤3
4 · a2
c = a2
c −(a/2)2
c
≤a2
c −b2
c .
Rearranging gives a2
b + b2
c ≤a2
c . Therefore, starting from the sequence (a0, a1, . . . , am), we may
repeatedly remove the elements am−1, am−2, . . . , a1 one by one. By doing so, the invariant aj/aj−1 ≥
2 is always maintained, and the value of Pm
j=1 a2
j/aj−1 never decreases. Therefore, we have
m
X
j=1
a2
j
aj−1
≤a2
m
a0
≤

min

max1≤t≤T w(t)
i ,
1
√
k/C
2
1/k
= min
(
k

max
1≤t≤T w(t)
i
2
, C
)
.
Recalling that the left-hand side of (9) is at most 1
C · Pm
j=1
a2
j
aj−1 , we have proved the inequality.
Proof of Equation (10)
For each integer j ∈[0, log2 k], let
Ij :=

i ∈[k] : max
1≤t≤T w(t)
i
∈
2j
k , 2j+1
k

denote the set of indices i ∈[k] on which the maximum weight over the T iterations is roughly
2j/k. Then, the upper bound Pk
i=1 max1≤t≤T w(t)
i
≤O(log8(k/ε)) from Lemma 3 implies |Ij| ≤
2−j · O(k log8(k/ε)). Note that I0, I1, . . . , I⌈log2 k⌉form a partition of [k], so we have
k
X
i=1
min
(
k
C

max
1≤t≤T w(t)
i
2
, 1
)
=
⌈log2 k⌉
X
j=0
X
i∈Ij
min
(
k
C

max
1≤t≤T w(t)
i
2
, 1
)
≤
⌈log2 k⌉
X
j=0
|Ij| · min
(
k
C
2j+1
k
2
, 1
)
≤O(k log8(k/ε)) ·
⌈log2 k⌉
X
j=0
2−j · min
(
k
C
2j+1
k
2
, 1
)
,
where the second step follows from i ∈Ij =⇒max1≤t≤T w(t)
i
< 2j+1/k, and the third step applies
the upper bound |Ij| ≤2−j · O(k log8(k/ε)).
The summation P⌈log2 k⌉
j=0
2−j · min

k
C

2j+1
k
2
, 1

is further upper bounded by
O(1) ·
+∞
X
j=0
min
 2j
Ck, 2−j

.
The summand min
n
2j
Ck, 2−jo
is given by the first term and thus geometrically increasing when
4j ≤Ck. The summand is geometrically decreasing when 4j ≥Ck. Therefore, the summation is
dominated by the term at j∗= ⌈log4(Ck)⌉, namely, 2−j∗= O(1/
√
Ck). Therefore, we have
k
X
i=1
min
(
k
C

max
1≤t≤T w(t)
i
2
, 1
)
≤O(k log8(k/ε)) · O(1/
√
Ck) = O(
p
k/C log8(k/ε)).
This proves Equation (10) and completes the proof.
39

C.4
Applications to Multi-Distribution Learning
We sketch how our proofs of Theorem 8 and Theorem 20 can be easily adapted to give MDL
algorithms with near-optimal sample complexities that run in either eO(k) or eO(
√
k) rounds. We
prove these results by modifying an algorithm of [ZZC+24], which we briefly describe below. We
refer the reader to [ZZC+24, Algorithm 1] for the full pseudocode description.
The MDL Algorithm of [ZZC+24]
Let H be the hypothesis class with VC dimension d. For
brevity, we treat the failure probability δ as a constant, and use the eO(·) and eΘ(·) notations to
suppress polylog(kd/ε) factors. The algorithm maintains k datasets S1, S2, . . . , Sk, where each Si
contains training examples drawn from the i-th data distribution Di. The algorithm runs the Hedge
dynamics for T = Θ((log k)/ε2) iterations starting at w(1) = (1/k, 1/k, . . . , 1/k). Each iteration
t ∈[T] consists of the following two steps:
• ERM step: For each i ∈[k], draw additional samples from Di and add them to Si until
|Si| ≥w(t)
i
· eΘ((d + k)/ε2).
Then, find a hypothesis h(t) ∈H that approximate minimizes the empirical error
ˆL(h) :=
k
X
i=1
w(t)
i
·
1
|Si|
X
(x,y)∈Si
1 [h(x) ̸= y] ,
which is an estimate of the error of h on Pk
i=1 w(t)
i Di.
• Hedge update step: For each i ∈[k], draw w(t)
i
· Θ(k) fresh samples from Di to obtain an
estimate r(t)
i
of the error of h(t) on Di. Compute w(t+1) from w(t) and r(t) via a Hedge update.
The crux of the analysis of [ZZC+24] is to show that the dataset sizes in the two steps above
are sufficient for finding a sufficiently accurate ERM h(t) as well as computing the loss vector r(t)
that is sufficiently accurate for the Hedge update.
Note that a straightforward implementation of the algorithm needs T = Θ((log k)/ε2) rounds
of sampling. While this round complexity is logarithmic in k, it has a polynomial dependence in
1/ε. In the following, we apply the ideas behind our OODS upper bounds to improve this round
complexity to eO(k) and then eO(
√
k), which is lower than (log k)/ε2 in the ε ≪1/k1/4 regime.
eO(k)-Round MDL from Theorem 8
The LazyHedge algorithm (Algorithm 3) suggests a nat-
ural modification to the algorithm of [ZZC+24]: To ensure |Si| ≥w(t)
i
· eΘ((d + k)/ε2), instead of
drawing additional samples at every round, we maintain a cap vector w(t) ∈[0, 1]k. We ensure the
invariant that, at the end of every iteration t, it holds for every i ∈[k] that |Si| ≥w(t)
i · eΘ((d+k)/ε2).
At each round t, if w(t)
i
≤w(t−1)
i
already holds for every i ∈[k], we use the current datasets to
perform the ERM step. Since |Si| ≥w(t−1)
i
· eΘ((d + k)/ε2) ≥w(t)
i
· eΘ((d + k)/ε2) holds for every
i ∈[k], the uniform convergence result of [ZZC+24, Lemma 1] guarantees that h(t) is an O(ε)-
accurate ERM. Otherwise, we update the cap vector to w(t) according to LazyHedge, and then add
fresh samples to each Si until |Si| ≥w(t)
i
· eΘ((d + k)/ε2).
40

It remains to handle the sampling in the Hedge update step, which requires w(t)
i
· Θ(k) fresh
samples from each Di. If we draw these samples in each round, the MDL algorithm would still
need T = Ω((log k)/ε2) rounds of sampling.
Instead, we maintain a dataset Si,t for each pair
(i, t) ∈[k] × [T] that is reserved for the Hedge update step for distribution Di in the t-th iteration.
We ensure the invariant that, at any iteration t,
|Si,t′| ≥w(t)
i
· Θ(k)
holds for every i ∈[k] and t′ ≥t. To this end, whenever the cap vector w(t) is updated, we add
fresh samples to each Si,t′ (where t′ ≥t) so that the invariant above still holds. By doing so, we
ensure that in the Hedge update step at any iteration t, we have |Si,t| ≥w(t)
i
· Θ(k) ≥w(t)
i
· Θ(k)
samples. Furthermore, we only need one round of sampling whenever the cap vector is updated.
The above exactly corresponds to the LazyHedge algorithm in the box setting of OODS: When-
ever the Hedge dynamics reaches a point w(t), the algorithm must update the cap w(t) to ensure
that w(t) ≥w(t). Every cap update in LazyHedge corresponds to a round of adaptive sampling in
MDL. By Lemma 7, the resulting MDL algorithm has an O(k log k) round complexity. Moreover,
the sample complexity of the MDL algorithm is at most
k
X
i=1
w(T)
i
· eΘ((d + k)/ε2) + T ·
k
X
i=1
w(T)
i
· Θ(k) = eO
C(d + k)
ε2

,
where we apply T = eO(1/ε2) and the bound Pk
i=1 w(T)
i
= O(C log8(k/ε)) = eO(C) from Lemma 6,
which in turn follows from [ZZC+24, Lemma 3]. Setting C = O(1) gives the following result for
MDL.
Corollary 21. There is an O(k log k)-round MDL algorithm with an eO((d + k)/ε2) sample com-
plexity.
eO(
√
k)-Round MDL from Theorem 20
We further improve the round complexity to eO(
√
k)
using our results for the ellipsoid setting (Theorem 20). The key observation is that the analysis
of [ZZC+24] does not require the “box” constraint w(t)
i
≤w(t)
i
for every i ∈[k]. Instead, the weaker
“ellipsoid” constraint Pk
i=1[w(t)
i ]2/w(t)
i
≤1 suffices.
To see this, we note that the analysis of [ZZC+24] uses the lower bounds on the sample size
at two different places, both of which go through under the ellipsoid constraint. The first place
is the proof of the uniform convergence result [ZZC+24, Lemma 1]. For fixed w ∈∆k−1, n ∈Nk
and h ∈H, the analysis boils down to showing that, if every dataset Si contains ni independent
samples from Di,
k
X
i=1
wi
ni
X
(x,y)∈Si
1 [h(x) ̸= y]
is an estimate for the population error of h on the mixture distribution Pk
i=1 wiDi with sub-
Gaussian parameter σ2 ≤Pk
i=1
w2
i
ni , and thus concentrates around the population error up to an
error of O(σ
p
log(1/δ)) except with probability δ. In particular, assuming that ni ≥wi · eΘ((d +
k)/ε2), we have
σ2 ≤
k
X
i=1
w2
i
wi · eΘ((d + k)/ε2)
= eΘ

ε2
d + k

k
X
i=1
wi = eΘ

ε2
d + k

.
41

We note that the same bound hold under the weaker assumption that, for some cap vector w, it
holds that Pk
i=1 w2
i /wi ≤1 and ni ≥wi · eΘ((d + k)/ε2):
σ2 ≤
k
X
i=1
w2
i
ni
≤
k
X
i=1
w2
i
wi · eΘ((d + k)/ε2)
= eΘ

ε2
d + k

·
k
X
i=1
w2
i /wi = eΘ

ε2
d + k

.
The second place is Step 3 in the proof of [ZZC+24, Lemma 17]. This step uses the fact that
w(t)
i
· Θ(k) samples are used to compute the estimate r(t)
i , so that the estimate has a variance of
O

1
kw(t)
i

. Fortunately, this variance bound is only used in aggregate over all i ∈[k] in their
Equation (111), which shows that the weighted average Pk
i=1 w(t)
i
· r(t)
i
has a variance of at most
k
X
i=1
[w(t)
i ]2 · O
 
1
kw(t)
i
!
= O(1/k) ·
k
X
i=1
w(t)
i
= O(1/k).
Again, this step would still go through under the ellipsoid constraint Pk
i=1[w(t)
i ]2/w(t)
i : As long as
at least w(t)
i
· Θ(k) fresh samples are used to estimate r(t)
i , the weighted average Pk
i=1 w(t)
i
· r(t)
i
has
a variance of at most
k
X
i=1
[w(t)
i ]2 · O
 
1
kw(t)
i
!
= O(1/k) ·
k
X
i=1
[w(t)
i ]2
w(t)
i
= O(1/k).
Therefore, Theorem 20 implies the following result for MDL.
Corollary 22. There is an eO(
√
k)-round MDL algorithm with an eO((d+k)/ε2) sample complexity.
D
Lower Bounds for OODS
We prove lower bounds on the sample-adaptivity tradeoff in the OODS model using two different
hard instances: one for the “large-ε” regime where ε ≤O(1/k), and the other for the “small-ε”
regime where ε ≤e−Ω(k).
D.1
Hard Instance for Large ε
Definition 4 (Hard instance for large ε). Given k ≥m ≥1, draw m different indices i⋆
1, i⋆
2, . . . , i⋆
m ∈
[k] uniformly at random. The objective function f : ∆k−1 →[0, 2] is
f(w) := min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆m + m
m2

.
Note that we allow the co-domain of f to be [0, 2] instead of [0, 1] for brevity; scaling everything
down by a factor of 2 gives a hard instance and thus lower bounds for the formulation in Definition 1.
Before we formally state and prove the lower bounds, we make a few simple observations and
then sketch the intuition behind the lower bound proof.
42

Characterization of Approximate Maxima
If we set wi⋆
j = 3m+1
2m2 −
j
m2 for every j ∈[m] and
wi = 0 for every i ∈[k] \ {i⋆
1, i⋆
2, . . . , i⋆
m}, we have
wi⋆
1 + 1
m2 = wi⋆
2 + 2
m2 = · · · = wi⋆m + m
m2 = 3m + 1
2m2 ,
which gives f(w) = 3m+1
2m2 . Furthermore, it can be easily verified that w ∈∆k−1. We note that w
is the maximizer of f over ∆k−1, since achieving an objective strictly higher than 3m+1
2m2 requires a
strict increase in each of wi⋆
1, . . . , wi⋆m, which would violate the constraint that w ∈∆k−1.
More generally, we have a “robust” version of this observation: As long as ε ≤O(1/k), every
ε-approximate maximum of f must put a significant weight of Ω(1/m) on at least half of the critical
indices i⋆
1, . . . , i⋆
m.
Lemma 23. For every ε ≤1/(2k) and every w ∈∆k−1 that satisfies f(w) ≥3m+1
2m2 −ε, we have
|{i ∈[k] : wi ≥1/(2m)} ∩{i⋆
1, i⋆
2, . . . , i⋆
m}| ≥m/2.
Proof. Suppose towards a contradiction that strictly fewer than m/2 entries among wi⋆
1, wi⋆
2, . . . , wi⋆m
are at least 1/(2m). Then, there exists j ∈{1, 2, . . . , ⌈m/2⌉} such that wi⋆
j < 1/(2m). It follows
that
f(w) ≤wi⋆
j + j
m2 <
1
2m + (m + 1)/2
m2
= 2m + 1
2m2 .
On the other hand, since m ≤k and ε ≤1/(2k), we have
3m + 1
2m2
−ε ≥3m + 1
2m2
−1
2m = 2m + 1
2m2
> f(w),
a contradiction.
Limited Information from a First-Order Oracle
Therefore, it remains to argue that an
OODS algorithm cannot identify Ω(m) critical indices using ≪m rounds while incurring a polylog(k)
sample overhead. To this end, we observe that a first-order oracle provides little information on
the critical indices, unless we have already found many such indices.
By definition of f from Definition 4, the following is a valid first-order oracle: Given w ∈∆k−1,
find the minimum j ∈[m] such that f(w) = wi⋆
j + j/m2.
Return the value of f(w) and the
supergradient ei⋆
j . In other words, via a first-order oracle, the optimization algorithm only gets to
know the critical index that accounts for the value of f(w).
Our next lemma suggests that, unless we put a weight of > 1/m2 on each of i⋆
1, i⋆
2, . . . , i⋆
j, the
value of f(w) is determined by the first j terms in the minimum. In other words, unless we already
“know” the first j critical indices, we cannot learn the value of i⋆
j+1, . . . , i⋆
m from the oracle.
Lemma 24. For every w ∈∆k−1 and j ∈[m], assuming min{wi⋆
1, wi⋆
2, . . . , wi⋆
j } ≤1/m2, we have
f(w) = min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

.
Proof. Suppose towards a contradiction that some w ∈∆k−1 satisfies wi⋆
j1 ≤1/m2 for some j1 ≤j,
but f(w) ̸= min
n
wi⋆
1 +
1
m2 , wi⋆
2 +
2
m2 , . . . , wi⋆
j +
j
m2
o
. Then, there exists j2 > j such that
wi⋆
j2 + j2
m2 < min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

≤wi⋆
j1 + j1
m2 ,
which implies wi⋆
j1 > wi⋆
j2 + (j2 −j1)/m2 ≥1/m2, a contradiction.
43

Intuition behind Lower Bound
There is a natural m-round algorithm for solving the instance
from Definition 4: In the first round, we query the uniform weight vector w = (1/k, 1/k, . . . , 1/k)
to learn the value of i⋆
1. In the second round, we query f on some w with wi⋆
1 ≫1/m2, thereby
learning the value of i⋆
2. Repeating this gives a m-round algorithm with a low sample overhead.
One might hope to be “more clever” and learn more than one critical index in each round. For
example, the algorithm might put a cap of ≫1/m2 on several coordinates in the first round, in the
hope of hitting more than one indices in i⋆
1, i⋆
2, . . .. However, if the algorithm has a sample overhead
of s, only O(m2s) such guesses can be made. In particular, if m2s ≪k, the guesses only cover a
tiny fraction of the indices. Thus, over the uniform randomness in i⋆, the algorithm can still only
learn O(1) critical indices in expectation within each round.
D.2
Lower Bounds for Large ε
Now, we formally state and prove the lower bounds in the ε ≤O(1/k) regime, for both the box
and the ellipsoid settings.
Theorem 25. The following holds for all sufficiently large k, ε ≤1/(2k) and s ≥1: (1) In the
box setting, for m := ⌊1
2
p
k/s⌋, no OODS algorithm with r ≤m/24 rounds and sample overhead s
can find an ε-approximate maximum with probability at least 9/10; (2) In the ellipsoid setting, for
m := ⌊1
2(k/s)1/4⌋, no OODS algorithm with r ≤m/24 rounds and sample overhead s can find an
ε-approximate maximum with probability at least 9/10.
In particular, to have a sample overhead s ≤polylog(k), any OODS algorithm must take eΩ(
√
k)
rounds in the box setting, and eΩ(k1/4) rounds in the ellipsoid setting.
Proof. We will focus on the box setting; the ellipsoid setting follows easily with only a few changes
in the proof.
Suppose towards a contradiction that an OODS algorithm A for the box setting takes r ≤m/24
rounds and succeeds with probability at least 9/10. Consider the execution of A on a random OODS
instance defined in Definition 4 with m :=
j
1
2
p
k/s
k
.
For each round t ∈[r], let I(t) := {i ∈[m] : w(t)
i
> 1/m2} denote the indices on which algorithm
A sets a cap strictly higher than 1/m2. Since A has a sample overhead of s, we have ∥w(t)∥1 ≤s
and thus |I(t)| ≤m2s. By definition of the box setting, within each round t, A cannot query the
oracle on w if wi > 1/m2 holds for some i ∈[k] \ I(t). Therefore, Lemma 24 implies the following:
Unless {i⋆
1, i⋆
2, . . . , i⋆
j} ⊆I(t), for every point w that A queries in round t, it holds that
f(w) = min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

.
Deferring Randomness
Rather than drawing all the m critical indices at the beginning, in our
analysis, we choose these indices “on demand” as the OODS algorithm expands the observable
region. We do this carefully, so that the distribution of i⋆is not biased.
In the first round, after w(1) (and thus the set I(1)) has been determined by A, we draw i⋆
1 from
[k] uniformly at random. If i⋆
1 /∈I(1), we stop the process; otherwise, we draw i⋆
2 from [k] \ {i⋆
1}
uniformly at random. We keep doing this, until either all the m critical indices are determined, or
we draw some i⋆
j /∈I(1). Let n(1) denote the number of critical indices drawn in the first round.
44

By Lemma 24, for every w ∈O(w(1)) observable to A in the first round, f(w) is determined by
the first n(1) critical indices. In particular, the remaining m −n(1) critical indices (i⋆
n(1)+1 through
i⋆
m) are still uniformly distributed among [k] \ {i⋆
1, . . . , i⋆
n(1)}, conditioned on any information that
A obtains from the first-order oracle.
More generally, at the beginning of each round t ∈[r], we observe the set I(t) is determined by
algorithm A. We keep sampling i⋆
n(t−1)+1, i⋆
n(t−1)+2, . . ., until either all m indices are chosen or we
encounter an index outside I(t). Let n(t) denote the total number of critical indices that have been
sampled, including those sampled in the first t −1 rounds. Then, all queries made by A during the
t-th round can be answered solely based on the values of i⋆
1 through i⋆
n(t), as they do not depend on
the m −n(t) indices that have not been decided.
Control the Progress Measure
Let random variable N(t) denote the value of n(t) in round t,
over the randomness in both algorithm A and the random drawing of the critical indices. Next, we
upper bound the expectation of N(r) after all r rounds.
In each round t ∈[r], whenever we sample a critical index i⋆
j (j ∈[m]), there are k −(j −1) >
k −m possible choices (namely, [k] \ {i⋆
1, . . . , i⋆
j−1}). Among these choices, at most |I(t)| ≤m2s
fall into the set I(t). Recall our choice of m :=
j
1
2
p
k/s
k
, which ensures m2s ≤k/4 and m ≤k/2.
Thus, the probability of not stopping after drawing i⋆
j is at most
m2s
k−m ≤k/4
k/2 = 1
2. It follows that
the number of critical indices sampled in each round t is stochastically dominated by a geometric
random variable with parameter 1/2. Therefore, we conclude that E

N(r)
≤2r.
Control the Success Probability
Finally, we derive a contradiction by arguing that the prob-
ability that A finds an ε-approximate maximum of f is below 9/10. Let ˆw denote the output of
A, and ˆI := {i ∈[k] : ˆwi ≥1/(2m)} be the indices on which ˆw puts a weight of at least 1/(2m).
Since the entries of ˆw ∈∆k−1 sum up to 1, |ˆI| ≤2m. By Lemma 23, for ˆw to be an ε-approximate
maximum, ˆI ∩{i⋆
1, i⋆
2, . . . , i⋆
m} must have a size ≥m/2.
Conditioning on the event that N(r) ≤m/4, at least 3m/4 critical indices have not been
chosen, and they are uniformly distributed among the k−N(r) remaining indices. For the condition
|ˆI∩{i⋆
1, i⋆
2, . . . , i⋆
m}| ≥m/2 to hold, we must have |ˆI∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| ≥m/2−N(r) ≥m/4.
For any choice of ˆI, over the remaining randomness in {i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}, it holds that
E
h
|ˆI ∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| | N(r)i
≤|ˆI| · m −N(r)
k −N(r) ≤2m ·
m
k −m/4 ≤4m2
k .
Recall that m ≤1
2
p
k/s ≤
√
k/2. Markov’s inequality gives
Pr
h
|ˆI ∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| ≥m/4 | N(r) ≤m/4
i
≤
E
h
|ˆI ∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| | N(r) ≤m/4
i
m/4
≤4m2/k
m/4
≤
8
√
k
≤1
3,
45

where the last step holds for all sufficiently large k ≥576. In other words, conditioning on N(r) ≤
m/4, the probability of finding an ε-approximate maximum is at most 1/3. On the other hand, by
Markov’s inequality and the assumption that r ≤m/24,
Pr
h
N(r) > m/4
i
≤E

N(r)
m/4
≤
2r
m/4 ≤1
3.
Therefore, the overall probability for A to output an ε-approximate maximum is at most 2/3 < 9/10,
a contradiction.
The Ellipsoid Setting
In this setting, we consider the execution of A on a random OODS
instance defined in Definition 4 with m :=
 1
2(k/s)1/4
. Different from the analysis for the box
setting, we define I(t) := {i ∈[m] : w(t)
i
> 1/m4} using threshold 1/m4 instead of 1/m2. Then, we
have an analogous implication of Lemma 24: Unless {i⋆
1, i⋆
2, . . . , i⋆
j} ⊆I(t), for every point w that
A queries in round t, it holds that
f(w) = min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

.
To see this, note that if i⋆
j0 /∈I(t) holds for some j0 ∈[j], we have w(t)
i⋆
j0 ≤1/m4. Then, for any
w ∈O(w(t)), we must have
w2
i⋆
j0
w(t)
i⋆
j0
≤
k
X
i=1
w2
i
w(t)
i
≤1,
which implies wi⋆
j0 ≤
r
w(t)
i⋆
j0 ≤1/m2. Then, Lemma 24 implies
f(w) = min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

.
To control the expectation of N(r), we note that the definition of I(t) and the assumption on A
having a sample overhead of s together imply |I(t)| ≤m4s. Then, the choice of m ≤1
2(k/s)1/4 ≤k/2
guarantees that the sampling process stops at each step except with probability m4s
k−m ≤k/16
k/2 ≤1/2.
The rest of the proof goes through.
D.3
Hard Instance for Small ε
When the accuracy parameter ε is exponentially small, we give a slightly different instance on which
any OODS algorithm must take Ω(k) rounds in the box setting and Ω(
√
k) rounds in the ellipsoid
setting, matching the exponents in Theorems 8 and 20.
Definition 5 (Hard instance for small ε). Given k ≥m ≥1, draw m different indices i⋆
1, i⋆
2, . . . , i⋆
m ∈
[k] uniformly at random. The objective function f : ∆k−1 →[0, 2] is
f(w) := min
j∈[m](aj · wi⋆
j + bj),
where aj = 2−j and bj = (1 −2−j)/m.
Again, we allow the co-domain of f to be [0, 2] instead of [0, 1] for brevity. We start with a few
simple observations and the intuition behind the lower bound proof.
46

Characterization of Approximate Maxima
We first note that the maximizer of f over ∆k−1
is the following vector w: wi = 1/m for every i ∈{i⋆
1, . . . , i⋆
m} and wi = 0 for i ∈[k] \ {i⋆
1, . . . , i⋆
m}.
Indeed, such w ensures that
aj · wi⋆
j + bj = 2−j · 1
m + 1 −2−j
m
= 1
m
for every j ∈[m] and thus f(w) = 1/m. Furthermore, to achieve an objective strictly higher than
1/m, we must strictly increase each of wi⋆
1, . . . , wi⋆m, which would violate w ∈∆k−1.
Analogous to Lemma 23, the following lemma states that for every ε ≤e−Ω(k), every ε-
approximate maximum of f must put a significant weight of Ω(1/m) on at least half of the critical
indices i⋆
1, . . . , i⋆
m.
Lemma 26. For every ε ≤2−(k+1)/2/(2k) and every w ∈∆k−1 that satisfies f(w) ≥
1
m −ε, we
have
|{i ∈[k] : wi ≥1/(2m)} ∩{i⋆
1, i⋆
2, . . . , i⋆
m}| ≥m/2.
Proof. Suppose towards a contradiction that strictly fewer than m/2 entries among wi⋆
1, wi⋆
2, . . . , wi⋆m
are at least 1/(2m). Then, there exists j ∈{1, 2, . . . , ⌈m/2⌉} such that wi⋆
j < 1/(2m). It follows
that
f(w) ≤aj · wi⋆
j + bj < 2−j
2m + 1 −2−j
m
= 1
m −2−j
2m ≤1
m −2−(m+1)/2
2m
.
On the other hand, since m ≤k and ε ≤2−(k+1)/2/(2k), we have
1
m −ε ≥1
m −2−(k+1)/2
2k
= 1
m −2−(m+1)/2
2m
.
This contradicts the assumption that f(w) ≥1
m −ε.
Limited Information from a First-Order Oracle
Again, the lower bound proof amounts to
showing that an OODS algorithm cannot identify Ω(m) critical indices using ≪m rounds while
having a low sample overhead. To this end, we note that the following is a valid first-order oracle
for f: Given w ∈∆k−1, find the minimum j ∈[m] such that f(w) = aj · wi⋆
j + bj. Return the value
of f(w) and the supergradient aj · ei⋆
j . Again, the optimization algorithm only gets to know the
critical index that accounts for the value of f(w).
The following lemma is an analogue of Lemma 24: We must put a weight of Ω(1/m) on each of
i⋆
1, i⋆
2, . . . , i⋆
j to learn the values of i⋆
j+1, . . . , i⋆
m from the first-order oracle.
Lemma 27. For every w ∈∆k−1 and j ∈[m], assuming min{wi⋆
1, wi⋆
2, . . . , wi⋆
j } ≤1/(2m), we have
f(w) = min
n
a1 · wi⋆
1 + b1, a2 · wi⋆
2 + b2, . . . , aj · wi⋆
j + bj
o
.
Proof. Suppose towards a contradiction that some w ∈∆k−1 satisfies wi⋆
j1 ≤1/(2m) for some
j1 ≤j, but f(w) ̸= min
n
a1 · wi⋆
1 + b1, a2 · wi⋆
2 + b2, . . . , aj · wi⋆
j + bj
o
. Then, there exists j2 > j
such that
aj2 · wi⋆
j2 + bj2 < min
n
a1 · wi⋆
1 + b1, a2 · wi⋆
2 + b2, . . . , aj · wi⋆
j + bj
o
≤aj1 · wi⋆
j1 + bj1.
47

Plugging aj = 2−j and bj = (1 −2−j)/m into the above gives
2−j2 · wi⋆
j2 + 1 −2−j2
m
< 2−j1 · wi⋆
j1 + 1 −2−j1
m
.
Recalling the assumption that wi⋆
j1 ≤1/(2m), we have
wi⋆
j2 < 1
m + 2j2−j1 ·

wi⋆
j1 −1
m

≤1
m −1
2m · 2j2−j1 ≤0,
a contradiction.
Intuition behind Lower Bound
As in the large-ε regime, there is a natural m-round algorithm
for solving the instance from Definition 5, so the proof amounts to arguing that the OODS algorithm
cannot be “more clever” and learn more than one critical index in each round. For example, the
algorithm might put a cap of ≫1/m on several coordinates in the first round, in the hope of hitting
more than one indices in i⋆
1, i⋆
2, . . .. However, any algorithm with a sample overhead of s can make
only O(ms) such guesses. Then, assuming ms ≪k, the guesses only cover a tiny fraction of the
indices. Thus, over the uniform randomness in i⋆, the algorithm can still only learn O(1) critical
indices in expectation within each round.
D.4
Lower Bounds for Small ε
Finally, we state and prove the lower bounds in the ε ≤e−Ω(k) regime, for both the box and the
ellipsoid settings.
Theorem 28. The following holds for all sufficiently large k, ε ≤2−(k+1)/2/(2k) and s ≥1: (1)
In the box setting, for m := min{⌊k/(8s)⌋, ⌊k/48⌋}, no OODS algorithm with r ≤m/24 rounds and
sample overhead s can find an ε-approximate maximum with probability at least 9/10; (2) In the
ellipsoid setting, for m := min{⌊1
4
p
k/s⌋, ⌊k/48⌋}, no OODS algorithm with r ≤m/24 rounds and
sample overhead s can find an ε-approximate maximum with probability at least 9/10.
In particular, to have a sample overhead s ≤polylog(k), any OODS algorithm must take eΩ(k)
rounds in the box setting, and eΩ(
√
k) rounds in the ellipsoid setting. These match the exponents
in the upper bounds (Theorems 8 and 20).
The proof is analogous to the one for Theorem 25, so we will be brief.
Proof. Again, we focus on the box setting; the ellipsoid setting follows easily with only a few
changes in the proof.
Suppose towards a contradiction that an OODS algorithm A for the box setting takes r ≤m/24
rounds and succeeds with probability at least 9/10. Consider the execution of A on a random
OODS instance defined in Definition 4 with m := min{⌊k/(8s)⌋, ⌊k/48⌋}. For each round t ∈[r],
let I(t) := {i ∈[m] : w(t)
i
> 1/(2m)} denote the indices on which algorithm A sets a cap of Ω(1/m).
Since A has a sample overhead of s, we have ∥w(t)∥1 ≤s and thus |I(t)| ≤2ms. By definition of
the box setting, within each round t, A cannot query the oracle on w if wi > 1/(2m) holds for some
i ∈[k] \ I(t). Therefore, Lemma 27 implies the following: Unless {i⋆
1, i⋆
2, . . . , i⋆
j} ⊆I(t), for every
point w that A queries in round t, it holds that
f(w) = min
n
a1 · wi⋆
1 + b1, a2 · wi⋆
2 + b2, . . . , aj · wi⋆
j + bj
o
.
48

Deferring Randomness
Again, we draw the m critical indices “on demand” in our analysis.
We start with n(0) = 0. In each round t ∈[r], after A decides on w(t) and I(t), we keep sampling
i⋆
n(t−1)+1, i⋆
n(t−1)+2, . . ., until either all m indices are chosen or we encounter an index outside I(t).
Let n(t) denote the total number of critical indices that have been sampled by the end of round t.
By the implication of Lemma 27, all queries made by A during the t-th round can be answered
solely based on the values of i⋆
1 through i⋆
n(t), as they do not depend on the m −n(t) indices that
have not been decided.
Control the Progress Measure
Let random variable N(t) denote the value of n(t) over the
randomness in both algorithm A and the critical indices.
In each round t ∈[r], whenever we
sample a critical index i⋆
j (j ∈[m]), there are k −(j −1) > k −m possible choices (namely,
[k] \ {i⋆
1, . . . , i⋆
j−1}). Among these choices, at most |I(t)| ≤2ms fall into the set I(t). Recall our
choice of m ≤k/(8s), which ensures both 2ms ≤k/4 and m ≤k/2. Thus, the probability of not
stopping after drawing i⋆
j is at most
2ms
k−m ≤k/4
k/2 = 1
2. It follows that at most 2 critical indices are
sampled in each round t in expectation, so we have E

N(r)
≤2r.
Control the Success Probability
Finally, we derive a contradiction by arguing that the prob-
ability that A finds an ε-approximate maximum of f is below 9/10. Let ˆw denote the output of
A, and ˆI := {i ∈[k] : ˆwi ≥1/(2m)} be the indices on which ˆw puts a weight of at least 1/(2m).
Since the entries of ˆw ∈∆k−1 sum up to 1, |ˆI| ≤2m. By Lemma 26, for ˆw to be an ε-approximate
maximum, ˆI ∩{i⋆
1, i⋆
2, . . . , i⋆
m} must have a size ≥m/2.
Conditioning on the event that N(r) ≤m/4, at least 3m/4 critical indices have not been
chosen, and they are uniformly distributed among the k−N(r) remaining indices. For the condition
|ˆI∩{i⋆
1, i⋆
2, . . . , i⋆
m}| ≥m/2 to hold, we must have |ˆI∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| ≥m/2−N(r) ≥m/4.
For any choice of ˆI, over the remaining randomness in {i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}, it holds that
E
h
|ˆI ∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| | N(r)i
≤|ˆI| · m −N(r)
k −N(r) ≤2m ·
m
k −m/4 ≤4m2
k .
Recall that m ≤1
2
p
k/s ≤
√
k/2. Markov’s inequality gives
Pr
h
|ˆI ∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| ≥m/4 | N(r) ≤m/4
i
≤
E
h
|ˆI ∩{i⋆
N(r)+1, i⋆
N(r)+2, . . . , i⋆
m}| | N(r) ≤m/4
i
m/4
≤4m2/k
m/4
= 16m
k
≤1
3,
where the last step follows from our choice of m ≤k/48. In other words, conditioning on N(r) ≤
m/4, the probability of finding an ε-approximate maximum is at most 1/3. On the other hand, by
Markov’s inequality and the assumption that r ≤m/24,
Pr
h
N(r) > m/4
i
≤E

N(r)
m/4
≤
2r
m/4 ≤1
3.
Therefore, the overall probability for A to output an ε-approximate maximum is at most 2/3 < 9/10,
a contradiction.
49

The Ellipsoid Setting
Finally, for the ellipsoid setting, we instead consider the execution of A
on a random OODS instance defined in Definition 4 with m := min{⌊1
4
p
k/s⌋, ⌊k/48⌋}, and define
I(t) := {i ∈[m] : w(t)
i
> 1/(4m2)} using threshold 1/(4m2) instead of 1/(2m). Then, we have an
analogous implication of Lemma 24: Unless {i⋆
1, i⋆
2, . . . , i⋆
j} ⊆I(t), for every point w that A queries
in round t, it holds that
f(w) = min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

.
To see this, note that if i⋆
j0 /∈I(t) holds for some j0 ∈[j], we have w(t)
i⋆
j0 ≤1/(4m2). Then, for any
w ∈O(w(t)), we must have
w2
i⋆
j0
w(t)
i⋆
j0
≤
k
X
i=1
w2
i
wi
≤1,
which implies wi⋆
j0 ≤
r
w(t)
i⋆
j0 ≤1/(2m). Then, Lemma 27 implies
f(w) = min

wi⋆
1 + 1
m2 , wi⋆
2 + 2
m2 , . . . , wi⋆
j + j
m2

.
To control the expectation of N(r), we note that the definition of I(t) and the assumption on
A having a sample overhead of s together imply |I(t)| ≤4m2s. Then, the choice of m ≤1
4
p
k/s
guarantees that the sampling process stops at each step except with probability 4m2s
k−m ≤k/4
k/2 = 1/2.
The rest of the proof goes through.
50
