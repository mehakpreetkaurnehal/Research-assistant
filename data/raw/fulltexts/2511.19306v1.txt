Dual-Granularity Semantic Prompting for Language Guidance
Infrared Small Target Detection
Zixuan Wang
Haoran Sun
Jiaming Lu
Wenxuan Wang
Zhongling Huang
Dingwen Zhang
Xuelin Qian
Junwei Han
School of Automation, Northwestern Polytechnical University
Xi‚Äôan, Shaanxi 710072, China
Abstract
Infrared small target detection remains challenging due to
limited feature representation and severe background in-
terference, resulting in sub-optimal performance.
While
recent CLIP-inspired methods attempt to leverage textual
guidance for detection, they are hindered by inaccurate
text descriptions and reliance on manual annotations. To
overcome these limitations, we propose DGSPNet, an end-
to-end language prompt-driven framework. Our approach
integrates dual-granularity semantic prompts:
coarse-
grained textual priors (e.g., ‚Äúinfrared image‚Äù, ‚Äúsmall tar-
get‚Äù) and fine-grained personalized semantic descriptions
derived through visual-to-textual mapping within the im-
age space. This design not only facilitates learning fine-
grained semantic information but also can inherently lever-
age language prompts during inference without relying on
any annotation requirements. By fully leveraging the pre-
cision and conciseness of text descriptions, we further in-
troduce a text-guide channel attention (TGCA) mechanism
and text-guide spatial attention (TGSA) mechanism that en-
hances the model‚Äôs sensitivity to potential targets across
both low- and high-level feature spaces.
Extensive ex-
periments demonstrate that our method significantly im-
proves detection accuracy and achieves state-of-the-art per-
formance on three benchmark datasets.
1. Introduction
Infrared small target detection (IRSTD) aims to accurately
localize tiny objects in infrared imagery captured by aerial
and satellite platforms [1].
However, these targets often
lack distinctive structural or textural features due to ther-
mal diffusion, making them extremely difficult to differen-
tiate from complex backgrounds. This challenge is further
compounded by factors such as sensor noise, atmospheric
interference, and highly dynamic scenes. Despite these dif-
ficulties, IRSTD remains crucial for mission-critical appli-
cations, including early-warning military systems, civil in-
frastructure monitoring, disaster response, and search-and-
rescue operations. Consequently, the growing demand for
robust and reliable detection has driven extensive research
efforts in this field [19, 39].
Existing infrared small target detection (IRSTD) ap-
proaches, such as SCTrans [35] and DCFR-Net [8], primar-
ily enhance target saliency by leveraging spatial‚Äìchannel
transformer mechanisms or detail-preserving convolutional
networks.
While these techniques improve feature rep-
resentation and emphasize potential target regions, they
predominantly rely on visual cues extracted from the im-
age domain. This dependence becomes a major limitation
in infrared imagery, where targets exhibit extremely low
signal-to-noise ratios, weak contrast against complex back-
grounds, and minimal structural or textural features [25].
Consequently, these methods often struggle to achieve
robust detection performance in cluttered or highly dy-
namic scenes, underscoring the need for more adaptive and
context-aware solutions.
To address these challenges, several pioneering stud-
ies have investigated visual‚Äìlanguage models (VLMs) [13],
leveraging text descriptions as semantic priors to guide tar-
get localization in infrared imagery. Although these text-
guided approaches show considerable promise by intro-
ducing high-level contextual cues, they suffer from two
critical limitations:
(1) Annotation overhead.
Gener-
ating precise text labels (e.g., ‚Äúsmall aerial target in a
sky‚Äìground background‚Äù) requires domain expertise and
extensive manual annotation, while automatically produc-
ing such descriptions using VLMs demands substantial
computational resources.
Both approaches impose pro-
hibitive costs, severely limiting their scalability for large-
scale IRSTD deployment.
(2) Description discrepancy.
Text annotations‚Äîwhether manually crafted or VLM-
generated‚Äîoften fail to accurately capture infrared-specific
1
arXiv:2511.19306v1  [cs.CV]  24 Nov 2025

properties. For example, low-intensity thermal signatures
may be misinterpreted as noise, or target features may be
confused with background clutter. Such semantic inaccu-
racies can propagate through the detection pipeline, ulti-
mately degrading model performance.
To address these limitations, we propose an end-to-end
framework for language-guided infrared small target detec-
tion, centered on Dual-Granularity Semantic Prompts.
Specifically, (i) coarse-grained textual priors (e.g., ‚Äúinfrared
image,‚Äù ‚Äúsmall target‚Äù) provide general domain-aware con-
text, while (ii) fine-grained, instance-specific semantic de-
scriptions are generated via a visual-to-textual mapping
within the image, capturing target-specific cues. This dual
design enriches the feature space with complementary se-
mantic cues, facilitating more discriminative representa-
tions of small targets.
Moreover, this framework naturally supports the use of
language prompts during inference without requiring man-
ual annotations, significantly reducing deployment over-
head. To further exploit these semantic cues, we introduce
a Text-Guide Channel and Spatial Attention mechanism,
which integrates refined textual semantics into the model‚Äôs
attention module. The mechanisms guide the model to con-
centrate on semantically relevant regions while suppressing
background noise, thereby enhancing target saliency and
improving localization precision. Our contributions can be
summarized as follows:
‚Ä¢ We propose a novel dual-granularity semantic prompt-
ing strategy that combines coarse-grained textual priors
with fine-grained personalized semantic descriptions via
visual-to-textual mapping, enhancing feature representa-
tion for small targets.
‚Ä¢ We design a Text-Guide Channel and Spatial Attention
mechanism that integrates refined semantic cues into the
model, effectively directing focus toward semantically
relevant regions while suppressing background interfer-
ence, thereby enhancing target saliency and localization
accuracy.
‚Ä¢ Extensive experiments on benchmark IRSTD datasets
demonstrate that our framework significantly outperforms
state-of-the-art methods, achieving superior detection in
complex and dynamic infrared scenarios.
2. Related Work
2.1. Infrared Small Target Detection
Early infrared small target detection (IRSTD) methods were
heavily reliant on handcrafted priors designed to enhance
faint targets against noisy thermal backgrounds.
Tech-
niques such as filter-based methods, including Top-Hat and
Laplacian-of-Gaussian (LoG) transformations, were com-
monly used to highlight target features by emphasizing dif-
ferences in intensity [7, 41]. Additionally, local contrast-
based approaches that exploited regional saliency [2, 5]
and sparse representation models for decomposing target
and background signals [27] were also frequently applied.
While these classical methods were computationally effi-
cient, they proved highly sensitive to background clutter and
often struggled in complex infrared scenes featuring non-
uniform noise or dynamic structures, where distinguishing
small targets from the background became difficult. With
the advent of deep learning, convolutional neural networks
(CNNs) revolutionized IRSTD by offering more robust de-
tection.
Methods like ACUNet, IRNet, and IRSTDNet
[6, 20, 43] utilized techniques such as multiscale fusion,
encoder-decoder consistency, and detail-preserving designs
to improve accuracy in detecting small targets. However,
despite their advancements, CNN-based models still largely
rely on low-level texture cues and lack strong semantic rea-
soning, leading to poor generalization when faced with am-
biguous or unseen conditions.
To address these shortcomings, Transformer-based ar-
chitectures, such as ISFormer and DGFormer [11, 30], were
introduced.
These models employ self-attention mecha-
nisms to capture long-range dependencies and global con-
text, which enhances feature representation, especially in
cluttered scenes. However, even these Transformer-based
models remain limited by their reliance on purely visual in-
formation, which restricts their semantic expressiveness and
adaptability to diverse imaging conditions. The limitations
of both classical methods and deep learning models high-
light the need for more comprehensive approaches that can
incorporate multi-modal or semantic information to better
adapt to the complexities of infrared target detection.
2.2. Language-guided IRSTD
Vision‚ÄìLanguage Models (VLMs), such as CLIP [23] and
ALIGN [14], have made significant strides by aligning
images and text in a shared embedding space using con-
trastive learning.
This alignment has led to remarkable
performance across a wide range of tasks, including open-
vocabulary detection (e.g., ViLD [10], GLIP [16]), image
captioning, visual question answering, and dense predic-
tion through multimodal frameworks like OFA [29], BLIP-
2 [17], and UniVL [44]. Moreover, generative models like
GIT [31] treat vision-language tasks as conditional gener-
ation problems, while instruction-following detectors, such
as LLMDet and DetGPT [21, 22], combine large language
models with detection heads to enable natural-language-
driven vision tasks. Despite their tremendous success in the
realm of RGB imagery, VLMs face significant challenges
when adapted to infrared data, primarily due to modal-
ity gaps, such as low signal-to-noise ratios, weak target
contrast, and the distinct physics behind infrared imaging.
These differences make it difficult for VLMs to effectively
interpret and detect targets in infrared images, limiting their
2

Re-encoder
Layer1
Layer2
Layer3
Layer4
Layer5
Down
Down
Down
C flatten
Learnable 
token
MultiHead
Attention
MLP
S1
‚àó
S2
‚àó
CLIP
Avg
&
Projection
Contrast
Loss
Block1
Block2
Block3
DeConv
Skip
Connect
Conv
Block5
Inversion Net
MSE Loss
Reconstruction Decoder
Pre-encoder
Re-encoder
Layer1
Layer3
Layer5
Block1
Block2
Block3
Conv
Skip
Connect
TGSA
Block5
Decoder
Pool
Conv
CA
Pool
Conv
TGCA
Down
Down
Down
C
Inversion Net
CLIP
Cross
Attention
Pre-encoder
Pre-training
Normal-training
A photo of an infrared imageÔºå
with ùë†1
‚àóin ùë†2
‚àóbackground.
A photo of an infrared imageÔºå
with ùë†1
‚àóin ùë†2
‚àóbackground.
‚®â2
BCE Loss
+
SoftIoU Loss
Conv
Convolutional Block
Deconvolutional Block
Channel Attention Module
Text-guide Channel Attention 
Module
Text-guide Spatial Attention 
Module
Deconv
CA
TGCA
TGSA
Maxpooling with stride of 2
Pool
‚®â2
‚®â2
Figure 1. Overview of DGSPNet. The entire training process is divided into two phases: pre-training and normal training. During the pre-
training phase, the network‚Äôs decoder is replaced with a reconstruction decoder, and contrastive loss is introduced to supervise the training
of the inversion network. During the normal training phase, the weights of the inversion net are frozen. Each convolutional block consists
of one convolution unit, a normalization layer, and a ReLU activation layer, and the deconvolutional block has the same composition.
performance in infrared small target detection (IRSTD).
To address this, recent efforts have sought to bridge the
modality gap by exploring language-guided IRSTD. For ex-
ample, Text-IRSTD [13] introduces fuzzy semantic prompts
and a Progressive Cross-modal Semantic Interaction De-
coder to dynamically align textual and visual features, im-
proving target localization. However, these approaches still
face challenges such as high annotation or computational
overhead and an inability to effectively capture infrared-
specific attributes, such as low-intensity thermal signals and
background noise. These limitations often result in seman-
tic mismatches between the textual descriptions and the in-
frared image content, which in turn degrade the overall de-
tection performance. As a result, there remains a need for
more efficient and precise methods that can bridge these
gaps and fully leverage language-guided infrared detection.
3. Methodology
3.1. Overview
Figure 1 illustrates the schematic overview of the proposed
DGSPNet, an end-to-end dual-granularity semantic prompt-
ing network for language-guided infrared small target de-
tection. DGSPNet takes an infrared image as input and out-
puts a binary segmentation mask to localize small targets.
The framework consists of three principal components:
hierarchical image encoder, dual-granularity semantic
prompt and prompt-driven semantic guidance. The hier-
archical image encoder is divided into a pre-encoder and a
re-encoder, responsible for ‚Äúlow-to-middle-level feature ex-
traction‚Äù and ‚Äútext-guided high-level feature optimization‚Äù,
respectively. The dual-granularity semantic prompt mecha-
nism constructs multi-granularity textual descriptions that
semantically align with the input infrared image, while
the prompt-driven semantic guidance leverages these text
prompts to dynamically guide the decoding process, en-
abling more accurate segmentation of small targets.
During training, the weights of the inversion net, which
is designed for dual-granularity semantic prompt genera-
tion, are frozen. To obtain the weights of the inversion net,
we elaborately design a reconstruction pre-training method.
After pre-training, both the inversion net and the image en-
coder weights are loaded for formal training, providing a
strong initialization that improves overall learning perfor-
3

GAP
MLP
unsqueeze
+
C
‚®â
+
MLP
MLP
unsqueeze
+
¬∑
Softmax
‚®â
+
Linear
Sigmoid
+
(a) TGCA
(b) TGSA
Text feature
Visual feature
Text feature
Visual feature
Figure 2.
Illustration of TGCA and TGSA. Guided by text
features, these modules generate channel and spatial attention
weights, which are applied via multiplication and residual con-
nections to enhance visual features.
mance.
In the following sections, we first introduce the hier-
archical image encoder, which captures global contextual
features while leveraging dual-granularity semantic infor-
mation to emphasize target regions. Next, we describe the
semantic prompt learning process, covering both coarse-
grained priors (explicit descriptions) and fine-grained se-
mantic cues (instance-specific, implicit descriptions). We
then present the prompt-guided decoding strategy, which
integrates attention mechanisms to model interactions be-
tween image features and text embeddings. Finally, we de-
tail the training and inference procedures of DGSPNet.
3.2. Hierarchical Image Encoder
The hierarchical encoder comprises two stages:
pre-
encoding and re-encoding.
The pre-encoding stage ex-
tracts basic visual features, such as edges, textures, and
local brightness, providing a foundation for the inversion
net to generate fine-grained semantic prompts.
The re-
encoding stage then leverages these dual-granularity seman-
tic prompts to guide high-level feature optimization based
on the pre-encoder outputs. To balance performance and
efficiency, we adopt a U-Net architecture enhanced with at-
tention mechanisms, enabling multi-resolution feature ex-
traction and effective integration of semantic guidance.
Pre-encoder. Specifically, given an input infrared image
x ‚ààRH√óW , the pre-encoder consists of three convo-
lutional blocks that iteratively extract multi-scale features

f (i)	3
i=1 ‚ààRhi√ówi√ódi, where hi and wi denote the height
and width of the i-th feature map and di is the correspond-
ing feature dimension. With the exception of the first block,
each block performs a downsampling operation on the fea-
ture maps, producing a final output f (3) with a spatial reso-
lution reduced to 1
4 of the original image.
For each pre-coding layer, assuming the input feature is
f in, max pooling operation is first applied to halve its spa-
tial resolution, followed by two stakced 3 √ó 3 convolutional
layers to extract deep features. To further enhance the rep-
resentation ability of the encoder and focus on potential tar-
get regions, we integrate a channel attention layer at the end
of each block to output the refined feature f out. This layer
combines global average pooling (GAP) and global max
pooling (GMP) to compute channel-wise attention weights.
By selectively emphasizing informative channels and sup-
pressing redundant or noise signals, the encoder strengthens
semantic cues relevant to the detection task while reducing
the impact of complex background clutter. The overall pro-
cess can be formulated as,
f mid = Œ≥
 Conv3√ó3(Œ≥
 Conv3√ó3(MaxPool(f in)))

f out =f mid ¬∑ œÉ
 Conv1√ó1(GMP(f mid) + GAP(f mid))

(1)
where Convk√ók represents the convolution layer with k √ó
k kernel size, Œ≥ and œÉ denote the rectified linear unit and
sigmoid activation, respectively.
Re-encoder. Corresponding to the last two layers of the
original UNet encoder, it takes the output features of the
pre-encoder f (3) along with the dual-granularity semantic
feature f text as input, and outputs

f (i)	5
i=4 ‚ààRhi√ówi√ódi
by two re-encoder layers.
Like the layer-wise structure of the pre-encoder, each
layer of the re-encoder comprises max-pooling, two con-
volutional layers, and a specialized channel attention mech-
anism. This specialized channel attention is referred to as
Text-Guide Channel Attention(TGCA), which is designed
to leverage semantic information as guidance, enabling the
channel attention mechanism to further enhance or suppress
specific regions within the image. So the re-encoder process
can be illustrated as follows:
f mid = Œ≥
 Conv3√ó3(Œ≥
 Conv3√ó3(MaxPool(f in)))

f out = TGCA
 f mid, f text
(2)
where TGCA guides the learning of channel attention
weights for visual features through dual-granularity seman-
tic text features.
As shown in Figure 2(a), this module
first generates text channel attention weights from text fea-
tures via MLP, and adjusts their dimensionality through ex-
pansion to match the spatial dimensions of visual features.
Meanwhile, global average pooling is applied to the visual
features to obtain the channel attention weights inherent to
the image itself. Subsequently, the text-guide channel at-
tention weights and the image‚Äôs intrinsic channel attention
4

weights are concatenated fused by a linear layer to gener-
ate the final attention weights. After being normalized by
the Sigmoid function, these weights are multiplied with the
original visual features channel-wise.
The result is then
added to the original features via a residual connection,
which ultimately outputs the text-guided enhanced visual
feature representation. The specific formulas are as follows:
wch = œÉ
 Fn
 Concat
 GAP
 f mid
, MLP
 f text
f out = f mid + f mid ¬∑ wch
(3)
3.3. Dual-Granularity Semantic Prompt
Since small targets in infrared images lack clear structural
features, relying only on visual cues often leads to unclear
or incorrect detections, especially in complex backgrounds.
Previous studies[13] have shown that incorporating infor-
mation from the word-embedding space can improve the
expressiveness and understanding of image features. How-
ever, how to generate accurate and relevant text descrip-
tions in a cost-effective way is an inherent limitation. Un-
like prior approaches that depend on prompt engineering
or large pre-trained language models, we propose a dual-
granularity semantic prompt module, which leverages text
inversion techniques to provide fine-grained attribute infor-
mation that aligns with the visual content and is otherwise
hard to obtain. Details of coarse- and fine-grained prompts
are explained below.
Coarse-grained Textual Descriptions. The coarse-grained
text description is designed to capture easily accessible
prior knowledge about an input image, such as the modality
(e.g., infrared image) or scene type (e.g., sky, suburb). This
kind of information can be obtained directly from metadata
or generated using predefined templates, avoiding the cost
of additional annotations. Inspired by prior works[23], we
create prompt templates tailored for infrared small target
detection. For example,
‚ÄúA photo of an infrared image, with targets in the
sky/ground/ocean background.‚Äù
The prompts are encoded in natural language and processed
by a text encoder to generate semantic embeddings f text ‚àà
Rl√ódt, where l denotes the sequence length of text embed-
dings and dt is the hidden dimension. These embeddings,
combined with image features, are fed to the prompt-driven
modules for joint reasoning. Coarse-grained prompts serve
as global priors, providing initial semantic context and guid-
ing attention to task-relevant regions.
Fine-grained Semantic Tokens. Coarse-grained descrip-
tions are often too general to capture specific image details,
such as scene layout (e.g., the sea surface) or target cate-
gory (e.g., a ship). In practice, detailed information is more
helpful for accurately locating infrared targets. For exam-
ple, if the model is given the text prompt ‚Äúa ship on the sea
surface‚Äù, any prediction of a target in the sky can be con-
fidently treated as a false alarm and corrected. However,
such detailed information cannot be predefined using fixed
templates since they vary across different images.
Therefore, we introduce a set of semantic tokens
{s‚àó
i }|nt
i=1 ‚ààR1√ód that are dynamically generated for each
image using a text inversion strategy. nt denotes the number
of tokens and d denotes the number of dimension of the text
encoder. More concretely, these tokens serve as potential
language representations and are learned by the inversion
net after downsampling and fusing the features from the first
three layers of the image encoder. This allows the model
to directly extract detailed scene semantics and local target
features from the image itself. Afterwards, the refined to-
kens are projected into the text embedding space and placed
into the coarse-grained descriptions, effectively transform-
ing them into implicit, image-specific text prompts. Taking
n = 2 as an example,
‚ÄúA photo of an infrared image, with s‚àó
1 in s‚àó
2 back-
ground.‚Äù
Together, the coarse-grained and fine-grained prompts form
a complementary dual-granularity prompt. The former of-
fers stable, general prior knowledge to guide the learning
process, while the latter injects personalized, image-specific
semantics to dynamically adapt the detection process.
Here, we explain how this process is implemented. For
the first three layers of features (f (1), f (2), and f (3)) output
by the image encoder, we use depthwise convolutions with
a kernel size of 3 and strides of 16, 8, and 4, respectively,
to achieve downsampling. Afterward, we fuse these three
layers of features through concatenation and a 1 √ó 1 convo-
lution to obtain image features f img that contain both global
semantic information and local detailed information:
f img = Conv1√ó1

Concat

DWConv(f (1), f (2), f (3))

(4)
Containing rich spatial detailed features, f img is subse-
quently fed into the inversion net, which is designed to map
visual features from the pre-encoder into fine-grained se-
mantic tokens, serving as a bridge between low-to-middle-
level visual cues and text semantics. Given the fused visual
features from the pre-encoder, f img ‚ààRn√óc√óh√ów (where
n is the batch size, c is the number of channel dimen-
sion), the first step is to reshape f img into a sequence for-
mat f img
seq ‚ààR(h√ów)√ón√óc. After that, we introduce a set of
learnable tokens, denoted as T ‚ààRnt√ón√óc (where nt is the
number of fine-grained semantic tokens), and feed them into
the multi-head attention as the query. The key and value,
however, are the combined feature of T and f img
seq . This de-
sign enables T to capture spatially discriminative informa-
tion from visual features and interact semantically through
self-attention. After passing through the multi-head atten-
tion, this set of learnable tokens are mapped to the semantic
space via the MLP, ultimately resulting in tokens with fine-
grained semantic information that can be embedded into the
5

text encoder‚Äîthereby completing the image-to-text inver-
sion. The entire process is as follows:
imgtokens = MLP
 MHA
 T, (T, f img
seq ), (T, f img
seq )

(5)
3.4. Prompt-driven Semantic Guidance
To effectively integrate textual semantics into the visual
decoding process, we propose a Prompt-driven Seman-
tic Guidance decoder, which adaptively modulates feature
decoding based on language guidance.
The decoder be-
gins with a cross-attention mechanism that aligns the text
prompting f text with the high-level image features f (5) ex-
tracted from the final encoder stage. This initial interac-
tion produces semantically guided features that establish a
global alignment between the two modalities.
The decoder follows a five-stage cascade structure. At
each stage i, the input decoding feature f (i)
d
‚ààR
hi
2 √ó wi
2 √ód is
first upsampled via bilinear interpolation to match the spa-
tial resolution of the corresponding encoder feature f (i) ‚àà
Rhi√ówi√ódi. The features are then fused through element-
wise addition, with the encoder feature first projected via a
1 √ó 1 convolution for dimension alignment,
f (i)
fused = Up(f (i)
d ) + Conv1√ó1(f (i)).
(6)
Next, the fused feature f (i)
fused is refined using a Text-
Guided Spatial Attention (TGSA) block, which incorpo-
rates the [eot] token of the text embedding f [eot]
text
‚ààR1√ódt.
As shown in figure 2(b), both the visual and textual features
are projected into a shared latent space of dimension dc via
separate MLPs,
v(i) = MLPi
vis(f (i)
fused) ‚ààRhi√ówi√ódc
t(i) = MLPi
txt(f [eot]
text ) ‚ààR1√ódc
(7)
The text vector t(i) is then broadcast across the spatial di-
mensions of v(i), and we perform the dot-product operation
to measure the semantic similarity at each spatial location.
A softmax normalization yields the attention weight,
w(i) = Softmax
‚ü®v(i), t(i)‚ü©
‚àöc

‚ààRhi√ówi√ó1
(8)
Finally, the attention map w(i) acts as a residual gating
mechanism, selectively amplifying regions likely to contain
targets while suppressing background noise:
f (i‚àí1)
d
= Conv3√ó3(f (i)
fused + w(i) ¬∑ f (i)
fused)
(9)
This process is repeated at each decoding stage, progres-
sively refining the representation under the guidance of se-
mantic prompts. At the final stage, the decoder outputs f (1)
d ,
which is passed through a shared convolutional prediction
head to generate the binary target mask.
3.5. Optimization and Inference
Reconstruction Pretraining. Since the weights of the in-
version net are difficult to learn effectively during formal
training and the random initial weights of the encoder lead
to slow training convergence, we introduce the method of
reconstruction pretraining. Its core goal is to obtain mean-
ingful weights for the inversion net and, at the same time,
acquire effective initial weights for the encoder‚Äîlaying a
solid parameter foundation for formal training and acceler-
ating model convergence.
During the pretraining phase, we keep the structures of
the encoder and inversion net of the original model un-
changed, only modifying the decoder part: replacing the
original decoder with a reconstruction decoder composed of
transposed convolution layers. The core function of this re-
construction process is to obtain effective initial weights of
the network while learning to reconstruct the input infrared
image. Here express the pretraining loss function:
L =LContra

sg[f (5)], f [eot]
text

+ LMSE (out, input)
(10)
where sg[¬∑] is the stop-gradient operation. LMSE, the Mean
Squared Error loss, is employed to compute the loss be-
tween the reconstructed results and the input image, thereby
constraining the learning process of the entire network. Par-
ticularly, we introduce a contrastive loss for optimizing the
inversion network. We calculate LContra between the av-
erage image feature f (5) and the text embedding f [eot]
text ,
which maximizes their similarity and encourages the in-
version network to learn a semantically faithful vision-to-
language mapping.
Normal Training. DGSPNet generates a predicted target
mask ÀÜ
m from an input infrared image and is trained us-
ing the corresponding ground-truth mask m as supervision.
Specifically, for each input image, the training objective
minimizes the following loss function:
L =Œª1LBCE (m, ÀÜ
m) + Œª2LSoftIoU (m, ÀÜ
m)
(11)
where, LBCE denotes the binary cross-entropy loss to pe-
nalize pixel-level probability errors, LSoftIoU means SoftIoU
loss to optimize the segmentation error, and the coefficient
Œªi balances different loss components.
4. Experiments
4.1. Experimental Setup
Datasets. For our experiments, we conducted evaluations
on all three datasets, NUAA-SIRST, NUDT-SIRST, and
IRSTD-1K. Following the methodology of previous works,
specifically [35], the training and testing splits for both
NUAA-SIRST and NUDT-SIRST are set to 5:5, meaning
50% of the data was used for training and the remaining
6

Table 1. Comparison with existing IRSTD approaches on the IRSTD-lk, NUDT-SIRST, and NUAA-SIRST datasets. The evaluation
metrics are IoU (10‚àí2), Pd (10‚àí2), and Fa (10‚àí6).The first and second best results are highlighted in bold and underlined, respectively.
METHOD
BACKBONE
SIZE
IRSTD-1K
NUDT-SIRST
NUAA-SIRST
IoU ‚Üë
Pd ‚Üë
Fa ‚Üì
IoU ‚Üë
Pd ‚Üë
Fa ‚Üì
IoU ‚Üë
Pd ‚Üë
Fa ‚Üì
IPI [9]
Traditional
256
27.92
81.37
16.18
17.76
74.49
41.23
25.67
84.63
16.67
PSTNN [36]
Traditional
256
24.57
71.99
35.26
14.85
66.13
44.17
30.30
72.80
48.99
MSLSTIPT [26]
Traditional
256
11.43
79.03
1524
8.342
47.40
888.1
10.30
82.13
1131
RKformer [37]
Hybird
512
64.12
93.27
18.65
92.25
96.86
6.580
77.24
99.11
1.580
IAANet [28]
Hybird
256
66.25
93.15
14.20
90.22
97.26
8.320
74.22
93.53
22.70
MTU-Net [33]
Hybird
256
66.11
93.27
36.80
74.85
93.97
46.95
74.78
93.54
22.36
SCTransNet [35]
Hybird
256
68.03
93.27
10.74
94.09
98.62
4.290
77.50
96.95
13.92
ACM [3]
CNN
256
59.23
93.27
65.28
61.12
93.12
55.22
68.93
91.63
15.23
ALCNet [4]
CNN
256
60.60
92.98
58.80
64.74
94.18
34.61
70.83
94.30
36.15
ISNet [39]
CNN
256
61.85
90.24
31.56
81.24
97.78
6.340
70.49
95.06
67.98
FC3-Net [38]
CNN
256
65.07
91.54
15.55
78.56
93.86
23.92
72.44
98.14
10.85
DNA-Net [15]
CNN
256
65.90
90.91
12.24
88.19
98.83
9.000
75.80
95.82
8.780
UIU-Net [34]
CNN
256
66.15
93.98
22.07
93.48
98.31
7.790
76.91
95.82
14.13
ISTDU [12]
CNN
256
66.36
93.60
53.10
89.55
97.67
13.44
75.52
96.58
14.54
RDIAN [24]
CNN
256
56.45
88.55
26.63
76.28
95.77
34.56
68.72
93.54
43.29
AGPCNet [42]
CNN
256
66.29
92.83
13.12
88.87
97.20
10.02
75.69
96.48
14.99
Dim2Clear [40]
CNN
256
66.34
93.75
20.93
81.37
96.23
9.170
77.29
99.10
6.720
MMLNet [18]
CNN
256
67.21
94.28
14.00
81.81
98.43
11.77
78.71
98.88
25.71
L2SKNet [32]
CNN
256
67.81
90.24
17.46
93.58
97.57
5.330
73.43
98.17
20.82
Text-IRSTD [13]
CNN
256
69.57
92.59
14.97
95.84
99.73
1.032
-
-
-
DGSPNet (Ours)
CNN
256
70.87
93.26
14.23
96.13
99.15
0.32
80.32
97.33
9.26
Table 2. Ablation study on prompt-driven modules to evaluate the necessity and synergy of sub-components (TGCA, Cross-Attention,
TGSA). Experiments are conducted on IRSTD-1K and NUDT-SIRST datasets, with evaluation metrics including IoU(10‚àí2), Pd (10‚àí2),
and Fa (10‚àí6).
PROMPT-DRIVEN MODULE VARIANT
IRSTD-1K
NUDT-SIRST
Baseline
TGCA
Cross-Attention
TGSA
IoU ‚Üë
Pd ‚Üë
Fa ‚Üì
IoU ‚Üë
Pd ‚Üë
Fa ‚Üì
‚úì
65.31
91.91
17.72
94.38
98.15
8.66
‚úì
‚úì
66.99
92.25
20.89
94.43
98.83
2.69
‚úì
‚úì
‚úì
67.54
92.92
26.93
94.66
98.73
2.64
‚úì
‚úì
‚úì
67.87
92.59
27.97
95.27
99.04
2.57
‚úì
‚úì
‚úì
67.81
91.91
36.00
94.81
98.94
1.49
‚úì
‚úì
‚úì
‚úì
70.87
93.26
14.23
96.13
99.15
0.32
50% for testing. For IRSTD-1K, we used an 8:2 split, where
80% of the data was used for training and 20% for testing.
This setup aligns with the standard practice in this field, en-
abling a fair comparison with prior research.
Implementation Details. Our experiments are conducted
on the NVIDIA GeForce RTX 4090 GPU. All images are
normalized and random cropped to a size of 256√ó256. Dur-
ing pre-training phase, the AdamW optimizer is adopted
with a learning rate of 3e-4 on the inversion network and
1e-4 on other parameters of the model over 800 pretrain-
ing epochs. After pretraining, the encoder and inversion
net weights are directly transferred to the formal training
model, while the reconstruction decoder weights are dis-
carded. As to formal Training Phase, the Adam optimizer is
used and a Polynomial Decay learning rate scheduler is ap-
plied, with an initial learning rate of 1e-4 and a decay power
of 1.2, ensuring the learning rate gradually decreases over
800 training epochs with a batch size of 4. Additionally, we
choose frozen weights CLIP-ViT-B/32 as the text encoder.
We evaluate the proposed model using three complementary
metrics: Intersection over Union (IoU), detection probabil-
ity (Pd), and false-alarm rate (Fa). More explanations of
these metrics can be found in the supplementary.
7

Table 3. Ablation study on text designs to evaluate the influence of the number of fine-grained semantic tokens (#Tokens) on model
performance, implemented on the IRSTD-1K dataset. Various text variants (incorporating different quantities of fine-grained tokens such
as s‚àó
1, s‚àó
2) are assessed, with evaluation metrics including IoU(10‚àí2), Pd (10‚àí2), and Fa (10‚àí6).
#Tokens
TEXT VARIANT
IoU ‚Üë
Pd ‚Üë
Fa ‚Üì
0
A photo of an infrared image, with targets in the background.
66.41
91.92
22.43
1
A photo of an infrared image, with s‚àó
1 in the background.
69.29
92.25
17.78
2
A photo of an infrared image, with s‚àó
1 in s‚àó
2 background.
70.87
93.26
14.23
3
A photo of an infrared image, with s‚àó
1 s‚àó
2 in s‚àó
3 background.
67.37
93.26
16.45
4
A photo of an infrared image, with s‚àó
1 s‚àó
2 in s‚àó
3 s‚àó
4 background.
69.22
91.58
19.91
4.2. Comparisons with the State-of-the-Arts
Our method demonstrates a clear advantage over state-of-
the-art competitors. Table 1 presents a detailed compari-
son of the quantitative results between our proposed method
and other state-of-the-art approaches in IRSTD across three
datasets. Our approach achieves the best or second-best per-
formance across most metrics, highlighting the effective-
ness of our design.
Specifically, on IRSTD-1K, DGSP-
Net obtains an IoU of 70.87 (surpassing the second-best
method, Text-IRSTD [13], with 69.57), a high Pd of 93.26,
and an Fa of 14.23, excelling in both detection accuracy
and false alarm control. On NUDT-SIRST, it achieves a
remarkable IoU of 96.13, a Pd of 99.15, and an ultra-low
Fa of 0.32, significantly outperforming other methods and
demonstrating precise target localization and robust back-
ground suppression. On NUAA-SIRST, DGSPNet also de-
livers competitive results with an IoU of 80.32, leading in
detection integrity and accuracy.
Notably, although DGSPNet adopts a CNN backbone, it
outperforms methods that utilize hybrid architectures. For
example, on NUDT-SIRST, SCTransNet [35] achieves an
IoU of 94.09 and an Fa of 4.290, whereas DGSPNet im-
proves the IoU to 96.13 and reduces the Fa to 0.32, re-
vealing a substantial performance gap. This demonstrates
that our reconstruction pre-training strategy, hierarchical
encoder design, and dual-granularity semantic prompt mod-
ule effectively exploit the strengths of CNNs in feature ex-
traction and semantic fusion, while further extending the
capability of traditional visual models through language-
guided learning.
4.3. Ablation study
Component Effectiveness in the Prompt-Driven Mod-
ule. To clarify the necessity of each sub-component in the
prompt-driven semantic guidance module, we conduct ab-
lation studies on Text-Guide Channel Attention (TGCA),
Cross-Attention, and Text-Guide Spatial Attention (TGSA).
By incrementally adding these components, we compare
IoU, detection rate (Pd), and false alarm rate (Fa) on the
IRSTD-1K and NUDT-SIRST datasets, with the results
shown in Table 2. On IRSTD-1K, both Cross-Attention and
TGSA improve IoU and Pd, indicating enhanced localiza-
tion accuracy and completeness, but they also introduce a
higher Fa. In contrast, TGCA can strengthen text‚Äìvisual se-
mantic alignment at the channel level, effectively reducing
Fa and providing a basis for false-alarm suppression. Only
when all three components are integrated does the model
achieve the best trade-off among the metrics, yielding op-
timal overall performance on both IRSTD-1K and NUDT-
SIRST. These results verify the functional complementarity
of the components and the soundness of our designs.
Effectiveness of Dual-Granularity Semantic Prompt.
Table 3 presents the performance of different text designs
for the dual-granularity semantic prompts on the IRSTD-
1K dataset. The results show that using two semantic tokens
and constructing the prompt as ‚ÄúA photo of an infrared im-
age, with s‚àó
1 in s‚àó
2 background‚Äù yields the best performance.
Using only a single semantic token leads to suboptimal re-
sults. This suggests that while one token is generally suffi-
cient to describe either target characteristics or background
attributes, introducing too many tokens can cause semantic
ambiguity and degrade the guidance quality.
5. Conclusion
We introduce DGSPNet, an advanced end-to-end frame-
work designed to combine coarse-grained textual priors
with fine-grained, instance-adaptive prompts to provide ex-
plicit linguistic guidance for infrared small-target detection.
Unlike traditional models, DGSPNet enhances detection by
aligning hierarchical visual features with dynamically dis-
tilled semantic cues, enabling the decoder to focus attention
on faint targets while effectively minimizing background
noise. This approach leads to state-of-the-art performance
across three challenging infrared datasets, demonstrating
the model‚Äôs robust ability to handle diverse and complex
scenarios. Our experiments highlight the synergy between
fixed semantic anchors, which provide context, and learn-
able image tokens, which adapt to specific target features.
This combination not only improves detection accuracy
and robustness but also stabilizes training through a sim-
ple gradient-isolation trick. Additionally, DGSPNet shows
the broader potential of cross-modal prompting in low-SNR
8

detection tasks, positioning language as a lightweight, plug-
and-play supervisory signal that can significantly enhance
computer vision models, especially in resource-constrained
or noisy environments.
References
[1] CL Philip Chen, Hong Li, Yantao Wei, Tian Xia, and
Yuan Yan Tang. A local contrast method for small infrared
target detection. IEEE transactions on geoscience and re-
mote sensing, 52(1):574‚Äì581, 2013. 1
[2] Zhiwei Chen, Bin Wang, and Hongjun Zhou. Local con-
trast method based on directional information for infrared
small target detection. Optics Communications, 313:123‚Äì
130, 2014. 2
[3] Yimian Dai, Yiquan Wu, Fei Zhou, and Kobus Barnard.
Asymmetric contextual modulation for infrared small tar-
get detection. In Proceedings of the IEEE/CVF winter con-
ference on applications of computer vision, pages 950‚Äì959,
2021. 7
[4] Yimian Dai, Yiquan Wu, Fei Zhou, and Kobus Barnard. At-
tentional local contrast networks for infrared small target de-
tection. IEEE transactions on geoscience and remote sens-
ing, 59(11):9813‚Äì9824, 2021. 7
[5] Sameer D Deshpande,
Meng Joo Er,
Yu Chan,
and
Soon Huat Lee. Max-mean and max-median filters for de-
tection of small targets. IEEE Transactions on Aerospace
and Electronic Systems, 35(1):64‚Äì73, 1999. 2
[6] Yuan Ding, Zhi Ye, and Yuanjing Tang. Irnet: A two-stage
network for infrared small target detection. IEEE Access, 8:
52513‚Äì52522, 2020. 2
[7] Richard O Duda, Peter E Hart, and David G Stork. Pattern
Classification. John Wiley & Sons, 2012. 2
[8] Linyu Fan, Yingying Wang, Guoliang Hu, Feifei Li, Yuhang
Dong, Hui Zheng, Changqing Lin, Yue Huang, and Xinghao
Ding. Diffusion-based continuous feature representation for
infrared small-dim target detection. IEEE Transactions on
Geoscience and Remote Sensing, 62:1‚Äì17, 2024. 1
[9] Chenqiang Gao, Deyu Meng, Yi Yang, Yongtao Wang, Xi-
aofang Zhou, and Alexander G Hauptmann. Infrared patch-
image model for small target detection in a single image.
IEEE transactions on image processing, 22(12):4996‚Äì5009,
2013. 7
[10] Jiuxiang Gu and et al. Open-vocabulary object detection via
vision and language knowledge distillation. ICLR, 2022. 2
[11] Shuai Han, Bin Wang, and Yu Zhang.
Dgformer: Dual-
granularity transformer for infrared small target detection.
IEEE Transactions on Geoscience and Remote Sensing,
2023. 2
[12] Qingyu Hou, Liuwei Zhang, Fanjiao Tan, Yuyang Xi, Hao-
liang Zheng, and Na Li. Istdu-net: Infrared small-target de-
tection u-net. IEEE Geoscience and Remote Sensing Letters,
19:1‚Äì5, 2022. 7
[13] Feng Huang, Shuyuan Zheng, Zhaobing Qiu, Huanxian Liu,
Huanxin Bai, and Liqiong Chen. Text-irstd: Leveraging se-
mantic text to promote infrared small target detection in com-
plex scenes. arXiv preprint arXiv:2503.07249, 2025. 1, 3, 5,
7, 8
[14] Chao Jia, Yinfei Yang, Ye Xia, et al. Scaling up visual and
vision-language representation learning with noisy text su-
pervision. ICML, 2021. 2
[15] Boyang Li, Chao Xiao, Longguang Wang, Yingqian Wang,
Zaiping Lin, Miao Li, Wei An, and Yulan Guo. Dense nested
attention network for infrared small target detection. IEEE
Transactions on Image Processing, 32:1745‚Äì1758, 2022. 7
[16] Junnan Li and et al. Grounded language-image pretraining.
CVPR, 2022. 2
[17] Junnan Li and et al. Blip-2: Bootstrapping language-image
pre-training with frozen image encoders and large language
models. ICLR, 2023. 2
[18] Qiang Li, Wei Zhang, Wanxuan Lu, and Qi Wang. Multi-
branch mutual-guiding learning for infrared small target de-
tection. IEEE Transactions on Geoscience and Remote Sens-
ing, 2025. 7
[19] Fanzhao Lin, Kexin Bao, Yong Li, Dan Zeng, and Shim-
ing Ge. Learning contrast-enhanced shape-biased represen-
tations for infrared small target detection. IEEE Transactions
on Image Processing, 33:3047‚Äì3058, 2024. 1
[20] Jian Liu, Liang Liu, and Junlin Yang.
Acunet: Adaptive
context u-net for infrared small target detection. Sensors, 21
(13):4563, 2021. 2
[21] Yuxin Liu and et al.
Llmdet: Instruction-tuned vision-
language models for zero-shot object detection.
arXiv
preprint arXiv:2309.00676, 2023. 2
[22] Ziyi Liu and et al.
Detgpt: Detect what you want via
reasoning with large language models.
arXiv preprint
arXiv:2305.15062, 2023. 2
[23] Alec Radford, Jong Wook Kim, Jack Hallacy, et al. Learn-
ing transferable visual models from natural language super-
vision. ICML, 2021. 2, 5
[24] Heng Sun, Junxiang Bai, Fan Yang, and Xiangzhi Bai.
Receptive-field and direction induced attention network for
infrared dim small target detection with a large-scale dataset
irdst. IEEE Transactions on Geoscience and Remote Sens-
ing, 61:1‚Äì13, 2023. 7
[25] Haotian Sun, Qiuyu Jin, Jun Xu, and Linbo Tang.
In-
frared small-target detection based on multi-level local con-
trast measure. Procedia Computer Science, 221:549‚Äì556,
2023. 1
[26] Yang Sun, Jungang Yang, and Wei An. Infrared dim and
small target detection via multiple subspace learning and
spatial-temporal patch-tensor model. IEEE Transactions on
Geoscience and Remote Sensing, 59(5):3737‚Äì3752, 2021. 7
[27] Fang Wang, Lei Zhang, and Xiaodan Liang. Infrared small
target detection via simultaneous orthogonal matching pur-
suit and sparsity measure. Infrared Physics & Technology,
69:81‚Äì89, 2015. 2
[28] Kewei Wang, Shuaiyuan Du, Chengxin Liu, and Zhiguo Cao.
Interior attention-aware network for infrared small target de-
tection. IEEE Transactions on Geoscience and Remote Sens-
ing, 60:1‚Äì13, 2022. 7
[29] Peng Wang and et al. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. ICML, 2022. 2
9

[30] Yongtao Wang, Yifan Liu, and Liangpei Zhang. Isformer:
Infrared small target detection via transformer-based feature
enhancement. Remote Sensing, 14(18):4722, 2022. 2
[31] Zhe Wang and et al. Git: A generative image-to-text trans-
former for vision and language. CVPR, 2022. 2
[32] Fengyi Wu, Anran Liu, Tianfang Zhang, Luping Zhang, Jun-
hai Luo, and Zhenming Peng. Saliency at the helm: Steering
infrared small target detection with learnable kernels. IEEE
Transactions on Geoscience and Remote Sensing, 2024. 7
[33] Tianhao Wu, Boyang Li, Yihang Luo, Yingqian Wang, Chao
Xiao, Ting Liu, Jungang Yang, Wei An, and Yulan Guo. Mtu-
net: Multilevel transunet for space-based infrared tiny ship
detection.
IEEE Transactions on Geoscience and Remote
Sensing, 61:1‚Äì15, 2023. 7
[34] Xin Wu, Danfeng Hong, and Jocelyn Chanussot. Uiu-net: U-
net in u-net for infrared small object detection. IEEE Trans-
actions on Image Processing, 32:364‚Äì376, 2022. 7
[35] Shuai Yuan, Hanlin Qin, Xiang Yan, Naveed Akhtar, and Aj-
mal Mian. Sctransnet: Spatial-channel cross transformer net-
work for infrared small target detection. IEEE Transactions
on Geoscience and Remote Sensing, 62:1‚Äì15, 2024. 1, 6, 7,
8
[36] Landan Zhang and Zhenming Peng. Infrared small target
detection based on partial sum of the tensor nuclear norm.
Remote Sensing, 11(4):382, 2019. 7
[37] Mingjin Zhang, Haichen Bai, Jing Zhang, Rui Zhang,
Chaoyue Wang, Jie Guo, and Xinbo Gao. Rkformer: Runge-
kutta transformer with random-connection attention for in-
frared small target detection. In Proceedings of the 30th ACM
International Conference on Multimedia, pages 1730‚Äì1738,
2022. 7
[38] Mingjin Zhang, Ke Yue, Jing Zhang, Yunsong Li, and Xinbo
Gao. Exploring feature compensation and cross-level cor-
relation for infrared small target detection. In Proceedings
of the 30th ACM International Conference on Multimedia,
pages 1857‚Äì1865, 2022. 7
[39] Mingjin Zhang, Rui Zhang, Yuxiang Yang, Haichen Bai,
Jing Zhang, and Jie Guo. Isnet: Shape matters for infrared
small target detection. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
877‚Äì886, 2022. 1, 7
[40] Mingjin Zhang, Rui Zhang, Jing Zhang, Jie Guo, Yunsong
Li, and Xinbo Gao. Dim2clear network for infrared small
target detection. IEEE Transactions on Geoscience and Re-
mote Sensing, 61:1‚Äì14, 2023. 7
[41] Shuang Zhang, Lei Zhang, and Xuanjing Mou.
Infrared
small target detection via non-local low-rank regularization
and salient feature extraction. Infrared Physics & Technol-
ogy, 77:364‚Äì377, 2016. 2
[42] Tianfang Zhang, Siying Cao, Tian Pu, and Zhenming
Peng.
Agpcnet:
Attention-guided pyramid context net-
works for infrared small target detection.
arXiv preprint
arXiv:2111.03580, 2021. 7
[43] Yu Zhang, Xian Zhang, and Hao Sun. Infrared small target
detection based on multiscale residual fusion network. In
ICIP, pages 2508‚Äì2512, 2021. 2
[44] Liunian Harold Zhou and et al. Unified vision-language pre-
training for image captioning and vqa. In AAAI, 2021. 2
10
