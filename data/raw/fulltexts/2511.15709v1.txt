TOKENISATION OVER BOUNDED ALPHABETS IS HARD
Violeta Kastreva∗, ,†,1,2 Philip Whittington,∗,1 Dennis Komm,1 Tiago Pimentel1
1ETH Zürich, 2Sofia University “St. Kliment Ohridski”
vkastreva@uni-sofia.bg, {philip.whittington, dennis.komm, tiago.pimentel}@inf.ethz.ch
ABSTRACT
Recent works have shown that tokenisation is NP-complete. However, these works
assume tokenisation is applied to inputs with unboundedly large alphabets—an un-
realistic assumption, given that in practice tokenisers operate over fixed-size alpha-
bets, such as bytes or Unicode characters. We close this gap by analysing tokenisa-
tion over bounded n-ary alphabets, considering two natural variants: bottom-up to-
kenisation and direct tokenisation, where we must, respectively, select a sequence
of merge operations or a vocabulary whose application optimally compresses a
dataset. First, we note that proving hardness results for an n-ary alphabet proves the
same results for alphabets of any larger size. We then prove that even with binary
alphabets, both variants are not only NP-complete, but admit no polynomial-time
approximation scheme (unless P = NP). We further show that direct tokenisation
remains NP-complete even when applied to unary alphabets. While unary alpha-
bets may not be practically useful, this result establishes that the computational
intractability of tokenisation is not an artifact of large alphabets or complex con-
structions, but a fundamental barrier. Overall, our results explain why practical
algorithms such as BPE and UnigramLM are heuristic, and points toward approxi-
mation algorithms being an important path going forward for tokenisation research.
1
INTRODUCTION
Tokenisation is the first step in most natural language processing pipelines. Given a string of
characters c, a tokeniser maps it to a sequence of subwords s. Language models then operate on
these subword sequences rather than the raw characters. Despite its central role, however, we still
lack a comprehensive understanding of tokenisation; e.g., which properties of the produced strings
of subwords s actually help downstream modelling? A common property to aim for is compression
(Sennrich et al., 2016; Uzan et al., 2024; Zouhar et al., 2023b), as using shorter subword-strings to
encode a dataset allows for more efficient training and inference—more data can be passed through
the model with the same number of flops. While not a silver bullet (Schmidt et al., 2024; Ali et al.,
2024), compression has been shown to correlate with downstream model performance (Gallé, 2019;
Rust et al., 2021; Zouhar et al., 2023a; Goldman et al., 2024) and will be our work’s focus.
A practical concern follows immediately: once an objective (e.g., compression) is fixed, can an optimal
tokeniser be found efficiently? Popular algorithms such as BPE and UnigramLM are greedy or heuris-
tic and need not return an optimal tokeniser for the metrics they are designed to optimise. Further,
recent work has sharpened this picture, proving the NP-completeness of finding an optimal tokeniser
under a compression-style objective (Kozma and Voderholzer, 2024; Whittington et al., 2025; Lim
et al., 2025). These papers, however, show this for the tokenisation of strings over unboundedly large
alphabets. Conversely, in practice the strings we care about are typically composed of Unicode charac-
ters or bytes, thus using bounded alphabets. Whether it is possible to efficiently find optimal tokenisers
over Unicode-strings (which have an alphabet size of roughly 170,000), byte-strings (with an alphabet
size of 256), or bit-strings (with an alphabet size of 2) are open questions of practical relevance.
In this paper, we first define the n-ary tokenisation problem: the problem of finding an optimal
tokeniser on strings constrained to alphabets of size n. We examine this problem under two variants:
direct and bottom-up tokenisation, where—given a dataset over an n-ary alphabet and a vocabulary
size K—we must find the vocabulary (in direct tokenisation) or sequence of merges (in bottom-up
∗Equal contribution.
† Work was done during a research internship at ETH Zürich.
1
arXiv:2511.15709v1  [cs.CL]  19 Nov 2025

tokenisation) which, when applied to the dataset, maximally compresses it. We prove that (i)
assuming P ̸= NP, both direct and binary bottom-up tokenisation are not in the polynomial-time
approximation scheme (PTAS) class, meaning that they cannot be approximated arbitrarily well in
polynomial time,1 and that (ii) for the direct case, even unary tokenisation is NP-complete. Notably,
unary and binary are the easiest of the n-ary tokenisation problems, and thus these hardness results
also trivially extend to tokenisation problems with larger alphabets.
Our results thus indicate that the computational hardness of tokenisation is not an artifact of large
alphabets or elaborate merge operations: it already appears under direct tokenisation over unary alpha-
bets. This helps explain why practical algorithms (e.g., BPE) rely on approximations, and suggests that
future work should focus on provably good approximate methods or on relaxations for this problem.
2
TOKENISATION
Our notation’s colour-coding (following Whittington et al., 2025)
• Blue for raw data (i.e., characters c ∈Σ∗);
• Magenta for tokeniser-specific data (i.e., subwords s∈S∗and merges m∈M∗);
• Orange for functions (e.g., tok).
Let c ∈Σ∗be2 a character-string, composed of characters c from an alphabet Σ; for notational
convenience, we may write one such string as c = c1c2 . . . c|c|. Character-strings compose the raw
text data found, say, on the web, which make up the datasets on which language models are trained.
We denote one such dataset by D = {cm}M
m=1. Before feeding data to our models, however, we
typically convert them to strings of subwords, which is the job of a tokeniser.
Formally, a tokeniser can be defined as a tuple ⟨S, detok, tok⟩, composed of a vocabulary, a decoding
and an encoding function. A vocabulary S is a finite set of subwords, each of which is a non-empty
span of characters; we thus write S ⊂Σ+. A subword-string is then a sequence s ∈S∗and
represents a character-string via the concatenation of its subwords’ characters. We say that a pair of
character- and subword-strings are equivalent if
c
◦= s ⇐⇒c = concat(s),
concat(s) = s1 ◦s2 ◦· · · ◦s|s|
(1)
where s = ⟨s1, s2, · · · , s|s|⟩, each st ∈S is a subword, and operator ◦denotes string concatenation.
Notably, Σ⊆S is typically enforced to guarantee that every c∈Σ∗can be represented by at least one
subword-string s ∈S∗, and we say that a vocabulary’s size is |S| = |Σ| + K. Second in the tuple
above, a decoding function is defined as detok: S∗→Σ∗, and given a subword-string it outputs
the character-string it represents. This function thus is simply defined as detok(s)
def
= concat(s).
Finally, an encoding function tok: Σ∗→S∗maps character- to subword-strings while ensuring the
equivalence c
◦= s for s = tok(c). Several encoding functions may respect this constraint, as many
subword-strings may be equivalent to a specific character-string. For instance, given S ={a, aa, aaa},
the string c = aaa could be tokenised as s = ⟨aaa⟩or as s = ⟨a, aa⟩. We focus on two encoding
functions in this paper, which we follow Whittington et al. (2025) in labelling as direct and bottom-
up. The direct encoding function (tok	) only requires a vocabulary, which it applies optimally
to encode a character-string. In turn, the bottom-up encoding function (tok↑) takes a merge
sequence m = ⟨m1, . . . , mK⟩as input, which it applies in order to a character-string; each of these
merges mk is composed of a pair of subwords, which we represent as s[1]
k ⊚s[2]
k , and we write
mergem : S∗→S∗to represent a function which, given a subword-string, processes it left-to-right
and replaces any consecutive occurrence of the pair s[1]
k , s[2]
k with a new token s[new]
k
= s[1]
k ◦s[2]
k .
Defining M
def
= S ×S, we say m ∈M and m ∈M∗. We now formalise these encoding functions as
tok	[S](c)
def
= arg min
s∈S∗
|s|,
tok↑[m](c)
def
=
 K
m∈m
mergem

(c)
(2)
s.t. c
◦=s
1More specifically, we present a constant lower bound on the approximation ratio achievable by any
polynomial-time algorithm for this problem (again, assuming P ̸= NP).
2We note that Σ∗denotes the Kleene star of Σ (i.e., ∪∞
i=0Σi), and Σ+ denotes its Kleene plus (i.e., ∪∞
i=1Σi).
2

where J represents function composition. A tokeniser is thus fully determined by a vocabulary
or merge-sequence; for the direct case we have tok
def
=tok	[S], while for bottom-up tok
def
=tok↑[m].
Importantly, the direct encoding function (tok	[S]) can be efficiently computed in O(|c|2) time
using the methods from Schmidt et al. (2024).
2.1
OBJECTIVE FUNCTIONS AND THEIR OPTIMISATION
As described above, a direct tokeniser is fully determined by a vocabulary, while a bottom-up
tokeniser is identified by a merge-sequence. How to select a specific tokeniser, though? This
is typically done via defining an objective function G which, given an encoding function (tok)
and a dataset (D), returns a value representing the cost of that particular choice. Choosing a
tokeniser then “simply” requires optimising this objective: e.g., for direct tokenisation we must find
Sopt = arg minS⊂Σ+ G(tok	[S], D) under the constraint that |S| = |Σ| + K.
Several objective functions exist. UnigramLM (Kudo, 2018), for instance, selects a vocabulary
which optimises a dataset’s unigram negative log-probability. Other work has proposed alternative
measures, such as the frequency of the 5-th % least frequent token (Gowda and May, 2020), or the
tokeniser’s Rényi efficiency (Zouhar et al., 2023a). As mentioned above, we focus on compression
in this paper. We do so following a battery of previous work which formally analyses tokenisers
(Zouhar et al., 2023b; Kozma and Voderholzer, 2024; Whittington et al., 2025; Lim et al., 2025).
Prior work has shown that a tokeniser’s compression correlates with the downstream performance
of language models trained on its output subword-strings (Gallé, 2019; Zouhar et al., 2023a). We
note, however, that other recent work has criticised compression as the sole objective for tokenisation,
showing that these two properties (compression and downstream performance) may have a more
complex relationship than originally suspected (Ali et al., 2024; Schmidt et al., 2024).
There are two natural ways to define a compression objective: compressed length, which measures
the number of remaining symbols after a string is tokenised, and compression reduction, which
measures how many symbols are reduced in the string by a tokeniser. These are formalised as:
Gℓ(tok, D)
def
=
X
c∈D |tok(c)|
|
{z
}
compressed length, size of remaining string
,
Gr(tok, D)
def
=
X
c∈D

|tok(c)| −|c|

|
{z
}
compression reduction, number of reduced symbols
(3)
While equivalent in how they rank tokenisers, this choice can make a big difference when evaluating
the quality of an approximation. When using minimisation objectives, such as Gℓ, the approximation
ratio of an algorithm upper-bounds the ratio between the objective value achieved by the algorithm’s
solution and an optimal solution, being thus at least 1 by definition. A similar definition applies when
using maximisation objectives, such as Gr, but the approximation ratio is inversed. We say we have a
δ-approximation algorithm if, for every possible input, this ratio is bounded from above by δ. If
a dataset has 1,000 characters and would have 100 symbols if optimally compressed, a suboptimal
tokeniser which instead reduces it to at most 200 symbols would have an approximation ratio of 2
under Gℓbut of 1.125 under Gr. Notably, prior work has analysed both these measures. We argue
here that compressed length is the more natural objective, as it directly relates to the throughput
achieved by a language model processing that text, being thus connected to the model’s training and
inference costs. A 2-approximation for Gℓimplies that a language model using that tokeniser may be
2-times slower (and more costly) than optimal when processing the same text.3
After deciding on an objective function, such as Gℓabove, we must select a vocabulary (S ⊂Σ+)
or merge-sequence (m ∈M∗) which optimises it. Unfortunately, both these optimisation problems
have infinite search spaces (respectively, P(Σ+) and M∗, where P denotes the powerset operation),
which begs the question: is there an efficient way to find these optima? Recent work has shown that,
in general, this is not possible, proving compression-based tokenisation to be NP-complete; more
specifically, Kozma and Voderholzer (2024) showed this for bottom-up tokenisation, Whittington
et al. (2025) for direct and bottom-up tokenisation, and Lim et al. (2025) for direct tokenisation with
candidate tokens. This means that, unless P = NP, there exists no polynomial-time algorithm to find
compression-optimal tokenisers. Beyond that, using the Gr objective function, Kozma and Voder-
holzer (2024) showed that bottom-up tokenisation is not only NP-complete but also APX-hard, which
implies that it is not in the polynomial-time approximation scheme (PTAS) complexity class (unless
3Assuming that language models cannot achieve sub-linear computational complexity on their input’s length.
3

P = NP). The PTAS class is characterised by problems for which: for every constant ε > 0, there
exists a polynomial-time algorithm (whose run-time may depend on ε), which solves it with an approx-
imation ratio upper-bounded by 1+ε. Not being in PTAS thus implies that there is no polynomial-time
algorithm which can approximate the optimal solution with an approximation ratio arbitrarily close to
1. Notably, all of these complexity proofs apply to tokenisation problems over alphabets of arbitrarily
large sizes. Whether these results hold once alphabet sizes are bounded by a constant is thus left open.
3
TOKENISATION OVER BOUNDED ALPHABETS
We now move to the analysis of tokenisation over bounded alphabets. Let an n-ary alphabet be an al-
phabet with size |Σ| = n. We define the tokenisation problem over such bounded alphabets as follows.
Definition 1. Let K be a vocabulary size and D be a dataset composed of character-strings from
an alphabet of size |Σ| = n. For a given δ, the n-ary tokenisation decision problem requires
deciding whether there exists a vocabulary Sopt ⊆Σ+ (for direct tokenisation) or a merge-sequence
mopt ∈M∗(for bottom-up tokenisation) which compresses D to at most δ symbols. The n-ary
tokenisation optimisation problem is to find what the maximal such compression of D is. Formally:
δ ≥min
tok∈T
X
c∈D |tok(c)| , s.t. |tok| = K
|
{z
}
n-ary tokenisation decision problem
, δopt = min
tok∈T
X
c∈D |tok(c)| , s.t. |tok| = K
|
{z
}
n-ary tokenisation optimisation problem
(4)
where T
def
= {tok	[S]| S ⊂Σ+} for direct tokenisation and T
def
= {tok↑[m]| m∈M∗} for bottom-up.
We will more specifically call these the n-ary direct tokenisation problem and the n-ary bottom-up
tokenisation problem when dealing with, respectively, direct and bottom-up tokenisers, writing
Tokn
	(D, K, δ) and Tokn
↑(D, K, δ) for the functions which return the solution to their decision prob-
lems. Notably, the n-ary tokenisation problems form a clear hierarchy from easiest (n = 1) to hardest
(n →∞), with unary tokenisation being the easiest such problem. In the next sections, we first prove
that both binary direct and binary bottom-up tokenisation are hard to approximate, i.e., that both these
problems are not in PTAS (in §4). We then prove that unary direct tokenisation is NP-complete (in §5).
Fact 1. If n-ary tokenisation is NP-hard, all n′-ary tokenisation problems for n′ > n are NP-hard.
Proof. Let n, n′ ∈N with n′ ≥n. Any instance of the n-ary tokenisation problem is a valid instance
of the n′-ary problem with the same solutions, allowing for a trivial reduction between them. Thus,
any proof of hardness for the n-ary tokenisation problem immediately applies to n′-ary problems.
A Note on Optimisation vs. Decision Problems.
Typically, NP-hardness is discussed mainly
as a property of decision problems, while hardness of approximation (and consequently, being
contained or not in PTAS) is a notion regarding optimisation problems. There is, however, a notion
of equivalence between these classes of problems: if a polynomial-time algorithm exists to solve
a decision problem (i.e., if this problem is not NP-hard), it can usually be leveraged to also find
an efficient algorithm for its associated optimisation problem, and vice-versa. Similarly, if no
polynomial-time algorithm can solve an optimisation problem with an approximation ratio arbitrarily
close to 1 (i.e., if the problem is not in PTAS), this implies that there must be some constant ε
such that it is NP-hard to distinguish between instances that admit a solution with cost x and those
that admit a solution with cost (1 + ε)x. We will use this latter property here to show hardness of
approximation, relying on gap-preserving reductions.4 To this end, it will be useful to also define gap
versions of the problems we discuss. Formally, we will denote such gap versions similarly to their
decision versions (e.g., Tokn
	(D, K, δ) above), but while providing two decision boundaries instead
(e.g., Tokn
	(D, K, (δ−, δ+))). In minimisation gap problems, the task is then to decide whether their
optimal value is at most δ+ or at least δ−(with the opposite being true for maximisation problems); if
a value falls between these, any answer is acceptable. For n-ary tokenisation problems, for instance,
4We note that hardness of approximation is not formally the same as proving APX-hardness (as was done in
Kozma and Voderholzer, 2024). However, it allows for the same conclusion: the binary (and larger) tokenisation
problems cannot be approximated arbitrarily well in polynomial time, unless P = NP. Additionally, our
gap-preserving reductions allow us to find explicit constants to which the problems cannot be approximated.
4

we would require an algorithm which computes:
Tokn(D, K, (δ−, δ+)) =



T
if δ+ ≥mintok∈T
P
c∈D |tok(c)| , s.t. |tok| = K
F
elif δ−≤mintok∈T
P
c∈D |tok(c)| , s.t. |tok| = K
?
else
(5)
4
BINARY TOKENISATION IS HARD TO DECIDE AND APPROXIMATE
In this section, we will prove NP-hardness of the two binary tokenisation decision problems above,
and of their corresponding gap problems (for specific gaps). To this end, we will use a reduction
from the 3-occurrence maximum 2-satisfiability problem (3-OCC-MAX2SAT), which we define in
§4.1. We then move on to proving results showing hardness of approximation for the binary direct
and binary bottom-up tokenisation problems (in §4.2 and §4.3, respectively).
4.1
3-OCCURRENCE MAXIMUM 2-SATISFIABILITY
Let X be a Boolean variable assigned a value x ∈{F, T}, and let X = {Xj}J
j=1 be a set of such vari-
ables, with joint assignment x = {xj}J
j=1. Further, let C = {(L1
i ∨L2
i )}I
i=1 be a set of clauses,5 where
each literal Li is either a variable Xj or its negation ¬Xj. We define 3-OCC-MAX2SAT as follows.
Definition 2. Let X = {Xj}J
j=1 be a set of Boolean variables and C = {(L1
i ∨L2
i )}I
i=1 be a set
of clauses. Further, let each variable Xj occur in exactly three clauses. Given a target γ ∈N, the
3-OCC-MAX2SAT decision problem requires deciding whether there exists an assignment x ∈{F, T}J
such that at least γ clauses are satisfied. The 3-OCC-MAX2SAT optimisation problem requires finding
the maximum number of satisfiable clauses. Formally:
γ ≤
max
x ∈{F,T}J
XI
i=11x {L1
i ∨L2
i }
|
{z
}
3-OCC-MAX2SAT decision problem
γopt =
max
x ∈{F,T}J
XI
i=11x {L1
i ∨L2
i }
|
{z
}
3-OCC-MAX2SAT optimisation problem
,
(6)
We write 3OM2S(X, C, γ) to denote a function which, given an instance of the 3-OCC-MAX2SAT
decision problem, returns its solution. The 3-OCC-MAX2SAT problem was proven to be hard to
approximate arbitrarily well by Berman and Karpinski (1999), with their result also implying that
this problem is NP-hard.
4.2
BINARY DIRECT TOKENISATION IS HARD TO DECIDE AND APPROXIMATE
In this section, we prove that the binary direct tokenisation problem is both hard to decide and to
approximate beyond a certain constant r > 1. First, we will prove that the decision version is NP-hard
(in §4.2.1). Second, we will then use this initial result to prove that a gap version of the problem is simi-
larly NP-hard (in §4.2.2). This will complete our proof that this problem’s optimisation version is hard
to approximate arbitrarily well, as being contained in PTAS would allow us to solve the gap problem.
4.2.1
THE BINARY DIRECT TOKENISATION DECISION PROBLEM IS NP-HARD
We now prove NP-completeness of binary direct tokenisation, which requires two things: inclusion
in NP and being NP-hard. Inclusion in NP follows from the general (unbounded) case, which was
previously proven by Whittington et al. (2025). Proving NP-hardness requires a polynomial-time
reduction from another NP-hard problem to this problem, which we will design in what follows.
Reduction 1. Consider an instance of the 3-OCC-MAX2SAT decision problem and the binary alphabet
Σ = {0, 1}. Now, for each variable Xj, let xT
j = 02j−1 and xF
j = 02j, i.e., these are character-strings
formed of 0 repeated 2j −1 or 2j times. Then we build subdatasets:
D1 ={1xT
j, xT
j1, 1xF
j, xF
j1 | 1 ≤j ≤J} × f,
D2 ={1xT
j1, 1xF
j1 | 1 ≤j ≤J} × f ′ (7a)
D3 ={1xT
j1xF
j1 | 1 ≤j ≤J}
× f ′′,
D4 ={1L1
i 1L2
i 1 | 1 ≤i ≤I}
× 1
(7b)
5In some formalisations, 3-OCC-MAX2SAT allows clauses of size one. We work here, more specifically, with
the 3-occurrence maximum exact-2-satisfiability variant of this problem, thus not allowing single literal clauses.
5

where L1
i and L2
i are replaced by their respective variable characters as they appear in the i-th
clause (i.e., Li is replaced by xT
j if it is equal to Xj or by xF
j if it equals ¬Xj). Further, ×f denotes
that a set of strings should be repeated f times in the corresponding dataset. These multiplicities are
f ′′ def
= 7, f ′ def
= 2(f ′′ + 3) + 1 = 21, and f
def
= 2(f ′ + f ′′ + 3) + 1 = 63. A full dataset is then formed
by joining these subdatasets: D = D1 ∪D2 ∪D3 ∪D4. Finally, we set the number of allowed tokens
to K = 5J and the target compression to δ = 4fJ + 3f ′J + 2f ′′J + 3I −γ = 329J + 3I −γ. 6
We will write R1(X, C, γ) to represent the D-2-TOK instance which is output by this reduction,
represented by the tuple (D, K, δ). Notably, this reduction runs in polynomial time. By proving its
correctness, thus, we can show that binary direct tokenisation is NP-hard. For this reduction to be
correct, the given 3-OCC-MAX2SAT instance must be satisfiable if and only if its reduced tokenisation
instance is as well, i.e., 3OM2S(X, C, γ) ⇐⇒Tok2
	(R1(X, C, γ)). We now set out to prove both
directions of this iff clause.
Theorem 1. The binary direct tokenisation decision problem is NP-complete.
Proof sketch. This proof is done in two steps.
Forward step (3OM2S(X, C, γ) =⇒Tok2
	(R1(X, C, γ))). See a formal proof in Lemma 1 in §A.
Assuming an instance of 3-OCC-MAX2SAT is satisfied by assignment x ⋆= {x⋆
j}J
j=1, we build a direct
tokeniser with tokens 1xT
j, xT
j1, 1xF
j, xF
j1, and with token 1xT
j1 if x⋆
j = T, and 1xF
j1 otherwise. This
tokeniser compresses D1 to 252J, D2 to 63J, D3 to 14J, and D4 to 3I −γ⋆tokens, where γ⋆is
the number of clauses satisfied by x ⋆. Adding these compressed lengths together, we find that they
satisfy the direct tokenisation problem, as γ⋆≥γ by assumption.
Backward step (Tok2
	(R1(X, C, γ)) =⇒3OM2S(X, C, γ)). See full proof in Lemma 2 in §B. We
first show that an optimal tokeniser for the D-2-TOK instance is always sat-compliant: it contains
all tokens of the form 1xT
j, xT
j1, 1xF
j, xF
j1, and either 1xT
j1 or 1xF
j1 for each j ∈{1, . . . , J}. We do
this by showing that D1 guarantees that any optimal solution includes tokens 1xT
j, xT
j1, 1xF
j, xF
j1;
D2 guarantees that any optimal solution further only includes tokens of the form 1xT
j1, 1xF
j1; and
D3 guarantees that either token 1xT
j1 or 1xF
j1 exist for each j ∈{1, . . . , J}. Then, we show that if
such a sat-compliant tokeniser reaches the desired compression, it must correspond to an assignment
x ⋆which satisfies the desired number of clauses.
4.2.2
THE BINARY DIRECT TOKENISATION GAP PROBLEM IS NP-HARD
We now prove that not only the decision version of the binary direct tokenisation problem is NP-hard,
but so is its gap version. Proving NP-hardness of a gap problem is an indirect way of proving that its
optimisation version is hard to approximate: if an efficient algorithm can approximate the optimisation
problem arbitrarily well (which is thus contained in PTAS), it could be used to solve the gap problem.
Theorem 2. The binary direct tokenisation gap problem is NP-hard. Thus, the binary direct
tokenisation optimisation problem is not in PTAS, unless P = NP.
Proof sketch. See a formal proof in §C. As shown by Berman and Karpinski (1998; 1999), the
3-OCC-MAX2SAT gap problem is NP-hard to approximate for problems with I = 2016n clauses,
γ−= (2011 + ε)n lower bound, and γ+ = (2012 −ε)n upper bound. We can use Lemmas 1 and 2
to prove a reduction from this gap problem to D-2-TOK’s gap problem. Notably, our reduction equates
δ = 329J + 3I −γ, for both γ−and γ+. Analysing the gap of the resulting tokenisation problem,
we find that this problem is thus NP-hard for an approximation ratio δ−
δ+ of at least 1.000002.
This implies that no polynomial-time algorithm can approximate the binary direct tokenisation
optimisation problem with an approximation ratio better than this constant, unless P = NP.
While the constant above (i.e., 1.000002) is remarkably small, we note that our proof makes no attempt
to optimise this bound. Our lemma’s main takeaway is that it is not possible to compute D-2-TOK with
approximation ratios arbitrarily close to 1 in polynomial time. Other larger bounds likely exist and,
in fact, it might even be possible that there is no constant-factor approximation for D-2-TOK at all.
6This reduction is inspired by Whittington et al.’s 2025 reduction, which we update to (i) rely on binary, as
opposed to unbounded, alphabets; (ii) use constant-sized f’s, which allow us to prove approximation hardness.
6

4.3
BINARY BOTTOM-UP TOKENISATION IS HARD TO DECIDE AND APPROXIMATE
This section addresses the computational hardness of finding an optimal merge sequence in binary
bottom-up tokenisation. We establish that the problem is NP-hard in §4.3.1. We then prove that the
problem is also hard to approximate in §4.3.2, thus showing that it is not in PTAS, unless P = NP.
As for the direct case, our argument proceeds by first proving the hardness of the decision problem,
and then leveraging this result to demonstrate the hardness of a corresponding gap problem.
4.3.1
THE BINARY BOTTOM-UP TOKENISATION PROBLEM IS NP-COMPLETE
As before, we use a reduction from 3-OCC-MAX2SAT to prove this problem’s NP-hardness.
Reduction 2. Consider an instance of the 3-OCC-MAX2SAT decision problem and the binary alphabet
Σ = {0, 1}. Again, for each variable Xj, let xT
j = 02j−1 and xF
j = 02j. Then we build subdatasets:
D1 ={11} ∪{xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j}J
j=1
× f
(8a)
D2 ={1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1}J
j=1
× f ′
(8b)
D3 ={1xT
j1xF
j1, 11xF
j1xT
j11}J
j=1
× f ′′
(8c)
D4 ={1xF
j1xT
j11, 11xF
j1xT
j1}J
j=1
× f ′′′
(8d)
D5 =







1xT
j1xF
j′1
if L1
i = Xj, L2
i = ¬Xj′
1xT
j′1xF
j1
if L1
i = ¬Xj, L2
i = Xj′
11xF
j1xF
j′1
if L1
i = ¬Xj, L2
i = ¬Xj′
1xT
j1xT
j′11
if L1
i = Xj, L2
i = Xj′







I
i=1
× 1
(8e)
These subdataset multiplicities are f ′′′ def
= 4, f ′′ def
= 2(2f ′′′ +3)+1 = 23, f ′ def
= 2(2f ′′ +2f ′′′ +3)+
1 = 115, and f
def
= 2(2f ′+2f ′′+2f ′′′+3)+1 = 575. We set the vocabulary size to K = 10J and the
target compressed length to δ = (8J +1)f +6Jf ′+4Jf ′′+4Jf ′′′+3I −γ = 5398J +575+3I −γ.
We write R2(X, C, γ) to represent the B-2-TOK instance (D, K, δ) constructed by this reduction. As
before, this is a polynomial-time reduction. We now prove the equivalence 3OM2S(X, C, γ) ⇐⇒
Tok2
↑(R2(X, C, γ)) which shows the reduction’s correctness and thus that B-2-TOK is NP-hard.
Theorem 3. The binary bottom-up tokenisation decision problem is NP-complete.
Proof sketch. This proof is done in two steps.
Forward step (3OM2S(X, C, γ) =⇒Tok2
↑(R1(X, C, γ))). See full proof in Lemma 3 in §D. As-
sume the 3-OCC-MAX2SAT instance admits an assignment x ⋆= {x⋆
j}J
j=1 satisfying at least γ clauses.
We construct a merge sequence m = m1◦m2◦m3◦m4◦m5◦m6, where m1, m2, m4, m6 are struc-
tural merges that appear in every valid tokeniser solution and ensure that all strings corresponding to
single variables are properly compressed; and m3, m5 are assignment-dependent merges, chosen
according to x ⋆: for each variable x⋆
j, we merge 1 ⊚xT
j11, 1xT
j ⊚1 if x⋆
j = T, and 11xF
j ⊚1, 1 ⊚xF
j1
otherwise. Applying m to the string subdatasets D1, D2, D3, D4 gives the fixed compressed length
5398J + 575. For the strings D5, the construction ensures that each clause compresses to 2 tokens
if at least one of its two literals is true under x ⋆, and remains at 3 tokens otherwise. Since x ⋆satisfies
at least γ clauses, we obtain at most 3I −γ symbols. Compression thus satisfies the budget δ.
Backward step (Tok2
↑(R2(X, C, γ))
=⇒
3OM2S(X, C, γ)). See full proof in Lemma 4 in
§E. We consider sat-compliant direct tokenisers, which must contain all tokens of the form
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j and must contain either 1xT
j1, 1xT
j11 or 1xF
j1, 11xF
j1 for
each j ∈{1, . . . , J}. Again, an optimal direct tokeniser for the B-2-TOK-instance is always sat-
compliant, which is enforced by datasets D1 to D4. We then show that if such a tokeniser achieves
the desired compression, it must correspond to an assignment x ⋆which satisfies the desired number
of clauses. To finish, we show that, for any instance generated by Reduction 2, a sat-compliant
direct tokeniser always corresponds to a bottom-up tokeniser with the same compression quality.
4.3.2
THE BINARY BOTTOM-UP TOKENISATION GAP PROBLEM IS NP-HARD
As in §4.2.2, we perform a reduction from a gap variant to show hardness of approximation.
7

Theorem 4. The binary bottom-up tokenisation gap problem is NP-hard. Thus, the binary bottom-up
tokenisation optimisation problem is not in PTAS, unless P = NP.
Proof sketch. See proof in §F. A similar proof to Theorem 2 applies here, except with different
values. We find that no polynomial-time algorithm can solve the binary bottom-up tokenisation
optimisation problem with an approximation ratio better than 1.0000001, unless P = NP.
5
UNARY TOKENISATION IS HARD TO DECIDE
We now move on to the unary tokenisation case. Here, we work with alphabets composed of a single
symbol, i.e., Σ = {a}. As Σ∗= {aℓ| ℓ∈N}, it follows that unary character-strings c ∈Σ∗may
only differ from one another in their length. There exists thus an isomorphism (given by the function
|·| and its inverse) between these character-strings and their string-lengths, ℓ∈N. A natural notation
for such problems is then to work directly with string-lengths. In this section, we will thus represent
a character-string c ∈Σ∗by its length ℓ∈N; a dataset D = {cm}M
m=1 by the lengths of its strings
DN = {ℓm}M
m=1, where cm = aℓm; and a vocabulary S ⊂Σ+ by a set of string-lengths SN ⊂N+.
A subword-string is then a sequence of such string-lengths, sN ∈S∗
N and we have:
detok(sN)
def
= sum(sN)
tok	[SN](ℓ)
def
= arg min
sN∈S∗
N
|sN|, s.t., ℓ= sum(sN)
(9)
where we overload the functions detok and tok	 to handle this unary-strings representation. Note
that all these definitions are equivalent (up to an isomorphism) to the definitions in §3.
When posing either the optimisation or decision version of the unary tokenisation problems, we could
thus work with either representation of our data (as strings or string-lengths) and the solutions must
be the same. However, the complexity of an algorithm is typically measured as a function of the
length of its input. If this input is a unary string, the input will be as long as this string’s length.
If this input is a number, however, this input’s length behaves logarithmically on the value of the
number itself (as this number would typically be encoded in a compact binary representation). When
dealing with problems such as unary tokenisation, this introduces an important subtlety: the problem’s
complexity status may change depending on how we represent it (with strings or string-lengths).
If such a problem is NP-hard when either representation is given, it is called strongly NP-hard.
If this problem is NP-hard only in its string-length representation, but not when represented using
unary strings, it is weakly NP-hard.7 Importantly, Fact 1 applies only to strongly NP-hard unary
problems; as the trivial identity we use in its proof would not be valid for unary problems with
string-length representations. For unary tokenisation, the string representation (where strings are
explicitly represented) is more natural, and we are thus interested in strong NP-hardness.
5.1
UNARY DIRECT TOKENISATION IS STRONGLY NP-COMPLETE
In this section, we prove that the unary direct tokenisation problem is strongly NP-complete. In §G,
we prove that the problem is in NP, even if the input is in string-length representation. To prove
NP-hardness of unary direct tokenisation, we then design a polynomial-time reduction from the
well-known vertex cover problem (vertex-cover).8 Let (V, E) represent a finite, simple, undirected
graph with V = {v1, . . . , vJ} and E ⊆{(v, v′) | v, v′ ∈V, v ̸= v′}. A set C ⊆V is a vertex cover
if, for every edge (v, v′) ∈E, we have that either v or v′ is in C. Given a budget ψ ∈N, the vertex
cover problem requires deciding whether a graph has a vertex cover of at most ψ vertices.
Definition 3. Given a graph (V, E) and a budget ψ ∈N, the vertex cover decision problem asks
whether there exists a vertex cover C ⊆V with |C| ≤ψ in this graph.
For convenience, we will write VC(V, E, ψ) for a function which returns T if its input is a satisfiable
instance of the vertex-cover decision problem, and F otherwise. We now provide a polynomial-time
reduction from vertex-cover to D-1-TOK, which will prove D-1-TOK’s NP-hardness.
7Note that the opposite case—where a problem is NP-hard only when representing the data as strings, but
not strings-lengths—is not possible, as strings have a larger size than their lengths.
8The NP-hardness of vertex-cover was proven by Karp (1972) in his groundbreaking paper introducing
the very concept of NP-hardness, and can be found in the textbook by Garey and Johnson (1979).
8

Reduction 3. Consider an instance (V, E, ψ) of vertex-cover and let N
def
= (J + I + 1)4, where
J = |V| and I = |E|. Now, let enc
 vj

= j + j2N + j3N 2 and B = N 4. We construct three
subdatasets from this graph as:
D1 = {aℓj | vj ∈V} ∪{B},
where ℓj = enc
 vj

vertex-strings
(10a)
D2 = {aℓ′
j | vj ∈V},
where ℓ′
j = enc
 vj

+ B
cover-strings (10b)
D3 = {aℓ′′
j,j′ | (vj, vj′ ∈E)},
where ℓ′′
j,j′ = enc
 vj

+ enc
 vj′
+ B
edge-strings
(10c)
Finally, we merge these subdatasets to form a dataset D = D1 ∪D2 ∪D3, and set K = J + 1 + ψ
and δ = 3J + 2I + 1 −ψ.
As before, we complete our NP-hardness proof by showing this to be a valid reduction, i.e., that
VC(V, E, ψ) ⇐⇒Tok1
	(R3(V, E, ψ)). Notably, our reduction outputs (in polynomial time, as all
lengths are polynomially bounded in the size of the original instance) an instance of the unary direct
tokenisation problem in string form. As such, by proving the correctness of this reduction, we prove
the strong NP-hardness of D-1-TOK.
Theorem 5. The unary direct tokenisation decision problem is strongly NP-complete.
Proof sketch. This proof is done in two steps.
Forward step (VC(V, E, ψ) =⇒Tok1
	(R3(V, E, ψ))). See full proof in Lemma 6 in §H. Suppose
that the given instance of vertex-cover is true, i.e., that VC(V, E, ψ) = T. Now, let C⋆⊆V
be a vertex cover which satisfies this instance. Then we can build a tokeniser with vocabulary:
SN = {aℓj | vj ∈V} ∪{aB} ∪{aℓ′
j | vj ∈C⋆}. This tokeniser will encode: all strings in D1 as a
single symbol; ψ strings in D2 with a single symbol and others with 2; and all strings in D3 with
two symbols (as, per our assumption, all edges have at least one vertex in C⋆). This means the total
amount of tokens used is: (J + 1) + (2J −ψ) + 2I = δ. Therefore, Tok1
	(D, K, δ) = T.
Backward step (Tok1
	(R3(V, E, ψ))
=⇒
VC(V, E, ψ)). See full proof in Lemma 7 in §I. We
prove this lemma in 4 steps. First, we show that all string-lengths in DN are unique. Second, we
show that an optimal tokeniser’s vocabulary must contain only full strings in DN. Third, we show
that an optimal tokeniser’s vocabulary must include all strings in D1. Fourth, we show that if a
compression of δ is achieved, than the corresponding vertex-cover instance must be true. Notably,
three of these steps rely on the fact that we can use N as a numerical base to prove the uniqueness
of both: (i) individual string-lengths, as well as (ii) their pairwise summed values.
Interestingly, the unary direct tokenisation problem is tightly related to the problem of choosing
denominations for a coin system. In fact, the application of the function tok	[SN](ℓ) is equivalent
to the change-making problem; a problem shown to be (weakly) NP-hard by Lueker (1975) and a
classic example for dynamic programming. (Note that this problem is only weakly NP-hard, as we
can solve it in polynomial time when the input is given in string form.) The unary direct tokenisation
problem can thus be equivalently seen as a general optimal denomination problem, where—given
a set of common currency transactions—one must select optimal coin denominations for a currency;
see Shallit (2003) for a discussion of this problem.
Corollary 1. The general optimal denomination decision problem is strongly NP-complete.
5.2
A VARIANT OF UNARY BOTTOM-UP TOKENISATION IS (AT LEAST) WEAKLY NP-HARD
While direct tokenisation over a unary alphabet is strongly NP-complete, our current picture of the
complexity of its bottom-up counterpart is more nuanced. In bottom-up tokenisation, one must find a
merge sequence m which is then applied (by tok↑[m](c)) exhaustively and in sequence, replacing
all occurrences of each pair one at a time. A variant of this problem—termed optimal pair encoding
(OPE) tokenisation—relaxes this requirement, using the merge sequence for a different purpose:
to define a merge-extracted vocabulary Sm = Σ ∪{s1 ◦s2 | m ∈m, m = (s1, s2)} The final
tokenisation is then produced by optimally applying this vocabulary, which can be done using the
direct encoding function (tok	[Sm](c)). This approach thus ensures that a merge is used only if it
contributes to the most efficient segmentation overall. Notably, this variation was used by Schmidt
9

et al. (2024) and formally analysed by Kozma and Voderholzer (2024). Now, let the unary OPE
tokenisation problem be defined similarly to the other n-ary tokenisation problems (in Definition 1),
but while constraining the search space to the set of OPE tokenisers: Tope
def
={tok	[Sm] | m ∈M∗}.
Having defined the decision problem, we now establish its computational hardness.
Theorem 6. The unary optimal pair encoding decision problem is (at least) weakly NP-complete.
Proof sketch. The full proof can be found in §K. Inclusion in NP follows from Kozma and Voder-
holzer (2024). The proof of NP-hardness is achieved via a polynomial-time reduction from the
addition chain sequence decision problem (see §J for a formal definition), which is known to be
NP-complete when its input numbers are encoded in binary (Downey et al., 1981). The reduction
reveals a natural connection between the two problems: finding the shortest addition chain for a set
of numbers is equivalent to a special case of unary optimal pair encoding where every string in the
dataset must be compressed into a single token.
6
CONCLUSION AND LIMITATIONS
We provided several hardness results on bottom-up and direct tokenisation with bounded alphabets,
thus answering open questions posed by both Kozma and Voderholzer (2024) and Whittington et al.
(2025). A number of open questions remain, however, in particular with respect to approximability.
For instance, while we showed that the binary tokenisation optimisation problems cannot be approx-
imated arbitrarily well (unless P = NP)—and while it seems likely that the lower bound provided in
the proof of Theorem 2 can be significantly lifted—it is unclear whether any constant approximation
ratio can even be obtained. With respect to decision problems, while we showed strong NP-hardness
of unary direct tokenisation, we were so far only able to prove: (i) weak NP-hardness of unary OPE to-
kenisation, and (ii) no hardness result for unary (standard) bottom-up tokenisation. Finally, the results
of our work are limited in that we consider (i) compression as objective, and (ii) bottom-up and direct
tokenisation only; the hardness of both other objectives and variants remains open. Overall, however,
our results show that tokenisation remains a hard problem, even when restricted to small (even binary
or unary) alphabets. Future work should thus explore provably good approximation algorithms.
ACKNOWLEDGMENTS
We would like to thank Giulia Lanzillotta, Weronika Ormaniec, Dimitri von Rütte and Felix Sarnthein
for their helpful feedback on the introduction. We are also grateful to Pietro Lesci, Amit Moryossef
and Marius Mosbach for their comments on the manuscript, and to Thomas Hofmann for insightful
discussions about the paper. We further thank Gregor Bachmann, Hans-Joachim Böckenhauer,
Emanuel Skodinis and Moritz Stocker for their contributions in the early stages of this work, and
Stefan Gerdjikov for his valuable early input and discussions.
REFERENCES
Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes
Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Buschhoff, Charvi Jain, Alexander Weber,
Lena Jurkschat, Hammam Abdelwahab, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff,
Samuel Weinbach, Rafet Sifa, Stefan Kesselheim, and Nicolas Flores-Herr. 2024. Tokenizer
choice for LLM training: Negligible or crucial? In Findings of the Association for Computational
Linguistics: NAACL 2024, pages 3907–3924, Mexico City, Mexico. Association for Computational
Linguistics.
Piotr Berman and Marek Karpinski. 1998.
On some tighter inapproximability results, further
improvements. In Electronic Colloquium on Computational Complexity, volume 65.
Piotr Berman and Marek Karpinski. 1999. On some tighter inapproximability results (extended
abstract). In Automata, Languages and Programming — 26th International Colloquium, ICALP
1999, Proceedings, volume 1644 of Lecture Notes in Computer Science, pages 200–209, Berlin,
Heidelberg. Springer.
10

Peter Downey, Benton Leong, and Ravi Sethi. 1981. Computing sequences with addition chains.
SIAM Journal on Computing, 10(3):638–646.
Matthias Gallé. 2019. Investigating the effectiveness of BPE: The power of shorter sequences. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
1375–1381, Hong Kong, China. Association for Computational Linguistics.
Michael R. Garey and David S. Johnson. 1979. Computers and Intractability: A Guide to the Theory
of NP-Completeness. W. H. Freeman, San Francisco.
Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao, Idan Szpektor, and Reut Tsarfaty. 2024.
Unpacking tokenization: Evaluating text compression and its correlation with model performance.
In Findings of the Association for Computational Linguistics: ACL 2024, pages 2274–2286,
Bangkok, Thailand. Association for Computational Linguistics.
Thamme Gowda and Jonathan May. 2020. Finding the optimal vocabulary size for neural machine
translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages
3955–3964, Online. Association for Computational Linguistics.
Richard M. Karp. 1972. Reducibility among combinatorial problems. In Raymond E. Miller and
James W. Thatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum Press,
New York.
László Kozma and Johannes Voderholzer. 2024. Theoretical analysis of byte-pair encoding. Preprint,
arXiv:2411.08671.
Taku Kudo. 2018. Subword regularization: Improving neural network translation models with
multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pages 66–75, Melbourne, Australia.
Association for Computational Linguistics.
Jia Peng Lim, Davin Choo, and Hady W. Lauw. 2025. A partition cover approach to tokenization.
Preprint, arXiv:2501.06246.
G.S. Lueker. 1975. Two NP-complete Problems in Nonnegative Integer Programming. Technical
report: Computer Science Laboratory. Univ.
Phillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder, and Iryna Gurevych. 2021. How good is your
tokenizer? On the monolingual performance of multilingual language models. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3118–3135,
Online. Association for Computational Linguistics.
Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri Uzan, Yuval Pinter,
and Chris Tanner. 2024. Tokenization is more than compression. In Proceedings of the 2024
Conference on Empirical Methods in Natural Language Processing, pages 678–702, Miami,
Florida, USA. Association for Computational Linguistics.
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin, Germany. Association for
Computational Linguistics.
Jeffrey Shallit. 2003. What this country needs is an 18c piece. In Math. Intelligencer 25(2).
Omri Uzan, Craig W. Schmidt, Chris Tanner, and Yuval Pinter. 2024. Greed is all you need: An
evaluation of tokenizer inference methods. In Proceedings of the 62nd Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 813–822, Bangkok,
Thailand. Association for Computational Linguistics.
11

Philip Whittington, Gregor Bachmann, and Tiago Pimentel. 2025. Tokenisation is NP-complete. In
Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), Vienna, Austria. Association for Computational Linguistics.
Vilém Zouhar, Clara Meister, Juan Gastaldi, Li Du, Mrinmaya Sachan, and Ryan Cotterell. 2023a.
Tokenization and the noiseless channel.
In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 5184–5207, Toronto,
Canada. Association for Computational Linguistics.
Vilém Zouhar, Clara Meister, Juan Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan
Cotterell. 2023b. A formal perspective on byte-pair encoding. In Findings of the Association
for Computational Linguistics: ACL 2023, pages 598–614, Toronto, Canada. Association for
Computational Linguistics.
12

A
PROOF OF FORWARD STEP OF THEOREM 1
Lemma 1. If a 3-OCC-MAX2SAT instance is satisfiable, then the D-2-TOK instance output by Reduc-
tion 1 is also satisfiable. Formally: 3OM2S(X, C, γ) =⇒Tok2
	(R1(X, C, γ)).
Proof. To prove this forward step of Theorem 1, we first establish that a satisfiable 3-OCC-MAX2SAT
instance guarantees the existence of a binary direct tokeniser that meets the compression target.
Assume a (X, C, γ) instance of the 3-OCC-MAX2SAT problem is satisfiable, i.e., that 3OM2S(X, C, γ)
is true. We must prove that, in this case, Tok2
	(R1(X, C, γ)) is also true. Now, let x ⋆= {x⋆
j}J
j=1 be
any satisfying solution to the (X, C, γ) instance. We will denote the number of clauses satisfied by x ⋆
by γ⋆, noting that γ⋆≥γ by assumption. We can construct a tokeniser from this solution as follows:
S = Σ
[ n
1xT
j, xT
j1, 1xF
j, xF
j1
oJ
j=1
[
n
1xT
j1 if x⋆
j = T else 1xF
j1
oJ
j=1
(11)
Note that—as required by our reduction—this tokeniser has vocabulary size |S| = |Σ| + K, since
K = 5J tokens were added. Under this tokeniser, we have:
tok	[S](D1) =
n
⟨1xT
j⟩, ⟨xT
j1⟩, ⟨1xF
j⟩, ⟨xF
j1⟩
(length 1)
| 1 ≤j ≤J
o
× f
(12a)
tok	[S](D2) =
 ⟨1xT
j1⟩, ⟨1xF
j, 1⟩
(length 3)
if x⋆
j = T
⟨1xT
j, 1⟩, ⟨1xF
j1⟩
(length 3)
else
| 1 ≤j ≤J

× f ′ (12b)
tok	[S](D3) =
 ⟨1xT
j1, xF
j1⟩
(length 2)
if x⋆
j = T
⟨1xT
j, 1xF
j1⟩
(length 2)
else
| 1 ≤j ≤J

× f ′′ (12c)
tok	[S](D4) =
( ⟨1L1
i 1, L2
i 1⟩
(length 2)
if L1
i = T
⟨1L1
i , 1L2
i 1⟩
(length 2)
elif L2
i = T
⟨1L1
i , 1, L2
i 1⟩
(length 3)
else
| 1 ≤i ≤I
)
× 1
(12d)
where we override function tok	[S] to apply elementwise to a full dataset of character-strings,
instead of to a unique c. Consequently, we get the compressed lengths:
Gℓ(tok	[S], D1) = 4 J f = 252 J f,
Gℓ(tok	[S], D2) = 3 J f ′ = 63 J,
(13a)
Gℓ(tok	[S], D3) = 2 J f ′′ = 14 J,
Gℓ(tok	[S], D4) = 3 I −γ⋆
(13b)
We have that each character-string in dataset D4 is compressed to 2 symbols if either L1
i or L2
i are
true, and otherwise is kept at 3 symbols; the γ⋆satisfied clauses in x ⋆will thus be compressed to
2 symbols and the unsatisfied clauses to 3. Summing these values together, we get the compressed
length of the entire dataset under this tokeniser: Gℓ(tok	[S], D) = 329J + 3 I −γ⋆. Finally:
γ⋆≥γ =⇒329J + 3 I −γ⋆≤329J + 3 I −γ
(14)
This completes this proof.
B
PROOF OF BACKWARD STEP OF THEOREM 1
Before starting our lemma’s proof, we define a few notions which will be useful throughout it. First,
we define a sat-compliant tokeniser to be any tokeniser which: (i) contains all tokens of the form
1xT
j, xT
j1, 1xF
j, xF
j1; and (ii) contains either 1xT
j1 or 1xF
j1 for each j ∈{1, . . . , J}. Otherwise, we
call the tokeniser sat-noncompliant. Given the vocabulary of a sat-compliant tokeniser, we can
easily build an assignment to a 3-OCC-MAX2SAT instance with the following function:
g(S) = {xj}J
j=1, where
 xj = T
if 1xT
j1 ∈S
xj = F
elif 1xF
j1 ∈S
(15)
Further, we will define as a 101-string any character-string of the form 10+1, and as a 10101-string
any character-string of the form 10+10+1. (The 0+ notation stands for a sequence of one or more 0
characters.) Considering the datasets output by Reduction 1, we know that there are no 101-strings in
D1. Further, we know that each unique 101-string appears in datasets D2 and D3 exactly f ′ and f ′′
times, respectively, and exactly 3 times in D4. (This is due to us working with the three-occurrences
variant of MAX2SAT and to the fact that xT
j = 02j−1 and xF
j = 02j.) We now prove the following
lemma.
13

Lemma 2. If the D-2-TOK instance output by Reduction 1 is satisfiable, then the 3-OCC-MAX2SAT
instance which generated it is as well. Formally: Tok2
	(R1(X, C, γ)) =⇒3OM2S(X, C, γ).
Proof. Assume this (D, K, δ) instance of D-2-TOK—where (D, K, δ) = R1(X, C, γ)—is satisfiable,
i.e., that Tok2
	(R1(X, C, γ)) evaluates to true. We must prove that, in this case, 3OM2S(X, C, γ)
also evaluates to true. Now, let Sopt be an arbitrary optimal solution to (D, K, δ). We know, by
definition, that:
Tok2
	(R1(X, C, γ)) ⇐⇒

Gℓ(tok	[Sopt], D) ≤δ

(16)
We can thus prove this lemma by showing the following implication:

Gℓ(tok	[Sopt], D) ≤δ

=⇒3OM2S(X, C, γ)
(17)
We will proceed in four steps:
1
we prove that Sopt must include all tokens of the form 1xT
j, xT
j1, 1xF
j, xF
j1;
2
we prove that Sopt must, in addition to the tokens above, only include tokens of the form
1xT
j1, 1xF
j1;
3
we prove that Sopt may only include, for each j, either token 1xT
j1 or 1xF
j1;
4
finally, we prove that, if

Gℓ(tok	[Sopt], D) ≤δ

, we can build a variable assignment
which satisfies this 3-OCC-MAX2SAT instance (X, C, γ).
Note that, together, steps 1 to 3 show that Sopt must be the vocabulary of a sat-compliant
tokeniser; in step 4 , we will then rely on the function g (defined above in Eq. (15)) to convert this
vocabulary into a satisfying assignment x = g(Sopt) of the 3-OCC-MAX2SAT instance.
LemmaProofStep 1. (Step
1 ).
An optimal tokeniser must include all tokens of the form
1xT
j, xT
j1, 1xF
j, xF
j1, i.e.:
n
1xT
j, xT
j1, 1xF
j, xF
j1
oJ
j=1 ⊆Sopt
(18)
Proof. We prove this step by contradiction. Assume there exists an optimal tokeniser with vocabulary
S✗which does not include t > 0 of the tokens above. Now, choose an arbitrary set of t tokens in this
vocabulary which are not of the above form, and replace them with the missing tokens in this set. We
denote this new tokeniser’s vocabulary by S✓. Note that the strings in D1 with these missing tokens
were represented with at least 2 symbols under S✗, but with a single token under S✓, i.e.:
Gℓ(tok	[S✗], D1) ≥(4J + t)f,
Gℓ(tok	[S✓], D1) = 4Jf
(19)
Further, note that under S✓, we have that strings in dataset D2 are compressed to at most two symbols,
while strings in D3 and D4 are compressed to at most three symbols:
∀c ∈D2 : Gℓ(tok	[S✓], c) ≤2,
∀c ∈D3 ∪D4 : Gℓ(tok	[S✓], c) ≤3
(20)
To improve on this compressed length, S✗must, thus, compress strings in D2 to a single symbol, or
strings in D3 and D4 to one or two symbols. Notably, this can only be done if the non-compliant
tokens in S✗contain 101-strings. This is because, to compress a string in D2 to a single symbol, the
full character-string must become a token, and D2 only includes 101-strings. Moreover, under S✓,
strings in D3 and D4 are already compressed to at most ⟨1xT
j, 1, xT
j1⟩. To further compress them,
tokeniser S✗must include tokens which cross the “middle” of this character-string, which would
make this tokens at least have a 101 prefix or suffix. We consider the best case scenario, which is if
they are exactly 101-strings, as any longer string will be at most as frequent as it.
As discussed above, however, each 101-string appears at most: f ′ times in D2, f ′′ times in D3, and
3 times in D4. This gives us a best case scenario—in which all the strings in which a new token
appears are compressed to a single symbol—where:
Gℓ(tok	[S✓], D2 ∪D3 ∪D4) −Gℓ(tok	[S✗], D2 ∪D3 ∪D4) ≤t(f ′ + 2(f ′′ + 3))
(21)
14

As the difference in Eq. (19) is of at least tf tokens, we put these together:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤t(f ′ + 2(f ′′ + 3)) −tf
(22)
As f > f ′ + 2(f ′′ + 3), this difference is smaller than zero, implying that S✓improves on S✗. This
shows a contradiction, which completes our proof.
LemmaProofStep 2. (Step
2 ).
An optimal tokeniser must include all tokens of the form
1xT
j, xT
j1, 1xF
j, xF
j1, and further only tokens of the form 1xT
j1, 1xF
j1, i.e.:
n
1xT
j, xT
j1, 1xF
j, xF
j1
oJ
j=1 ⊆Sopt
and
Sopt ⊂
n
1xT
j, xT
j1, 1xF
j, xF
j1, 1xT
j1, 1xF
j1
oJ
j=1
(23)
Proof. As before, we prove this step by contradiction. Given step 1 , we know an optimal tokeniser
includes all tokens 1xT
j, xT
j1, 1xF
j, xF
j1. Now, assume there exists an optimal tokeniser with vocabulary
S✗with t > 0 tokens which are not of the form 1xT
j, xT
j1, 1xF
j, xF
j1 or 1xT
j1, 1xF
j1; note that these t
tokens are sat-noncompliant. Choose an arbitrary set of t unused compliant tokens—i.e., with form
1xT
j1, 1xF
j1—to replace the non-compliant tokens with, forming a new tokeniser’s vocabulary S✓.
Both these vocabularies compress strings in D1 equally:
Gℓ(tok	[S✗], D1) = 4Jf,
Gℓ(tok	[S✓], D1) = 4Jf
(24)
For strings in D2: if the entire string is in the vocabulary, it is encoded as a single token; otherwise, it
is represented with two symbols. Under S✓, there are J tokens covering strings in D2. Under S✗,
there are only (J −t) tokens covering strings in D2. This implies:
Gℓ(tok	[S✗], D2) = (3J + t)f ′,
Gℓ(tok	[S✓], D2) = 3Jf ′
(25)
Finally, for strings in D3 and D4, a similar argument to the previous step applies: (i) only tokens
containing 101-strings can compress these datasets; (ii) each 101-string appears at most f ′′ + 3 times
in them; (iii) each 101-string will lead to at most two symbols being saved. As S✗differs from S✓in
t tokens, we get that it will improve on it by at most:
Gℓ(tok	[S✓], D3 ∪D4) −Gℓ(tok	[S✗], D3 ∪D4) ≤2t(f ′′ + 3)
(26)
Summing together the compression on all datasets, we get that their difference is bounded by:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤2t(f ′′ + 3) −tf ′
(27)
As f ′ > 2(f ′′ + 3), this difference is smaller than zero, implying that S✓improves on S✗. This
shows a contradiction, which completes our proof.
LemmaProofStep 3. (Step 3 ). An optimal tokeniser must be sat-compliant: it must contain all
tokens of the form 1xT
j, xT
j1, 1xF
j, xF
j1 and it must contain either 1xT
j1 or 1xF
j1 for each 1 ≤j ≤J.
Proof. As before, we prove this step by contradiction. Given step 1 , we know an optimal tokeniser
includes all tokens 1xT
j, xT
j1, 1xF
j, xF
j1. Further, given step 2 , we know its other tokens all have form
1xT
j1, 1xF
j1. Now, assume there exists an optimal tokeniser with vocabulary S✗which includes both
1xT
j1 and 1xF
j1 for t > 0 variables, and thus neither of those two for t > 0 other variables. Then,
define S✓as a vocabulary where the 1xF
j1 token of all t doubly assigned variables are replaced with
the 1xT
j1 token of all non-assigned variables. Note that S✓is sat-compliant. These two tokenisers
achieve the same compression on D1 and D2:
Gℓ(tok	[S✗], D1 ∪D2) = 4Jf + 3Jf ′,
Gℓ(tok	[S✓], D1 ∪D2) = 4Jf + 3Jf ′
(28)
The tokeniser with vocabulary S✓will then compress each string in D3 to 2 symbols, while S✗will
compress the t strings 1xT
j1xF
j1 with unassigned variables to 3 symbols. This will lead to a total
compression of:
Gℓ(tok	[S✗], D3) = (2J + t)f ′′,
Gℓ(tok	[S✓], D3) = 2Jf ′′
(29)
Finally, the t doubly assigned tokens of the form 1xF
j1 (which S✓does not contain) appear at most
three times in D4 and will lead to at most one symbol being saved, leading to a bound:
Gℓ(tok	[S✓], D4) −Gℓ(tok	[S✗], D4) ≤3t
(30)
15

Putting these compressed lengths together, we get:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤3t −tf ′′
(31)
As f ′′ > 3, this difference is smaller than zero, implying that S✓improves on S✗. This shows a
contradiction, which completes our proof.
LemmaProofStep 4. (Step 4 ). If an optimal tokeniser achieves a compressed length of at most
329J + 3I −γ, the original 3-OCC-MAX2SAT instance is satisfiable, i.e.:

Gℓ(tok	[Sopt], D) ≤329J + 3I −γ

=⇒3OM2S(X, C, γ)
(32)
Proof. Given steps 1 to 3 , we know that an optimal tokeniser will be sat-compliant. We will
now denote this optimal tokeniser’s vocabulary by Sopt and use Eq. (15) to extract a 3-OCC-MAX2SAT
assignment x ⋆= g(Sopt) which corresponds to this tokeniser’s vocabulary. From the previous proof
steps we see that any sat-compliant tokeniser achieves the following compressed length in D1, D2,
and D3:
Gℓ(tok	[Sopt], D1 ∪D2 ∪D3) = 4Jf + 3Jf ′ + 2Jf ′′ = 329J
(33)
Now, note that a character-string 1L1
i 1L2
i 1 in D4 will be: compressed to two symbols if at least one of
the tokens 1L1
i 1 or 1L2
i 1 exists, or compressed to three symbols if neither exists. Equivalently, a clause
L1
i ∨L2
i in 3-OCC-MAX2SAT is: satisfied if either L1
i or L2
i evaluates to true, or not satisfied if both evalu-
ate to false. Given our construction of function g above, one of 3-OCC-MAX2SAT’s clauses will be satis-
fied if and only if its corresponding string in D4 is compressed to two symbols. We can thus state that:

Gℓ(tok	[Sopt], D4) = 3I −γ⋆

⇐⇒
 I
X
i=1
1x ⋆{L1
i ∨L2
i } = γ⋆
!
(34)
Given the construction of δ as 329J + 3I −γ, we conclude that a sat-compliant tokeniser which
compresses the full dataset to at least that size can be mapped to a 3-OCC-MAX2SAT assignment
which satisfies at least γ clauses. This concludes the proof.
C
PROOF OF THEOREM 2
Theorem 2. The binary direct tokenisation gap problem is NP-hard. Thus, the binary direct
tokenisation optimisation problem is not in PTAS, unless P = NP.
Proof. For this proof, we rely on a result by Berman and Karpinski (1998; 1999) that, for specific
instances of 3-OCC-MAX2SAT with I = 2016n clauses, it is NP-hard to distinguish whether at least
(2012 −ε)n or at most (2011 + ε)n of these clauses are satisfiable, for any ε > 0. We will
denote this 3-OCC-MAX2SAT gap problem by 3OM2S(X, C, (γ−, γ+)), with γ−= (2011 + ε)n and
γ+ = (2012−ε)n. We can now prove the NP-hardness of the binary direct tokenisation gap problem
by reducing 3-OCC-MAX2SAT’s gap problem to it. To this end, we rely on a reduction identical to
R1(X, C, γ), but where we define:
δ−= 329J + 3I −γ−
δ+ = 329J + 3I −γ+
(35)
= 329J + 3I −2011 + ε
2016
I
= 329J + 3I −2012 −ε
2016
I
Lemmas 1 and 2 trivially show the validity of this reduction:
3OM2S(X, C, (γ−, γ+)) ⇐⇒Tok2
	(D, K, (δ−, δ+))
(36)
which holds since 3OM2S(X, C, γ+) ⇐⇒Tok2
	(D, K, δ+) and the same for γ−and δ−. It is
therefore NP-hard to distinguish whether a dataset can be compressed to at most 329J +3I −2012−ε
2016 I
symbols, or if at least 329J+3I−2011+ε
2016 I symbols remain (with an allowed vocabulary size K = 5J).
Since each variable occurs exactly three times in 3-OCC-MAX2SAT, we have that 3
2J = I. We now
compute a lower bound on the best achievable compression ratio:
δ−
δ+ = 329J + 3I −2011+ε
2016 I
329J + 3I −2012−ε
2016 I
(37a)
16

= 667 −6033+3ε
2016
667 −6036−ε
2016
(37b)
= 1338639 −3ε
1338636 + 3ε
(37c)
= 446213 −ε
446212 + ε
(37d)
Thus, binary direct tokenisation cannot be approximated in polynomial time with an approximation
ratio better than 446213
446212 > 1.000002, unless P = NP.
D
PROOF OF FORWARD STEP OF THEOREM 3
We first need another lemma in preparation for the actual proof of this forward step. Note that
Reduction 2 produces character-strings xT
j and xF
j, with form {0j | 1 ≤j ≤2J}, which our tokeniser
must compress. However, a merge-sequence m = ⃝2J−1
j=1 [0j ⊚0] does not compress all these targets
into a single symbol; character-string 0000, for instance, would be merged into ⟨00, 00⟩by the first
merge 0 ⊚0 in this sequence, and merge 03 ⊚0 would not be applied to it. Thus, we need to describe
a more unwieldy merge sequence to achieve this with the same number of merges. We do so in §D.1,
where we show that exactly 2J −1 merges are required to compress all these strings into a single
symbol. With this, we now prove the forward step of Theorem 3 in the following lemma.
Lemma 3. If a 3-OCC-MAX2SAT instance is satisfiable, then the B-2-TOK instance output by Reduc-
tion 2 is also satisfiable. Formally: 3OM2S(X, C, γ) =⇒Tok2
↑(R1(X, C, γ)).
Proof. Assume this (X, C, γ) instance of the 3-OCC-MAX2SAT decision problem is satisfiable, i.e.,
that 3OM2S(X, C, γ) is true. We must prove that in this case, Tok2
↑(R1(X, C, γ)) is also true. We
define the following list of merges which, as shown in SubLemma 1, compresses every target of type
xT
j or xF
j into a single token:
m1 = (1 ⊚1) ◦⃝⌊log2 J⌋
j=1
[(02j ⊚02j)] ◦⃝⌊log2 J⌋
j=1
⃝2j−1
j′=1 [(02j ⊚0j′)]
|
{z
}
2J −1 merges which compress each xT
j and xF
j to a single token
(38)
Note that the merge 1 ⊚1 is independent of the merges on 0 and could thus be placed at any point in
the sequence. We also define the following lists of merges, which will be included in any satisfying
solution to the tokenisation problem:
m2 = ⃝J
j=1[(11 ⊚xF
j), (xT
j ⊚11)]
m4 = ⃝J
j=1[(xF
j ⊚1), (1 ⊚xT
j)]
(39a)
m6 = ⃝J
j=1[(1 ⊚xF
j), (xT
j ⊚1)]
(39b)
Now, let x ⋆= {x⋆
j}J
j=1 be any satisfying solution to the 3-OCC-MAX2SAT instance (X, C, γ). We
define the following instance-specific merges:
m3 =
J
⃝
j=1
 (1 ⊚xT
j11)
if x⋆
j = T
(11xF
j ⊚1)
else

,
m5 =
J
⃝
j=1
 (1xT
j ⊚1)
if x⋆
j = T
(1 ⊚xF
j1)
else

(40)
In words, we include merges 1 ⊚xT
j11 and 1xT
j ⊚1 if x⋆
j is true, or 11xF
j ⊚1 and 1 ⊚xF
j1 if x⋆
j is
false. We then create a merge sequence by concatenating these lists in order:
m = m1 ◦m2 ◦m3 ◦m4 ◦m5 ◦m6
(41)
This gives us a total of |m| = K = 10J merges. Now we just need to count the symbols output by
this solution to check whether the bound is satisfied.
By applying the merges m, each string in D1 will be compressed into a single symbol, obtaining:
Gℓ(tok↑[m], D1) = (1 + 8J)f
(42)
For each pair of strings 1xT
j1 and 1xF
j1 in D2, one is compressed into a single symbol while the other
is only compressed to two symbols—the one with xT
j is compressed into a single symbol if x⋆
j = T
17

Assignment
Condition
c
tok↑[m1](c)
tok↑[m1 ◦m2](c)
tok↑[m1 ◦m2 ◦m3](c)
tok↑[m1 ◦· · · ◦m4](c)
tok↑[m1 ◦· · · ◦m5](c)
|tok↑[m](c)|
L1
i = Xj and L2
i = ¬Xj′
x⋆
j = T ∧x⋆
j′ = T
⟨1, 0, . . . , 0
| {z }
2j−1
, 1, 0, . . . , 0
| {z }
2j′
, 1⟩
⟨1, xT
j, 1, xF
j′, 1⟩
·
·
⟨1xT
j, 1, xF
j′1⟩
⟨1xT
j1, xF
j′1⟩
2
x⋆
j = F ∧x⋆
j′ = T
·
·
⟨1xT
j, 1, xF
j′1⟩
3
x⋆
j = T ∧x⋆
j′ = F
·
·
⟨1xT
j1, xF
j′1⟩
2
x⋆
j = F ∧x⋆
j′ = F
·
·
⟨1xT
j, 1xF
j′1⟩
2
L1
i = ¬Xj and L2
i = Xj′
x⋆
j = T ∧x⋆
j′ = T
⟨1, 0, . . . , 0
| {z }
2j′−1
, 1, 0, . . . , 0
| {z }
2j
, 1⟩
⟨1, xT
j′, 1, xF
j, 1⟩
·
·
⟨1xT
j′, 1, xF
j1⟩
⟨1xT
j′1, xF
j1⟩
2
x⋆
j = F ∧x⋆
j′ = T
·
·
⟨1xT
j′1, xF
j1⟩
2
x⋆
j = T ∧x⋆
j′ = F
·
·
⟨1xT
j′, 1, xF
j1⟩
3
x⋆
j = F ∧x⋆
j′ = F
·
·
⟨1xT
j′, 1xF
j1⟩
2
L1
i = ¬Xj and L2
i = ¬Xj′
x⋆
j = T ∧x⋆
j′ = T
⟨1, 0, . . . , 0
| {z }
2j
, 1, 0, . . . , 0
| {z }
2j′
, 1⟩
⟨11, xF
j, 1, xF
j′, 1⟩
·
⟨11xF
j, 1, xF
j′1⟩
⟨11xF
j, 1, xF
j′1⟩
·
3
x⋆
j = F ∧x⋆
j′ = T
⟨11xF
j1, xF
j′, 1⟩
⟨11xF
j1, xF
j′1⟩
·
2
x⋆
j = T ∧x⋆
j′ = F
·
⟨11xF
j, 1, xF
j′1⟩
⟨11xF
j, 1xF
j′1⟩
2
x⋆
j = F ∧x⋆
j′ = F
⟨11xF
j1, xF
j′, 1⟩
⟨11xF
j1, xF
j′1⟩
·
2
L1
i = Xj and L2
i = Xj′
x⋆
j = T ∧x⋆
j′ = T
⟨1, 0, . . . , 0
| {z }
2j−1
, 1, 0, . . . , 0
| {z }
2j′−1
, 1⟩
⟨1, xT
j, 1, xT
j′, 11⟩
⟨1, xT
j, 1xT
j′11⟩
⟨1xT
j, 1xT
j′11⟩
⟨1xT
j, 1xT
j′11⟩
·
2
x⋆
j = F ∧x⋆
j′ = T
2
x⋆
j = T ∧x⋆
j′ = F
·
⟨1xT
j, 1, xT
j′11⟩
2
x⋆
j = F ∧x⋆
j′ = F
·
3
Table 1: Performance of merges on strings in D5, adapted from Whittington et al. (2025). The dot
symbol · denotes the string not changing under the given merge.
and the one with xF
j otherwise. The same is true for each pair of strings 1xT
j11 and 11xF
j1, also in D2.
We thus have that, for each variable Xj, the strings in D2 will occupy a total of (1 + 2 + 1 + 2)f ′
symbols, and:
Gℓ(tok↑[m], D2) = 6f ′J
(43)
Similarly, each string in D3 and D4 will be compressed into only 2 symbols after this tokeniser is
applied to it. We thus have:
Gℓ(tok↑[m], D3) = 4f ′′J,
Gℓ(tok↑[m], D4) = 4f ′′′J
(44)
Finally, we have the strings in D5. These strings are constructed such that they will be compressed
into 2 symbols if either L1
i or L2
i evaluates to T, and kept with 3 symbols otherwise; see Tab. 1 for a
detailed simulation of why this is the case. We thus have:
Gℓ(tok↑[m], D5) =
I
X
i=1









3 −1

















L1
i = Xj
and (1 ⊚xT
j11), (1xT
j ⊚1) ∈m
or
L1
i = ¬Xj and (11xF
j ⊚1), (1 ⊚xF
j1) ∈m
or
L2
i = Xj′
and (1 ⊚xT
j′11), (1xT
j′ ⊚1) ∈m
or
L2
i = ¬Xj′ and (11xF
j′ ⊚1), (1 ⊚xF
j′1) ∈m


























(45a)
= 3I −
I
X
i=1
1x ⋆{L1
i ∨L2
i }
(45b)
≤3I −γ
(45c)
where, by construction, we have a merge in our sequence (e.g., 1 ⊚xT
j11 or 11xF
j ⊚1) if and only if
its value is in a satisfying assignment (e.g., x⋆
j = T or x⋆
j = F, respectively). Summing together the
lengths in Eqs. (42) to (44) and (45), we get that:
Gℓ(tok↑[m], D) ≤δ = (1 + 8J)f + (6f ′ + 4f ′′ + 4f ′′′) J + 3 I −γ
(46)
which concludes the proof.
D.1
PROOF THAT EXACTLY 2J −1 MERGES OPTIMALLY COMPRESS THE 2J 0j STRINGS
SubLemma 1. Given character-strings {0j | 1 ≤j ≤2J}, an optimal bottom-up tokeniser requires
exactly 2J −1 merges to encode all these strings into a single token.9
Proof. We establish the result in two steps:
1
we prove that at least 2J −1 merges are required to reduce these strings to a single token;
2
we prove that 2J −1 merges are sufficient to reduce these character-strings to a single token.
9Note the same is true for both optimal direct tokenisers, and optimal OPE tokenisers (defined in §5.2).
18

Given these upper and lower bounds, we conclude exactly 2J −1 merges are required to reduce these
character-strings to a single token. This completes the proof.
SubLemmaProofStep 1. (Step 1 ). Given character-strings {0j | 1 ≤j ≤2J}, an optimal
bottom-up tokeniser requires at least 2J −1 merges to encode all these strings into a single token.
Proof. The target set contains 2J distinct values, one of which is the base symbol 0. Whenever a
target becomes a single token through a merge, that merge must combine exactly two tokens whose
concatenation equals that specific target. Since all targets are distinct, this concatenation cannot
simultaneously equal any other target. Hence, a single merge can complete at most one target. It
follows that at least 2J −1 merges are required to reduce all targets to single tokens.
SubLemmaProofStep 2. (Step 2 ). Given character-strings {0j | 1 ≤j ≤2J}, there exists an
explicit merge sequence that reduces all these strings to single tokens in exactly 2J −1 merges.
Proof. For the matching upper bound, we will construct an explicit merge sequence, composed of
two types of merges:
1. Binary stage. For each power of two up to J—i.e., with j ∈N such that 2j ≤J—
incrementally include binary merges as 02j ⊚02j.
2. Extension stage. For each power of two up to J—i.e., with j ∈N such that 2j ≤J—
incrementally create non-binary merges by merging binary tokens 02j with non-binary ones
0j′ (with j′ ∈N such that j′ < 2j). These merges will thus be 02j ⊚0j′.10
These merges are combined as:
⃝⌊log2 J⌋
j=1
[(02j ⊚02j)]
|
{z
}
binary merges
◦
⃝⌊log2 J⌋
j=1
⃝2j−1
j′=1 [(02j ⊚0j′)]
|
{z
}
extension merges
(47)
Note that, as merges are created incrementally, token 02j always exists when merge 02j⊚02j is applied.
Similarly, for j′ < 2j, both token 02j and 0j′ will exist when merge 02j ⊚0j′ is applied. Finally, for
j′, j′′ < 2j, tokens 0j′ and 0j′′ will both be created before any extension merge with left-side 02j is
applied; thus, a merge 02j ⊚0j′ will never affect subword-string ⟨02j, 0j′′⟩or vice-versa. After these
merges are applied, it is easy to see that each character-string {0j | 1 ≤j ≤2J} will be represented as
a single symbol. As the merge-sequence above contains 2J −1 merges, this completes the proof.
E
PROOF OF BACKWARD STEP OF THEOREM 3
We again start with defining some useful notions and redefine compliant tokenisers.
First, even though this section is about bottom-up tokenisers, we define a term addressing direct
tokenisers, meaning this definition describes tokenisers with tokens instead of merges. We will use this
definition in our lemma to prove a fact about direct tokenisers, which we later generalise by showing
that it applies to bottom-up tokenisers as well. We define a sat-compliant (direct) tokeniser to be
any tokeniser which: (i) contains all tokens of the form 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j;
and (ii) contains either 1xT
j1, 1xT
j11 or 1xF
j1, 11xF
j1 for each j ∈{1, . . . , J}. Otherwise, we call the
tokeniser sat-noncompliant.
We further adapt the definition of 101-strings to also include all character-strings of the form 110+1
and 10+11. Considering the datasets output by Reduction 2, we know that there are no 101-strings
in dataset D1. Further, we know that each unique 101-string appears in datasets D2, D3, and D4
exactly 2f ′, 2f ′′, and 2f ′′′ times, respectively, and exactly 3 times in D5 (this is due to us working
with the three-occurrences variant of MAX2SAT and to the fact that xT
j = 02j−1 and xF
j = 02j). We
now prove the following lemma.
10As we will see in the proof, the smallest non-fully merged value always consists of only two symbols, such
that the “rightmost” tiebreaker is not necessary.
19

Lemma 4. If the B-2-TOK instance output by Reduction 2 is satisfiable, the 3-OCC-MAX2SAT instance
which generated it is as well. Formally: Tok2
↑(R2(X, C, γ)) =⇒3OM2S(X, C, γ).
Proof. Assume this B-2-TOK instance (D, K, δ)—where (D, K, δ) = R2(X, C, γ)—is satisfiable,
i.e., that Tok2
↑(R2(X, C, γ)) evaluates to true. We must prove that, in this case, 3OM2S(X, C, γ)
also evaluates to true. Now, let mopt be an arbitrary optimal solution to (D, K, δ). We know, by
definition, that:
Tok2
↑(R2(X, C, γ)) ⇐⇒

Gℓ(tok↑[mopt], D) ≤δ

(48)
We can thus prove this lemma by showing the following implication:

Gℓ(tok↑[mopt], D) ≤δ

=⇒3OM2S(X, C, γ)
(49)
When comparing two bottom-up tokenisers, things quickly get messy, because we have to not only
consider the merges, but also their order. For this reason, we show that sat-compliant direct tokenisers
can be transformed into bottom-up tokenisers without loss of compression quality. Thus, for the
sat-compliant tokeniser, we can consider the direct tokeniser instead. The key idea for this to work
is that all target strings are hit via a sequence of merges such that each intermediate merge also hits a
target (which has high multiplicity), such that this target must also be included as a token by a direct
tokeniser. We also compare to sat-noncompliant direct tokenisers, which are by definition at least as
strong as bottom-up tokenisers; thus we can compute upper bounds on their performance.
Let Sopt again be the optimal direct tokeniser for (D, K, δ). We will proceed in six steps:
1
we
prove
that
Sopt
must
include
all
tokens
of
the
form
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j;
2
we prove that Sopt must, in addition to the tokens above, only include tokens of the form
1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1;
3
we prove that Sopt may only include, for each j, either token 1xT
j1 or 1xF
j1, and either token
11xT
j1 or 1xF
j11;
4
we prove that Sopt may only include, for each j, either tokens 11xT
j1, 1xT
j1 or 1xF
j1, 1xF
j11
5
we prove that for any sat-compliant Sopt, there exists a merge sequence mopt with the
same performance;
6
finally, we prove that if

Gℓ(tok↑[mopt], D) ≤δ

, we can build a variable assignment
which satisfies this 3-OCC-MAX2SAT instance (X, C, γ).
Note that, together, steps 1 to 4 show that Sopt must be the vocabulary of a sat-compliant direct
tokeniser; in step 5 , we show that we can convert any sat-compliant Sopt to a merge sequence
mopt without losing compression, showing an equivalence between the two types of tokenisers for
these reduced instances; and in step 6 , we will then rely on function g (defined above) to convert
this merge sequence into a satisfying assignment x = g(Sopt) for the instance (X, C, γ).
LemmaProofStep 1. (Step 1 ). An optimal (direct) tokeniser must include all tokens of the form
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j, 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1, i.e.:
n
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j, 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1
oJ
j=1 ⊆Sopt
(50)
Proof. We prove this step by contradiction. Assume there exists an optimal tokeniser with vocabulary
S✗which does not include t > 0 of the tokens above. Now, remove t arbitrarily chosen tokens in this
vocabulary which are not of the form above, and replace them with the missing tokens in this set. We
20

denote this new tokeniser’s vocabulary by S✓. Note that the strings in D1 with these missing tokens
were represented with at least 2 symbols under S✗, but with a single token under S✓, i.e.:
Gℓ(tok	[S✗], D1) ≥(8J + 1 + t)f,
Gℓ(tok	[S✓], D1) = (8J + 1)f
(51)
Further, note that under S✓, we have that strings in dataset D2 are compressed to at most two symbols,
while strings in D3, D4, and D5 are compressed to at most three symbols:
∀c ∈D2 : Gℓ(tok	[S✓], c) ≤2,
∀c ∈D3 ∪D4 ∪D5 : Gℓ(tok	[S✓], c) ≤3
(52)
To improve on this compressed length, S✗must, consequently, compress strings in D2 to a single
symbol, or strings in D3, D4, and D5 to one or two symbols. As before, this can only be done if
the noncompliant tokens in S✗contain 101-strings.11 As discussed above, however, each 101-string
appears, as a prefix or suffix, at most: 2f ′ times in D2, 2f ′′ times in D3, 2f ′′′ times in D4, and 3
times in D5. This gives us a best case scenario—in which all the strings in which a new token appears
are compressed to a single symbol—where:
Gℓ(tok	[S✓], D2 ∪D3 ∪D4 ∪D5)−Gℓ(tok	[S✗], D2 ∪D3 ∪D4 ∪D5)
(53)
≤t(2f ′ + 2(2f ′′ + 2f ′′′ + 3))
As the difference in Eq. (51) is of at least tf tokens, we put these together:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤t(2f ′ + 2(2f ′′ + 2f ′′′ + 3)) −tf
(54)
Since f > 2(2f ′ + 2f ′′ + 2f ′′′ + 3), this difference is smaller than zero, implying that S✓improves
on S✗. This shows a contradiction, which completes our proof.
LemmaProofStep 2.
(Step
2 ).
An optimal tokeniser must include all tokens of
the form 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j,
and further only tokens of the form
1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1, i.e.:
n
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j
oJ
j=1 ⊆Sopt
and
(55)
Sopt ⊂
n
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j, 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1
oJ
j=1
Proof. As before, we prove this step by contradiction. Given step 1 , we know that an opti-
mal tokeniser includes all tokens 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j. Now, assume there
exists an optimal tokeniser with vocabulary S✗with t > 0 tokens which are not of the form
11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j or 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1; we will call these tokens
non-compliant here.
Choose an arbitrary set of t unused compliant tokens—i.e., with form
1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1—to replace the non-compliant tokens with, forming a new tokeniser’s
vocabulary S✓. Both these vocabularies compress strings in D1 equally:
Gℓ(tok	[S✗], D1) = (8J + 1)f,
Gℓ(tok	[S✓], D1) = (8J + 1)f
(56)
For strings in D2: if the entire string is in the vocabulary, it is encoded as a single token; else, it is
represented with two symbols. Under S✓, there are 2J tokens covering strings in D2. Under S✗,
there are only (2J −t) tokens covering strings in D2. This implies:
Gℓ(tok	[S✗], D2) = (6J + t)f ′,
Gℓ(tok	[S✓], D2) = 6Jf ′
(57)
For strings in D3, D4, and D5, an argument similar to the previous step applies: (i) only tokens
containing 101-strings can compress these datasets; (ii) each 101-string appears, as a prefix or suffix,
at most 2f ′′ + 2f ′′′ + 3 times in them; (iii) each 101-string will lead to at most two symbols being
saved. As S✗differs from S✓in t tokens, we get that it will improve on it by at most:
Gℓ(tok	[S✓], D3 ∪D4 ∪D5) −Gℓ(tok	[S✗], D3 ∪D4 ∪D5) ≤2t(2f ′′ + 2f ′′′ + 3)
(58)
Summing together the compression on all datasets, we get that their difference is bounded by:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤2t(2f ′′ + 2f ′′′ + 3) −tf ′
(59)
As f ′ > 2(2f ′′ + 2f ′′′ + 3), this difference is smaller than zero, implying that S✓improves on S✗.
This shows a contradiction, which completes our proof.
11This follows the same argument as in the proof of Lemma 2. Note that the extension of 101-strings to
include strings of the form 110+1 and 10+11 does not break the argument, as the substring has to appear as a
prefix or suffix to yield a saving. Thus, even though the strings of form 10+1 are included in the new strings, we
do not have to count those occurrences.
21

LemmaProofStep 3.
(Step
3 ).
An optimal tokeniser must contain all tokens of
the form 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j
and further only tokens of the form
1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1, and for each 1 ≤j ≤J, it must contain exactly one of 1xT
j1, 1xF
j1,
and exactly one of 1xT
j11, 11xF
j1.
Proof. As before, we prove this step by contradiction. Given step 1 , we know an optimal tokeniser
includes all tokens 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j. Further, given step 2 , we know its
other tokens all have form 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1. Now, assume there exists an optimal tokeniser
with vocabulary S✗which includes both tokens in a pair 1xT
j1, 1xF
j1 or 11xT
j1, 1xF
j11 for t > 0 such
pairs, and thus neither of those two for t > 0 other such pairs. Then, define S✓as a vocabulary
where the 1xF
j1 respectively 1xF
j11 token of all t doubly assigned pairs are replaced with the 1xT
j1
respectively 11xT
j1 token of all uncovered pairs.
These two tokenisers achieve the same compression on D1 and D2:
Gℓ(tok	[S✗], D1 ∪D2) = (8J + 1)f + 6Jf ′,
Gℓ(tok	[S✓], D1 ∪D2) = (8J + 1)f + 6Jf ′
(60)
The tokeniser with vocabulary S✓will then compress each string in D3 to 2 symbols, while S✗will
compress the t strings of the form 1xT
j1xF
j1 or 11xF
j1xT
j11 for which their respective pair 1xT
j1, 1xF
j1
or 11xT
j1, 1xF
j11 is uncovered, to 3 symbols. This will lead to a total compression of:
Gℓ(tok	[S✗], D3) = (4J + t)f ′′,
Gℓ(tok	[S✓], D3) = 4Jf ′′
(61)
Finally, the t doubly assigned tokens (which S✓does not contain) appear at most f ′′′ times in D4
and three times in D5, and will lead to at most one symbol being saved, leading to a bound:
Gℓ(tok	[S✓], D4 ∪D5) −Gℓ(tok	[S✗], D4 ∪D5) ≤t(f ′′′ + 3)
(62)
Putting these compressed lengths together, we get:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤t(f ′′′ + 3) −tf ′′
(63)
As f ′′ > f ′′′ + 3, this difference is smaller than zero, implying that S✓improves on S✗. This shows
a contradiction, which completes our proof.
LemmaProofStep 4. (Step 4 ). An optimal tokeniser must be sat-compliant: it must contain
all tokens of the form 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j and further only tokens of the
form 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1, and for each 1 ≤j ≤J, it must include either 1xT
j1, 1xT
j11 or
1xF
j1, 11xF
j1.
Proof. As before, we prove this step by contradiction. Given step 1 , we know that an optimal
tokeniser includes all tokens 11, xT
j, xF
j, 1xT
j, xT
j1, 1xF
j, xF
j1, xT
j11, 11xF
j. Further, given step 2 , we
know that its other tokens all have form 1xT
j1, 1xF
j1, 1xT
j11, 11xF
j1. Finally, given step 3 , we know
that it contains exactly one token for each pair 1xT
j1, 1xF
j1 and 1xT
j11, 11xF
j1.
Now, assume there exists an optimal tokeniser with vocabulary S✗which includes both tokens in a
pair 1xF
j1, 1xT
j1 or 1xT
j11, 11xF
j1 for t > 0 such pairs, and thus neither of those two for t > 0 other
such pairs. Then, define S✓as a vocabulary where the 1xF
j1 respectively, 11xF
j1) token of all t doubly
assigned pairs are replaced with the 1xT
j1 (respectively, 1xT
j11) token of all uncovered pairs. These
two tokenisers achieve the same compression on D1, D2, and D3:
Gℓ(tok	[S✗], D1 ∪D2 ∪D3) = (8J + 1)f + 6Jf ′ + 4Jf ′′
(64)
Gℓ(tok	[S✓], D1 ∪D2 ∪D3) = (8J + 1)f + 6Jf ′ + 4Jf ′′
(65)
The tokeniser with vocabulary S✓will then compress each string in D4 to 2 symbols, while S✗will
only compress the t strings of the form 1xF
j1xT
j11 or 11xF
j1xT
j1, for which the pair is uncovered, to 3
symbols. This will lead to a total compression of:
Gℓ(tok	[S✗], D4) = (4J + t)f ′′′,
Gℓ(tok	[S✓], D4) = 4Jf ′′′
(66)
22

Finally, the t doubly assigned tokens of the form 1xF
j1 or 11xF
j1 (which S✓does not contain) appear,
as a prefix or suffix, at most three times in D5, and will lead to at most one symbol being saved,
leading to a bound:
Gℓ(tok	[S✓], D5) −Gℓ(tok	[S✗], D5) ≤3t
(67)
Putting these compressed lengths together, we get:
Gℓ(tok	[S✓], D) −Gℓ(tok	[S✗], D) ≤3t −tf ′′′
(68)
As f ′′′ > 3, this difference is smaller than zero, implying that S✓improves on S✗. This shows a
contradiction, which completes our proof.
LemmaProofStep 5. (Step 5 ). A sat-compliant direct tokeniser can be transformed into a bottom-
up tokeniser without changing its performance. As any optimal direct tokeniser is sat-compliant,
this implies:
Tok2
↑(R2(X, C, γ)) ⇐⇒Tok2
	(R2(X, C, γ))
(69)
Proof. As every bottom-up tokeniser can be interpreted as a direct tokeniser with the same vocabulary
size and a possibly suboptimal application of its tokens, it holds that:
Tok2
↑(R2(X, C, γ)) =⇒Tok2
	(R2(X, C, γ))
(70)
This is to say, direct tokenisers always compress at least as well as bottom-up tokenisers, when
allowed the same vocabulary size.
Given a sat-compliant direct tokeniser, we first describe how to transform it into a bottom-up
tokeniser. We always include merges:
m1 = (1 ⊚1) ◦⃝⌊log(2J−1)⌋
j=1
[(02i ⊚02i)] ◦⃝⌊log(2J−1)⌋
j=1
⃝2j−1
j′=1 [(02j ⊚0j′)]
(71a)
m2 = ⃝J
j=1[(11 ⊚xF
j), (xT
j ⊚11)]
(71b)
m4 = ⃝J
j=1[(xF
j ⊚1), (1 ⊚xT
j)]
(71c)
m6 = ⃝J
j=1[(1 ⊚xF
j), (xT
j ⊚1)]
(71d)
and additionally, depending on which tokens are included in the direct tokeniser’s vocabulary S:
m3 =
J
⃝
j=1
 (1 ⊚xT
j11)
if 1xT
j11 ∈S
(11xF
j ⊚1)
else

,
(71e)
m5 =
J
⃝
j=1
 (1xT
j ⊚1)
if 1xT
j1 ∈S
(1 ⊚xF
j1)
else

(71f)
Note that since the tokeniser is sat-compliant, we have
1xT
j11 ∈S ⇐⇒1xT
j1 ∈S
(72)
It is easy to verify that the resulting bottom-up tokeniser has the same performance on datasets D1
to D4. For D5, Tab. 1 shows that each string which could be reduced to two symbols by the direct
tokeniser is also reduced to two symbols by the bottom-up tokeniser. Thus, we get:
Tok2
↑(R2(X, C, γ)) ⇐⇒Tok2
	(R2(X, C, γ))
(73)
which completes this proof.
LemmaProofStep 6. (Step 6 ). If an optimal (direct) tokeniser achieves a compressed length of at
most 5398J + 575 + 3I −γ, the original 3-OCC-MAX2SAT instance is satisfiable, i.e.:

Gℓ(tok	[Sopt], D) ≤5398J + 575 + 3I −γ

=⇒3OM2S(X, C, γ)
(74)
23

Proof. Given steps 1 to 4 , we know that an optimal tokeniser will be sat-compliant. We will
now denote this optimal tokeniser’s vocabulary by Sopt and use Eq. (15) to extract a 3-OCC-MAX2SAT
assignment x ⋆= g(Sopt) which corresponds to this tokeniser’s vocabulary. From the previous proof
steps, we know that any sat-compliant tokeniser achieves the following compressed length in D1,
D2, D3, and D4:
Gℓ(tok	[Sopt], D1 ∪D2 ∪D3 ∪D4) = (8J + 1)f + 6Jf ′ + 4Jf ′′ + 4Jf ′′′
(75a)
= (8 · 575 + 6 · 115 + 4 · 23 + 4 · 4)J + 575
(75b)
= 5398J + 575
(75c)
Now, note that any target string in D5 will be: (i) compressed to two symbols either if L1
i or L2
i
is Xj and tokens 1xT
j1 and 1xT
j11 exist or if L1
i or L2
i is ¬Xj and tokens 1xF
j1 and 11xF
j1 exist;
or (ii) compressed to three symbols if neither case is satisfied. Equivalently, a clause L1
i ∨L2
i in
3-OCC-MAX2SAT is: satisfied if either L1
i or L2
i evaluates to true; or not satisfied if both evaluate to
false. Given our construction of function g above, one of 3-OCC-MAX2SAT’s clauses will be satisfied
if and only if its corresponding string in D4 is compressed to two symbols. We can thus state that:

Gℓ(tok	[Sopt], D5) = 3I −γ⋆

⇐⇒
 I
X
i=1
1x ⋆{L1
i ∨L2
i } = γ⋆
!
(76)
Given the construction of δ as 5398J + 575 + 3I −γ, we conclude that a sat-compliant tokeniser
which compresses the full dataset to at most that size can be mapped to a 3-OCC-MAX2SAT assignment
which satisfies at least γ clauses. This concludes the proof.
F
PROOF OF THEOREM 4
Theorem 4. The binary bottom-up tokenisation gap problem is NP-hard. Thus, the binary bottom-up
tokenisation optimisation problem is not in PTAS, unless P = NP.
Proof. For this proof, we again rely on the result by Berman and Karpinski (1998; 1999) that, for
specific instances of 3-OCC-MAX2SAT with I = 2016n clauses, it is NP-hard to distinguish whether
at least (2012 −ε)n or at most (2011 + ε)n of these clauses are satisfiable, for any ε > 0. We
will denote this 3-OCC-MAX2SAT gap problem by 3OM2S(X, C, (γ−, γ+)), with γ−= (2011 + ε)n
and γ+ = (2012 −ε)n. We can now prove the NP-hardness of the binary bottom-up tokenisation
gap problem by reducing 3-OCC-MAX2SAT’s gap problem to it. To this end, we rely on a reduction
identical to R2(X, C, γ), but where we define:
δ−= 5398J + 575 + 3I −γ−
δ+ = 5398J + 575 + 3I −γ+
(77)
= 5398J + 575 + 3I −2011 + ε
2016
I
= 5398J + 575 + 3I −2012 −ε
2016
I
Lemmas 3 and 4 trivially show the validity of this reduction:
3OM2S(X, C, (γ−, γ+)) ⇐⇒Tok2
↑(D, K, (δ−, δ+))
(78)
which holds since 3OM2S(X, C, γ+) ⇐⇒Tok2
↑(D, K, δ+) and the same for γ−and δ−. It is
therefore NP-hard to distinguish whether a dataset can be compressed to at most 5398J +575+3I −
2012−ε
2016 I symbols, or if at least 5398J +575+3I −2011+ε
2016 I symbols remain (with an allowed vocab-
ulary size K = 10J). Since each variable occurs exactly three times in any 3-OCC-MAX2SAT instance,
we have that 3
2J = I. We now compute a lower bound on the best achievable compression ratio:
δ−
δ+ = 5398J + 575 + 3I −2011+ε
2016 I
5398J + 575 + 3I −2012−ε
2016 I
(79a)
= 10805J + 575 −6033+3ε
2016
J
10805J + 575 −6036−3ε
2016
J
(79b)
≥10805 −6033+3ε′
2016
10805 −6036−3ε′
2016
additive 575 omitted in ε′ for sufficiently large J
(79c)
= 7258949 −ε′
7258948 + ε′
(79d)
We conclude that binary bottom-up tokenisation cannot be approximated in polynomial time with
an approximation ratio better than 7258949
7258948 > 1.0000001, unless P = NP.
24

G
PROOF THAT UNARY DIRECT TOKENISATION IS IN NP
A decision problem is in the nondeterministic polynomial-time class (NP) if it can be verified in
polynomial time in the presence of a certificate: a string designed to verify that the current instance
is a “yes”-instance, typically encoding an optimal solution to its search problem. When inputs are
represented as strings, the following lemma follows trivially from the unbounded-alphabet case
discussed by Whittington et al. (2025). Their proof, however, relies on the explicit computation of
the direct tokenisation function:
tok	[S](c) = arg min
s∈S∗
|s|,
s.t. c
◦=s
(80)
While we can efficiently compute this when inputs are given in string form, this is not known
to be the case for the string-length representation. In this case, the optimal application of tokens
corresponds to the change-making problem, which is itself weakly NP-hard, as shown by Lueker
(1975). Thus, we now include the optimal application of tokens as a part of the certificate, in order
to prove that this decision problem is still in NP when inputs are represented as string-lengths.
Lemma 5. The unary direct tokenisation decision problem is in NP.
Proof. We use as certificate a set of string-lengths composing the tokeniser’s vocabulary SN, as well
as a set Z = {zm}M
m=1, where each zm ∈N|SN| shows how many tokens of each length should
be used to tokenise each string in the dataset DN. Verifying this certificate then simply requires
computing the sum of tokens used for each target.
If |DN| ≤K, each ℓ∈DN can be included as a token, and thus all entries in our dataset can be
compressed into single token; consequently, the certificate can simply be empty and we verify the prob-
lem’s satisfiability by checking whether δ ≥|DN| holds. Assuming |DN| > K—and therefore that
K’s value is polynomial in the input—we have that the certificate also has polynomial length, and that,
in particular, the size of Z is bounded by |DN| K log ℓmax, where ℓmax is the maximum string-length
in DN. Thus, all that is left is to compute the sum of Z and check whether P
z∈Z sum(z) ≤δ.
H
PROOF OF FORWARD STEP OF THEOREM 5
Lemma 6. If a vertex-cover instance is satisfiable, then the D-1-TOK instance output by Reduc-
tion 3 is also satisfiable. Formally: VC(V, E, ψ) =⇒Tok1
	(R3(V, E, ψ)).
Proof. Suppose the given instance of vertex-cover is satisfiable. Then there exists a vertex cover
C⋆⊆V which uses ψ vertices. As a consequence, we can choose as tokens those with the following
lengths:
SN = {ℓj | vj ∈V} ∪{B} ∪{ℓ′
j | vj ∈C⋆}
(81)
We have that every vertex-string in D1 is covered by a single token, either of length ℓj or B.
All ψ cover-strings aℓ′
j in D2 which encode a vertex that belongs to C⋆are also covered by a single
token. The remaining J −ψ cover-strings in D2 are covered by 2 tokens, as for every target of length
ℓ′
j = enc
 vj

+ N 3 there exist the tokens of length ℓj = enc
 vj

and B = N 3.
As C⋆is a vertex cover, we have that for every edge (vj, vj′) ∈E at least one of the vertices belongs
to C⋆. It follows that for every edge-string of length ℓ′′
j,j′ = enc
 vj

+ enc
 vj′
+ N 3 in D3, at
least one of the tokens of length ℓ′
j = enc
 vj

+ B or ℓ′
j′ = enc
 vj′
+ B belongs to SN. Thus, all
edge-strings are covered by two tokens.
We can now count the number of symbols used in each dataset: D1 uses J + 1 symbols; D2 uses
2J −ψ symbols; and D3 uses 2I symbols. This gives us a total of 3J + 2I + 1 −ψ = δ symbols,
which satisfies this tokenisation instance. Thus, we have that Tok1
	(D, K, δ) = T.
I
PROOF OF BACKWARD STEP OF THEOREM 5
Lemma 7. If the D-1-TOK instance output by Reduction 3 is satisfiable, then the vertex-cover
instance which generated it is as well. Formally: Tok1
	(R3(V, E, ψ)) =⇒VC(V, E, ψ).
25

Proof. Assume this instance (DN, K, δ) of D-1-TOK—where (DN, K, δ) = R3(V, E, ψ)—is satisfi-
able, i.e., that Tok1
	(R3(V, E, ψ)) evaluates to true. We must prove that, in this case, VC(V, E, ψ)
also evaluates to true. Now, let SN be an arbitrary optimal solution to (DN, K, δ). By construction,
we have that K = J + 1 + ψ and δ = 3J + 2I + 1 −ψ. We proceed in four steps:
1
We prove that all target strings in DN are unique;
2
We prove that an optimal tokeniser must only include full target strings in its vocabulary;
3
We prove that an optimal tokeniser will include all target strings in D1 in its vocabulary;
4
We prove that, if an optimal tokeniser achieves compression δ, then the instance of
vertex-cover which was reduced to it is satisfiable.
These steps first show that an optimal tokeniser must admit a certain form (Steps 1 – 3 ), and that
from this form (Step 4 ), we can deduce a valid vertex cover, which concludes the proof.
LemmaProofStep 1. (Step 1 ). All strings in DN are unique.
Proof. Now we show that all strings in DN are unique. These strings all have lengths:
ℓj1 = enc
 vj1

,
B = N 4,
ℓ′
j1 = enc
 vj1

+ B,
ℓ′′
j1,j2 = enc
 vj1

+ enc
 vj2

+ B (82)
for 1 ≤j1, j2 ≤J and with enc
 vj1

= j1 + j2
1N + j3
1N 2. Notably, our reduction defines N ≫J3
and it will be useful to think about these lengths in base N. Let a number (a, b, c, d, e)N denote
aN 4 + bN 3 + cN 2 + dN 1 + e. For example, we can write:
(a, b, c, d, N −1)N + (a, 0, 0, 0, 1)N = (2a, b, c, d + 1, 0)N
(83)
We can similarly write in this base:
ℓj1 = (0, 0, j3
1, j2
1, j1)N,
B = (1, 0, 0, 0, 0)N,
ℓ′
j1 = (1, 0, j3
1, j2
1, j1)N,
(84a)
ℓ′′
j1,j2 = (1, 0, j3
1 +j3
2, j2
1 +j2
2, j1+j2)N,
ℓ′
j1 + ℓ′
j2 = (2, 0, j3
1 +j3
2, j2
1 +j2
2, j1+j2)N
(84b)
Two numbers are the same only if each “digit” in this base system is the same. Given this structure,
we see that ℓj1 and B are all unique string-lengths. Further, the string-lengths ℓ′
j1 are all different
from one another. It is left to show that: (i) all string-lengths ℓ′
j1 are different from all ℓ′′
j1,j2;
and (ii) that string-lengths ℓ′′
j1,j2 are different among themselves. Requirement (i) is proven by
SubLemma 2, which shows that there is no set of numbers j1, j2, j3 ∈N for which j1 = j2 + j3
and j2
1 = j2
2 + j2
3. Requirement (ii) is proven by SubLemma 3, which shows that there is no set of
numbers j1, j2, j3, j4 ∈N for which j1 + j2 = j3 + j4 and j2
1 + j2
2 = j2
3 + j2
4. It follows that all
strings in DN are unique.
LemmaProofStep 2. (Step 2 ). An optimal tokeniser must only include full character-strings in DN
and compress all other strings to two symbols.
Proof. Note that, since all strings in DN are unique, the best compression one could possibly achieve
would result from compressing K strings into a single symbol, and the remaining |DN| −K to two
symbols. As |DN| = 2J + I + 1, this (hypothetical) optimal compression would lead to:
Gℓ(Sopt, DN) = K + 2(|DN| −K)
(85a)
= J + 1 + ψ + 2(2J + I + 1 −J −1 −ψ)
(85b)
= 3J + 2I + 1 + ψ −ψ) = δ
(85c)
As by assumption Tok1
	(R3(V, E, ψ)) evaluates to true, our tokeniser must achieve this compression,
and is thus composed of K full strings in DN. Further, it must compress all other strings to at most
two symbols.
LemmaProofStep 3. (Step 3 ). An optimal tokeniser selects every vertex-string in D1 as a token.
26

Proof. Suppose that some vertex-string of length ℓj1 in D1 is not chosen as a token. Then ℓj1 must
be the sum of two tokens. No tokens of cover- or vertex-strings (in datasets D2 and D3) can be
used, since such tokens contain a summand B, which is significantly larger than ℓj1. Hence, both
summands would have to also be vertex-strings ℓj2, ℓj3. These string-lengths have values:
ℓj1 = (1, 0, j3
1, j2
1, j1)N,
ℓj2 = (1, 0, j3
2, j2
2, j2)N,
ℓj3 = (1, 0, j3
3, j2
3, j3)N
(86)
Again, by SubLemma 2, it is impossible that ℓj1 = ℓj2 + ℓj3. Thus, no target string in D1 can be
covered by two other tokens; but, as argued in Step 2 , the tokeniser may use at most two symbols per
target. This concludes the proof that all character-strings in D1 must be included in the vocabulary
SN. Further, every cover and edge-string is larger than B, while all vertex-strings are significantly
smaller than it; B thus cannot be written as two other tokens and must hence also be part of SN.
LemmaProofStep 4. (Step 4 ). If an optimal tokeniser achieves compression δ, then the original
vertex-cover instance is satisfiable.
Proof. Step 3 shows that J +1 tokens in any optimal tokeniser must correspond to the target strings
in D1. Using only these tokens, every target in D2 needs two symbols, and every target in D3 needs
three symbols. With step 2 , the remaining ψ tokens must correspond to target strings from D2 ∪D3.
We are going to show that: a token corresponding to an edge target can only contribute to itself; a
token corresponding to a cover target can only contribute to itself and edge targets which include
the vertex the cover consists of. Without loss of generality, let t with 0 ≤t ≤ψ of these remaining
tokens be edge-strings (from D3) and let the remaining ψ −t be cover-strings (from D2).
We are now going to show that a token from D3 can only improve the solution by reducing its
target string to a single token. Pick any of the selected edge tokens, having a length of ℓ′′
j1,j2 =
(1, 0, j3
1 +j3
2, j2
1 +j2
2, j1+j2)N. For this edge token to contribute to compressing a target string, it
must be combined with another token; let its length be ℓ⊕, such that their sum equals the length
of one of the target strings already present in the dataset. Since ℓ′′
j1,j2 already contains a summand
B, the token ℓ⊕cannot contain B, as any target string length in the dataset is strictly less than 2B.
As established in step 2 , tokens must correspond to target string-lengths themselves. Since any
non-vertex target string contains a summand B, the additional token ℓ⊕must therefore be a vertex
token, with length ℓ⊕= (0, 0, j3
⊕, j2
⊕, j⊕)N. Now, assume that the edge token ℓ′′
j1,j2 is combined
with this vertex token ℓ⊕to compress an arbitrary target from one of the three datasets:
ℓj3 = ℓ′′
j1,j2 + ℓ⊕⇐⇒
(87a)
(0, 0, j3
3, j2
3, j3)N = (1, 0, j3
⊕+ j3
1 + j3
2, j2
⊕+ j2
1 + j2
2, j⊕+ j1 + j2)N
ℓ′
j3 = ℓ′′
j1,j2 + ℓ⊕⇐⇒
(87b)
(1, 0, j3
3, j2
3, j3)N = (1, 0, j3
⊕+ j3
1 + j3
2, j2
⊕+ j2
1 + j2
2, j⊕+ j1 + j2)N
ℓ′′
j3,j4 = ℓ′′
j1,j2 + ℓ⊕⇐⇒
(87c)
(1, 0, j3
3 + j3
4, j2
3 + j2
4, j3 + j4)N = (1, 0, j3
⊕+ j3
1 + j3
2, j2
⊕+ j2
1 + j2
2, j⊕+ j1 + j2)N
The first case clearly cannot be satisfied, as any vertex target has length strictly smaller than any edge
target. Additionally, the two other cases cannot be satisfied per SubLemmas 4 and 5. In other words,
this shows that edge-strings cannot contribute to any other target value.
We are now left with ψ −t tokens formed of cover-strings. Recall that using only the tokens obtained
from step 3 , every target in D2 needs two symbols, and every target in D3 needs three symbols.
Having the newly obtained tokens corresponding to a target from D2 we will show that they can only
be applied optimally on: themselves, resulting in one symbol used; target strings from D3, reducing
the symbols to two. Any other application of these tokens would not yield a better compression,
as improving compression is only possible if the token obtained from D2 compresses a target from
D2 ∪D3 other than itself to a single token. But step 1 shows that all target strings are unique,
meaning a token cannot reduce any target string apart from itself to a single symbol.
Finally, these selected tokens must be used, in conjunction with vertex-strings, to compress all the
remaining edge-strings to two tokens. Note that only composing a cover and a vertex-string can
compress an edge-string to two symbols:
ℓj1 + ℓj2 = (0, 0, j3
1 +j3
2, j2
1 +j2
2, j1+j2)N
(88a)
27

ℓ′
j1 + ℓ′
j2 = (2, 0, j3
1 +j3
2, j2
1 +j2
2, j1+j2)N
(88b)
ℓ′
j1 + ℓj2 = (1, 0, j3
1 +j3
2, j2
1 +j2
2, j1+j2)N
(88c)
Additionally, an edge-string can only be compressed by a cover- or vertex-string subword which
contains exactly the vertices that the edge consists of. Assume there exists another set of vertex-
and cover-strings such that their tokens ℓj1, ℓj2 can compress an edge-string consisting of different
vertices ℓ′′
j3,j4. Then we have that:
ℓ′′
j3,j4 = ℓj1 + ℓj2 ⇐⇒(1, 0, j3
3 + j3
4, j2
3 + j2
4, j3 + j4)N = (1, 0, j3
1 + j3
2, j2
1 + j2
2, j1 + j2)N
(89)
From SubLemma 3 it follows that this cannot be the case. This means that, for each edge-string of
length ℓ′′
j1,j2 not in our tokeniser, we must have a subword (of length either ℓ′
j1 or ℓ′
j2) which “covers”
it to obtain our target compression. Consider thus, the subgraph (V, E′), where:
E′ = E \ {(vj1, vj2) ∈E | ℓ′′
j1,j2 /∈SN}
(90)
There exists a vertex cover of size ψ −t for this subgraph composed of vertices {vj ∈V | ℓ′
j ∈SN}.
Now, if we expand this set of ψ −t vertices by picking one arbitrary vertex, vj1 or vj2, for each edge-
string of length ℓ′′
j1,j2 in our vocabulary, we get a cover C = {vj ∈V | ℓ′
j ∈SN}∪{vj1 | ℓ′′
j1,j2 ∈SN}
of size at most ψ for the original graph. Thus, it follows that VC(V, E, ψ) = T.
I.1
PROOFS THAT STRING-LENGTHS IN REDUCTION 3 ARE UNIQUE
We now show the technical sublemmas used in the previous proof.
SubLemma 2. For any r ∈N, there do not exist non-zero i, j ∈N such that:
i + j = r,
i2 + j2 = r2
(91)
Proof. From (i + j)2 = i2 + 2ij + j2 and the equations i + j = r and i2 + j2 = r2, we obtain:
r2 = i2 + j2 + 2ij = r2 + 2ij ⇒ij = 0,
(92)
contradicting i, j > 0.
SubLemma 3. There do not exist two distinct pairs {i, j} ̸= {a, b} of positive integers such that:
i + j = a + b,
i2 + j2 = a2 + b2
(93)
Proof. Due to i + j = a + b, there is an integer s such that:
a = i −s,
b = j + s
(94)
Then, replacing a and b with their corresponding expressions from Eq. (94), we obtain:
a2 + b2 −(i2 + j2) = (i −s)2 + (j + s)2 −i2 −j2 = 2s2 + 2s(j −i).
(95)
By the lemma statement a2 + b2 = i2 + j2, so 2s2 + 2s(j −i) = 0, i.e.:
s
 s + j −i

= 0.
(96)
Hence either s = 0 or s = i−j. If s = 0, then a = i and b = j. If s = i−j, then a = i−(i−j) = j
and b = j + (i −j) = i. In both cases, it follows that {a, b} = {i, j}, contradicting distinctness.
SubLemma 4. For any r ∈N, there do not exist non-zero i, j, k ∈N such that:
i + j + k = r,
i2 + j2 + k2 = r2
(97)
Proof. Using (i + j + k)2 = i2 + j2 + k2 + 2(ij + ik + jk) and the given equations:
r2 = r2 + 2(ij + ik + jk) =⇒ij + ik + jk = 0.
(98)
With i, j, k > 0, each product ij, ik, jk is positive, which yields a contradiction. Hence, no solution
exists.
28

SubLemma 5. Let r, p ∈N. There do not exist non-zero i, j, k ∈N such that:
i + j + k = r + p,
i2 + j2 + k2 = r2 + p2,
i3 + j3 + k3 = r3 + p3.
(99)
Proof. Let p1 = i+j+k, p2 = i2+j2+k2, p3 = i3+j3+k3, and e1 = i+j+k, e2 = ij+ik+jk,
e3 = ijk. From the first two equations:
e1 = r + p,
e2 = p2
1 −p2
2
= (r + p)2 −(r2 + p2)
2
= rp.
(100)
Newton’s identity for three variables gives:
p3 = e1p2 −e2p1 + 3e3.
(101)
Substituting p1 = r + p, p2 = r2 + p2, e2 = rp yields:
p3 = (r + p)(r2 + p2) −rp(r + p) + 3e3 = (r + p)
 r2 + p2 −rp

+ 3e3 = r3 + p3 + 3e3.
(102)
By the third equation in the lemma statement, p3 = r3 + p3; hence 3e3 = 0 and so e3 = ijk = 0,
contradicting i, j, k > 0.
J
DEFINITION OF THE ADDITION CHAIN PROBLEM
An addition chain is a sequence of integers that provides an efficient way to “build” a target set of
numbers starting from 1.
Definition 4. Let t = {t1, t2, . . . , tJ} be a finite set of positive integer targets. An addition chain
for t is a sequence of integers b = ⟨b0, b1, . . . , bR⟩with the following properties:
1. The sequence starts with b0 = 1.
2. Every subsequent element bi is the sum of two preceding elements:
bi = bj + bk,
for some 0 ≤k ≤j < i.
(103)
3. The sequence contains all targets: for every tj ∈t, there is some br ∈b such that tj = br.
4. By convention, the length of the chain b is R.
Definition 5. Let t be a set of positive integers. Given a maximum length ζ, the addition chain
decision problem (add-chain) requires deciding whether there exists an addition chain for t with
length at most ζ. The addition chain optimisation problem is to find the minimal length for such an
addition chain.
We denote by AddChain(t, ζ) a function which returns T if such a chain exists (meaning the
add-chain instance is satisfied), and F otherwise.
K
PROOF THAT THE UNARY OPE DECISION PROBLEM IS (AT LEAST)
WEAKLY NP-COMPLETE
Theorem 6. The unary optimal pair encoding decision problem is (at least) weakly NP-complete.
Proof. We write Tok1
OPE(D, K, δ) for a function which returns T if its input corresponds to a satisfiable
instance of the OPE-1-TOK decision problem, and F otherwise. To prove weak NP-completeness, we
must show that the problem is in NP and that it is weakly NP-hard. Inclusion in NP was already
established by Kozma and Voderholzer (2024). We prove weak NP-hardness via a reduction from the
add-chain decision problem. First, we define this reduction.
Reduction 4. Given an instance of add-chain consisting of a targets t = {t1, . . . , tJ} and a length
limit ζ, we construct an instance of the OPE-1-TOK problem with the following parameters. The
dataset D is the set of unary strings corresponding to the targets: D = {at1, at2, . . . , atJ}; the merge
budget is set to the addition chain length limit: K = ζ; and the token count threshold is set to the
number of targets δ = J.
29

Note that setting the threshold δ to the number of strings in the dataset implies that a valid solution
must represent every string as a single token. The proof proceeds in two parts, showing both directions
of the equivalence:
AddChain(t, ζ) ⇐⇒Tok1
OPE(D, K, δ)
(104)
Forward Step (AddChain(t, ζ)
=⇒
Tok1
OPE(D, K, δ)).
We first show that a solution to the
add-chain problem implies a solution to the OPE-1-TOK problem. Assume there exists a valid
addition chain b⋆= ⟨b0, b1, . . . , bR⟩of length R ≤ζ for the target set t. By definition, for each
element b⋆
r ∈b⋆(where r ≥1), there exist indices r′, r′′ < r such that b⋆
r = b⋆
r′ +b⋆
r′′. We construct a
merge sequence m = ⟨m1, . . . , mR⟩of length R where each merge is defined as mr = (ab⋆
r′ ⊚ab⋆
r′′ ),
corresponding to the predecessors of br in the addition chain. The length of this merge sequence
m is R ≤ζ, satisfying the merge budget K = ζ. By the iterative definition of the merge-extracted
vocabulary, the resulting vocabulary Sm = {ab⋆
r′+b⋆
r′′ | mr ∈m} will contain a token ab⋆
r for every
element b⋆
r in the addition chain b⋆. Since the addition chain b⋆contains all targets tj ∈t, the
vocabulary Sm is guaranteed to contain a single token for each target string atj ∈D. Consequently,
the direct encoding function tok	[Sm] can represent each string c ∈D with exactly one token. The
total token count is therefore:
X
c∈D
|tok	[Sm](c)| =
X
c∈D
1 = |D|
(105)
By the construction used in our reduction, |D| = δ. The condition is met, thus proving the implication.
Backward Step (Tok1
OPE(D, K, δ) =⇒AddChain(t, ζ)).
Next, we show that a solution to the
OPE-1-TOK problem implies a solution to the add-chain problem. Assume there exists a merge
sequence mopt of length K = R ≤ζ that satisfies the OPE-1-TOK decision problem. The condition is:
X
c∈D
|tok	[Sm](c)| ≤δ
(106)
where, by the reduction’s construction, we have δ = |D|. Since the tokenisation of any string must
contain at least one token, this sum is also lower-bounded by |D|. Therefore, the inequality must
hold with equality, which is only possible if every string is tokenised into exactly one token:
|tok	[Sm](c)| = 1
for all c ∈D.
(107)
This implies that for every target tj ∈t, the corresponding string atj must exist as a single token
in the merge-extracted vocabulary, i.e., atj ∈Sm. Now, construct an addition chain from Sm as:
b = [ℓr | aℓr ∈Sm]. The construction of Sm from merges guarantees that b is a valid addition
chain.12 Since Sm contains all strings atj, then b must contain all targets tj ∈t. Further, b was
constructed from K ≤ζ merges. There is thus an addition chain for t of length at most ζ, which
concludes the proof.
12If any subword produced by a non-reachable merge exists in Sm, it should be pruned from b. A non-
reachable merge is a merge mr = (aℓr′ ⊚aℓr′′ ) whose pair of subwords aℓr′ and aℓr′′ cannot be generated.
30
