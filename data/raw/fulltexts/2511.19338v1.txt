Electrochemical Interfaces at Constant Potential: Data-Efficient Transfer
Learning for Machine-Learning-Based Molecular Dynamics
Michele Giovanni Bianchi1,2, Michele Re Fiorentin1, Francesca Risplendi1,
Candido Fabrizio Pirri1,3, Michele Parrinello2, Luigi Bonati2*, Giancarlo Cicero1*
1Department of Applied Science and Technology, Politecnico di Torino, corso Duca degli Abruzzi 24,
Torino, 10129, Italy.
2Atomistic Simulations, Italian Institute of Technology, via Enrico Melen 83, Genova, 16152, Italy.
3Centre for Sustainable Future Technologies, Italian Institute of Technology, via Livorno 60, Torino,
10144, Italy.
*Corresponding authors. E-mails: luigi.bonati@iit.it; giancarlo.cicero@polito.it;
Abstract
Simulating electrified metal/water interfaces with explicit solvent under constant potential is essential for under-
standing electrochemical processes, yet remains prohibitively expensive with ab initio methods. We present TRECI,
a data-efficient workflow for constructing machine learning force-fields (ML-FFs) that achieve ab initio-level accu-
racy in electronically grand-canonical molecular dynamics. By leveraging transfer learning from general-purpose and
domain-specific models, TRECI enables stable and accurate simulations across a wide potential range using a reduced
number of reference configurations. This efficiency allows the use of high-level meta-GGA functionals and rigor-
ous surface-electrification schemes. Applied to Cu(111)/water, models trained on just one thousand configurations
yield accurate molecular dynamics simulations, capturing bias-dependent solvent restructuring effects not previously
reported. TRECI offers a general strategy for characterising diverse materials and interfacial chemistries, significantly
lowering the cost of realistic constant-potential simulations and expanding access to quantitative electrochemical
modelling.
Keywords: electrified metal/water interface, constant potential, machine learning force-field, transfer learning
Introduction
Charged metal/H2O interfaces play an important role in
fields such as corrosion, heterogeneous catalysis, and elec-
trochemistry [1, 2]. Despite decades of study, recently
intensified by the rise of electrochemical technologies
for the energy transition, understanding these interfaces
at the atomic level remains a major challenge in sur-
face science [3–9]. Ab initio molecular dynamics (AIMD)
simulations are an ideal candidate to provide atomistic
insight in these systems [10]. However, the heterogene-
ity of the interfaces composed of metal surfaces, solvents
and solutes clashes with the problem of small spatial and
short time scales achievable in AIMD. In recent years,
machine learning interatomic potentials, here referred to
as ML force-fields (ML-FFs) to avoid confusion with the
applied electric potential, have emerged as a promising
solution to overcome the limits of AIMD. They bridge the
accuracy of ab initio approaches with the computational
feasibility of force-field schemes [11]. However, the pres-
ence of an applied electrical bias significantly increases
the simulation complexity both at the ab initio and ML
level. The simplest approaches for the electrification of the
interface are based on constant charge schemes [12], which
are not fully representative of the experiments, since the
applied bias is not controllable. In principle, constant
potential methods align more closely with experimental
ensembles [13]. However, they are more computationally
expensive, and in practice they are applied in conjunc-
tion with implicit solvent schemes [14–16] which can
introduce artefacts. In addition to these issues associated
with the electrification schemes, ML-FFs face a further
complication: the treatment of long-range electrostatic
interactions. The presence of charged species can give rise
to interactions that extend beyond the local nature of
the force-fields [17, 18]. As a result, the development of
ML-FFs for electrochemical systems constitutes an area
of research that remains largely unexplored. Preliminary
ML-FFs in the electrochemistry domain neglect the effects
of the applied bias, focusing only on the potential of zero
charge (PZC), i.e., when there is no extra charge on the
surface [19–21]. To overcome these limitations, ML-FFs
employing a constant charge approach were developed [9],
while subsequent advances proposed constant potential
ML models employing hybrid explicit-implicit solvent
schemes [22–25].
1
arXiv:2511.19338v1  [physics.comp-ph]  24 Nov 2025

Fig. 1 General scheme of TRECI for electronically grand-canonical machine learning force-fields: the pillars of the methods are a high-quality
dataset at multiple applied bias values, a data-efficient ML architecture via transfer learning and an effective active learning approach.
In this work, we present TRECI, a data-efficient strat-
egy that uniquely leverages transfer learning [26, 27] and
active learning to construct ML-FFs tailored for electri-
fied metal/water interfaces. By featuring a fully explicit
solvent model and a rigorous constant potential scheme
(the “double reference method” [28]), TRECI allows for
higher fidelity simulations, with a level of accuracy that
was not possible before. We apply TRECI to a copper/wa-
ter interface, a representative electrode used in many
electrocatalytic applications [7, 29]. TRECI achieves stable
and accurate simulations with only a thousand configura-
tions, a significant reduction of the dataset size in com-
parison to similar models trained from scratch. Notably,
TRECI enables direct observation of bias-dependent sol-
vent restructuring phenomena, offering insights that were
inaccessible with prior models. The TRECI code is openly
available at https://github.com/michelegiovannibianchi/
TRECI, and its versatility allows straightforward applica-
tion to other electrochemical systems. This paves the way
for broader access to advanced constant potential simula-
tions in a wide range of electrochemical applications.
Results
Data-efficient workflow for
electrochemical interfaces
The main outcome is the development of TRECI (TRansfer
learning for ElectroChemical Interfaces), a computational
workflow for ML-FFs that provides a high-fidelity descrip-
tion of electrified metal/water interfaces while remaining
data-efficient. The success of TRECI relies on three core
pillars: 1) a high-quality quantum mechanical description
of the interface, 2) a data-efficient transfer learning frame-
work for building the ML-FFs and 3) an effective active
learning strategy for collecting the required configurations
(Fig. 1).
High-quality description at multiple applied
bias values. A high-quality dataset and rigorous labelling
are essential elements to provide an accurate description
of electrified interfaces via ML-FFs. Any artifact in the
solvent representation or ambiguities in the definition of
the electrode potential can introduce inconsistencies in
the labels, which become particularly detrimental when
working with the small datasets employed in this work.
TRECI achieves a high-quality modelling of the interfaces
via a) a comprehensive description of the solvent dynam-
ics, b) an accurate treatment of the liquid water network,
and c) a robust electrification scheme (cf. left side of
Fig. 1).
The comprehensive description of the solvent is achieved
by following a fully explicit solvent approach without any
artificial vacuum or implicit solvent region. This allows
for capturing in the dataset both the overall dielectric
response of the solvent and the localised interactions
between molecules and the surface.
A high-quality description of the liquid water network
implies the choice of advanced DFT functionals due
to the well-known shortcomings of the common PBE-
GGA approximation in water-based systems [30, 31]. We
develop the datasets using the meta-GGA SCAN func-
tional [32] as the DFT reference (cf. Section “Methods”).
The inclusion of the effects of the applied bias in
the framework of ML-FFs represents the most cru-
cial element. This is realised within an electronically
grand-canonical ensemble using the “double reference
method” [28]. By employing this scheme, TRECI con-
structs multiple datasets corresponding to distinct poten-
tial energy surfaces (PESs), each associated with a well-
defined and macroscopically accessible quantity, the bias
value. The “double reference method” determines the
electrode potential by using a series of potential ref-
erences computed in auxiliary systems. This approach
eliminates artifacts from solvent/vacuum interfaces and
avoids inconsistencies between the energies computed in
the systems with and without extra electrons (see Section
“Methods”). As a result, it provides a consistent rela-
tionship between surface charge and electrode potential,
2

Fig. 2 TRECI strategy for the training of the constant potential ML-FFs via transfer learning: the node features of a pre-trained general-
purpose or domain-specific GNNs are used as descriptors for a multi-head Franken model. Each readout block implements a large-scale kernel
regression model targeting an applied bias value. Kernel functions are approximated by means of Random Fourier Features (RF)
removes ambiguities in data labelling, and guarantees that
each PES is uniquely identified. Such a rigorous electrifi-
cation scheme represents the main step ahead with respect
to other ML works.
To the best of our knowledge, a single framework inte-
grating all these three modelling choices has not been
realised in prior studies, largely due to the high com-
putational cost associated with each component: SCAN
is approximately 2.5 times more expensive than PBE,
and a simulation with the “double reference method” is
up to 10 times more costly than a calculation without
external bias. When used together, these methods become
prohibitively expensive, calling for highly data-efficient
approaches.
ML force-fields via transfer learning. Given the
high cost of generating such data across multiple poten-
tials, training independent ML-FFs for each applied bias
value would be highly inefficient. TRECI instead adopts
a transfer learning paradigm [33] that reuses information
from models trained in the absence of an electric field to
efficiently learn the corresponding constant applied bias
PESs.
Specifically,
we
draw
on
the
recently
introduced
Franken framework [34]. In this scheme, energies and
forces are predicted using a large-scale kernel regres-
sion model. Differently from the hand-crafted descriptors
of traditional kernel-based ML-FFs, here they are built
upon the node features of a pre-trained graph neural
network (GNN), such as the general-purpose potentials
from the MACE-MP family [35], inheriting the represen-
tations learnt by the deep neural network (cf. Section
“Methods”). Within TRECI, we extend this idea into a
multi-head architecture, where a shared set of GNN-based
descriptors is reused to simultaneously learn several PESs
corresponding to different applied biases. Each head is
trained to address a specific bias value (Fig. 2). Because
only a small regression block is bias-specific, the over-
all data requirement is drastically reduced. Furthermore,
TRECI generalises the Franken concept beyond fine-tuning
of universal ML-FFs: it also leverages domain-specific
models trained for the same system at the PZC, in the
absence of an applied field. In the following we refer to
models constructed using general-purpose descriptors and
domain-specific ones as Franken-MP0 and Franken-PZC,
respectively. As detailed in the Section “From stability
to accuracy” below, this combination of general-purpose
and domain-specific models provides a route from stable
exploratory simulations to accurate production runs.
Data-efficient active learning. The efficiency of
TRECI ultimately hinges on identifying the few thermo-
dynamically relevant configurations at each target bias.
Simply re-labelling equilibrium structures obtained in
the absence of an electric field is insufficient, as the
applied potential can stabilise entirely different interfacial
arrangements. To address this, TRECI adopts an iterative
data acquisition strategy based on active learning. The
dataset is progressively expanded by exploring the PESs
with preliminary ML-FFs and selecting a limited number
of new configurations for DFT labelling.
Specifically, for the selection, we exploit the capabilities of
the “Data-Efficient Active Learning” (DEAL) protocol [36],
which evaluates the similarity among the local environ-
ments of the sampled configurations to identify a reduced
set of geometries to label. This filtering step is based on
the Bayesian predictive variance of a Sparse Gaussian
Process model (see Section “Methods”). This approach
yields a compact and non-redundant dataset while ensur-
ing efficient coverage of the relevant configurational space.
3

Fig. 3 Evolution of the learning process for TRECI models at representative target biases during the active learning. (a) Stability range of the
different descriptors (Franken-MP0 vs Franken-PZC ): coloured bars show the iterations at which the ML-FFs are stable; dark colours identify
the descriptors employed for the sampling at the different iterations. (b) Force RMSE for Franken-MP0 and Franken-PZC . Solid lines identify
the descriptors employed for the sampling. The accuracy is not reported when the models are not stable. (c) Peak value of the oxygen density
in the different interfacial regions.
Complete strategy. Here we summarise the steps
of TRECI to construct ML-FFs at constant potential.
The workflow begins with the construction of an ML-
FF trained for the PZC case using the MACE architecture
(see Section “Methods”). The PZC model is developed
to have domain-specific descriptors useful for the transfer
learning. In addition, this gives a starting dataset for the
ML-FFs with applied bias and serves as a baseline for the
results in the absence of an electric field.
An iterative strategy composed of four steps is then
applied (Fig. 1). The cycle starts with electronically
grand-canonical DFT calculations. Using this data, ML-
FFs are optimised for the corresponding constant bias
values using transfer learning, employing both general-
purpose (from the MACE-MP family) and domain-specific
descriptors (from the model trained at the PZC). The
models are then benchmarked for stability: if domain-
specific descriptors fail to yield stable MD trajecto-
ries, the workflow proceeds using the general-purpose
ones. Instead, once enough data are available and
Franken-PZC models become robust, they are preferred
due to their superior accuracy. Then, new atomic config-
urations are sampled using MD and selected through a
data-efficient active learning protocol. The cycle continues
until both the accuracy of energy and force predictions, as
well as relevant physical observables, reach convergence.
Only at this point, the ML-FFs are considered sufficiently
reliable for use in production-level MD simulations.
Validation of the workflow
We validate the workflow of TRECI on a real case sys-
tem. In particular, we study the copper-water interface at
negative potentials, a representative cathodic system used
in electrochemical applications such as the CO2 electro-
reduction reaction [7]. In Fig. 3 we report the evolution
of the learning process, from which it is clear that after
a few active learning iterations and the collection of one
thousand structures, TRECI achieves stable, accurate and
converged results.
From stability to accuracy. We begin constructing
the constant bias models using a subset of 250 con-
figurations selected from the PZC dataset and labelled
with grand-canonical DFT. This small dataset proves
insufficient for training not only a standard MACE archi-
tecture but also domain-specific Franken-PZC ML-FFs:
trajectories generated with these models lack stability. In
contrast, the fine-tuned Franken-MP0 consistently yields
stable results across all applied potentials and through-
out all active learning cycles (Fig. 3a). For this reason, we
start expanding the dataset in an active learning frame-
work using configurations from MD trajectories generated
with Franken-MP0 models. As the dataset is enlarged,
Franken-PZC ML-FFs begin to produce stable results: at
low applied bias (V = −0.50 V), stability is achieved after
a single active learning cycle, while more negative poten-
tials require additional iterations (Fig. 3a). In particular,
we initially use equivariant domain-specific models, which
are computationally more expensive but become robust in
earlier iterations (Supplementary Section S1.2), and then
we transition to invariant ML-FFs for production MD
simulations.
Switching
from
general-purpose
to
domain-specific
descriptors leads to a reduction in the prediction error
(approximately 30% lower RMSE), ensuring a uniform
accuracy across all the bias values (cf. Fig. 3b and
Table 1). Notably, the accuracy achieved with PZC
descriptors is comparable to the corresponding PZC
MACE model one, but TRECI obtains these results using an
amount of data equal to 25% of the PZC dataset.
From
accuracy
to
convergence. When using
Franken-PZC models, the force accuracy rapidly con-
verges to a plateau (Fig. 3b). Despite this convergence, we
perform a more rigorous check by analysing the evolution
4

Table 1 Root-Mean-Squared-Error (RMSE) in meV/˚A for the prediction of forces using TRECI models at different bias values, with
descriptors from either MACE-MP-O (Franken-MP0) or our domain-specific PZC model (Franken-PZC).
Case [V vs SHE]
−0.50
−0.75
−1.00
−1.25
−1.50
−1.75
−2.00
Franken-MP0
31.8
30.9
32.2
31.7
29.8
29.9
30.5
Franken-PZC
24.5
23.7
24.3
23.0
21.3
22.1
22.3
of key physical observables. As a representative quan-
tity, we monitor the peak heights in the oxygen density
profile at the interface, which are highly sensitive to struc-
tural changes induced by the applied bias (see Section
“Structure of interfacial water”). The analysis of these
quantities during the active learning provides insights
into the learning process (Fig. 3c). As the models are
progressively refined, the first peak height (chemisorbed
molecules) decreases while the second one (physisorbed
water) increases until both converge to stable values. This
modulation of the peak density is a well-known effect
of the applied potential, and these variations during the
active learning cycles are an expected behaviour for a
transfer learning process from an electric-field-agnostic
model to ML-FFs in the presence of applied bias. By
tracking these physical observables, we notice that even
after the convergence of the force RMSE, approximately
three additional active learning iterations are required
for the peak densities to reach a plateau. At this point,
we further validate the convergence of the solvent den-
sity profiles, comparing TRECI models built upon different
descriptors, as detailed in Supplementary Section S1.3.
Even if the starting models for the transfer learning
are different, the ML-FFs predict reciprocally consis-
tent observables, confirming the effectiveness of TRECI’s
strategy.
Application to Cu(111)/water interface
Having validated TRECI’s workflow, the ML-FFs are
employed in large-scale MD simulation to investigate the
properties of interfacial water in contact with a Cu(111)
surface, focusing on the effects of reducing bias. Based
on an estimation of the PZC for this system (≈−0.3 ÷
−0.4 V vs SHE, see Section “Methods”), we consider
values smaller than −0.50 V up to −2.00 V vs SHE as
reducing potentials.
Structure of interfacial water. Firstly, we examine
the density profile of interfacial water at different poten-
tials (Fig. 4a). Looking at these profiles, it is possible to
identify, at least, three visible regions in the solvent, close
to the interface. These regions are sequentially labelled as
I, II and III, moving from the surface towards the water
bulk. There are still some minimal fluctuations in the den-
sity after 8 ˚A far from the surface, but they are completely
smoothed out at 12 ˚A.
The most evident signature of the applied potential is the
variation in the magnitude of the peak density (Fig. 4a).
The density of peak I drastically decreases at increas-
ing negative potentials, and it almost disappears for
V ≤−1.25 V. A more detailed analysis, focusing only on
the oxygen role (Fig. 4b), reveals that the O contribution
in region I is completely suppressed for V ≤−1.25 V: there
are no more H2O molecules in region I, and the residual
contribution to the total density in region I is associated
with hydrogens of region II molecules.
The application of the bias also causes the density in
peak II to increase to a value 4.5 times larger than the
bulk water one (Fig. 4b). As discussed in the Supplemen-
tary Section S3, this peak modulation is associated with
a variation in the preferential orientation of the water
dipoles. Region I is composed of molecules with oxygen
atoms exposed towards the surface and hydrogen atoms
pointing away towards other water molecules (Fig. 4e).
This orientation is compatible with a chemisorption pro-
cess due to a dative bond between the oxygen lone pair
and a copper atom. On the contrary, molecules in region
II are characterised by an orientation in which H atoms
are mainly exposed towards the surface (Fig. 4e): these
molecules are considered as physisorbed. This dipole flip
mechanism with increasing negative bias has already been
reported [39, 40] and is related to the variation of the
strength of the Cu-O chemical interaction and the Cu-
H electrostatic attraction at variable surface charge. At
reducing potentials, the increase of the negative surface
charge enhances the repulsion of negative oxygen atoms
while attracting the positive hydrogens, flipping the water
molecules (Fig. 4f). These effects are also illustrated in
the Supplementary Video through MD snapshots, high-
lighting the Bader charge computed on the electrode at
the DFT level and the behaviour of interfacial molecules.
The effectiveness of TRECI in studying multiple PESs is
demonstrated by its ability to replicate these well-known
effects and, more importantly, to produce monotonous
and consistent trends among the various potential values.
Even if AIMD can already give some information about
these phenomena, our model provides a more detailed
picture, affording an analysis at several applied poten-
tials and without finite-size effects. In particular, two new
elements are identified: an evident transition in region
I-II between −1.00 and −1.25 V and a more complex
behaviour of region III. As soon as peak I disappears,
between −1.00 and −1.25 V, there is an abrupt increase
in the O density in region II, sharpening the peak. At the
same time, this transition also has consequences in region
III. Here, the O density is almost unperturbed at low volt-
ages (Fig. 4b), but peak III markedly increases at higher
negative values. The bias influences not only the height
of the peak but also its position: peak III tends to shift
closer to the surface at more negative V (∆zpeak ≈0.3 ˚A
at V = −2.00 V vs SHE). To the best of our knowl-
edge, the effects of the applied potential in region III have
never been discussed before. Indeed, this region and the
complete transition to the water bulk are commonly not
accessible via AIMD due to the computational cost of
considering water layers thicker than 6 ÷ 8 ˚A.
Features of the interfacial H-bond network.
Another important feature of interfacial water is the
strength of the H-bonds and how this is altered by the
5

Fig. 4 Properties of interfacial water. (a) Density profile in the solvent region vs the distance z with respect to the surface zsurf, at different
values of external potential. Arrows emphasise trends moving towards more negative potentials. (b) Summary of the oxygen peak density
height in the interfacial regions vs the applied potential. VDOS spectra for hydrogen atoms in region I (c) and region II (d). From low to
high frequencies, it is possible to identify the peaks associated with the H-bond bending (≈40 ÷ 50 cm-1), the libration (150 ÷ 700 cm-1),
the H-O-H bending (≈1200 cm-1), the O-H symmetric and asymmetric stretching (2300 ÷ 2800 cm-1) [37, 38]. Spectra are computed using
the deuterium mass for the hydrogen species. Frames of the MD trajectory in which specific molecules are emphasised to show the typical
water orientation in regions I and II at PZC (e) and at −2.00 V vs SHE (f). O and H atoms of emphasised molecules are depicted in red and
white, respectively.
applied bias. This information can be inferred from the
analysis of the vibrational density of states (VDOS) spec-
tra for H atoms (Fig. 4c-d). For example, the relative
strength of the H-bonds vs the O-H covalent bonds in
water is correlated to O-H stretching vibration modes [37].
As emphasised with the black arrow in Fig. 4d, the O-H
stretching band in region II visibly red-shifts. This shift
is the typical signature of the weakening of the O-H cova-
lent bond due to stronger H-bonds [37].
The stretching band reflects the strength of H-bonds
through the relative motion of oxygen and hydrogen
atoms along the covalent bond axis, while the libra-
tion modes capture the restricted rotational motion of
water molecules caused by H-bonding interactions [41]. As
emphasised by the red arrows in Fig. 4c-d, the libration
band in regions I and II is altered by the applied bias. The
shift at lower energies in region I is proof that molecules
can more easily rotate in region I due to the weaker Cu-O
interaction. Whereas, in region II, the high-energy shoul-
der of the libration band is more suppressed at increasing
negative potentials. This red-shift can be interpreted as
the signature of a less restricted rotational motion at
increasing negative potentials due to a smaller number of
H-bonds in region II (H atoms pointing towards the sur-
face are not involved in H-bonds with other molecules).
It is important to note that computing VDOS spectra
via AIMD is often limited by short simulation times,
which hinder statistical convergence and hide spectral
features. In contrast, the spectra presented here benefit
from nanosecond-scale sampling, significantly improving
the resolution of peak positions and revealing fine details
in the spectra. This enhanced statistical quality allows for
a more reliable interpretation of bias-induced changes in
interfacial water dynamics.
6

Discussion
We presented TRECI, a general and data-efficient work-
flow for developing ML-FFs to investigate metal/H2O
interfaces under external electrical bias. This enables a
high-fidelity description of these systems by combining
a fully explicit solvent model, a DFT-accurate repre-
sentation of the liquid water network via the SCAN
functional, and a constant potential framework that
avoids explicit-implicit solvent compromises through the
“double reference method”.
To address the computational cost of this rigorous setup,
TRECI leverages a data-efficient transfer learning algo-
rithm for the ML-FFs training and an effective active
learning protocol. To ensure model stability in the early
stages of the ML-FF development, we perform transfer
learning from general-purpose potentials. In subsequent
iterations, we refine the ML-FFs by transferring informa-
tion from domain-specific models to achieve a superior
accuracy. This dual strategy enables the construction
of robust and precise ML-FFs across a wide range of
potential values. Our workflow achieves stable MD tra-
jectories with only a few hundred configurations, reaches
the same accuracy as models trained from scratch, and
converges physical observables with a final dataset of
just one thousand configurations. This data efficiency
is critical for enabling DFT-level labelling in constant
potential simulations, especially with our computation-
ally demanding settings. Importantly, the applicability of
the TRECI workflow is not constrained by our specific ab
initio setup used for data labelling. Its modular design
allows for easy customisation (in the choice of DFT func-
tional, electrification scheme, or pre-trained model for
transfer learning), making it adaptable to the require-
ments of diverse electrochemical interfaces, not limiting
to metal/H2O systems.
To
demonstrate
its
capabilities,
we
applied
the
TRECI workflow to Cu/water interfaces as a represen-
tative system. Our ML models successfully revealed
detailed structural and dynamical properties of electrified
metal/water interfaces, achieving a level of resolution
rarely accessible with conventional approaches. This
picture can be further enriched by incorporating surface
defects, ions, adsorbates, and electrocatalytic reactions
to simulate specific operando conditions. While these
more complex scenarios introduce new challenges, such
as accounting for long-range electrostatic interactions
from charged species, the core strengths of TRECI remain
unaffected. Its data-efficient and robust protocol provides
a solid foundation for developing more advanced ML-FFs
in which the TRECI models are a modular element within
a more complex architecture. For these reasons, we
believe that the TRECI strategy paves the way for high-
quality, atomistic simulations of electrified interfaces,
driving progress in both the development of modelling
techniques and the advancement of fundamental and
applied research in electrochemistry.
Methods
Details of DFT calculations
The ab initio calculations required to generate data for
the ML dataset are based on non-polarised DFT simula-
tions, as implemented in the Vienna Ab initio Simulation
Package (VASP), version 6.4.3 [42–44]. The projector-
augmented-wave (PAW) method is employed to describe
the electron-ion interaction [45]. Electronic wave func-
tions are expanded in plane waves with a cut-off energy
of 800 eV in conjunction with a dense FFT grid to prop-
erly guarantee numerical convergence of energies as well
as of forces. The Brillouin zone is sampled by employing
a Gamma-centred (11 11 1) Monkhorst-Pack mesh in
the case of the unit cell of the Cu slab and consistently
reduced for the supercells. A Gaussian smearing with a
spreading of 0.1 eV is employed. Gamma-point-only cal-
culations are performed for systems including only H2O
molecules.
The
meta-generalised-gradient
approximation
(meta-
GGA)
“Strongly
Constrained
and
Appropriately
Normed” (SCAN) functional is employed [30, 32]. Indeed,
it is well known that common GGA functionals, such as
PBE, tend to markedly over-structure the liquid water
network [30, 46]. Whereas, SCAN succeeds in describing
the H2O behaviour due to its capability to reproduce the
correct relative magnitude of the covalent O-H bonds,
the hydrogen bonds and the dispersion forces that deter-
mine the H-bond network [30]. In view of the numerical
instability and convergence issues associated with the
original formulation of the SCAN functional, the revisited
r2SCAN version is adopted in this work [47].
Interface electrification scheme
The electrification of the interface at the DFT level is
performed following the “double reference method” [28].
By allowing the surface charge to adjust to reach a
target potential and compensating with a homogeneous
background charge for the added or removed electrons,
this approach is compatible with an electronically grand-
canonical ensemble [48]. The robust definition of the
electrode potential is established via a chain of poten-
tial references involving two auxiliary systems such that
this definition is not influenced by the counter-charge and
any fictitious solvent-vacuum interface. Specifically, the
first auxiliary system has no extra charge (i.e., no applied
potential) and a vacuum region to define the vacuum level;
the second is with no extra charge and no vacuum and
represents a reference for the same system but with extra
charge (i.e., with applied potential).
Here, we employ the definition of the electrode poten-
tial within the “double reference method” to accomplish
two tasks. First, by applying this scheme to the config-
urations in the PZC dataset, we estimate the potential
of zero charge to be approximately −0.3 to −0.4 V vs
SHE. This value serves as an upper bound for the range
of reducing potentials. Second, by using the same method
on geometries with added or removed charge, we establish
a direct link between the net charge and the correspond-
ing applied bias. This enables us to define a criterion for
tuning the system charge to reach a target bias value. In
7

this approach, the same geometry is labelled at multiple
bias values. These calculations share the chain of refer-
ences. As a result, the computational cost of the auxiliary
systems is distributed across datasets corresponding to
different applied biases.
Practically, the grand-canonical DFT is performed by
modifying the “Fully Constant Potential” (FCP) ASE cal-
culator proposed in Ref. [49], with the definition of the
electrode potential in the “double reference method”.
This calculator is then integrated into a Python routine
that implements an automatic workflow to evaluate the
auxiliary systems of the “double reference method” and
perform constant potential calculations at fixed geometry
for more target potential values (see Code Availability).
Machine learning force-fields
Constant potential models via Franken transfer
learning. The training of the constant potential force-
fields is performed within the Franken transfer learning
approach [34]. This scheme is based on the integration
of the accurate descriptors of a pre-trained deep neu-
ral network with an efficient large-scale kernel regression
approach. The kernel function is approximated by means
of Random Fourier Features (RF) maps to overcome the
scalability issues of kernel methods in the case of large
datasets [50]. This structure allows for fast training and
inference typical of kernel methods, but irrespective of the
dataset size.
As shown in Fig. 2, starting from common descriptors
(the backbone) extracted as the node features of a graph
neural network (GNN), different readout blocks are used
to target specific bias values: we trained seven RF-based
readouts for potentials ranging from −0.50 V to −2.00
V vs SHE (with a spacing of 0.25 V). This approach is
inherently scalable, enabling the simultaneous generation
of a greater number of regression blocks that encompass
an even broader spectrum of applied biases. In addition
to using descriptors from the pre-trained general-purpose
potential MACE-MP0, as done in Ref. [34], we employ
domain-specific ones. That is, we extract the descriptors
from a MACE model trained on a set of data of the
same interface but without an applied electric field (cf.
Section “MACE model for the PZC”). While this approach
requires training an additional model, the computational
cost of labelling configurations at PZC is significantly
lower (approximately 10 to 20 times less than perform-
ing full constant potential calculations across seven target
bias values). Therefore, the increased accuracy with a
reduced number of grand-canonical DFT evaluations jus-
tifies the additional effort required to develop the PZC
model.
After testing different descriptors and numbers of RFs
(Supplementary Section S1.1), final results are achieved
with the descriptors from our PZC MACE model and 8192
RFs.
MACE model for the PZC. The ML force-fields
(ML-FFs) at the PZC are built using the MACE software,
version 0.3 [51]. The peculiarity of MACE is the construc-
tion of a high-order message-passing scheme based on a
many-body atomic cluster expansion (ACE) representa-
tion and the adoption of equivariant internal features for
the message [52]. It results in a data-efficient structure
with appreciable in-domain and out-domain predictions.
Specifically, we employ a 4-body term expansion and two
interaction layers, each with a cut-off radius of 6 ˚A. Con-
sidering the possible presence of non-local contribution
outside this receptive field, we test models with cut-off
radii up to 9 ˚A, as well as a model with a cut-off of
5 ˚A but with three interaction layers (Supplementary
Section S2). In all cases, we observe that the RMSE of
the force predictions decreases only marginally as the
receptive field increases. At the same time, the solvent
density profile remains unchanged with the expansion of
the receptive field, indicating that non-local contributions
do not significantly affect the solvent structure. These
results demonstrate that the selected cut-off radius and
number of interaction layers are sufficient to capture all
relevant interactions within the system.
During the construction of the ML-FF, an architecture
with equivariant messages (L = 1) and 128 channels is
employed to better optimise the stability of the model in
case of a limited-size database. Once the dataset is com-
plete, the final ML-FF is trained with invariant messages
(L = 0) and 256 channels, resulting in a reasonable bal-
ance between accuracy and computational efficiency. The
dataset is split into training/validation/test subsets with
a ratio of 85:10:5. The model is optimised with the AMS-
Grad algorithm, using a learning rate of 0.01, a batch size
of 4 and a maximum number of epochs equal to 1000.
Performance is evaluated on energy and forces with a
weighted root mean square error (RMSE) loss function.
The weights for energy and forces in the loss function are
initially set to 1 and 100, respectively. In the last 20% of
the training, the weight for energy is increased to 1000.
Dataset
generation.
Different
strategies
are
adopted to generate a preliminary dataset for the PZC.
A pre-existing dataset [53] is used to describe the pure
water bulk. Configurations for bare Cu surfaces are
included after sampling AIMD trajectories at a lower
accuracy level (i.e., PBE). A ML universal model [35]
is also used to collect data for Cu/H2O systems. For all
these cases, the configurations are labelled consistently
with our DFT setup.
Starting from this initial pool of configurations, the
training dataset is iteratively refined through the “Data-
Efficient
Active
Learning”
(DEAL)
scheme,
recently
introduced in Ref. [36]. This method allows us to pro-
gressively enlarge the dataset and refine the ML-FF in
a data-efficient manner, providing new configurations
selected from MD trajectories obtained with preliminary
ML-FFs. Firstly, the MD configurations are screened,
relying on the uncertainty prediction obtained by a query-
by-committee approach. To avoid redundant labelling,
this set of selected configurations is then filtered by
DEAL using a sparse Gaussian process model, trained
on-the-fly on the pre-selected data. More details on this
approach are reported in the original article [36]. This
strategy ensures broad and uniform coverage of the rele-
vant configuration space while dramatically reducing the
number of costly ab initio calculations.
At the end of the active learning, the complete dataset
for the Cu/H2O interface at the PZC is composed of
about 5320 configurations, including systems of bulk
8

H2O, bare Cu surfaces and Cu/H2O interfaces.
A similar iterative strategy is used to construct datasets
at multiple applied potentials. Starting from a subset of
250 configurations sampled from the PZC dataset, mod-
els are progressively refined using the DEAL protocol. In
this case, a two-step DEAL selection is performed. First,
configurations are filtered independently for each applied
bias; then, the selected configurations are merged across
all bias values and screened again alongside the existing
dataset. This second DEAL selection reduces redundancy
among configurations sampled at the different biases and
with respect to the existing dataset. This iterative refine-
ment continues until the TRECI models reach an accuracy
comparable to the reference PZC model and demonstrate
stable convergence of key physical observables.
Molecular dynamics simulations
Classical molecular dynamics (MD) simulations are per-
formed with the Large-scale Atomic / Molecular Mas-
sively Parallel Simulator (LAMMPS) software [54], supple-
mented by MACE v0.3 [51]. NVT simulations are performed
with an integration step of 0.5 fs, and the deuterium mass
is used for the hydrogen atoms to properly describe the
dynamics of light atoms. The temperature is controlled
using a Nos´e–Hoover thermostat with a damping param-
eter of 100 times the integration step. The thermostat
temperature is properly set to ensure that the outermost
metal layers of the slab and water molecules are in thermo-
dynamic equilibrium at 330 K. This slight increase above
room temperature is required to overcome the residual
over-structuring of water predicted by SCAN [30]. The
dynamical properties are computed in the microcanonical
ensemble to remove any spurious contribution from the
thermostat. For the VDOS spectra, the velocity autocor-
relation function is evaluated by recording the velocities
every 5 fs for several time windows of 6 ps. MD runs
are performed in a slab system composed of six layers of
metal atoms, with the two innermost ones fixed. Periodic
boundary conditions are imposed in all directions, and no
regions of vacuum are present. MD runs are performed in
a slab system with a lateral size of 10 ÷ 15 ˚A (4×4 up
to 6×6 supercells) during the active learning phase and
20 ˚A (8×8 supercells) in the final production run. In the
latter case, a region of 30 ˚A (i.e., 15 ˚A per side) is filled
with water molecules.
Supplementary information.
Supplementary infor-
mation: Supplementary Sections 1–3, Figures 1–4.
Supplementary video: Illustrative video on the effects of
the applied potential on the surface charge and the sol-
vent structure. This material is available at https://bit.
ly/copper water bias movie.
Acknowledgements.
The authors acknowledge the
support of the Data Science and Computation Facil-
ity at the Fondazione Istituto Italiano di Tecnolo-
gia and the CINECA award under the ISCRA initia-
tive. M.G.B. thanks F. Raffone, P. J. Buigues and S.
Perego for useful discussions. L.B. and M.P. acknowl-
edge funding from the European Union - NextGenera-
tionEU initiative and the Italian National Recovery and
Resilience Plan (PNRR) from the Ministry of University
and Research (MUR), under Project PE0000013 CUP
J53C22003010006 ”Future Artificial Intelligence Research
(FAIR)”.
Declarations
Data availability.
A minimal dataset to evaluate the
TRECI workflow is provided alongside the code at https://
github.com/michelegiovannibianchi/TRECI. The molec-
ular dynamics trajectories underlying the analysis of the
interface will be available upon publication.
Code
availability.
The
TRECI
code
under-
lying
this
work
is
freely
available
at
https:
//github.com/michelegiovannibianchi/TRECI.
The
FCP
calculator
employed
in
TRECI
is
available
at
https://github.com/michelegiovannibianchi/
DoubleReferenceMethod-FCP-calculator.
TRECI
work-
flow
requires
the
MACE
code
available
at
https:
//github.com/ACEsuit/mace, the Franken code avail-
able
at
https://github.com/CSML-IIT-UCL/franken
and the DEAL code available at https://github.com/
luigibonati/DEAL.
Author contribution.
L.B. and G.C. designed and
supervised the experiment. M.G.B. implemented the code
and performed the calculations. All authors analysed the
simulations. M.G.B. and L.B. wrote the first draft, and
all authors revised it.
Competing interests.
The authors declare no com-
peting interests.
References
[1] Carrasco, J., Hodgson, A., Michaelides, A.: A molec-
ular perspective of water at metal interfaces. Nature
Materials 11(8), 667–674 (2012) https://doi.org/10.
1038/nmat3354
[2] Gonella, G., Backus, E.H.G., Nagata, Y., Bon-
thuis, D.J., Loche, P., Schlaich, A., Netz, R.R.,
K¨uhnle, A., McCrum, I.T., Koper, M.T.M., Wolf,
M., Winter, B., Meijer, G., Campen, R.K., Bonn, M.:
Water at charged interfaces. Nature Reviews Chem-
istry 5(7), 466–485 (2021) https://doi.org/10.1038/
s41570-021-00293-2
[3] Kolb, D.: Reconstruction phenomena at metal-
electrolyte
interfaces.
Progress
in
Surface
Sci-
ence 51(2), 109–173 (1996) https://doi.org/10.1016/
0079-6816(96)00002-0
[4] Xia, X., Berkowitz, M.L.: Electric-Field Induced
Restructuring of Water at a Platinum-Water Inter-
face: A Molecular Dynamics Computer Simulation.
Physical Review Letters 74(16), 3193–3196 (1995)
https://doi.org/10.1103/PhysRevLett.74.3193
[5] Iwasita, T., Nart, F.C.: In situ infrared spectroscopy
at electrochemical interfaces. Progress in Surface Sci-
ence 55(4), 271–340 (1997) https://doi.org/10.1016/
S0079-6816(97)00032-4
9

[6] Cicero, G., Calzolari, A., Corni, S., Catellani, A.:
Anomalous Wetting Layer at the Au(111) Sur-
face. The Journal of Physical Chemistry Letters
2(20), 2582–2586 (2011) https://doi.org/10.1021/
jz200989n
[7] Nitopi, S., Bertheussen, E., Scott, S.B., Liu, X.,
Engstfeld, A.K., Horch, S., Seger, B., Stephens,
I.E.L., Chan, K., Hahn, C., Nørskov, J.K., Jaramillo,
T.F., Chorkendorff, I.: Progress and Perspectives of
Electrochemical CO2 Reduction on Copper in Aque-
ous Electrolyte. Chemical Reviews 119(12), 7610–
7672
(2019)
https://doi.org/10.1021/acs.chemrev.
8b00705
[8] Auer, A., Ding, X., Bandarenka, A.S., Kunze-
Liebh¨auser, J.: The Potential of Zero Charge and
the Electrochemical Interface Structure of Cu(111)
in Alkaline Solutions. The Journal of Physical Chem-
istry C 125(9), 5020–5028 (2021) https://doi.org/10.
1021/acs.jpcc.0c09289
[9] Zhu, J.-X., Cheng, J.: Machine Learning Potential for
Electrochemical Interfaces with Hybrid Representa-
tion of Dielectric Response. Physical Review Letters
135(1) (2025) https://doi.org/10.1103/48ct-3jxm
[10] Groß,
A.,
Sakong,
S.:
Ab
Initio
Simula-
tions
of
Water/Metal
Interfaces.
Chemical
Reviews
122(12),
10746–10776
(2022)
https:
//doi.org/10.1021/acs.chemrev.1c00679
[11] Unke, O.T., Chmiela, S., Sauceda, H.E., Gastegger,
M., Poltavsky, I., Sch¨utt, K.T., Tkatchenko, A.,
M¨uller, K.-R.: Machine Learning Force Fields. Chem-
ical Reviews 121(16), 10142–10186 (2021) https://
doi.org/10.1021/acs.chemrev.0c01111
[12] Le, J.-B., Yang, X.-H., Zhuang, Y.-B., Jia, M.,
Cheng, J.: Recent progress toward ab initio modeling
of electrocatalysis. The Journal of Physical Chem-
istry Letters 12(37), 8924–8931 (2021) https://doi.
org/10.1021/acs.jpclett.1c02086
[13] H¨ormann, N.G., Beinlich, S.D., Reuter, K.: Converg-
ing divergent paths: Constant charge vs constant
potential energetics in computational electrochem-
istry. The Journal of Physical Chemistry C 128(13),
5524–5531 (2024) https://doi.org/10.1021/acs.jpcc.
3c07954
[14] Kastlunger,
G.,
Lindgren,
P.,
Peterson,
A.A.:
Controlled-Potential Simulation of Elementary Elec-
trochemical Reactions: Proton Discharge on Metal
Surfaces. The Journal of Physical Chemistry C
122(24),
12771–12781
(2018)
https://doi.org/10.
1021/acs.jpcc.8b02465
[15] Islam, S.M.R., Khezeli, F., Ringe, S., Plaisance, C.:
An implicit electrolyte model for plane wave density
functional theory exhibiting nonlinear response and
a nonlocal cavity definition. The Journal of Chemical
Physics 159(23), 234117 (2023) https://doi.org/10.
1063/5.0176308
[16] Le, D.: An Explicit-Implicit Hybrid Solvent Model
for Grand Canonical Simulations of the Electrochem-
ical Environment (2023). https://doi.org/10.26434/
chemrxiv-2023-z2n4n
[17] Ko, T.W., Finkler, J.A., Goedecker, S., Behler, J.:
A fourth-generation high-dimensional neural net-
work potential with accurate electrostatics includ-
ing non-local charge transfer. Nature Communi-
cations 12(1), 398 (2021) https://doi.org/10.1038/
s41467-020-20427-2
[18] Zhang, C., Calegari Andrade, M.F., Goldsmith, Z.K.,
Raman, A.S., Li, Y., Piaggi, P.M., Wu, X., Car, R.,
Selloni, A.: Molecular-scale insights into the electrical
double layer at oxide-electrolyte interfaces. Nature
Communications 15(1), 10270 (2024) https://doi.
org/10.1038/s41467-024-54631-1
[19] Natarajan, S.K., Behler, J.: Neural network molec-
ular dynamics simulations of solid–liquid interfaces:
water at low-index copper surfaces. Physical Chem-
istry Chemical Physics 18(41), 28704–28725 (2016)
https://doi.org/10.1039/C6CP05711J
[20] Mikkelsen, A.E.G., Schiøtz, J., Vegge, T., Jacob-
sen, K.W.: Is the water/Pt(111) interface ordered at
room temperature? The Journal of Chemical Physics
155(22) (2021) https://doi.org/10.1063/5.0077580
[21] Rice, P.S., Liu, Z.-P., Hu, P.: Hydrogen Coupling on
Platinum Using Artificial Neural Network Potentials
and DFT. The Journal of Physical Chemistry Let-
ters 12(43), 10637–10645 (2021) https://doi.org/10.
1021/acs.jpclett.1c02998
[22] Chen, X., El Khatib, M., Lindgren, P., Willard,
A., Medford, A.J., Peterson, A.A.: Atomistic learn-
ing in the electronically grand-canonical ensemble.
npj Computational Materials 9(1), 73 (2023) https:
//doi.org/10.1038/s41524-023-01007-6
[23] Bergmann, N., Bonnet, N., Marzari, N., Reuter, K.,
H¨ormann, N.G.: Machine Learning the Energetics of
Electrified Solid-Liquid Interfaces. Physical Review
Letters 135(14), 146201 (2025) https://doi.org/10.
1103/lm64-m3bn
[24] Tian, X., Tosello Gardini, A., Raucci, U., Xiao, H.,
Zhuo, Y., Parrinello, M.: Electrochemical Potential-
Driven Water Dynamics Control CO2 Electroreduc-
tion at the Ag/H2O Interface. Chemistry (2025).
https://doi.org/10.26434/chemrxiv-2025-n41q2
[25] Chen, L., Tian, Y., Hu, X., Chen, S., Wang, H.,
Zhang, X., Zhou, Z.: A constant potential reactor
framework for electrochemical reaction simulations
(2024). https://arxiv.org/abs/2411.16330
[26] Chen, M.S., Lee, J., Ye, H.-Z., Berkelbach, T.C.,
Reichman, D.R., Markland, T.E.: Data-Efficient
Machine Learning Potentials from Transfer Learning
of Periodic Correlated Electronic Structure Meth-
ods: Liquid Water at AFQMC, CCSD, and CCSD(T)
10

Accuracy. Journal of Chemical Theory and Compu-
tation 19(14), 4510–4519 (2023) https://doi.org/10.
1021/acs.jctc.2c01203
[27] Zaverkin, V., Holzm¨uller, D., Bonfirraro, L., K¨astner,
J.: Transfer learning for chemically accurate inter-
atomic neural network potentials. Phys. Chem.
Chem. Phys. 25(7), 5383–5396 (2023) https://doi.
org/10.1039/D2CP05793J
[28] Taylor, C.D., Wasileski, S.A., Filhol, J.-S., Neurock,
M.: First principles reaction modeling of the electro-
chemical interface: Consideration and calculation of a
tunable surface potential from atomic and electronic
structure. Physical Review B 73(16), 165402 (2006)
https://doi.org/10.1103/PhysRevB.73.165402
[29] Naher, M., Gonz´alvez, M.A., Williams, C.M., Bern-
hardt, P.V.: Emerging applications and mechanis-
tic insights of copper mediated electrocatalysts in
organic transformations. Chemical Society Reviews
54(15), 7304–7338 (2025) https://doi.org/10.1039/
D5CS00382B
[30] Chen,
M.,
Ko,
H.-Y.,
Remsing,
R.C.,
Cale-
gari Andrade, M.F., Santra, B., Sun, Z., Selloni, A.,
Car, R., Klein, M.L., Perdew, J.P., Wu, X.: Ab ini-
tio theory and modeling of water. Proc. Natl. Acad.
Sci. U.S.A. 114(41), 10846–10851 (2017) https://doi.
org/10.1073/pnas.1712499114
[31] Gartner, T.E., Zhang, L., Piaggi, P.M., Car, R.,
Panagiotopoulos, A.Z., Debenedetti, P.G.: Signa-
tures of a liquid–liquid transition in an ab initio deep
neural network model for water. Proceedings of the
National Academy of Sciences 117(42), 26040–26046
(2020) https://doi.org/10.1073/pnas.2015440117
[32] Sun, J., Ruzsinszky, A., Perdew, J.: Strongly con-
strained and appropriately normed semilocal density
functional. Phys. Rev. Lett. 115(3), 036402 (2015)
https://doi.org/10.1103/PhysRevLett.115.036402
[33] Falk, J., Bonati, L., Novelli, P., Parrinello, M., Pon-
til, M.: Transfer learning for atomistic simulations
using gnns and kernel mean embeddings, vol. 36, pp.
29783–29797 (2023)
[34] Novelli, P., Meanti, G., Buigues, P.J., Rosasco, L.,
Parrinello, M., Pontil, M., Bonati, L.: Fast and
Fourier features for transfer learning of interatomic
potentials. npj Computational Materials 11(1), 293
(2025) https://doi.org/10.1038/s41524-025-01779-z
[35] Batatia, I., Benner, P., Chiang, Y., Elena, A.M.,
Kov´acs, D.P., Riebesell, J., Advincula, X.R., Asta,
M., Avaylon, M., Baldwin, W.J., Berger, F., Bern-
stein,
N.,
Bhowmik,
A.,
Bigi,
F.,
Blau,
S.M.,
C˘arare, V., Ceriotti, M., Chong, S., Darby, J.P.,
De, S., Della Pia, F., Deringer, V.L., Elijoˇsius,
R., El-Machachi, Z., Fako, E., Falcioni, F., Ferrari,
A.C., Gardner, J.L.A., Gawkowski, M.J., Genreith-
Schriever, A., George, J., Goodall, R.E.A., Grandel,
J., Grey, C.P., Grigorev, P., Han, S., Handley, W.,
Heenen, H.H., Hermansson, K., Ho, C.H., Hofmann,
S., Holm, C., Jaafar, J., Jakob, K.S., Jung, H.,
Kapil, V., Kaplan, A.D., Karimitari, N., Kermode,
J.R., Kourtis, P., Kroupa, N., Kullgren, J., Kuner,
M.C., Kuryla, D., Liepuoniute, G., Lin, C., Margraf,
J.T., Magd˘au, I.-B., Michaelides, A., Moore, J.H.,
Naik, A.A., Niblett, S.P., Norwood, S.W., O’Neill,
N., Ortner, C., Persson, K.A., Reuter, K., Rosen,
A.S., Rosset, L.A.M., Schaaf, L.L., Schran, C., Shi,
B.X., Sivonxay, E., Stenczel, T.K., Sutton, C., Svahn,
V., Swinburne, T.D., Tilly, J., Oord, C., Vargas, S.,
Varga-Umbrich, E., Vegge, T., Vondr´ak, M., Wang,
Y., Witt, W.C., Wolf, T., Zills, F., Cs´anyi, G.: A
foundation model for atomistic materials chemistry.
The Journal of Chemical Physics 163(18), 184110
(2025) https://doi.org/10.1063/5.0297006
[36] Perego, S., Bonati, L.: Data efficient machine learn-
ing potentials for modeling catalytic reactivity via
active learning and enhanced sampling. npj Compu-
tational Materials 10(1), 291 (2024) https://doi.org/
10.1038/s41524-024-01481-6
[37] Le, J., Fan, Q., Perez-Martinez, L., Cuesta, A.,
Cheng, J.: Theoretical insight into the vibrational
spectra of metal–water interfaces from density func-
tional theory based molecular dynamics. Physical
Chemistry Chemical Physics 20(17), 11554–11558
(2018) https://doi.org/10.1039/C8CP00615F
[38] Jin, S., Fan, X., Stamper, C., Mole, R.A., Yu, Y.,
Hong, L., Yu, D., Baggioli, M.: On the tempera-
ture dependence of the density of states of liquids at
low energies. Scientific Reports 14(1), 18805 (2024)
https://doi.org/10.1038/s41598-024-69504-2
[39] Toney, M.F., Howard, J.N., Richer, J., Borges, G.L.,
Gordon, J.G., Melroy, O.R., Wiesler, D.G., Yee, D.,
Sorensen, L.B.: Distribution of water molecules at
Ag(111)/electrolyte interface as studied with sur-
face X-ray scattering. Surface Science 335, 326–
332 (1995) https://doi.org/10.1016/0039-6028(95)
00455-6
[40] Raffone, F., Khatib, R., Sulpizi, M., Cucinotta,
C.: Revealing the molecular interplay of coverage,
wettability, and capacitive response at the Pt(111)-
water solution interface under bias. Communications
Chemistry 8(1), 58 (2025) https://doi.org/10.1038/
s42004-025-01446-w
[41] Tong, Y., Kampfrath, T., Campen, R.K.: Experimen-
tally probing the libration of interfacial water: the
rotational potential of water is stiffer at the air/wa-
ter interface than in bulk liquid. Physical Chemistry
Chemical Physics 18(27), 18424–18430 (2016) https:
//doi.org/10.1039/C6CP01004K
[42] Kresse,
G.,
Hafner,
J.:
Ab
initio
molecular-
dynamics simulation of the liquid-metal–amorphous-
semiconductor transition in germanium. Phys. Rev.
B 49, 14251–14269 (1994) https://doi.org/10.1103/
PhysRevB.49.14251
11

[43] Kresse, G., Furthm¨uller, J.: Efficiency of ab-initio
total energy calculations for metals and semicon-
ductors using a plane-wave basis set. Computational
Materials Science 6(1), 15–50 (1996) https://doi.org/
10.1016/0927-0256(96)00008-0
[44] Kresse,
G.,
Furthm¨uller,
J.:
Efficient
iterative
schemes for ab initio total-energy calculations using a
plane-wave basis set. Phys. Rev. B 54, 11169–11186
(1996) https://doi.org/10.1103/PhysRevB.54.11169
[45] Kresse, G., Joubert, D.: From ultrasoft pseudopo-
tentials to the projector augmented-wave method.
Phys. Rev. B 59, 1758–1775 (1999) https://doi.org/
10.1103/PhysRevB.59.1758
[46] LaCount, M.D., Gygi, F.: Ensemble first-principles
molecular dynamics simulations of water using the
SCAN meta-GGA density functional. The Journal of
Chemical Physics 151(16) (2019) https://doi.org/10.
1063/1.5124957
[47] Furness, J.W., Kaplan, A.D., Ning, J., Perdew, J.P.,
Sun, J.: Accurate and numerically efficient r 2 SCAN
meta-generalized gradient approximation. J. Phys.
Chem. Lett. 11(19), 8208–8215 (2020) https://doi.
org/10.1021/acs.jpclett.0c02405
[48] Hagopian, A., Doublet, M.-L., Filhol, J.-S., Bin-
ninger, T.: Advancement of the homogeneous back-
ground method for the computational simulation of
electrochemical interfaces. Journal of Chemical The-
ory and Computation 18(3), 1883–1893 (2022) https:
//doi.org/10.1021/acs.jctc.1c01237
[49] Xia, Z., Xiao, H.: Grand Canonical Ensemble Model-
ing of Electrochemical Interfaces Made Simple. Jour-
nal of Chemical Theory and Computation 19(15),
5168–5175 (2023) https://doi.org/10.1021/acs.jctc.
3c00237
[50] Dhaliwal, G., Nair, P.B., Singh, C.V.: Machine
learned interatomic potentials using random fea-
tures. npj Computational Materials 8(1), 7 (2022)
https://doi.org/10.1038/s41524-021-00685-4
[51] Batatia, I., Kov´acs, D.P., Simm, G.N.C., Ortner,
C., Cs´anyi, G.: MACE: Higher Order Equivariant
Message Passing Neural Networks for Fast and Accu-
rate Force Fields (2023). http://arxiv.org/abs/2206.
07697
[52] Batatia, I., Batzner, S., Kov´acs, D.P., Musaelian,
A., Simm, G.N.C., Drautz, R., Ortner, C., Kozin-
sky, B., Cs´anyi, G.: The design space of E(3)-
equivariant
atom-centred
interatomic
potentials.
Nature Machine Intelligence 7(1), 56–67 (2025)
https://doi.org/10.1038/s42256-024-00956-x
[53] Gartner, T.I., Zhang, L., Piaggi, P., Car, R., Pana-
giotopoulos, A., Debenedetti, P.: Data from ”Signa-
tures of a liquid-liquid transition in an ab initio deep
neural network model for water”. Princeton Univer-
sity (2020). https://doi.org/10.34770/45m3-am91
[54] Thompson, A.P., Aktulga, H.M., Berger, R., Bolin-
tineanu, D.S., Brown, W.M., Crozier, P.S., Veld,
P.J., Kohlmeyer, A., Moore, S.G., Nguyen, T.D.,
Shan, R., Stevens, M.J., Tranchida, J., Trott, C.,
Plimpton, S.J.: LAMMPS - a flexible simulation tool
for particle-based materials modeling at the atomic,
meso, and continuum scales. Comp. Phys. Comm.
271, 108171 (2022) https://doi.org/10.1016/j.cpc.
2021.108171
12

Supplementary Information
Electrochemical Interfaces at Constant Potential:
Data-Efficient Transfer Learning for Machine-Learning-Based
Molecular Dynamics
Michele Giovanni Bianchi1, Michele Re Fiorentin1, Francesca Risplendi1, Candido Fabrizio Pirri1,3,
Michele Parrinello2, Luigi Bonati2,∗, Giancarlo Cicero1,∗
1 Department of Applied Science and Technology, Politecnico di Torino, corso Duca degli Abruzzi 24,
Torino, 10129, Italy
2 Atomistic Simulations, Italian Institute of Technology, via Enrico Melen 83, Genova, 16152, Italy
3 Centre for Sustainable Future Technologies, Italian Institute of Technology, via Livorno 60, Torino,
10144, Italy
*Corresponding authors. E-mails: luigi.bonati@iit.it; giancarlo.cicero@polito.it
S1 Additional tests on the constant potential models
S1.1 Accuracy test on the descriptors and the number of random features
Figure S1 shows a comparison of the accuracy of the Franken models based on MACE-MP-0 (i.e., Franken-MP0) and
on the domain-specific PZC descriptors (i.e., Franken-PZC). Accuracy is evaluated on the force RMSE on a validation
Supplementary Figure S1 Comparison of the RMSE error on force prediction at V = -0.50 V and -2.00 vs SHE, using different descriptors
(Franken-MP0 vs Franken-PZC) for different training set sizes and numbers of random features.
set of about 150 configurations. This analysis is performed by varying the size of the training set (from 16 up to 1024
configurations) and the number of the random features used to approximate the kernel function. We evaluate the cases
where V is -0.50 and -2.00 V vs SHE, as these represent the nearest and most distant conditions for the descriptors
trained on the PZC geometries. It is evident that the domain-specific PZC descriptors are superior to the general-purpose
MACE-MP-0 ones at -0.50 V as well as at -2.00 V, regardless of the training set size and the number of random features.
This test further proves the importance of the domain-specific descriptors in the TRECI strategy.
S1.2 Assessing the stability of domain-specific models
As discussed in the main text, models based on domain-specific descriptors generally require more data than general-
purpose models to ensure stable MD trajectories, particularly in cases involving highly negative potentials. To address
this, we tested Franken models built from various domain-specific models, including both equivariant GNNs and larger
datasets.
As expected, models employing equivariant descriptors proved more expressive, delivering stable results with smaller
datasets compared to their invariant counterparts. This is the case of the descriptors from a MACE model with equivariant
messages (L = 1) and 128 channels, trained exclusively on geometries at PZC (Franken-PZC-L1). However, equivariant
S1

architectures are computationally more demanding, increasing inference costs.
For this reason, we also explored invariant descriptors trained on larger datasets. One such case is a MACE model with
invariant messages (L = 0) and 256 channels, trained on a combined dataset comprising PZC geometries and config-
urations compatible with applied potentials (Franken-PZC+Charged-L0)1. Within this framework, the active learning
process enables not only the progressive optimisation of the readout blocks but also the retraining of descriptors
using the expanded dataset, which includes the newly acquired geometries associated with the applied potential from
the previous iterations. Interestingly, after completing the active learning, we observed on the final dataset that even
domain-specific MACE descriptors with invariant messages (L = 0) and 256 channels, trained only on PZC geometries
(Franken-PZC-L0), can guarantee stable results. This demonstrates that stable performance can be achieved with a
thousand of data points without resorting to expensive equivariant models or continuously retraining the descriptors.
Based on these findings, we recommend using equivariant descriptors for the dataset expansion during the active learn-
ing process. Whereas, for production MD runs, invariant descriptors are preferable due to their lower computational
cost during inference.
S1.3 Cross-check among different descriptors
A characteristic of TRECI workflow is the monitoring of physical observables during the active learning to control their
convergence and identify a condition to stop the iterative refinement of the model. In addition, at the end of the active
learning, we further validate the results: we compare a physical quantity (e.g., the solvent density profile) using models
with different descriptors. As shown in Fig. S2, it is evident that the final results are almost independent from the
employed descriptors, confirming that the generated small datasets are sufficient for accurate and reliable models.
Supplementary Figure S2 Comparison of the oxygen density profiles of interfacial solvent at different applied potential values, as predicted
by Franken models with different descriptors. The same PZC profile computed with a standard MACE model is reported in each panel for
reference.
S2 Test on the locality of the ML model
An important parameter of an ML-FF is the receptive field, i.e., the spatial region around an atom that is considered
in the determination of its interactions with neighbouring atoms. MACE, and consequently Franken models based on
MACE descriptors are able to expand the receptive field beyond the cut-off radius r through a sequence of n interaction
blocks. The resulting nominal receptive field is n × r. Here, we benchmark the effects of different cut-off radii and/or
numbers of interaction blocks on the accuracy and on the predicted physical observables. Specifically, we perform these
tests on the ML-FF for the PZC and for V = - 2.00 V. As shown in Fig. S3a, there is a minimal improvement in the
RMSE on the force prediction as the receptive field is expanded for the PZC as well as for V = - 2.00 V. We also evaluate
if this small reduction of the error has an evident impact on the physical observables, such as the oxygen density profile.
Fig. S3b-c shows the O density profile at the PZC and at V = - 2.00 V for ML-FFs with different receptive fields. It is
evident that the solvent density profile is independent from the receptive field of the ML model since a constant density
is recovered within 12 ˚A in all the cases. This proves that the recovery of the water bulk properties far from the surface
is associated with the screening due to the water molecules rather than the local nature of the ML-FFs. For that reason,
all the models are developed with r = 6 ˚A and n = 2.
1Geometries compatible with applied potentials were relabelled without extra charge to maintain consistency with the PZC dataset. This relabelling
incurs no additional cost, as this DFT calculation is already required for the auxiliary systems in the “double reference method.”
S2

Supplementary Figure S3 Comparison of the effects of different cut-off radii r and/or number n of interaction blocks on the accuracy of
the models (a) and on the oxygen density profile (b-c) at the PZC and at V = - 2.00 V.
S3 Additional analysis of the effects of the applied bias
The modulation of the solvent density profile due to the applied potential, as discussed in the main text, is strictly cor-
related with a re-orientation of the dipole of water molecules. In this section, we more deeply analyse the water molecule
orientation at the different potentials, considering the distribution of two angles. The first, β, is associated with the ori-
entation of the dipole and is defined as the angle between the dipole vector of the molecule and the vector perpendicular
to the surface (see inset of Fig. S4a). Whereas the second angle, γ, is linked to the orientation of the O-H covalent bond
and is defined as the angle between the vector of the O-H bond and the vector perpendicular to the surface (see inset
of Fig. S4b).
At the PZC (Fig. S4a) and at low potentials (V ≥-1.0 V vs SHE), region I (i.e., chemisorbed molecules) is characterised
by a β distribution peaked at around 40° ÷ 60°: the resulting molecular orientation has oxygen atoms exposed towards
the surface and hydrogen atoms pointing away towards other water molecules, as visible in the MD frame reported in
Fig. S4g. Both hydrogens within the same molecule behave in a similar way and are at the same height with respect
to the surface. This can be deduced from the distribution of the γ angle in region I (Fig. S4b): even if it is possible to
define two γ angles for each molecule, both distributions are equivalent and overlap in a unique unimodal distribution.
The picture is completely different in region II: here, the β distribution reaches its maximum value in the range
120° ÷ 140°, meaning that the water dipole is mainly pointing towards the surface. This orientation is weakly depen-
dent on the applied potential value: the peak of the β distribution only slightly shifts at higher angles at more negative
potentials due to a stronger electrostatic interaction between hydrogens and the surface atoms. The analysis of the
γ distribution provides additional details. For all the applied potentials, this distribution has the first lobe at about
140° ÷ 170°, where the O-H bond points towards the surface. The second lobe in the γ distribution is at about 80° ÷ 110°.
This is associated with the second O-H bond that is almost parallel to the surface, and its H atom is interacting with
other water molecules within region II (see the orientations reported in Fig. S4h).
In addition to the previously described general picture of water orientation, our ML-enhanced model provides a finer
picture of the water structure. Indeed, a detailed analysis reveals a second weaker signal in the β and γ distributions
from a sub-region between regions II and III: this is visible at PZC and progressively more blurred at increasing nega-
tive potentials. Looking at Fig. S4a, it is possible to appreciate that there are a few molecules with a β angle at about
40° ÷ 70° in region II. This contribution is associated with water molecules with dipoles pointing away from the surface,
towards region III (see the orientation reported as “Region II-b” in Fig. S4g). This is also visible in the γ distribution
(Fig. S4b) with a contribution at around 90° ÷ 110° (O-H bond parallel to the surface and H interacting with other
molecules of region II) and at around 10° ÷ 30° (O-H bond pointing towards region III and H interacting with molecules
of region III). Notice that this detail is usually not accessible in standard AIMD due to the high signal-to-noise ratio of
the statistics but provides interesting information: molecules passing from region II to III (and vice versa) have to flip the
dipole with a β angle from 120° ÷ 140° to 40° ÷ 70°. This observation is valid at PZC and at low potentials (V ≥-1.0 V).
The description completely changes at V = -1.25 V (Fig. S4c-d) and at more negative potentials (Fig. S4e-f). The applied
potential has an impact on the structure of region III, where it becomes evident that there are two water orientations,
at least. The dipole angle is about 160° ÷ 180°, while γ is about 110° ÷ 130° in region III in proximity to the edge with
region II. These contributions are associated with water molecules with the dipole perpendicular and pointing towards
the surface. Both hydrogen atoms are almost at the same height (single spot for γ at about 110° ÷ 130°) and are inter-
acting with O of region II. This orientation can be visualised in the MD frame of Fig. S4h, where this orientation is
identified as “Region III”. On the contrary, the β and γ distributions in the middle of region III are more similar to
the region II one. An O-H bond is perpendicular to the surface (γ at 140° ÷ 170°) and the other one is almost parallel
to it (γ at 70° ÷ 100°), as emphasised in Fig. S4h, with the label “Region III-b”. The resulting dipole is not exactly
perpendicular to the surface (differently from region III close to region II) but β is peaked at 110° ÷ 150°.
We emphasise once more that these findings cannot be obtained through conventional AIMD; however, they could play
S3

a crucial role in surface reactivity. At considerably negative potentials, molecules that interact directly on the surface
(region II) are not in contact with an area resembling bulk water; rather, they can migrate in a zone that retains partial
structural organisation (region III).
Supplementary Figure S4 Colour maps for the distributions of the β dipole and γ O-H-bond angle at PZC (a-b), -1.25 (c-d) and -2.00 (e-
f) V vs SHE. The vertical dashed lines identify the different regions. Frames of the MD trajectory in which specific molecules are emphasised
to show the water orientation: (a) configuration of molecules in region I, region II and between region II and III (“Region II-b”), at low
potentials, (b) configuration in region II, in region III close to region II (“Region III”) and in the middle of region III (“Region III-b”) at
high negative potentials. O and H atoms of emphasised molecules are depicted in red and white, respectively.
S4
