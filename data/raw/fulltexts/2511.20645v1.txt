PixelDiT: Pixel Diffusion Transformers for Image Generation
Yongsheng Yu1,2 *
Wei Xiong1 â€ 
Weili Nie1
Yichen Sheng1
Shiqiu Liu1
Jiebo Luo2
1NVIDIA
2University of Rochester
â€  Project Lead and Main Advising
Abstract: Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline
where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization.
To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and
learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level
design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of
a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential
to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256Ã—256, surpassing existing pixel generative models
by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 10242 resolution in pixel space. It
achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.
(a) High-resolution text-to-image samples at the megapixel scale (approximately 1024Ã—1024) generated by PixelDiT-T2I, which is
directly trained on pixel space.
Stable Diffusion 3
FLUX
PixelDiT
Real Image
â€œA bicycle parked on the sidewalkâ€¦â€ â†’  â€œA motorcycle parked on the sidewalkâ€¦â€
SD3 VAE Recon.
FLUX VAE Recon.
(b) Train-free image editing with FlowEdit [1] based on different diffusion models. The imperfect reconstruction of VAEs used by
FLUX [2] and Stable Diffusion 3 [3] produce severe and non-invertible distortions on the small details such as the scene texts on the
wall, therefore the subsequent editing of the latent diffusion models fails. In contrast, by denoising directly in pixel space without a
VAE, our PixelDiT avoids VAE reconstruction artifacts and helps maintain background consistency during local manipulations. For a
fair comparison, all models are evaluated with nmin= 0 in FlowEdit to preserve more structures.
Figure 1 | Visual results of PixelDiT on text-to-image generation and training-free image editing. Please zoom in for
the details. Additional examples are provided in the Appendix.
* Work was done while Yongsheng was an intern at NVIDIA.
Contact: {yongshengy, wxiong}@nvidia.com
Â© 2025 NVIDIA. All rights reserved.
arXiv:2511.20645v1  [cs.CV]  25 Nov 2025

PixelDiT: Pixel Diffusion Transformers for Image Generation
1. Introduction
Latent diffusion models (LDMs) [4] perform denoising in
a compressed representation space and have become the
standard paradigm for Diffusion Transformers [5, 6]. This
choice yields substantial savings in compute and mem-
ory. However, it also inherits two structural limitations.
First, LDMs couple diffusion to a separately pretrained au-
toencoder whose reconstruction objective is only partially
aligned with the downstream generative objective [7, 8].
Second, the autoencoder introduces a lossy reconstruction
that can remove high-frequency details and cap sample
fidelity for image generation and editing tasks even when
the diffusion model is strong [3, 9, 10, 11], as shown in
Figure 1b. These limitations motivate us to revisit pixel-
space diffusion, where both learning and sampling operate
directly in the original pixels without autoencoders.
The core challenge in pixel-space diffusion can be framed
as pixel modeling. By pixel modeling, we refer to the
mechanism of capturing dense, per-pixel interactions and
high-frequency details, which is distinct from the semantic
structural generation typically handled by coarse patch
tokens. Effectively modeling these per-pixel tokens is
crucial for texture fidelity but computationally expensive.
Prior attempts expose a fundamental trade-off in how pixel
interactions are organized. One line of work adopts aggres-
sive patchification [12, 13, 14, 15, 16] to keep attention
affordable, but this significantly weakens per-pixel token
modeling and hinders the generation of finer visual con-
tents. Another line pushes toward near-pixel granularity
(e.g., small patch sizes or U-ViT-like designs) to better
preserve details [17, 18, 19], but the global attention must
process very long token sequences with quadratic complex-
ity, or rely on heavy decoder-style stacks, resulting in high
training and sampling costs. Cascaded pipelines [20, 21]
mitigate some cost, but may introduce additional stages
and accumulated errors. Taken together, these observa-
tions suggest that the obstacle to practical pixel-space
diffusion is the lack of an efficient pixel modeling mecha-
nism that can model both global semantics and per-pixel
updates.
To address these challenges, we propose PixelDiT, a single-
stage, fully transformer-based diffusion model that per-
forms end-to-end training and sampling in pixel space
while explicitly structuring pixel modeling. PixelDiT de-
couples image semantics from per-pixel learning with a
dual-level architecture design: a patch-level DiT with an
aggressive patch size that performs long-range attention
on short patch token sequences to capture global layout
and content, and a pixel-level DiT that performs dense,
per-pixel token modeling to refine local texture details.
We propose two key techniques to make this design effec-
tive: (i) a pixel-wise AdaLN modulation that conditions
each pixel token using semantic tokens, aligning per-pixel
updates with global context; and (ii) a pixel token com-
paction mechanism that compresses each pixel token be-
fore full attention and decompresses them back afterward,
enabling per-pixel token modeling while keeping global
attention efficient. The dual-level pathways, together with
efficient pixel modeling via pixel-wise AdaLN and token
compaction, yield high training efficiency and faster con-
vergence while preserving fine details.
Extensive experiments show that PixelDiT generates high-
quality images when trained end-to-end on pixel space
without any autoencoders. On ImageNet 256 Ã— 256, Pix-
elDiT achieves an FID of 1.61, outperforming recent pixel-
space models by a large margin. We further demonstrate
the scalability of our architecture by extending PixelDiT to
text-to-image generation. Using multi-modal DiT blocks
in the patch-level pathway, we directly train PixelDiT at
10242 resolution in pixel space. To our knowledge, this
represents a significant breakthrough, as prior pixel-space
models have struggled to scale effectively to such reso-
lutions. PixelDiT can generate high-fidelity text-aligned
1K resolution images as shown in Figure 1a and achieves
competitive scores on standard benchmarks compared to
state-of-the-art latent diffusion models. Moreover, oper-
ating directly in pixel space enables PixelDiT to bypass
VAE reconstruction artifacts, leading to significantly better
content preservation for image editing tasks, as shown in
Figure 1b.
We highlight our main contributions as follows:
â€¢ We
propose
PixelDiT,
a
single-stage,
fully
transformer-based pixel-space diffusion model that is
trained end-to-end without a separate autoencoder.
â€¢ We demonstrate that efficient pixel modeling is the
key factor to practical pixel-space diffusion and pro-
pose a dual-level DiT architecture that disentangles
the learning of global semantics from pixel-level tex-
ture details.
â€¢ We introduce a pixel-wise AdaLN modulation mech-
anism and a pixel token compaction mechanism that
jointly enable dense per-pixel token modeling.
â€¢ PixelDiT achieves high image quality in both class-
conditioned image generation and text-to-image gen-
eration, significantly outperforming existing pixel-
space generative models and approaching the state-
of-the-art latent diffusion models.
2. Related Works
2.1. Latent Diffusion Models with Autoencoders
Latent diffusion models (LDMs) perform denoising in
a latent space produced by an autoencoder, delivering
substantial compute and memory savings and enabling
training at higher resolutions with larger backbones on a
fixed budget [4]. In practice, most LDMs adopt a varia-
2

PixelDiT: Pixel Diffusion Transformers for Image Generation
Noised
Image
1Ã—1
Patchify
16Ã—16
Patchify
DiT Blocks
Timestep and Class
Embedding
AdaLN-Zero
Condition
Semantic Token
Ã—N
PiT Blocks
AdaLN-Zero
Condition
Pixel Token
Ã—M
RMSNorm
RoPE
MHSA
Pixel-wise
Gate
Linear Expand
Linear Compress
RMSNorm
Pixel-wise
Gate
FFN
MLP
Pixel-wise
Scale & Shift
Pixel-wise
Scale & Shift
Figure 2 | Overview of PixelDiT: a dual-level, fully transformer-based diffusion architecture that operates directly
in pixel space. The left figure shows the overall framework of PixelDiT, while the right figure illustrates the detailed
structure of the PiT blocks.
tional autoencoder (VAE) that trades off reconstruction
fidelity and compression rate; a rich line of work im-
proves the autoencoder via better architecture or learn-
ing objective, including stronger compression schemes,
tokenizers [22, 23, 24, 25] and analysis of the reconstruc-
tionâ€“generation optimization dilemma [7]. End-to-end ap-
proaches have also been explored: REPA-E jointly tunes
the VAE and diffusion transformer to align the latent rep-
resentation with the generative objective [8]. Recently,
several works [11, 26] replace the variational bottleneck
with representation autoencoders and report competitive
latents without explicit variational modeling.
Why revisit pixel space? Despite their efficiency, LDMs
inherit a reconstruction bottleneck: sample fidelity is
bounded by the autoencoder, and aggressive compres-
sion tends to remove high-frequency details and fine struc-
tures [4, 22]. Pretraining or co-training a large autoencoder
adds additional data and compute overhead compared to
pixel-space training. Moreover, misalignment between
the autoencoderâ€™s reconstruction objective and the down-
stream generative objective introduces distribution shift in
latent space (e.g., texture smoothing or color shifts) that
the diffusion model must compensate for [7, 8]. Finally,
decoding latency at sampling time incurs additional cost
compared to pixel-space models.
2.2. Pixel-Space Diffusion Models
Pixel-space diffusion models preceded latent-space meth-
ods and remain an active area of research. Early work
establishes high image quality via direct denoising in
pixel space [27], while cascaded diffusion improves high-
resolution synthesis through multi-scale pipelines [20].
However, the quadratic compute and memory cost in im-
age resolution renders end-to-end training at megapixel
scales prohibitively expensive.
Recent efforts revisit
pixel space with improved architectures and training: Jet-
Former formulates autoregressive generation over raw pix-
els and text [13]; Simple Diffusion proposes simplified,
memory-efficient convolutional networks with skip con-
nections [18, 19]; Fractal Generative Models introduce
fractal designs for long-range structure [21]; PixelFlow
develops hierarchical flow-based pixel-space models [17];
and PixNerd employs lightweight neural field layers for
efficient pixel-space diffusion [12]. In contrast to these
efforts, we present a purely transformer-based pixel-space
diffusion architecture trained directly at 10242 resolution.
Concurrent to our work, EPG [15] adopts a two-stage
framework bridging self-supervised pre-training and gen-
erative fine-tuning, while FARMER [14] integrates nor-
malizing flows with autoregressive modeling to handle
high-dimensional pixels. Distinctively, JiT [16] demon-
strates that plain Transformers can efficiently model
high-dimensional data by predicting clean images (ğ‘¥0-
prediction).
3. Method
In this section, we present PixelDiT, a transformer-based
diffusion model that directly performs denoising in pixel
space. Our objective is to make pixel token modeling com-
putationally efficient while preserving the convergence
behavior and sample quality that have motivated latent
space approaches.
3.1. Dual-level DiT Architecture
As illustrated in Figure 2, we adopt a dual-level trans-
former organization that concentrates semantic learning
on a coarse patch-level pathway and leverages dedicated
Pixel Transformer (PiT) blocks in the pixel-level path-
way for detail refinement. This organization allows most
semantic reasoning to occur on the low-resolution grid,
which reduces the burden on the pixel-level pathway and
accelerates learning, consistent with observations found
in [11, 28, 29].
Patch-level architecture:
Let the input image be ğ‘¥âˆˆ
3

PixelDiT: Pixel Diffusion Transformers for Image Generation
RğµÃ—ğ¶Ã—ğ»Ã—ğ‘Š. We form non-overlapping ğ‘Ã— ğ‘patch to-
kens ğ‘¥patch âˆˆRğµÃ—ğ¿Ã—(ğ‘2ğ¶), where ğ¿=( ğ»
ğ‘)( ğ‘Š
ğ‘) is the
number of tokens, and project them to hidden size ğ·:
ğ‘ 0 = ğ‘Špatch ğ‘¥patch,
(1)
ğ‘= SiLU(ğ‘Šğ‘¡ğ‘¡+ ğ‘Šğ‘¦ğ‘¦+ ğ‘) âˆˆRğµÃ—1Ã—ğ·.
(2)
Following [7], we augment the DiT block by replacing
LayerNorm with RMSNorm and applying 2D RoPE in all
attention layers. The patch-level pathway consists of ğ‘
augmented DiT blocks; for block ğ‘–, we write
Ëœğ‘ ğ‘–= RMSNorm(ğ‘ ğ‘–),
(3)
ğ‘ ğ‘–= ğ‘ ğ‘–+ ğ›¼1(ğ‘) âŠ™Attn
(ï¸€
ğ›¾1(ğ‘) âŠ™Ëœğ‘ ğ‘–+ ğ›½1(ğ‘); RoPE
)ï¸€
,
(4)
ğ‘ ğ‘–+1 = ğ‘ ğ‘–+ ğ›¼2(ğ‘) âŠ™MLP
(ï¸€
ğ›¾2(ğ‘) âŠ™RMSNorm(ğ‘ ğ‘–) + ğ›½2(ğ‘)
)ï¸€
,
(5)
where AdaLN modulation parameters are produced from
the global conditioning vector ğ‘and then broadcast across
the ğ¿patch tokens. This global-to-patch broadcasting ap-
plies identical per-feature AdaLN parameters to all patch
tokens (i.e., token-independent at the patch level), in con-
trast to the pixel-wise AdaLN used later in the pixel-level
pathway.
After ğ‘blocks, we obtain semantic tokens ğ‘ ğ‘
âˆˆ
RğµÃ—ğ¿Ã—ğ·. In the spirit of designs [11, 28], we define
the conditioning signal for the pixel-level pathway as
ğ‘ cond := ğ‘ ğ‘+ğ‘¡, where ğ‘¡is the timestep embedding. These
tokens provide semantic context to the PiT blocks via
pixel-wise AdaLN.
Pixel-level architecture:
The pixel-level DiT is com-
posed of ğ‘€layers of PiT Blocks. It takes the pixel tokens
and the output of the patch-level DiT ğ‘ cond as inputs to
perform the pixel token modeling and generate the final
result. The details of each PiT block are described below.
Design notes.
The patch-level pathway exclusively pro-
cesses patch tokens to capture global semantics. By del-
egating detail refinement to the pixel-level pathway, we
can employ larger patch sizes ğ‘, which shortens the se-
quence length and accelerates inference while preserving
per-pixel fidelity. Furthermore, the pixel-level pathway
operates with a reduced hidden dimension ğ·pix â‰ªğ·(e.g.,
ğ·pix=16), ensuring that dense per-pixel computations re-
main highly efficient.
3.2. Pixel Transformer Block
Each PiT block has two core components. First, pixel-
wise AdaLN enables dense conditioning at the level of
individual pixels, aligning per-pixel updates with global
context. Second, a pixel token compaction mechanism
reduces redundancy among pixel tokens so that global
attention operates on a manageable sequence length.
Global Vector
1Ã—D
Repeat
Semantic Token
Repeat
LÃ—D
Semantic Token
MLP
LÃ—D
LÃ—D
LÃ—D
AdaLN modulation 
parameters
Ã—LÃ—P2
Ã—P2
ğ”¸
ğ”¹
â„‚
Figure 3 | AdaLN modulation strategies. (A) A naive
AdaLN broadcasts a global conditioning vector to all pix-
els. (B) Patch-wise AdaLN expands semantic tokens to
the ğ‘2 pixels within each patch. (C) Pixel-wise AdaLN
applies an MLP to each semantic token to produce per-
pixel scale, shift, and gating parameters, enabling fully
context-aligned updates at every pixel.
Pixel-wise AdaLN Modulation.
In the pixel-level path-
way, each image is embedded into one token per pixel with
a linear layer:
ğ‘‹âˆˆRğµÃ—ğ¶Ã—ğ»Ã—ğ‘Š
reshape + linear
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’RğµÃ—ğ»Ã—ğ‘ŠÃ—ğ·pix.
(6)
To align with patch-level semantic tokens, we reshape
into ğµÂ· ğ¿sequences of ğ‘2 pixel tokens, i.e., ğ‘‹
âˆˆ
R(ğµÂ·ğ¿)Ã—ğ‘2Ã—ğ·pix.
For each patch, we form a semantic
conditioning token ğ‘ cond âˆˆR(ğµÂ·ğ¿)Ã—ğ·that summarizes
global context. A straightforward patch-wise modulation
would repeat the same parameters for all ğ‘2 pixels within
a patch, as illustrated in Figure 3(B). However, this can-
not capture dense per-pixel variation. Instead, we assign
independent modulation to each pixel by expanding ğ‘ cond
into ğ‘2 sets of AdaLN parameters via a linear projection
Î¦ : Rğ·â†’Rğ‘2Â·6ğ·pix:
Î˜ = Î¦(ğ‘ cond) âˆˆR(ğµÂ·ğ¿)Ã—ğ‘2Ã—6ğ·pix,
(7)
and we partition the last dimension of Î˜ into six
groups of size ğ·pix, yielding (ğ›½1, ğ›¾1, ğ›¼1, ğ›½2, ğ›¾2, ğ›¼2) âˆˆ
(ï¸€
R(ğµÂ·ğ¿)Ã—ğ‘2Ã—ğ·pix)ï¸€6.
These modulation parameters are
learned and are distinct at every pixel, as illustrated in
Figure 3(C). They are applied to ğ‘‹via pixel-wise AdaLN,
enabling pixel-specific updates; in contrast, patch-wise
AdaLN broadcasts a single set of parameters to all pix-
els within a patch and thus cannot capture such spatial
variation.
Pixel Token Compaction.
In the pixel-level pathway,
direct attention over all ğ»Ã— ğ‘Špixel tokens is computa-
4

PixelDiT: Pixel Diffusion Transformers for Image Generation
Method
Training
Epochs
#params
Generation@256
gFIDâ†“
sFIDâ†“
ISâ†‘
Precisionâ†‘
Recallâ†‘
Latent Generative Models
LDM-4-G [4]
170
400M
3.60
-
247.6
0.87
0.48
DiT-XL [5]
1400
675M
2.27
4.60
278.2
0.83
0.57
SiT-XL [6]
1400
675M
2.06
4.50
270.3
0.82
0.59
MaskDiT [30]
1600
675M
2.28
5.67
276.5
0.80
0.61
REPA [31]
800
675M
1.42
4.70
305.7
0.80
0.65
LightningDiT [7]
800
675M
1.35
4.15
295.3
0.79
0.65
SVG-XL [26]
1400
675M
1.92
-
264.9
-
-
DDT-XL [28]
400
675M
1.26
-
310.6
0.79
0.65
RAE-XL [11]
800
839M
1.13
-
262.6
0.78
0.67
Pixel Generative Models
StyleGAN-XL [32]
/
/
2.30
4.02
265.1
0.78
0.53
ADM-U [27]
400
554M
4.59
5.25
186.7
0.82
0.52
CDM [20]
2160
/
4.88
-
158.7
-
-
RIN [33]
480
410M
3.42
-
182.0
-
-
VDM++ [34]
/
/
2.12
-
267.7
-
-
JetFormer [13]
/
2.8B
6.64
-
-
0.69
0.56
Simple Diffusion [18]
/
2.0B
2.44
-
256.3
-
-
FractalMAR-H [21]
600
844M
6.15
-
348.9
0.81
0.46
FARMER [14]
320
1.9B
3.60
-
269.2
0.81
0.51
EPG [15]
800
583M
2.04
-
283.2
0.80
0.56
PixelFlow-XL [17]
320
677M
1.98
5.83
282.1
0.81
0.60
PixNerd-XL [12]
320
700M
1.93
-
298.0
0.80
0.60
JiT-G [16]
600
2B
1.82
-
292.6
0.79
0.62
PixelDiT-XL
80
797M
2.36
5.11
282.3
0.80
0.57
PixelDiT-XL
320
797M
1.61
4.68
292.7
0.78
0.64
Table 1 | Quantitative results on ImageNet 256Ã—256 for class-conditioned generation.
tionally prohibitive. We therefore compress the ğ‘2 pixel
tokens inside each patch into a compact patch token before
global attention, and later expand the attended representa-
tion back to pixels. This reduces the attention sequence
length from ğ»Ã— ğ‘Što ğ¿=( ğ»
ğ‘)( ğ‘Š
ğ‘), a ğ‘2-fold reduction;
with ğ‘=16, this yields a 256Ã— shrinkage while preserv-
ing per-pixel updates through pixel-wise AdaLN and the
learned expansion.
We instantiate the compaction operators with a learned flat-
tening: a linear map ğ’: Rğ‘2Ã—ğ·pix â†’Rğ·that jointly mixes
spatial and channel dimensions, paired with an expan-
sion â„°: Rğ·â†’Rğ‘2Ã—ğ·pix. This compressâ€“attendâ€“expand
pipeline keeps global attention efficient. Unlike the lossy
bottleneck in VAEs, this mechanism only compresses the
representation momentarily for the attention operation.
Crucially, this compaction operates purely to reduce the
computational overhead of self-attention; it does not com-
promise fine-grained details, because high-frequency in-
formation is preserved through residual connections and
learned expansion layers that effectively bypass the pixel-
token bottleneck.
3.3. PixelDiT for Text-to-Image Generation
We extend the patch-level pathway with multi-modal DiT
(MM-DiT) blocks [3] that fuse text and image semantics
while leaving the pixel-level pathway unchanged. In each
MM-DiT block, image and text tokens form two streams
with separate QKV projections.
Text embeddings ğ‘¦âˆˆRğµÃ—ğ¿txtÃ—ğ·txt are produced by
a frozen Gemma-2 encoder [35]. Following [36], we
prepend a concise system prompt to the user prompt before
feeding the sequence to the text encoder. The resulting
token embeddings are projected to the model width and
used as the text stream in MM-DiT.
Empirically, we find the semantic tokens from the patch-
level pathway are sufficient to convey textual intent to
the pixel updates. The pixel-level pathway is therefore
architecturally identical to the class-conditioned model: it
operates on pixel tokens and is conditioned only through
the semantic tokens together with the timestep. No text
tokens are routed directly to the pixel stream.
3.4. Training Objectives
We adopt the Rectified Flow formulation [37] in pixel
space and train the model with its velocity-matching loss:
â„’diff = Eğ‘¡,ğ‘¥,ğœ€
[ï¸€
â€–ğ‘“ğœƒ(ğ‘¥ğ‘¡, ğ‘¡, ğ‘¦) âˆ’ğ‘£ğ‘¡â€–2
2
]ï¸€
.
(8)
Following [31], we include an alignment objective that
encourages mid-level patch-pathway tokens to agree with
features from a frozen DINOv2 encoder [38]. The overall
5

PixelDiT: Pixel Diffusion Transformers for Image Generation
Figure 4 | Qualitative results on ImageNet 256 Ã— 256 using PixelDiT-XL. We use a classifier-free guidance scale
ğ›¼cfg = 4.0.
objective is â„’= â„’diff + ğœ†repa â„’repa. We use the same
formulation for class- and text-conditional models.
4. Experiments
We evaluate the effectiveness of PixelDiT through exten-
sive experiments. To demonstrate the scalability of our
approach, we instantiate PixelDiT with three model sizes:
Base (B), Large (L), and Extra Large (XL) for experiments
on ImageNet. The detailed configurations are summarized
in Table 2. Unless otherwise specified, we use PixelDiT-
XL as the default model for all experiments on ImageNet.
Config
ğ‘
ğ‘€
ğ·
ğ·pix
Heads
Params (M)
PixelDiT-B
12
2
768
16
12
184
PixelDiT-L
22
4
1024
16
16
569
PixelDiT-XL
26
4
1152
16
16
797
Table 2 | Model configurations for PixelDiT variants for
experiments on ImageNet 256Ã—256. ğ‘and ğ‘€denote the
depth of the patch-level and pixel-level pathways, respec-
tively. ğ·and ğ·pix represent the hidden dimension of the
patch-level and pixel-level pathways.
4.1. Implementation Details
Class-conditioned Image Generation. We follow the
training setup of [5] and train our model on ImageNet-
1K [39] at 256Ã—256 resolution (denote as ImageNet
256Ã—256) for class-conditioned generation tasks. For the
representation alignment objective, we set ğœ†repa=0.5 and
apply alignment at the eighth block of the patch-level path-
way. Throughout diffusion training, we use logit-normal
sampling [3], EMA with decay 0.9999, AdamW with be-
tas (0.9, 0.999), a batch size of 256, and bfloat16 mixed
precision. We train the model with a constant learning rate
of 1 Ã— 10âˆ’4 for the first 160 epochs, then step down to
1 Ã— 10âˆ’5 for the remainder. We apply gradient clipping at
1.0 initially and 0.5 thereafter for stability.
Text-to-Image Generation. We use Gemma-2 [35] as the
text encoder and incorporate the MM-DiT conditioning
design [3] on the patch-level pathway. Our PixelDiT-T2I
uses hidden size 1536, patch-level pathway depth ğ‘=14,
and pixel-level pathway depth ğ‘€=2. We collect approx-
imately 26M imageâ€“text pairs at 10242 resolution with
various aspect ratios to train our model. We first pre-train
the model from scratch at 512 Ã— 512 resolution for 400K
iterations with AdamW (learning rate 1 Ã— 10âˆ’4; betas
(0.9, 0.999)), batch size 1,024, gradient clipping 0.5, and
the shifting strategy [3] with shift value ğ›¼=3.0. We then
finetune the model at 10242 resolution for another 100K
iterations with AdamW (learning rate 2 Ã— 10âˆ’5), batch
size 768, a higher shift value ğ›¼=4.0, and gradient clipping
0.1.
Evaluation Settings on ImageNet. Following ADM [27],
we report FID (gFID), sFID, Inception Score, and Preci-
sionâ€“Recall on 50K samples. On ImageNet 256Ã—256, we
use a guidance scale 3.25 with an interval [40] [0.1, 1.0]
for the 80-epoch checkpoint. All other evaluations use a
guidance 2.75 with an interval [0.1, 0.9], unless otherwise
stated.
Evaluation Settings for Text-to-Image. We evaluate
PixelDiT-T2I at 512 Ã— 512 and 10242 resolutions on
GenEval [41] and DPG-Bench [42] using 533 and 1,065
prompts, respectively. Unless noted, we use a fixed guid-
ance scale 4.5 and the shift value aligns with the training
setting at corresponding resolutions.
Sampling at Inference. For both class-conditioned and
text-conditional image generation tasks, we use FlowDPM-
Solver [36], a modified DPMSolver++ [43] in the Rectified
Flow formulation, with bfloat16 precision. By default, we
perform sampling with 100 steps on ImageNet and 25
steps for text-to-image generation.
6

PixelDiT: Pixel Diffusion Transformers for Image Generation
0K
200K
400K
600K
800K 1000K
Training Steps
10
20
30
40
50
gFID
B/16
B/4
B/32
0K
200K
400K
600K
800K 1000K
Training Steps
0
10
20
30
40
50
L/16
L/8
L/32
0K
200K
400K
600K
800K 1000K
Training Steps
2
3
4
5
6
XL/16
XL/8
(a) Patch-size ablations for B/L/XL models on ImageNet 256Ã—256.
0K
200K
400K
600K
800K 1000K
Training Steps
0
5
10
15
20
gFID
XL/16
L/16
B/16
700K
900K
1.8
2.0
2.2
2.4
(b) Comparison at fixed patch size.
Figure 5 | Convergence analysis of PixelDiT on ImageNet 256Ã—256. (a) gFID vs. training iterations for B, L, and XL
models with varying patch sizes. (b) Comparison of B/L/XL models at a fixed patch size ğ‘=16.
Methods
Params
GenEval â†‘
DPG â†‘
Throughput
(samples/s) â†‘
(B)
512 Ã— 512 resolution
PixArt-ğ›¼[44]
0.6
0.48
71.6
1.5
PixArt-Î£ [45]
0.6
0.52
79.5
1.5
PixelFlowâ€  [17]
0.9
0.60
77.9
0.05
PixNerdâ€  [12]
1.2
0.73
80.9
1.04
PixelDiT-T2Iâ€ 
1.3
0.78
83.7
1.07
1024 Ã— 1024 resolution
PixArt-Î£ [45]
0.6
0.54
80.5
0.4
LUMINA-Next [46]
2.0
0.46
74.6
0.12
SDXL [47]
2.6
0.55
74.7
0.15
Playground v2.5 [48]
2.6
0.56
75.5
0.21
Hunyuan-DiT [49]
1.5
0.63
78.9
0.05
DALLE 3 [10]
-
0.67
83.5
-
FLUX-dev [2]
12.0
0.67
84.0
0.04
PixelDiT-T2Iâ€ 
1.3
0.74
83.5
0.33
Table 3 | Comprehensive comparison of our method
with text-to-image approaches. We highlight the best
and second-best entries. â€  indicates pixel-space diffusion
models.
4.2. Class-conditioned Image Generation on ImageNet
As shown in Table 1, at 320 epochs, our PixelDiT-XL
obtains a gFID of 1.61, surpassing recent pixel-space
models including PixelFlow-XL (gFID 1.98), PixNerd-
XL (gFID 1.93), and EPG (gFID 2.04) by a large margin,
showing a very significant improvement in image quality.
Besides fidelity, PixelDiT-XL exhibits a stronger diver-
sityâ€“faithfulness trade-off, achieving a recall of 0.64 (vs.
0.60 for PixelFlow-XL), with competitive precision (0.78).
Notably, PixelDiT-XL converges quickly: after only 80
epochs it already reaches gFID 2.36 and IS 282.3, out-
performing classical pixel diffusion (ADM-U, gFID 4.59
at 400 epochs) and autoregressive baselines (JetFormer,
gFID 6.64), indicating that structuring pixel modeling ac-
celerates convergence. Note that SiD2 [19] reports a gFID
of 1.38 at 256 Ã— 256, but its model size, training epochs,
IS, and Precision/Recall are unknown. For fairness, we
include its gFID for reference but exclude it from Table 1.
While latent-space models such as REPA and RAE achieve
lower gFID, PixelDiT-XL narrows the gap without any
pretrained autoencoder, demonstrating the potential of
pixel-space diffusion models in generating high-quality
images. This is further supported by qualitative results
in Figure 4. PixelDiT-XL produces sharp textures, coher-
ent object boundaries, and reasonable global structures,
showing superiority of our dual-level architecture design.
4.3. Text-to-Image Generation
Table 3 presents the quantitative results of PixelDiT-T2I
on text-to-image generation task. At 512 Ã— 512 resolution,
PixelDiT-T2I attains a GenEval score of 0.78 and DPG
of 83.7, outperforming recent pixel-space models such as
PixNerd and PixelFlow. At 10242 resolution, PixelDiT-
T2I achieves a GenEval score of 0.74 and DPG of 83.5,
surpassing many latent diffusion models on GenEval and
remaining competitive on DPG. These results indicate that
end-to-end pixel-space diffusion with dual-level design
scales to text-to-image generation with strong text ground-
ing and robust compositionality. Qualitatively, Figure 1(a)
shows representative samples produced by PixelDiT-T2I,
illustrating high-resolution synthesis at the megapixel
scale and consistent multi-resolution generation. More
visualizations are provided in the appendix.
Efficiency. Throughput in Table 3 is measured with fp16
precision on a single NVIDIA A100 GPU. PixelDiT-T2I
reaches 1.07 samples per second at 512 Ã— 512 resolution,
exceeding pixel-space baselines PixelFlow and PixNerd.
At 10242 resolution, our PixelDiT-T2I reaches 0.33 sam-
ples per second, comparable to latent diffusion models,
despite denoising directly in pixel space.
4.4. Detail Preservation in Image Editing
In Figure 1(b),
we perform image editing with
FlowEdit [1] and compare the results using latent diffu-
sion models (Stable Diffusion 3, FLUX) and PixelDiT.
While both latent models generate motorcycle to replace
the bicycle, the small scene text on the wall is heavily
distorted. To investigate the cause of the distortion, Fig-
7

PixelDiT: Pixel Diffusion Transformers for Image Generation
Model Components
Epoch
gFIDâ†“
A: Vanilla DiT/16
80
9.84
A: + RoPE, RMSNorm
80
8.53
B: + Dual-level, patch-wise AdaLN (no compaction)
80
OOM
B: + Pixel Token Compaction
80
3.50
C: + Pixel-wise AdaLN
80
2.36
320
1.61
Table 4 | Ablations of PixelDiT-XL on ImageNet
256Ã—256. Results start from Vanilla DiT and incremen-
tally add architectural improvements and inference strate-
gies. OOM indicates the dual-level variant without token
compaction exceeds memory limits. Labels Aâ€“C match
the design schematic in Figure 3.
ure 1(b) presents the direct reconstruction results by the
latent modelsâ€™ VAEs, which show that the scene text has
already been distorted by the VAE encoder due to its lossy
compression nature, and the error further accumulates in
the latent diffusion process. In contrast, since PixelDiT is
end-to-end trained in pixel space without relying on VAEs,
it does not suffer from the lossy compression distortion.
Therefore, it not only makes correct modifications, but also
preserves the small scene text well, showing significant
advantages on preserving fine details for image editing.
4.5. Ablation Study
4.5.1. Contribution of Core Components
Table 4 quantifies the contribution of the proposed pixel-
modeling components by comparing different model vari-
ants. Note that labels Aâ€“C in Table 4 correspond to the
schematic variants in Figure 3. Specifically, we use a
30-layer, 16 Ã— 16 patchified DiT that directly performs
denoising in pixel space as the baseline model (â€œVanilla
DiT/16â€). This baseline operates solely on patch tokens
without a dedicated pixel-level pathway, treating each
16 Ã— 16 patch as a high-dimensional vector. It obtains
9.84 gFID at 80 epochs. Introducing a dual-level archi-
tecture without pixel token compaction causes global at-
tention to scale quadratically with the number of pixels
and leads to an out-of-memory condition (OOM). Adding
pixel token compaction resolves this bottleneck by short-
ening the global-attention sequence from ğ»Ã—ğ‘Špixels
to ğ¿=( ğ»
ğ‘)( ğ‘Š
ğ‘) patches, yielding a significant quality im-
provement to 3.50 gFID at the same 80-epoch budget.
Incorporating pixel-wise AdaLN further aligns per-pixel
updates with the semantic context produced by the patch-
level pathway, improving gFID to 2.36 at 80 epochs and
to 1.61 at 320 epochs.
The comparison between model variants A, B, and C
demonstrates the importance of each proposed compo-
nent. More importantly, the comparison between our full
PixelDiT model C and the vanilla DiT/16 A reveals that
pixel-level token modeling plays a key role in pixel gen-
Configuration
GFLOPs
Epoch 80
Epoch 160
FIDâ†“
ISâ†‘
FIDâ†“
ISâ†‘
PixelDiT-XL
311
2.36
282.3
1.97
299.4
No Pixel Token Compaction
82247
OOM
No Pixel-Pathway Attention
279
2.56
256.9
2.22
281.7
Table 5 | Computation and convergence analyses of
pixel token compaction. We report GFLOPs alongside
ImageNet 256Ã—256 metrics at 80 and 160 epochs. No
Pixel Token Compaction removes the compressâ€“expand
pathway, resulting in out-of-memory (OOM) at our train-
ing scale.
No Pixel-Pathway Attention ablates self-
attention in all PiT blocks.
erative models. Without pixel modeling, i.e., the visual
content is only learned at the patch level, it will be chal-
lenging for the model to learn the fine details, and the
visual quality will degrade significantly.
4.5.2. Analysis on Pixel Token Compaction
Token compaction is essential for making pixel-space train-
ing feasible. A global attention over ğ‘=ğ»Ã— ğ‘Špixel
tokens incurs ğ‘‚(ğ‘2) memory and ğ‘‚(ğ‘2ğ·) FLOPs, pro-
ducing billions of attention entries even at 256 Ã— 256 res-
olution, as reflected by the 82,247 GFLOPs reported for
this variant in Table 5. Grouping pixels into ğ‘Ã— ğ‘patches
using pixel token compaction reduces the sequence length
to ğ¿=ğ‘/ğ‘2, yielding a ğ‘4-fold reduction in attention cost.
To analyze the role of attention in the pixel-level pathway,
we include a â€œNo Pixel-Pathway Attentionâ€ ablation that
removes the attention and only keeps pixel-wise AdaLN
and an MLP at the pixel level. As shown in Table 5, while
this variant reduces GFLOPs, it is consistently inferior to
our full PixelDiT model across different training iterations
(e.g., from 80 to 160 epochs), with visible degradation in
gFID and IS. This indicates that compact global attention
is necessary to align local updates with global context.
4.5.3. Impact of Model Size and Patch Size
We investigate the impact of the patch size ğ‘on the perfor-
mance of models at different scales: PixelDiT-B, PixelDiT-
L, and PixelDiT-XL. For all evaluations, we use the same
CFG guidance scale 3.25 with interval [0.10, 1.00]. We
evaluate patch sizes of 4, 8, 16, and 32 on ImageNet
256Ã—256; Figure 5(a) visualizes the resulting convergence
behavior. For the base model, moving from ğ‘=32 to ğ‘=16
and ğ‘=4 substantially accelerates convergence: at 200K
iterations gFID drops from 48.5 (B/32) to 15.1 (B/16)
and 6.7 (B/4), and B/4 ultimately reaches 3.4 gFID at
500K iterations. Larger models follow a similar trend, but
the benefit of very small patches diminishes with scale.
For PixelDiT-L, using ğ‘=8 rather than ğ‘=16 improves
gFID only modestly (from 2.72 to 2.15 at 300K itera-
tions), and for PixelDiT-XL the gap between ğ‘=8 and
8

PixelDiT: Pixel Diffusion Transformers for Image Generation
ğ‘=16 essentially vanishesâ€”both configurations converge
to gFID near 2.0. These results highlight a clear trade-off:
smaller patches yield better image quality or faster conver-
gence but incur a quadratic cost in sequence length, and
the relative gain shrinks as the model capacity increases.
In practice, we therefore use ğ‘=16 as the default patch
size for PixelDiT-XL, which offers near-optimal quality
at substantially lower compute.
To analyze the standalone effect of model size, Figure 5(b)
directly compares B, L, and XL variants at a fixed patch
size ğ‘=16. Scaling the model yields consistent gains
across the entire training trajectory: at 200K iterations,
gFID improves from 15.1 (B/16) to 4.95 (L/16) and 2.95
(XL/16), and at 1M iterations XL/16 reaches 1.94 gFID
compared to roughly 2.1 for L/16. Thus, increasing capac-
ity improves both image quality and the speed at which a
given quality level is reached, demonstrating the scalability
of PixelDiT.
We leave more ablation studies to the Appendix.
5. Conclusion
In this work, we revisited diffusion modeling in pixel
space and showed that, with appropriate architectural de-
sign, pixel-space Diffusion Transformers can achieve high
fidelity and efficiency without relying on a pretrained au-
toencoder. PixelDiT factors pixel modeling into a dual-
level transformer design and introduces pixel-wise AdaLN
and pixel token compaction to decouple global seman-
tics from per-pixel token learning while keeping attention
affordable. Experiments on class-conditioned image gener-
ation and text-to-image generation tasks demonstrate that
this design closes much of the gap between latent-space
and pixel-space methods and yields strong performance at
high resolutions.
While pixel-space diffusion incurs higher computational
costs than latent approaches due to the raw data dimen-
sionality, our work narrows this efficiency gap. Overall,
PixelDiT highlights that the main barrier to practical pixel-
space diffusion is not the representation space, but the
absence of efficient pixel modeling architectures. We hope
this perspective will inspire more fundamental research on
pixel-space generative modeling.
References
[1] Vladimir Kulikov, Matan Kleiner, Inbar Huberman-
Spiegelglas, and Tomer Michaeli. Flowedit: Inversion-free
text-based editing using pre-trained flow models. In ICCV,
pages 19721â€“19730, 2025.
[2] Black Forest Labs. Flux. https://github.com/black-
forest-labs/flux, 2024.
[3] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim
Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim
Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yan-
nik Marek, and Robin Rombach. Scaling rectified flow
transformers for high-resolution image synthesis. In ICML,
2024.
[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and BjÃ¶rn Ommer. High-resolution image
synthesis with latent diffusion models. In CVPR, 2022.
[5] William Peebles and Saining Xie. Scalable diffusion mod-
els with transformers. In ICCV, 2023.
[6] Nanye
Ma,
Mark Goldstein,
Michael
S Albergo,
Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie.
Sit: Exploring flow and diffusion-based generative models
with scalable interpolant transformers. In ECCV, 2024.
[7] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruc-
tion vs. generation: Taming optimization dilemma in latent
diffusion models. In CVPR, 2025.
[8] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang
Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking
vae for end-to-end tuning with latent diffusion transformers.
In ICCV, 2025.
[9] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks
Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail
Doshi, and Daiqing Li. Playground v3: Improving text-to-
image alignment with deep-fusion large language models.
arXiv preprint arXiv:2409.10695, 2024.
[10] OpenAI. Dalle-3, 2023.
[11] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining
Xie. Diffusion transformers with representation autoen-
coders. arXiv preprint arXiv:2510.11690, 2025.
[12] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and
Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv
preprint arXiv:2507.23268, 2025.
[13] Michael Tschannen, AndrÃ© Susano Pinto, and Alexander
Kolesnikov. Jetformer: An autoregressive generative model
of raw images and text. In ICLR, 2025.
[14] Guangting Zheng, Qinyu Zhao, Tao Yang, Fei Xiao, Zhijie
Lin, Jie Wu, Jiajun Deng, Yanyong Zhang, and Rui Zhu.
Farmer: Flow autoregressive transformer over pixels. arXiv
preprint arXiv:2510.23588, 2025.
[15] Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai
Zheng, Jiahong Wu, and Xiangxiang Chu. Advancing end-
to-end pixel space generative modeling via self-supervised
pre-training. arXiv preprint arXiv:2510.12586, 2025.
[16] Kaiming He Tianhong Li.
Back to basics:
Let de-
noising generative models denoise.
arXiv preprint
arXiv:2511.13720, 2025.
[17] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun,
and Ping Luo. Pixelflow: Pixel-space generative models
with flow. arXiv preprint arXiv:2504.07963, 2025.
9

PixelDiT: Pixel Diffusion Transformers for Image Generation
[18] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
Simple diffusion: End-to-end diffusion for high resolution
images. In ICML, 2023.
[19] Emiel Hoogeboom, Thomas Mensink, Jonathan Heek, Kay
Lamerigts, Ruiqi Gao, and Tim Salimans. Simpler diffusion
(sid2): 1.5 fid on imagenet512 with pixel-space diffusion.
In CVPR, 2025.
[20] Jonathan Ho, Chitwan Saharia, William Chan, David J
Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded
diffusion models for high fidelity image generation. Journal
of Machine Learning Research, 23(47):1â€“33, 2022.
[21] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal
generative models. arXiv preprint arXiv:2502.17437, 2025.
[22] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang
Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han.
Deep compression autoencoder for efficient high-resolution
diffusion models. In ICLR, 2025.
[23] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen,
Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating
diffusion model convergence with structured latent space,
2025.
[24] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong
Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou,
and Bhiksha Raj. Masked autoencoders are effective tok-
enizers for diffusion models. In ICML, 2025.
[25] Yongsheng Yu, Haitian Zheng, Zhifei Zhang, Jianming
Zhang, Yuqian Zhou, Connelly Barnes, Yuchen Liu, Wei
Xiong, Zhe Lin, and Jiebo Luo. Zipir: Latent pyramid
diffusion transformer for high-resolution image restoration.
arXiv preprint arXiv:2504.08591, 2025.
[26] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan,
Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and
Jiwen Lu. Latent diffusion model without variational au-
toencoder. arXiv preprint arXiv:2510.15301, 2025.
[27] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. In NeurIPS, 2021.
[28] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang.
Ddt: Decoupled diffusion transformer, 2025.
[29] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christo-
pher Pal, and Marc Aubreville. WÃ¼rstchen: An efficient
architecture for large-scale text-to-image diffusion models.
In ICLR, 2024.
[30] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima
Anandkumar.
Fast training of diffusion models with
masked transformers. TMLR, 2023.
[31] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon
Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie.
Representation alignment for generation: Training diffu-
sion transformers is easier than you think. In ICLR, 2025.
[32] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-
xl: Scaling stylegan to large diverse datasets.
In SIG-
GRAPH, 2022.
[33] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive
computation for iterative generation. In ICML, 2023.
[34] Diederik Kingma and Ruiqi Gao. Understanding diffu-
sion objectives as the elbo with simple data augmentation.
NeurIPS, 36, 2024.
[35] Gemma
Team,
Morgane
Riviere,
Shreya
Pathak,
Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,
LÃ©onard Hussenot, Thomas Mesnard, Bobak Shahriari,
Alexandre RamÃ©, et al.
Gemma 2: Improving open
language models at a practical size.
arXiv preprint
arXiv:2408.00118, 2024.
[36] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian
Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu,
Yao Lu, et al. Sana: Efficient high-resolution text-to-image
synthesis with linear diffusion transformers. In ICLR, 2025.
[37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow
straight and fast: Learning to generate and transfer data
with rectified flow. In ICLR, 2023.
[38] Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervi-
sion. In TMLR, 2023.
[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-
thy, Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision, 115(3):211â€“252, 2015.
[40] Tuomas KynkÃ¤Ã¤nniemi, Miika Aittala, Tero Karras, Samuli
Laine, Timo Aila, and Jaakko Lehtinen. Applying guid-
ance in a limited interval improves sample and distribution
quality in diffusion models. In NeurIPS, 2024.
[41] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt.
Geneval: An object-focused framework for evaluating text-
to-image alignment. Advances in Neural Information Pro-
cessing Systems, 36, 2024.
[42] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng,
and Gang Yu.
Ella:
Equip diffusion models with
llm for enhanced semantic alignment.
arXiv preprint
arXiv:2403.05135, 2024.
[43] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
Li, and Jun Zhu. Dpm-solver++: Fast solver for guided
sampling of diffusion probabilistic models. arXiv preprint
arXiv:2211.01095, 2022.
[44] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao,
Enze Xie, Zhongdao Wang, James Kwok, Ping Luo,
Huchuan Lu, and Zhenguo Li. Pixart-ğ›¼: Fast training
of diffusion transformer for photorealistic text-to-image
synthesis. In ICLR, 2024.
[45] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei
Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan
Lu, and Zhenguo Li. Pixart-ğœ: Weak-to-strong training of
diffusion transformer for 4k text-to-image generation. In
ECCV, 2024.
10

PixelDiT: Pixel Diffusion Transformers for Image Generation
[46] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang
Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun
Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-
t2x stronger and faster with next-dit.
arXiv preprint
arXiv:2406.18583, 2024.
[47] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952, 2023.
[48] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Lin-
miao Xu, and Suhail Doshi. Playground v2. 5: Three in-
sights towards enhancing aesthetic quality in text-to-image
generation. arXiv preprint arXiv:2402.17245, 2024.
[49] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong,
Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu,
Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: A power-
ful multi-resolution diffusion transformer with fine-grained
chinese understanding. arXiv preprint arXiv:2405.08748,
2024.
[50] I Loshchilov. Decoupled weight decay regularization. In
ICLR, 2019.
11

PixelDiT: Pixel Diffusion Transformers for Image Generation
A. Architecture and System Details
A.1. Summary of Model Size
To study the impact of model size, we evaluate the base
(B), large (L), and extra-large (XL) variants of PixelDiT
on ImageNet 256Ã—256. Tables 6 and 7 summarize the
detailed architectural specifications and training settings
for B, L, XL and T2I variants. Note that the default
configuration of all experiments in the main paper is
PixelDiT-XL. If not otherwise specified, we use the XL
configuration for all ImageNet 256Ã—256 experiments in
this appendix.
A.2. Text-to-Image Architecture with MM-DiT
Figure 6 illustrates the T2I variant of PixelDiT, where the
patch-level pathway is extended with MM-DiT blocks [3]
to fuse text embeddings, while the pixel-level pathway
remains unchanged. The figure emphasizes stream sepa-
ration, conditioning flow, and the pixel-wise modulation
interface used by the pixel-level pathway.
B. Solvers and Guidance Scales
B.1. Ablation of Solvers
We compare three diffusion samplers for denoising on Im-
ageNet 256Ã—256: FlowDPMSolver [36, 43], Euler, and
Heun, all run for 100 steps without classifier-free guidance.
Figure 7 plots gFID, sFID, Inception Score (IS), precision,
and recall as training progresses from 100K to 1,200K iter-
ations. Across most of the training trajectory, FlowDPM-
Solver achieves lower or comparable gFID and sFID than
Euler and Heun, with the gap particularly pronounced in
the low- and mid-epoch regimes (up to roughly 1â€“2 gFID
points around 400Kâ€“800K iterations). FlowDPMSolver
maintains the best overall trade-off: it matches or exceeds
the competing solvers on sFID and IS while keeping pre-
cision and recall high. These results motivate our choice
of FlowDPMSolver as the default sampler for all main
ImageNet and text-to-image evaluations.
B.2. Inference Steps
We further analyze the impact of the number of inference
steps when using FlowDPMSolver. Figure 8 shows gFID
for PixelDiT-XL at three training stages (100K, 400K, and
1.6M iterations) as we vary the sampling budget from 25
to 100 steps. At 100K iterations the model is undertrained
and additional steps give only modest improvements, with
gFID remaining in the 6â€“7 range. Once the model has
learned reasonable global structure (400K iterations), in-
creasing the budget from 25 to 50 steps reduces gFID from
about 3.19 to 2.51, and 100 steps further improves it to
2.36. For the fully converged checkpoint at 1.6M itera-
PixelDiT-B
PixelDiT-L
PixelDiT-XL
Architecture
Input dim.
256 Ã— 256 Ã— 3
Patch-level depth ğ‘
12
22
26
Pixel-level depth ğ‘€
2
4
4
Hidden size ğ·
768
1024
1152
Heads
12
16
16
Pixel hidden size ğ·pix
16
16
16
Patch size ğ‘
16
16
16
#Params (M)
184
569
797
Representation Alignment [31]
Alignment depth (patch-level)
8-th layer
Loss weight ğœ†repa
0.5
Alignment encoder
Frozen DINOv2 [38]
Optimization
Training iteration
800K
1M
1.6M
Batch size
256
Timestep reweighting
Logit-normal [3]
Optimizer
AdamW [50], ğ›½1=0.9, ğ›½2=0.999
EMA decay
0.9999
Class drop prob.
0.1
Gradient clipping
1.0
1.0
1.0 â†’0.5
Learning rate
1e-4
1e-4
1e-4 â†’1e-5
Weight decay
0
0
0
Inference
Sampler
FlowDPMSolver [36, 43]
Sampling steps
100
CFG scale (80-ep)
3.25
3.25
3.25
CFG interval (80-ep)
[0.10, 1.00]
CFG scale (320-ep)
â€“
â€“
2.75
CFG interval (320-ep)
â€“
â€“
[0.10, 0.90]
Guidance for Figs. 5, 8â€“10
3.25, [0.10, 1.00] for all checkpoints
Table 6 | Detailed architecture and training configurations
for PixelDiT B/L/XL models on ImageNet-256.
Hyperparameter
PixelDiT-T2I
Architecture
Input dim.
5122 Ã— 3
10242 Ã— 3
Patch-level depth ğ‘
14
Pixel-level depth ğ‘€
2
Hidden size ğ·
1536
Heads
24
Pixel hidden size ğ·pix
16
Patch size ğ‘
16
#Params (M)
1311
Representation Alignment [31]
Alignment depth (patch-level)
6-th layer
â€“
Loss weight ğœ†repa
0.5
0.0 (disabled)
Alignment encoder
Frozen DINOv2 [38]
Optimization
Training iteration
400K
100K
Batch size
1024
768
Learning rate
1e-4
2e-5
Gradient clipping
0.5
0.1
Text drop prob.
0.1
Inference
Sampler
FlowDPMSolver [36, 43]
Sampling steps
25
CFG scale
4.5
Table 7 | Implementation details for PixelDiT-T2I model.
12

PixelDiT: Pixel Diffusion Transformers for Image Generation
Noised
Image
1Ã—1
Patchify
16Ã—16
Patchify
MM-DiT Blocks
TimestepÂ Embedding
AdaLN-Zero
Semantic Token
Ã—N
PiT Blocks
AdaLN-Zero
Condition
Pixel Token
Ã—M
AdaLN-Zero
Text Token
User
Prompt
System
Prompt
Text Encoding
MLP
RMSNorm
Scale & Shift
Gate
FFN
RMSNorm
Scale & Shift
Gate
FFN
RMSNorm
Scale & Shift
RMSNorm
Scale & Shift
Joint MHSA
Gate
Gate
k
k
v
v
q
q
Figure 6 | T2I architecture of PixelDiT with MM-DiT blocks on the patch-level pathway. The pixel-level pathway
performs dense per-pixel modeling conditioned on semantic tokens.
500K
1000K
Training Steps
15
20
25
30
35
gFID (
)
500K
1000K
Training Steps
6
8
10
12
sFID (
)
500K
1000K
Training Steps
40
50
60
70
80
90
IS (
)
500K
1000K
Training Steps
0.475
0.500
0.525
0.550
0.575
0.600
Precision (
)
500K
1000K
Training Steps
0.65
0.66
0.67
0.68
0.69
Recall (
)
FlowDPMSolver
Euler
Heun
Figure 7 | Comparison of FlowDPMSolver, Euler, and Heun samplers on ImageNet 256Ã—256 with 100 inference steps
and no classifier-free guidance. FlowDPMSolver achieves the best combined trade-off between fidelity in gFID and
sFID and diversity in IS, precision, and recall, which motivates its use as our default sampler.
tions, 25 steps already achieve gFID â‰ˆ2.50, but 50â€“75
steps lower it to around 1.76â€“1.74, and 100 steps obtain
the best score of approximately 1.61 gFID. Overall, more
inference steps consistently benefit well-trained models,
though the marginal gain beyond 50 steps becomes small.
In practice we therefore use 100 steps for class-conditioned
ImageNet experiments to match the strongest quality, and
25 steps for text-to-image generation where sampling la-
tency is more critical.
B.3. Guidance Scale and Interval
We report the classifier-free guidance (CFG) settings used
for PixelDiT-XL at 80 and 320 epochs on ImageNet
256Ã—256. Table 8 lists the CFG scale, active time in-
terval, and the resulting gFID, sFID, IS, precision, and
recall. For the 80-epoch checkpoint, the best gFID is 2.36,
obtained with a relatively strong guidance scale of 3.25
applied over the entire denoising trajectory from ğ‘¡=0.10
to ğ‘¡=1.00. Increasing the scale to 3.50 or decreasing it to
3.00 slightly worsens gFID while mainly trading off IS
and recall, and restricting the active interval to [0.10, 0.95]
or [0.10, 0.90] does not lead to better performance. For the
320-epoch checkpoint, the optimum shifts toward milder
guidance: a scale of 2.75 active on the interval [0.10, 0.90]
achieves the best gFID of 1.61 together with strong re-
call of 0.64, whereas both larger and smaller scales yield
at most marginal IS gains at the cost of higher gFID.
In the main paper we therefore adopt a guidance scale
of 3.25 with interval [0.10, 1.00] for the 80-epoch Ima-
geNet 256Ã—256 results, and a scale of 2.75 with interval
[0.10, 0.90] for the 320-epoch checkpoint that underpins
our best reported scores.
C. Model Architecture Design
C.1. Ablation on Depth N and M
We analyze how to allocate depth between the patch-level
pathway (ğ‘layers) and the pixel-level pathway (ğ‘€layers)
under a fixed total budget of roughly ğ‘+ğ‘€â‰ˆ30 layers.
Figure 9 shows convergence curves for several (ğ‘, ğ‘€)
configurations evaluated on ImageNet 256Ã—256. All eval-
uations use the same CFG guidance scale 3.25 with inter-
val [0.10, 1.00]. Introducing even a shallow pixel pathway
(e.g., ğ‘=28, ğ‘€=2) dramatically improves convergence
and reduces final gFID to around 2.1. Our default configu-
ration (ğ‘=26, ğ‘€=4) provides the best overall behavior:
13

PixelDiT: Pixel Diffusion Transformers for Image Generation
Model
Epochs
Training Steps
CFG
Interval
gFIDâ†“
sFIDâ†“
ISâ†‘
Prec.â†‘
Rec.â†‘
PixelDiT-XL
80
400K
3.25
[0.10, 1.00]
2.36
5.11
282.3
0.80
0.57
PixelDiT-XL
80
400K
3.50
[0.10, 1.00]
2.60
5.07
305.2
0.82
0.57
PixelDiT-XL
80
400K
3.00
[0.10, 1.00]
2.60
5.06
277.6
0.80
0.58
PixelDiT-XL
80
400K
2.75
[0.10, 1.00]
2.76
5.17
259.2
0.79
0.59
PixelDiT-XL
80
400K
3.25
[0.10, 0.95]
2.73
5.24
285.8
0.80
0.58
PixelDiT-XL
80
400K
3.25
[0.10, 0.90]
2.75
5.32
285.4
0.80
0.58
PixelDiT-XL
320
1600K
2.75
[0.10, 0.90]
1.61
4.68
292.7
0.78
0.64
PixelDiT-XL
320
1600K
2.75
[0.10, 0.95]
1.65
4.64
293.8
0.77
0.64
PixelDiT-XL
320
1600K
2.75
[0.10, 1.00]
1.66
4.60
294.2
0.78
0.64
PixelDiT-XL
320
1600K
2.50
[0.10, 0.95]
1.69
4.68
276.9
0.77
0.65
PixelDiT-XL
320
1600K
2.50
[0.10, 0.90]
1.71
4.60
275.2
0.77
0.65
PixelDiT-XL
320
1600K
2.50
[0.10, 1.00]
1.71
4.62
277.9
0.77
0.65
Table 8 | CFG settings and results for PixelDiT-XL on ImageNet 256Ã—256.
Model
Params (B)
Overall â†‘
Objects
Counting
Colors
Position
Color
Single
Two
Attribution
512 Ã—512 resolution
PixArt-ğ›¼
0.6
0.48
0.98
0.50
0.44
0.80
0.08
0.07
PixArt-Î£
0.6
0.52
0.98
0.59
0.50
0.80
0.10
0.15
PixelFlow [17]
0.9
0.60
-
-
-
-
-
-
PixNerd [12]
1.2
0.73
0.97
0.86
0.44
0.83
0.71
0.53
PixelDiT-T2I
1.3
0.78
1.00
0.94
0.70
0.90
0.53
0.65
1024 Ã—1024 resolution
LUMINA-Next [46]
2.0
0.46
0.92
0.46
0.48
0.70
0.09
0.13
SDXL [47]
2.6
0.55
0.98
0.74
0.39
0.85
0.15
0.23
PlayGroundv2.5 [48]
2.6
0.56
0.98
0.77
0.52
0.84
0.11
0.17
Hunyuan-DiT [49]
1.5
0.63
0.97
0.77
0.71
0.88
0.13
0.30
DALLE3 [10]
-
0.67
0.96
0.87
0.47
0.83
0.43
0.45
FLUX-dev [2]
12.0
0.67
0.99
0.81
0.79
0.74
0.20
0.47
PixelDiT-T2I
1.3
0.74
1.00
0.95
0.55
0.88
0.41
0.68
Table 9 | GenEval category-wise results at 512 Ã— 512 and 1024 Ã— 1024 for text-to-image generation. Overall is the
unweighted mean over Single Object, Two Objects, Counting, Colors, Position, and Color Attribution.
it reaches gFID 2.34 by 300K iterations and continues to
improve to 1.94 at 1M iterations, outperforming both the
shallower pixel pathway (ğ‘=28, ğ‘€=2) and the deeper
one (ğ‘=22, ğ‘€=8). The latter attains similar final gFID
but converges more slowly in early epochs. These trends
indicate that dedicating a moderate but not excessive num-
ber of layers to the pixel-level pathway is crucial for effi-
cient pixel modeling and underpins the strong ImageNet
results reported in the main paper.
C.2. Study on Pixel Token Compaction (PTC) Rate
We investigate the effectiveness of Pixel Token Com-
paction (PTC) by varying the compaction rate. Recall
that our default patch size is ğ‘. Without compaction, the
pixel-level pathway would process a sequence of length
ğ»Ã— ğ‘Š. With standard compaction (denoted as â€œSeq Len
ğ¿(1Ã—)â€ or Base), the ğ‘Ã— ğ‘pixels in a patch are com-
pressed into a single token, reducing the sequence length
to ğ¿= (ğ»/ğ‘) Ã— (ğ‘Š/ğ‘). We explore relaxing this com-
pression by allowing the compacted sequence length to be
multiples of the base length ğ¿. Specifically:
â€¢ Seq Len ğ¿(1Ã—): The default setting. Compresses ğ‘2
pixels to 1 token. Compression rate: ğ‘2.
â€¢ Seq Len 2ğ¿(2Ã—): Compresses ğ‘2 pixels to 2 tokens.
Compression rate: ğ‘2/2.
â€¢ Seq Len 4ğ¿(4Ã—): Compresses ğ‘2 pixels to 4 tokens.
Compression rate: ğ‘2/4.
Figure 10 presents the ablation results. Across training, all
three settings converge to strong gFID values around 2.0,
while the model with the most aggressive compression
(Seq Len 1Ã—) obtains slightly better results. For example,
at 300K iterations the three configurations obtain gFID of
roughly 2.34 (1Ã—), 2.38 (2Ã—), and 2.43 (4Ã—), respectively.
At 1M iterations the 1Ã— variant is further improved to 1.94
while the longer sequences plateau slightly higher. The
result suggests that, for the pixel-level pathway, a compact
representation is sufficient to capture the residual infor-
mation needed for texture refinement. This could be due
to the redundant nature of the pixel-space tokens. Inter-
14

PixelDiT: Pixel Diffusion Transformers for Image Generation
Model
Params (B)
Overall â†‘
Global
Entity
Attribute
Relation
Other
512 Ã—512 resolution
PixArt-ğ›¼[44]
0.6
71.6
81.7
80.1
80.4
81.7
76.5
PixArt-Î£ [45]
0.6
79.5
87.5
87.1
86.5
84.0
86.1
PixelFlow [17]
0.9
77.9
-
-
-
-
-
PixNerd [12]
1.2
80.9
80.5
87.9
87.2
91.3
72.8
PixelDiT-T2I
1.3
83.7
88.0
90.9
87.6
89.8
88.5
1024 Ã—1024 resolution
LUMINA-Next [46]
2.0
74.6
82.8
88.7
86.4
80.5
81.8
SDXL [47]
2.6
74.7
83.3
82.4
80.9
86.8
80.4
PlayGroundv2.5 [48]
2.6
75.5
83.1
82.6
81.2
84.1
83.5
Hunyuan-DiT [49]
1.5
78.9
84.6
80.6
88.0
74.4
86.4
PixArt-Î£ [45]
0.6
80.5
86.9
82.9
88.9
86.6
87.7
DALLE3 [10]
-
83.5
91.0
89.6
88.4
90.6
89.8
FLUX-dev [2]
12.0
84.0
82.1
89.5
88.7
91.1
89.4
PixelDiT-T2I
1.3
83.5
83.0
88.6
87.8
91.2
89.6
Table 10 | DPG-Bench category-wise results at 512 Ã— 512 and 1024 Ã— 1024 resolutions for text-to-image generation.
200K 400K 600K 800K 1000K1200K1400K1600K
Training Steps
2
3
4
5
6
gFID
100 Steps
75 Steps
50 Steps
25 Steps
Figure 8 | Effect of the number of FlowDPMSolver infer-
ence steps on gFID for PixelDiT-XL at different training
stages on ImageNet 256Ã—256. Increasing the number of
steps is most beneficial once the model is moderately or
fully trained (400K and 1.6M iterations), with diminishing
returns beyond 50 steps; we adopt 100 steps as the default
for ImageNet experiments.
estingly, the results indicate that lower compression rates
do not necessarily lead to better image quality. We sus-
pect that a longer, redundant token sequence and a larger
attention space can be more challenging to optimize and
slower to converge under the same training setting. Using
more tokens in the pixel-level pathway may require care-
fully adjusted training settings to unlock its full potential.
Since the attention cost grows almost quadratically with
the compressed token length, the model with 1Ã— com-
paction performs similarly to other configurations, thus
we adopt it as the default setting throughout the paper for
efficiency.
0K
200K
400K
600K
800K
1000K
Training Steps
2
3
4
5
6
7
8
9
gFID
N=28, M=2
N=24, M=6
N=22, M=8
N=26, M=4
600K
700K
800K
1.9
2.0
2.1
2.2
2.3
Figure 9 | Ablation of depth allocation between patch-level
(ğ‘) and pixel-level (ğ‘€) pathways on ImageNet 256Ã—256.
Our chosen configuration (ğ‘=26, ğ‘€=4), highlighted in
brown, offers the best trade-off between early convergence
and final image quality.
D. Benchmark Details
D.1. DPG-Bench and GenEval Category Breakdown
Tables 9 and 10 report category-wise results for GenEval
and DPG-Bench at 5122 and 10242 resolutions.
On
GenEval at 5122, PixelDiT-T2I outperforms prior pixel-
space models. At 10242 resolution, PixelDiT-T2I matches
or surpasses several widely used latent diffusion systems
despite using fewer parameters, and maintains competitive
performance across all individual categories. On DPG-
Bench, PixelDiT-T2I ranks among the top-performing
models while maintaining balanced scores across cate-
gories. These detailed breakdowns corroborate that Pix-
elDiT delivers strong textâ€“image alignment and compo-
sitional reasoning, closing much of the gap to heavily
engineered latent diffusion models.
15

PixelDiT: Pixel Diffusion Transformers for Image Generation
0K
200K
400K
600K
800K
1000K
Training Steps
2
3
4
5
6
7
gFID
Seq Len L (1 Ã— )
Seq Len 2L (2 Ã— )
Seq Len 4L (4 Ã— )
400K
500K
600K
2.0
2.2
2.4
Figure 10 | Ablation of Pixel Token Compaction rates
on ImageNet 256Ã—256. The curves compare three post-
compaction sequence lengths, denoted as â€œSeq Len ğ¿
(1Ã—)â€, â€œSeq Len 2ğ¿(2Ã—)â€, and â€œSeq Len 4ğ¿(4Ã—)â€, where
ğ¿is the number of patch tokens after compaction. â€œSeq
Len ğ¿(1Ã—)â€ corresponds to our default configuration in
which each ğ‘2 pixel block is compacted into a single token.
E. FLOPs Estimation and Comparison
We estimate GFLOPs for a single forward pass at 2562
input resolution. For the GFLOPs of prior work, we reuse
the numbers reported in their papers [16, 19] and convert
them to a unified convention where one multiply-add
counts as two FLOPs. Table 11 compares the compute
cost and FID of PixelDiT-XL with representative latent-
space and pixel-space generative models on ImageNet
256Ã—256. Latent models achieve very strong FIDs with
around 240â€“290 GFLOPs, whereas many pixel models
require several hundred to several thousand GFLOPs to
close this quality gap. In contrast, PixelDiT-XL obtains
a 1.61 FID with only 311 GFLOPs, offering superior im-
age quality over the state-of-the-art pixel generators and
closing much of the gap between the best latent models,
while using a compute that is close to latent models and
substantially less than most prior pixel-space models.
F. Qualitative Examples
We include additional qualitative results for ImageNet
256Ã—256 single-class-conditioned image generations, as
shown in Figures 15â€“22, together with high-resolution
(approximately 10242) text-to-image generation results in
Figures 11â€“14, illustrating the visual quality, diversity, and
prompt alignment achieved by PixelDiT.
G. Limitations
Due to the limited model capacity and insufficient high-
quality training data, our PixelDiT-T2I text-to-image
model (1.3B parameters) sometimes struggles to generate
objects that are both geometrically and texturally complex,
Methods
Params
GFLOPs
(multi-add = 2 FLOPs)
FIDâ†“
Latent Generative Models
DiT-XL/2 [5]
675+49M
238
2.27
SiT-XL/2 [6]
675+49M
238
2.06
REPA, SiT-XL/2 [31]
675+49M
238
1.42
LightningDiT-XL/2 [7]
675+49M
238
1.35
DDT-XL/2 [28]
675+49M
238
1.26
RAE, DiTDH-XL/2 [11]
839+415M
292
1.13
Pixel Generative Models
ADM-G [27]
559M
2240
7.72
RIN [33]
320M
668
3.95
SiD, UViT/2 [18]
2B
1110
2.44
VDM++, UViT/2 [34]
2B
1110
2.12
SiD2, UViT/2 [19]
N/A
274
1.73
SiD2, UViT/1 [19]
N/A
1306
1.38
PixelFlow-XL/4 [17]
677M
5818
1.98
PixNerd-XL/16 [12]
700M
268
2.15
JiT-G/16 [16]
2B
766
1.82
PixelDiT-XL (ours)
797M
311
1.61
Table 11 | Compute comparison on ImageNet 256Ã—256.
We report model parameters, GFLOPs per forward pass,
and FID under our convention that one multiply-add equals
two FLOPs.
such as human hands and intricate architectural scenes. In
future work, we will address these challenging cases by
scaling up the model capacity and collecting more high-
quality data.
16

PixelDiT: Pixel Diffusion Transformers for Image Generation
A group of figures are gathered at a table near a trellised terrace, overlooking a river with rowers and a boat. The scene, rendered in an Impressionistic style, employs loose brushstrokes and soft, diffused
light. The man in the foreground, dressed in a blue and white striped shirt, gestures casually, holding what appears to be a cigarette. The table is set with wine bottles and glasses, indicating a leisurely
gathering. Through the trellis, a glimpse of the river reveals rowers in action, with a lone figure in a boat further out. The greenery of the trellis and the river landscape blend, contributing to the paintingâ€™s
overall sense of depth and atmospheric perspective.
A woman stands on a rooftop at dusk, overlooking a cityscape illuminated by twinkling lights. Her curly hair frames a serious expression, and she leans casually against the rooftop railing. The soft lighting
of the sunset blends with the artificial glow of the city, creating a warm yet muted atmosphere. The out-of-focus background emphasizes the vastness of the urban landscape, dotted with skyscrapers and
distant roads. Her dark jacket adds a layer of contrast, focusing the viewerâ€™s attention on her face. The overall style evokes a sense of urban solitude and reflection against a backdrop of a vibrant cityscape.
A golden-hued lioness rests serenely in a sunlit grassy field. The lioness, bathed in the warm glow of the setting or rising sun, is positioned in the foreground, lying comfortably on a small mound of earth
covered with dry grass. Her paws are outstretched, relaxed and slightly crossed. The background is a soft, blurred mix of green and golden foliage, hinting at a savanna-like landscape. The golden light
emphasizes the texture of her fur and highlights the contours of her face, adding a sense of calm and natural grandeur to the scene.
Figure 11 | Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt
used to generate it.
17

PixelDiT: Pixel Diffusion Transformers for Image Generation
Photo of a person moving with motion blur, shot with
a Leica M6 and VISION3 500T Color Negative Film,
reminiscent of a Wong Kar Tai film set.
A young man wearing 18th century noble clothing in
blues and pinks and standing in front of the green grass
with white flowers.
Portrait shot of a pretty woman, latex suit fashion, con-
trasting background, fashion magazine cover, 35mm
kodachrome.
A portrait of a human growing colorful flowers from
her hair. Hyperrealistic oilpainting.
knitted cat-whale plush toy on rug in warm sunlit living
room, cozy decor.
Behold the Joymonger, photorealistic, 1990s, hyper
realism, extremely detailed.
Figure 12 | Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt
used to generate it.
18

PixelDiT: Pixel Diffusion Transformers for Image Generation
A young woman with striking eyes gazes directly at the viewer, set against a soft, blurred background. Her long, auburn hair falls loosely around her shoulders, partially
obscuring one side of her face, while a vertically striped, collared shirt completes her casual yet elegant look. The lighting is warm and natural, emphasizing the subtle
contours of her face and the slight flush in her cheeks. The photograph captures a blend of relaxed confidence and introspective beauty, rendered with a soft focus that lends
a dreamlike quality to the image. The surrounding environment is intentionally muted, ensuring that the woman remains the primary focal point. The color palette leans
towards earthy tones, enhancing the overall warmth and approachability of the portrait.
Close-up portrait of a beautiful Baltic model wearing
white flower-shaped earrings, emphasis on the earrings,
everything is in full focus, pores and skin imperfections
are visible, neutral lighting from a large studio softbox,
natural beauty, professional studio shooting.
funny Candid photo, cat sleeping across amanâ€™s eyes as
he sleeps on his back,blocking his face, man is 25 years
old,neck length frizzy black-brown hair,very unruly
hair, stubble, wearing blackdress pants, long sleeve
white button upshirt, socks, laying on a post bed.
Ultra-realistic photo of an anthropomorphic chili pep-
per with a glossy red surface, smiling with human teeth
and wearing black sunglasses. Surrounded by realistic
flames, the pepper is sharply in focus, while the fiery
background is slightly blurred.
Tiny fluffy lamb standing on a fingertip, ultra-detailed
wool texture, soft natural light.
Colorful woolen parrot plush wearing ornate psychic-
style vest, detailed fabric textures.
young woman in warm sunset light, soft shadows, long
blonde hair, pink pajamas, calm expression.
Figure 13 | Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt
used to generate it.
19

PixelDiT: Pixel Diffusion Transformers for Image Generation
An image of a very tired old man, fish-
erman, long beard, black background set-
tings. The facial expression reflects wis-
doms and test of time.
snow-covered mountains rising above a
calm turquoise lake, their peaks perfectly
mirrored in the water, framed by dense
autumn-tinged pines.
A leopard hiding in the jungle, a photo-
realistic portrait, captured with a Canon
EOS R5 camera and a macro lens for de-
tailed wildlife portraits.
A serene moment unfolds as a dog leisurely
navigates a tranquil lavender field at sunset.
The golden light enhances the beauty of
the swaying purple blooms.
Close-up shot of an African American man
wearing a blue beanie, against a beige back-
ground, with a vintage aesthetic, in the
style of Kodak film photography.
Two conjoined strawberry. The strawber-
rys are resting on undulating red-pink holo-
graphic icy slush. Vaporwave. Intense
color. Ice textures. Solar aesthetic.
Figure 14 | Additional Text-to-Image Generation Results. The caption below each image corresponds to the text prompt
used to generate it.
20

PixelDiT: Pixel Diffusion Transformers for Image Generation
Figure 15 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 291. CFG scale = 4.0.
Figure 16 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 259. CFG scale = 4.0.
Figure 17 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 24. CFG scale = 4.0.
Figure 18 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 970. CFG scale = 4.0.
21

PixelDiT: Pixel Diffusion Transformers for Image Generation
Figure 19 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 39. CFG scale = 4.0.
Figure 20 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 263. CFG scale = 4.0.
Figure 21 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 373. CFG scale = 4.0.
Figure 22 | Uncurated ImageNet 256Ã—256 PixelDiT-XL
samples. Class 386. CFG scale = 4.0.
22
