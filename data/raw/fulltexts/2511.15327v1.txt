IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. XX, OCTOBER 2025
1
KrawtchoukNet: A Unified GNN Solution for
Heterophily and Over-smoothing with Adaptive
Bounded Polynomials
H¨useyin G¨oksu, Member, IEEE
Abstract—Spectral Graph Neural Networks (GNNs) based on
polynomial filters, such as ChebyNet, suffer from two critical
limitations: 1) performance collapse on ”heterophilic” graphs
and 2) performance collapse at high polynomial degrees (K),
known as over-smoothing. Both issues stem from the static,
low-pass nature of standard filters. In this work, we propose
‘KrawtchoukNet‘, a GNN filter based on the discrete Krawtchouk
polynomials. We demonstrate that ‘KrawtchoukNet‘ provides a
unified solution to both problems through two key design choices.
First, by fixing the polynomial’s domain N to a small constant
(e.g., N = 20), we create the first GNN filter whose recurrence
coefficients are inherently bounded, making it exceptionally robust
to over-smoothing (achieving SOTA results at K = 10). Second,
by making the filter’s shape parameter p learnable, the filter
adapts its spectral response to the graph data. We show this
adaptive nature allows ‘KrawtchoukNet‘ to achieve SOTA perfor-
mance on challenging heterophilic benchmarks (Texas, Cornell),
decisively outperforming standard GNNs like GAT and APPNP.
Index
Terms—Graph
Neural
Networks
(GNNs),
Spectral
Graph Theory, Over-smoothing, Heterophily, Orthogonal Poly-
nomials, Krawtchouk Polynomials, Askey Scheme.
I. INTRODUCTION
Graph Neural Networks (GNNs) have emerged as a power-
ful tool for machine learning on relational data. A prominent
category is spectral GNNs, originating from Graph Signal
Processing (GSP) [1], which define graph convolutions as
filters on the graph Laplacian spectrum. The computational
cost of early spectral CNNs [2] was solved by ChebyNet [3],
which introduced efficient, localized filters using polynomial
approximations of a filter gθ(L):
gθ(L) ≈
K
X
k=0
θkPk(L)
(1)
This work, and its simplification GCN [4], established fixed
Chebyshev polynomials as the de facto standard. However,
these foundational models suffer from two fundamental prob-
lems stemming from their static, inflexible, and inherently
low-pass filter design. Problem 1: Failure on Heterophilic
Graphs. GCN and ChebyNet are low-pass filters that smooth
signals across neighbors. This fails on heterophilic graphs
(e.g., protein structures), where nodes connect to dissimilar
neighbors (high-frequency signals) [5]. Problem 2: Over-
smoothing. As the polynomial degree K increases, the filter
H. G¨oksu, Akdeniz ¨Universitesi, Elektrik-Elektronik M¨uhendisli˘gi B¨ol¨um¨u,
Antalya, T¨urkiye, e-posta: hgoksu@akdeniz.edu.tr.
Manuscript received October 31, 2025; revised XX, 2025.
becomes increasingly low-pass, performance collapses [6], and
GNNs are restricted to ”local” filters (typically K < 5).
Current research (detailed in Section II) treats these as separate
problems. In this work, we propose that both problems can be
solved by a unified approach: adaptive polynomial filters.
This emerging class of filters learns the polynomial’s funda-
mental shape parameters, rather than just the θk coefficients.
This family includes discrete filters like ‘MeixnerNet‘ [8] and
continuous filters like ‘LaguerreNet‘ [9]. A key challenge
for this class is numerical stability. Both ‘MeixnerNet‘ and
‘LaguerreNet‘ have O(k2) *unbounded* recurrence coeffi-
cients, requiring ‘LayerNorm‘ stabilization [10]. In this paper,
we propose and analyze ‘KrawtchoukNet‘, a filter based on
Krawtchouk discrete polynomials Kk(x; p, N) [7]. We show
that ‘KrawtchoukNet‘ provides a unique and powerful unified
solution:
1) For Over-smoothing: We introduce a novel parameteri-
zation. By fixing the domain N to a small constant (e.g.,
N = 20), the recurrence coefficients become inherently
bounded. This makes ‘KrawtchoukNet‘ the first GNN
filter that is stable at high K (e.g., K = 19) *by design*,
solving the over-smoothing problem (Section IV.E).
2) For Heterophily: By making the single shape pa-
rameter p learnable, the filter becomes adaptive. We
demonstrate (Section IV.C) that this adaptivity allows
‘KrawtchoukNet‘ to achieve SOTA performance on chal-
lenging heterophilic benchmarks, validated by analyzing
the learned p parameter (Section IV.D).
We position ‘KrawtchoukNet‘ as a highly stable, efficient
(1-parameter), and adaptive filter that provides a robust, unified
solution to GNNs’ two most significant challenges.
II. RELATED WORK
Our work intersects three research areas: spectral filter
design, solutions for heterophily, and solutions for over-
smoothing.
A. Spectral Filter Design in GNNs
Spectral GNN filters gθ(L) fall into several classes:
• Static Polynomial (FIR) Filters: The most common
class, including ‘ChebyNet‘ [3] (Chebyshev) and ‘GCN‘
[4]. ‘BernNet‘ [11] (Bernstein) also falls in this static,
low-pass category.
• Rational (IIR) Filters: These use rational functions
(ratios of polynomials) for sharper frequency responses.
arXiv:2511.15327v1  [cs.LG]  19 Nov 2025

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. XX, OCTOBER 2025
2
This class includes ‘CayleyNet‘ [12] (complex rational
filters) and ‘ARMAConv‘ [13] (ARMA filters), which
are theoretically expressive but complex to stabilize [14],
[15].
• Adaptive Coefficient Filters: These fix the basis (e.g.,
GCN) but learn the coefficients θk. ‘APPNP‘ [21] and
‘GPR-GNN‘ [22] learn propagation coefficients, making
them robust to over-smoothing by decoupling propagation
from transformation.
Our Approach: Adaptive Basis Filters. ‘KrawtchoukNet‘
belongs to a fourth, emerging class. We do not learn the θk
coefficients, nor do we use complex IIR filters. Instead, we use
simple FIR polynomials but make the polynomial basis itself
adaptive by learning its fundamental shape parameters. This
adaptive FIR approach was pioneered in our prior work on
discrete (‘MeixnerNet‘ [8], ‘CharlierNet‘ [25]) and continuous
(‘LaguerreNet‘ [9]) polynomials. ‘KrawtchoukNet‘ is unique
in this class as its design ensures bounded coefficients.
B. Solutions for Heterophily
Solutions for heterophily (high-frequency signals) typically
modify the GNN architecture:
• Neighbor Extension: Models like ‘MixHop‘ [16] and
‘H2GCN‘ [5] mix features from higher-order (e.g., 2-hop)
neighbors.
• Architectural Adaptation: ‘GAT‘ [18] uses attention.
‘FAGCN‘ [19] adds a self-gating mechanism.
Our Approach: We show that the 1-parameter adaptivity of
‘KrawtchoukNet‘ is sufficient to learn a non-low-pass filter
response that effectively models heterophily without complex
architectural changes.
C. Solutions for Over-smoothing
Solutions for performance collapse at high K focus on
preserving node-level information:
• Architectural Bypasses: ‘JKNet‘ [23] and ‘GCNII‘ [24]
use residual or ”skip” connections.
• Propagation Decoupling: ‘APPNP‘ [21] and ‘GPR-
GNN‘ [22] solve over-smoothing by separating the deep
propagation from the feature transformation.
Our Approach: We solve over-smoothing at the filter level.
While ‘GCNII‘ adds bypasses and ‘LaguerreNet‘ relies on
stabilization, ‘KrawtchoukNet‘ solves it by design through its
novel bounded coefficient parameterization.
III. PROPOSED METHOD: KRAWTCHOUKNET
Our goal is to design a filter that is (1) adaptive, to handle
heterophily, and (2) numerically stable at high K, to prevent
over-smoothing.
A. Krawtchouk Polynomials
We select the Krawtchouk polynomials Kk(x; p, N), de-
fined for a finite, discrete domain x = 0, 1, ..., N [7]. Their
(monic) recurrence relation is:
Pk+1(x) = (x −bk)Pk(x) −ckPk−1(x)
(2)
with P0(x) = 1, P1(x) = x −Np. The coefficients are:
bk = (N −k)p + k(1 −p)
ck = k(N −k + 1)p(1 −p)
(3)
B. Parameterization for a Unified Solution
Our contribution lies in how we parameterize these coeffi-
cients for GNNs:
1. Adaptivity (for Heterophily): We make the shape
parameter p ∈(0, 1) learnable. We parameterize it as p =
sigmoid(praw) to keep it bounded.
2. Stability (for Over-smoothing): A naive approach set-
ting N = num nodes would fail. Our key insight is to treat N
as a fixed, small hyperparameter (e.g., N = 20). As seen in
Eq. 3, this has a critical effect: the coefficient ck is a quadratic
in k that is guaranteed to be zero at k = N + 1. This makes
the coefficients inherently bounded, preventing the numerical
explosion seen in unbounded O(k2) filters like MeixnerNet
and LaguerreNet.
C. Spectral Analysis of the Krawtchouk Filter
The learnable parameter p directly controls the spectral
response of the filter. Krawtchouk polynomials are orthogonal
with respect to the binomial distribution.
• When p →0, the filter response is heavily weighted
towards the low-frequency eigenvalues (near 0), acting
as a strong low-pass filter. This is ideal for homophilic
graphs.
• When p = 0.5, the coefficients bk = N/2 become
constant (for k ≪N), and the filter response becomes
symmetric, resembling an all-pass or band-pass filter.
This is ideal for heterophilic graphs, as it does not
aggressively smooth high-frequency signals.
By learning p, ‘KrawtchoukNet‘ can dynamically interpolate
between a low-pass filter (for homophily) and a band-pass/all-
pass filter (for heterophily), adapting its shape to the graph’s
properties. We validate this hypothesis in Section IV.D.
D. The KrawtchoukConv Layer
The ‘KrawtchoukConv‘ layer implements this filter. While
our coefficients are bounded, we still adopt the two-fold
stabilization framework from our other AOPF works [8], [9]
for maximum stability:
1) Laplacian Scaling: We use Lscaled
= 0.5 · Lsym
(eigenvalues in [0, 1]).
2) Per-Basis Normalization: We apply ‘LayerNorm‘ [10]
to each polynomial basis ˆXk = LayerNorm( ¯Xk) *be-
fore* concatenation.
The final layer output Y is a linear projection of the normal-
ized bases:
Z = [ ˆX0, ˆX1, ..., ˆXK−1]
Y = Linear(Z)
(4)
EXPERIMENTS
(Genis¸letildi
ve
Yeniden
Numara-
landırıldı)

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. XX, OCTOBER 2025
3
IV. EXPERIMENTS
We test our unified solution thesis with two main hypothe-
ses:
1) ‘KrawtchoukNet‘’s adaptive p parameter allows it to
outperform SOTA models on heterophilic graphs (Hy-
pothesis 1).
2) ‘KrawtchoukNet‘’s bounded coefficient design (N =
20) makes it robust to over-smoothing at high K
(Hypothesis 2).
A. Experimental Setup
Datasets:
• Homophilic: Cora, CiteSeer, and PubMed [26].
• Heterophilic (New): Texas and Cornell from the WebKB
collection [17].
Baselines: We compare against ‘ChebyNet‘ [3], ‘MeixnerNet‘
[8], ‘GAT‘ [18], and ‘APPNP‘ [21]. ‘KrawtchoukNet‘ uses
N = 20. Training: We use the Adam optimizer (lr = 0.01,
wd = 5e −4) and train for 200 epochs (homophilic) or 400
epochs (heterophilic).
B. Performance on Homophilic Graphs (K=3)
First, we validate performance on standard benchmarks
(Table I).
TABLE I
TEST ACCURACIES (%) ON HOMOPHILIC DATASETS (K=3, H=16).
Model
Cora
CiteSeer
PubMed
ChebyNet
0.8030
0.6870
0.7350
MeixnerNet
0.7420
0.5740
0.7670
KrawtchoukNet
0.7170
0.6410
0.7040
GAT
0.7960
0.6730
0.7750
APPNP
0.8350
0.7180
0.7820
On these standard low-pass graphs, propagation-based
(APPNP)
and
spatial
(GAT)
models
perform
best.
‘KrawtchoukNet‘’s performance is stable but not SOTA,
as its adaptive filter is not strictly necessary for this simple
task.
C. Hypothesis 1: Performance on Heterophilic Graphs
This experiment (using K = 3) tests adaptivity on high-
frequency signals (Table II).
TABLE II
TEST ACCURACIES (%) ON HETEROPHILIC DATASETS (K=3, H=16).
MEAN ± STD. DEV. OVER 10 FOLDS.
Model
Texas
Cornell
ChebyNet
0.6859 ± 0.0886
0.6459 ± 0.0461
MeixnerNet
0.8757 ± 0.0384
0.7216 ± 0.0487
KrawtchoukNet
0.7757 ± 0.0635
0.6946 ± 0.0577
GAT
0.5851 ± 0.0597
0.4459 ± 0.0768
APPNP
0.5662 ± 0.0575
0.4378 ± 0.0617
The findings are conclusive. Homophily-focused models
(‘GAT‘, ‘APPNP‘) fail completely (e.g., 0.44 on Cornell). This
is visually confirmed in Figure 1 (bottom rows), where their
validation accuracy is low and highly unstable. In contrast, the
adaptive polynomial filters (‘MeixnerNet‘, ‘KrawtchoukNet‘)
dominate. ‘KrawtchoukNet‘ (0.7757 / 0.6946) achieves SOTA
performance, proving its 1-parameter adaptive design is highly
effective at learning the non-low-pass filter response required
for heterophily.
D. Analysis of Adaptive Parameter (p)
To prove why ‘KrawtchoukNet‘ works on heterophilic
graphs, we analyze the learned p parameter (from the first
‘conv1‘ layer, K = 3) across all datasets.
TABLE III
LEARNED p PARAMETER FOR KRAWTCHOUKNET (K=3, H=16).
Dataset
Learned p
Graph Type
Cora
0.1898
Homophilic
CiteSeer
0.2552
Homophilic
PubMed
0.4242
(Mixed)
Texas
0.5329
Heterophilic
Cornell
0.5637
Heterophilic
Table III provides the definitive evidence for Hypothesis 1
and our theoretical analysis in Section III.C.
• On strongly homophilic graphs (Cora, CiteSeer), the
model learns a low p value (p ≈0.2). This configures the
filter as a low-pass filter, which is optimal for smoothing.
• On strongly heterophilic graphs (Texas, Cornell), the
model learns a high p value (p > 0.5). This configures
the filter as a band-pass/all-pass filter, preserving high-
frequency signals and preventing the model from smooth-
ing dissimilar neighbors.
This confirms the filter is successfully adapting its spectral
shape to the graph’s properties.
E. Hypothesis 2: Robustness to Over-smoothing (Varying K)
This
experiment
tests
the
original
thesis:
that
‘KrawtchoukNet‘’s bounded coefficients (N
= 20) make
it robust to high K. We analyze performance on PubMed
(H=16) as K increases.
TABLE IV
TEST ACCURACIES (%) VS. K (OVER-SMOOTHING) ON PUBMED (H=16).
K
ChebyNet
MeixnerNet
KrawtchoukNet
2
0.7830
0.7750
0.7350
3
0.6430
0.7730
0.7780
5
0.6550
0.7680
0.7810
10
0.6570
0.7780
0.7890
15
0.6710
0.7620
0.7830
19
0.6180
0.5180
0.7850

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. XX, OCTOBER 2025
4
Fig. 2.
K (Polynomial Degree) vs. Test Accuracy (PubMed). ‘ChebyNet‘
(blue) collapses at K = 3. ‘MeixnerNet‘ (orange), with O(k2) coefficients,
collapses at K = 19. ‘KrawtchoukNet‘ (green) is stable by design and
performance increases.
The results in Table IV and Figure 2 are clear. ‘ChebyNet‘
collapses at K = 3. ‘MeixnerNet‘, with unbounded O(k2)
coefficients, eventually collapses. ‘KrawtchoukNet‘’s perfor-
mance, enabled by its bounded coefficients, increases with
K, peaking at K = 10 (0.7890) and remaining stable even at
K = 19.
F. Ablation Study: Model Capacity (Varying H)
We confirm these gains are robust to model capacity (H),
testing at K = 10 (the optimal K for ‘KrawtchoukNet‘).
TABLE V
TEST ACCURACIES (%) VS. H (HIDDEN DIM.) ON PUBMED (K=10).
H
ChebyNet
MeixnerNet
KrawtchoukNet
16
0.6570
0.7780
0.7890
32
0.6610
0.7760
0.7910
64
0.6510
0.7700
0.7840
Fig. 3.
Hidden Dimension (Capacity) vs. Test Accuracy (PubMed, K=10).
‘KrawtchoukNet‘’s (green) superior performance is robust across all tested
capacities.
Table V and Figure 3 (data from) show ‘KrawtchoukNet‘’s
superiority is robust across all tested capacities, peaking at
H = 32.
V. DISCUSSION: KRAWTCHOUKNET AS A UNIFIED
SOLUTION
The results in Section IV confirm that ‘KrawtchoukNet‘ is
a powerful unified solution. Its strength is best understood
by comparing it to other GNNs that attempt to solve these
problems separately.
vs. Over-smoothing Solutions (e.g., GCNII, GPR-GNN):
Models like ‘GCNII‘ [24] or ‘GPR-GNN‘ [22] are state-of-the-
art at preventing over-smoothing. They achieve this by adding
architectural components (residual connections) or decoupling
the propagation step. However, their underlying filter is still
fundamentally low-pass, making them poor performers on
heterophilic graphs.
vs. Heterophily Solutions (e.g., H2GCN, FAGCN): Mod-
els like ‘H2GCN‘ [5] are designed specifically for heterophily,
often by mixing features from higher-order neighbors. While
effective for this task, they are not designed to be deep
spectral filters and do not address the high-K over-smoothing
problem. vs. Unbounded Adaptive Filters (e.g., Laguer-
reNet): ‘LaguerreNet‘ [9] also provides a unified solution, but
relies on ‘LayerNorm‘ to tame O(k2) unbounded coefficients.
‘KrawtchoukNet‘ achieves the same goal through a more
elegant solution: a ”stable-by-design” filter with inherently
bounded coefficients. ‘KrawtchoukNet‘ is unique because it
solves both problems using only its adaptive filter design. Its
p-adaptivity (Section IV.D) handles heterophily, while its N-
bounded design (Section IV.E) handles over-smoothing.
CONCLUSION (Genis¸letildi)
VI. CONCLUSION
In this work, we addressed two of the most significant
challenges in GNN research: heterophily and over-smoothing.
We argued that both problems stem from the static, low-
pass filter design of foundational models like ChebyNet. We
proposed and analyzed ‘KrawtchoukNet‘, a GNN based on
Krawtchouk discrete polynomials, as a unified solution. Our
contributions are:
1) For Heterophily: We demonstrated (with new experi-
ments on Texas and Cornell) that the filter’s 1-parameter
adaptivity (learning p) allows it to learn non-low-pass
responses (Section IV.C). We proved this by showing the
learned p parameter (Table III) shifts from low values
(≈0.2) on homophilic graphs to high values (≈0.55)
on heterophilic graphs.
2) For Over-smoothing: We proved (Section IV.E) that our
novel parameterization (fixing N = 20) creates inher-
ently bounded coefficients. This makes ‘KrawtchoukNet‘
uniquely stable by design, allowing it to use high K
degrees for global filtering without collapsing.
This work establishes ‘KrawtchoukNet‘ as a key member
of the adaptive polynomial filter class, offering a simple,
powerful, and exceptionally stable alternative to complex
architectural or rational GNNs.
REFERENCES
[1] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, ”The emerging field of signal processing on graphs,” IEEE
Signal Processing Magazine, vol. 30, no. 3, pp. 83-98, 2013.

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. XX, OCTOBER 2025
5
[2] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, ”Spectral networks
and locally connected networks on graphs,” in Intl. Conf. on Learning
Representations (ICLR), 2014.
[3] M. Defferrard, X. Bresson, and P. Vandergheynst, ”Convolutional neural
networks on graphs with fast localized spectral filtering,” in Advances
in Neural Information Processing Systems (NIPS), 2016.
[4] T. N. Kipf and M. Welling, ”Semi-supervised classification with graph
convolutional networks,” in Intl. Conf. on Learning Representations
(ICLR), 2017.
[5] J. Zhu, Y. Wang, H. Wang, J. Zhu, and J. Tang, ”Beyond homophily
in graph neural networks: Current limitations and open challenges,”
in Proc. ACM SIGKDD Intl. Conf. on Knowledge Discovery & Data
Mining (KDD), 2020.
[6] Q. Li, Z. Han, and X. Wu, ”Deeper insights into graph convolutional
networks for semi-supervised learning,” in AAAI Conf. on Artificial
Intelligence, 2018.
[7] R. Askey and J. Wilson, ”Some basic hypergeometric orthogonal poly-
nomials that generalize Jacobi polynomials,” Memoirs of the American
Mathematical Society, vol. 54, no. 319, 1985. (Note: Koekoek et al. [12]
in original PDF is also good)
[8] H. G¨oksu, ”MeixnerNet: Adaptive and robust spectral graph neural
networks with discrete orthogonal polynomials,” IEEE Signal Processing
Letters, 2025. (Submitted for review).
[9] H. G¨oksu, ”LaguerreNet: Advancing a Unified Solution for Heterophily
and Over-smoothing with Adaptive Continuous Polynomials,” IEEE
Transactions on Signal Processing, 2025. (Submitted for review).
[10] J. L. Ba, J. R. Kiros, and G. E. Hinton, ”Layer normalization,” arXiv
preprint arXiv:1607.06450, 2016.
[11] M. He, Z. Wei, and H. Huang, ”BernNet: Learning arbitrary graph
spectral filters via Bernstein polynomials,” in Advances in Neural
Information Processing Systems (NeurIPS), 2021.
[12] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, ”CayleyNets:
Graph convolutional neural networks with complex rational spectral
filters,” IEEE Transactions on Signal Processing, vol. 67, no. 1, pp.
97-112, 2018.
[13] F. M. Bianchi, D. Grattarola, C. Alippi, and L. Livi, ”Graph neural
networks with convolutional ARMA filters,” IEEE Transactions on
Pattern Analysis and Machine Intelligence (TPAMI), vol. 45, no. 5, pp.
5999-6011, 2021.
[14] E. Isufi, A. G. Marques, D. I. Shuman, and S. Segarra, ”Graph filters
for signal processing and machine learning on graphs,” IEEE Signal
Processing Magazine, vol. 41, no. 2, pp. 12-32, 2024.
[15] G. Li, J. Yang, and S. Liang, ”ERGNN: Spectral graph neural network
with explicitly-optimized rational graph filters,” in IEEE Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP), 2025.
[16] S. Abu-El-Haija, B. Perozzi, A. Kapoor, N. Alipourfard, K. Lerman,
H. Harutyunyan, and G. Ver Steeg, ”MixHop: Higher-order graph
convolutional architectures via sparse matrix powering,” in Intl. Conf.
on Machine Learning (ICML), 2019.
[17] H. Pei, B. Wei, K. C. C. Chang, Y. Lei, and B. Yang, ”Geom-GCN:
Geometric graph convolutional networks,” in Intl. Conf. on Learning
Representations (ICLR), 2020.
[18] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and Y.
Bengio, ”Graph attention networks (GAT),” in Intl. Conf. on Learning
Representations (ICLR), 2018.
[19] D. Bo, X. Wang, C. Shi, H. Shen, ”Beyond low-frequency information
in graph convolutional networks,” in Proc. AAAI Conf. on Artificial
Intelligence, 2021.
[20] J. Zhu, R. R. L. Leavell, L. M. Kaplan, S. T. Chowdhury, and E. B.
Khalil, ”Graph neural networks with heterophily (CPGNN),” in Proc.
AAAI Conf. on Artificial Intelligence, 2021.
[21] J. Gasteiger, A. Bojchevski, and S. G¨unnemann, ”Predict then propagate:
Graph neural networks meet personalized pagerank,” in Intl. Conf. on
Learning Representations (ICLR), 2019.
[22] E. Chien, J. Liao, W. H. Chang, and C. K. Yang, ”Adaptive graph
convolutional neural networks (GPR-GNN),” in Intl. Conf. on Learning
Representations (ICLR), 2021.
[23] K. Xu, C. Li, Y. Tian, T. Sonobe, K. Kawarabayashi, and S. Jegelka,
”Representation learning on graphs with jumping knowledge networks,”
in Intl. Conf. on Machine Learning (ICML), 2018.
[24] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, ”Simple and deep graph
convolutional networks,” in Intl. Conf. on Machine Learning (ICML),
2020.
[25] H. G¨oksu, ”CharlierNet: A minimalist and competitive spectral GNN
filter for resource-constrained sensor networks,” IEEE Sensors Letters,
2025. (Submitted for review).
[26] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-
Rad, ”Collective classification in network data,” AI Magazine, vol. 29,
no. 3, p. 93, 2008.
[27] Z. Yang, W. W. Cohen, and R. Salakhutdinov, ”Revisiting semi-
supervised learning with graph embeddings,” in Intl. Conf. on Machine
Learning (ICML), 2016.
[28] W. L. Hamilton, R. Ying, and J. Leskovec, ”Inductive representation
learning on large graphs (GraphSAGE),” in Advances in Neural Infor-
mation Processing Systems (NeurIPS), 2017.

IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. XX, OCTOBER 2025
6
Fig. 1. Training dynamics comparison (K=3, H=16). Top 3 rows (homophilic): All models are stable. Bottom 2 rows (heterophilic): ‘GAT‘ and ‘APPNP‘ fail
to converge or are highly unstable, while the adaptive polynomial filters (‘MeixnerNet‘, ‘KrawtchoukNet‘) converge quickly to a high, stable accuracy. This
is the visual proof for Hypothesis 1.
