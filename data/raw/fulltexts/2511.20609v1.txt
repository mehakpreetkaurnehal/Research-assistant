Under review as a conference paper
ADAPTIVE HOPFIELD NETWORK: RETHINKING SIMI-
LARITIES IN ASSOCIATIVE MEMORY
Shurong Wang1,2, Yuqi Pan2, Zhuoyang Shen1, Meng Zhang1,
Hongwei Wang1âˆ—& Guoqi Li2âˆ—
1 Zhejiang University
2 Institute of Automation, Chinese Academy of Sciences
shurong.22@intl.zju.edu.cn
hongweiwang@intl.zju.edu.cn
guoqi.li@ia.ac.cn
ABSTRACT
Associative memory models are content-addressable memory systems fundamen-
tal to biological intelligence and are notable for their high interpretability. How-
ever, existing models evaluate the quality of retrieval based on proximity, which
cannot guarantee that the retrieved pattern has the strongest association with the
query, failing correctness. We reframe this problem by proposing that a query
is a generative variant of a stored memory pattern, and define a variant distri-
bution to model this subtle context-dependent generative process. Consequently,
correct retrieval should return the memory pattern with the maximum a posteri-
ori probability of being the queryâ€™s origin. This perspective reveals that an ideal
similarity measure should approximate the likelihood of each stored pattern gen-
erating the query in accordance with variant distribution, which is impossible for
fixed and pre-defined similarities used by existing associative memories. To this
end, we develop adaptive similarity, a novel mechanism that learns to approximate
this insightful but unknown likelihood from samples drawn from context, aiming
for correct retrieval. We theoretically prove that our proposed adaptive similar-
ity achieves optimal correct retrieval under three canonical and widely applicable
types of variants: noisy, masked, and biased. We integrate this mechanism into
a novel adaptive Hopfield network (A-Hop), and empirical results show that it
achieves state-of-the-art performance across diverse tasks, including memory re-
trieval, tabular classification, image classification, and multiple instance learning.
Our code is publicly available here.
1
INTRODUCTION
Associative memory represents a fundamental paradigm in information storage and retrieval, func-
tioning as a content-addressable memory system that serves as a cornerstone of biological intelli-
gence (Miyashita, 1988; Pearce & Bouton, 2001), particularly in the hippocampus and neocortex
(Wang et al., 2014). Unlike conventional computer memory, which retrieves data based on a spe-
cific address, associative memory retrieves stored patterns by using a partial or noisy variant of the
pattern itself as a cue. This memory paradigm enables robust pattern completion, error correction,
and fault-tolerant information processing, making it a compelling model for both understanding
biological cognition and developing artificial intelligence systems.
The computational modeling of associative memory has evolved dramatically since its inception.
Hopfield (1982) pioneered this field by introducing a recurrent neural network, dubbed Hopfield
network, capable of storing and retrieving patterns through energy minimization. Subsequent work
(Krotov & Hopfield, 2016; Demircigil et al., 2017) extended memory capacity using a steeper energy
function. A pivotal breakthrough came with the establishment of a profound connection between
the modern Hopfield network and the attention mechanism (Vaswani et al., 2017), achieved by
using the softmax(Â·) function to further separate memories (Ramsauer et al., 2021). This insight not
*Corresponding authors.
1
arXiv:2511.20609v1  [cs.LG]  25 Nov 2025

Under review as a conference paper
only unified two previously disparate fields but also inspired further refinements that strengthened
associative memoryâ€™s performance from different perspectives (Millidge et al., 2022; Hu et al., 2023;
Wu et al., 2024a), and broadened applications to tasks like clustering (Saha et al., 2023), time series
prediction (Wu et al., 2024b), and more (Krotov et al., 2025).
Despite these significant advances, a critical and unaddressed limitation pervades the literature: the
absence of a rigorous framework for assessing retrieval accuracy. Current evaluations typically rely
on proximity-based criteria, such as Ïµ-retrieval (Ramsauer et al., 2021; Hu et al., 2023; Wu et al.,
2024a; Hu et al., 2024; 2025), which deem retrieval successful if the retrieved pattern is sufficiently
close to a certain stored pattern. However, proximity does not establish correctness; ensuring the
retrieval is a valid memory provides no guarantee that it is the correct one, that is, the one that has the
strongest association with the query. This oversight leads to a universal reliance on fixed, pre-defined
similarity measures (e.g., inner product or Euclidean distance between two memory patterns). Such
one-size-fits-all metrics fail to capture the nuanced, context-dependent association, or similarity,
between the query and the stored memory patterns. For instance, the word click is semantically
similar to tap, phonetically similar to clique, and orthographically to clock â€” illustrating that an
appropriate notion of similarity is context and task dependent while fixed metrics cannot adapt to
such context, nor can they certify correctness.
Our central premise is that correctness is inherently generative: a query x emerges as a variant of
an unknown stored pattern Î¾k. So, to properly define and achieve correct retrieval, we should model
the generative process that transforms a stored pattern Î¾k into a query x. To this end, we encapsulate
the context-dependent and application-related subtleness into a probabilistic framework centered
on the concept of variant distribution V(Î¾1Â·Â·Â·N), a joint distribution over stored patterns Î¾1Â·Â·Â·N and
memory variants x, where the likelihood pV(Î¾k, x) captures how probable that we observe Î¾k and it
coincidentally generates x for (Î¾k, x) âˆ¼V(Î¾1Â·Â·Â·N). Under this view, a correct retrieval returns the
memory pattern Î¾k maximizing the posterior pV(Î¾k|x), that is, the likelihood of x originates from
Î¾k when observed x as query. With further decomposition, maximizing pV(Î¾k|x) is equivalent to
maximizing pV(x|Î¾k), i.e., given Î¾k, how probable would it generates x as its variant. The correct
retrieval is therefore finding the pattern Î¾k that is most likely to produce x by varianting itself. This
perspective yields an insight that optimal correct retrieval can be achieved by forcing the similarity
measure to mimic the behavior of pV(x|Î¾k).
However, it is not possible to derive the variant distribution V(x1Â·Â·Â·N) and the likelihood pV(x|Î¾k)
on most occasions. Thus, we need to reconstruct the unknown by mining deeply from what is
observable: the query x, stored patterns Î¾1Â·Â·Â·N, and samples matching the context that vaguely
describe V. Building on these motivations, we introduce an adaptive similarity framework that learns
to approximate pV(x|Î¾k) from samples observed from the variant distribution, without assuming the
variant type is known a priori. Integrating this novel similarity measure into the Hopfield energy
yields an adaptive Hopfield network that strives for correct retrieval by capturing the underlying
variant distribution. Our key contributions are as follows:
â€¢ We introduce the variant distribution to model how queries emerge from stored patterns,
and formalize correct retrieval as a robust and meaningful criterion for evaluating the
theoretical accuracy of associative memories.
â€¢ We propose adaptive similarity derived from this framework and prove its optimality for
three canonical and widely applicable types of memory variants: noisy, masked, and biased.
â€¢ We build a novel adaptive Hopfield network (A-Hop) that incorporates learnable adaptive
similarity, achieving state-of-the-art performance among computational associative mem-
ories on tasks including memory retrieval, tabular classification, image classification, and
multiple instance learning.
2
BACKGROUND
We consider an associative memory that stores N memory patterns denoted by the memory matrix
Îž = [Î¾1; Î¾2; Â· Â· Â· ; Î¾N] âˆˆRdÃ—N, where each column vector Î¾i âˆˆRd represents a memory pattern.
Given a memory variant (query) x âˆˆRd, the goal is to retrieve the stored memory that is most
associated with it. For simplicity, we denote [n] â‰œ{k âˆˆZ | 1 â‰¤k â‰¤n}, and Î¾ âˆˆÎž means Î¾ is one
of the column vectors of the memory matrix Îž. Appendix A.1 contains a collection of notations.
2

Under review as a conference paper
2.1
HOPFIELD NETWORKS
Table 1: Summary of all Hopfield network by components; Hop (Hopfield, 1982), D-Hop (Krotov
& Hopfield, 2016), E-Hop (Demircigil et al., 2017), M-Hop (Ramsauer et al., 2021), U-Hop (Mil-
lidge et al., 2022), S-Hop (Hu et al., 2023), K-Hop (Wu et al., 2024a), and A-Hop (Ours).
Model
sim(Î¾, x)
sep(s)
mod(Î¾)
E(x)
Hop (Original)
Î¾âŠ¤x
s
Î¾
âˆ’1
2xâŠ¤ÎžÎžâŠ¤x
D-Hop (Dense)
Î¾âŠ¤x
sk
Î¾
âˆ’(1âŠ¤ÎžâŠ¤x)k+1
E-Hop (Exponential)
Î¾âŠ¤x
exp(s)
Î¾
âˆ’exp(1âŠ¤ÎžâŠ¤x)
M-Hop (Modern)
Î¾âŠ¤x
softmax(s)
Î¾
1
2xâŠ¤x âˆ’lse(ÎžâŠ¤x)
U-Hop (Universal)
âˆ’âˆ¥Î¾ âˆ’xâˆ¥1
arg max(s)
Î¾
/
S-Hop (Sparse)
Î¾âŠ¤x
sparsemax(s)
Î¾
1
2xâŠ¤x âˆ’Î¨â‹†
2(Î² ÎžâŠ¤x)
K-Hop (Kernelized)
Î¾âŠ¤x
Î±âˆ’entmax(s)
Î¦âŠ¤Î¦Î¾
1
2xâŠ¤Î¦âŠ¤Î¦x âˆ’Î¨â‹†
Î±(Î² ÎžâŠ¤Î¦âŠ¤Î¦x)
A-Hop (Adaptive)
wâŠ¤ftpt(Î¾, x)
multiple
Î¾
âˆ’lse(s(Îž, x))
Hopfield network is a line of associative memory that retrieves the most relevant stored memory
through a similarity-based matching process. The original Hopfield network (Hopfield, 1982) uses
d binary neurons Ïƒ âˆˆ{âˆ’1, +1}d to represent the states of the memory system that is limited to
storage of binary values. For retrieving, the model sets the query as the initial state (i.e., Ïƒ(0) = x),
and updates one or more neuron(s) iteratively through the following dynamics until convergence:
Ïƒ(t+1)
i
= sgn
ï£«
ï£­
d
X
j=1
Ti,jÏƒ(t)
j
ï£¶
ï£¸,
where Ti,j =
N
X
k=1
Î¾k,i Â· Î¾k,j.
A vectorized retrieval dynamics exists when updating all neurons simultaneously in one iteration:
Ïƒ(t+1) = sgn

Îž ÎžâŠ¤Ïƒ(t)
.
Years later, Krotov & Hopfield (2016) improves the memory capacity of Hopfield network from
O(d) to O(2d/2) when storing random samples. They adopted higher-order polynomial or expo-
nential function (Demircigil et al., 2017) to distinguish each stored memory to alleviate the fuzzy
memory problem: Ïƒ(t+1) = sgn
 Îž(ÎžâŠ¤Ïƒ(t))k
or Ïƒ(t+1) = sgn
 Îž exp(ÎžâŠ¤Ïƒ(t))

.
This concept has evolved significantly and was extended to memories with continuous value. Mod-
ern Hopfield networks abstract retrieval as a one-iteration update (Ramsauer et al., 2021), and their
retrieval dynamics T (x) can be unified under a three-step procedure (Millidge et al., 2022):
(1) Similarity [s = sim(Îž, x)]: The query x is compared against all stored patterns Î¾1Â·Â·Â·N using the
similarity function sim(Â·, Â·), obtaining a vector of similarity scores s âˆˆRN, where sk = sim(Î¾k, x).
(2) Separation [p = sep(s)]: The similarity scores (logits) s are transformed by a separation
function sep(Â·) into a probability distribution p. The separation function sharpens the scores and
emphasizes patterns with high similarity.
(3) Readout [y = Îž p]: The final retrieved pattern y is computed as a weighted combination of
stored memory patterns, using the weights p provided by the separation function.
Combining each step gives the unified retrieval dynamics:
y = T (x) = Îž sep(sim(Îž, x)).
Concretely, Ramsauer et al. (2021) proposed using softmax(Â·) function as the separation function,
which further enlarges the memory capacity and draws a tight connection between associative mem-
ory and attention mechanism, with the retrieval dynamics being T (x) = Îž softmax
 ÎžâŠ¤x

. Later,
Hu et al. (2023) proposed sparse Hopfield network substituting softmax(Â·) with sparsemax(Â·), for
inducing sparse selection while retaining differentiability. More recently, Wu et al. (2024a) at-
tempted to store memory patterns in a kernel space with greater separation among patterns, giving
rise to adding a new modulation step to the existing three-step unified framework. For clarity, we
use the modulation function mod(Â·) to describes how memory patterns are stored or pre-trained for
better retrieval and larger capacity. So, it broadens the unified framework to:
y = Îž sep(sim(mod(Îž), x)).
3

Under review as a conference paper
The kernelized Hopfield network (Wu et al., 2024a) adopted mod(Îž) = Î¦âŠ¤Î¦ Îž for a learnable
matrix Î¦ âˆˆRDÎ¦Ã—d, that projects memory patterns into a kernel space with the retrieval dynamics
being T (x) = Îž sep((Î¦ Îž)âŠ¤(Î¦ x)) = Îž sep((Î¦âŠ¤Î¦ Îž)âŠ¤x). The kernel Î¦ is trained to minimize
a separation loss defined on Îž, so that the expected Euclidean distance between any two memory
patterns is maximized. A succeeding work (Hu et al., 2024) uses spherical codes to find the optimal
kernel Îž that maximizes the capacity of the kernelized Hopfield network.
Furthermore, the energy-based view is a defining feature of Hopfield networks: memory retrieval
can be viewed as descending on a energy landscape (a Lyapunov function) E(Â·) whose minima
coincide with stored patterns (or their modulated version). Formally, the retrieval dynamics T (x)
and the corresponding energy function E(x) are jointly and carefully designed such that each update
monotonically decreases the energy (i.e., E(T (x)) < E(x)), and successful retrieval occurs when
being sufficiently close to a generalized fixed point near a specific memory pattern Î¾k âˆˆÎž (i.e.,
âˆ¥T (x)âˆ’Î¾kâˆ¥2 < Ïµ). This principled linkage between dynamics and energy ensures convergence and
provides a powerful interpretable model of memory retrieving for Hopfield networks. Connecting
to the previous unified framework, the separation function decides the direction of the retrieval
dynamics T (Â·), and the modulation function reshapes the geometry of the energy landscape E(Â·),
and we organize all existing Hopfield networksâ€™ components and energy function in Table 1.
However, across existing formulations, the energy and retrieval dynamics are anchored to a fixed,
task-agonistic similarity measure (typically the dot product). Apart from that, the energy and dy-
namics are solely determined by stored memories Îž that overlook the nature of the subtle, nuanced,
context-specific association between queries and memories required for â€œcorrectâ€ retrieval. This
fundamental gap motivates our work: to refine the energy and retrieval dynamics around a learnable,
adaptive similarity measure while preserving the precious interpretability of Hopfield networks.
3
METHODS
In this section, we first establish a rigorous probabilistic framework to define correct retrieval, elim-
inating limitations of conventional proximity-based metrics (Section 3.1). To make this concept
practical, we develop the similarity footprint (Section 3.2), a multi-scale descriptor measuring asso-
ciation between queries and memory patterns, and use it to learn an adaptive similarity integrated
into Adaptive Hopfield Network (A-Hop) that achieves optimal correct retrieval for noisy, masked,
and biased types of variants, and has a decreasing, convergent, and bounded energy (Section 3.3).
3.1
VARIANT DISTRIBUTION AND CORRECT RETRIEVAL
Conventional analyses of associative memory (Ramsauer et al., 2021; Hu et al., 2023; Wu et al.,
2024a; Hu et al., 2024) mostly focus on Ïµ-retrieval:
Definition 1: Ïµ-retrieval (Hu et al., 2023; Wu et al., 2024a; Hu et al., 2025)
Given a query x âˆˆRd and the retrieval result y âˆˆRd given by the memory system, a memory
pattern Î¾ âˆˆÎž is said to be Ïµ-retrieved if âˆ¥y âˆ’Î¾âˆ¥2 â‰¤Ïµ.
While Ïµ-retrieval ensures the retrieval result y lies near a certain stored memory pattern Î¾k âˆˆÎž,
it provides no guarantee that Î¾k is the most appropriate match for query x. The query x may
have stronger associations with a different pattern Î¾j (j Ì¸= k), denoting that Î¾j could be the more
appropriate match for x. This identifies that proximity alone is an insufficient proxy for correctness.
To address this limitation, we use a probability distribution to model the generative process of the
query x. We posit that a query x is not an arbitrary vector but a variant of a specific stored mem-
ory pattern Î¾ âˆˆÎž generated by a context-dependent process. We formalize this via the variant
distribution, which models the relation of memory patterns Î¾ âˆˆÎž and queries x âˆˆRd as variants:
Definition 2: Variant distribution
A variant distribution V(Îž) is a joint distribution over pair (Î¾, x) âˆˆÎž Ã— Rd where Î¾ âˆˆÎž is one
of the stored memory patterns and x âˆˆRd is an arbitrary query.
For (Î¾, x) âˆ¼V(Îž), the probability density function pV(Îž)(Î¾, x) (or pV(Î¾, x) when unambiguous)
measures the likelihood of observing Î¾ and x at the same time.
4

Under review as a conference paper
Additionally, the posterior pV(Î¾|x) represents the likelihood that query x originates from memory
pattern x, and the likelihood pV(x|Î¾) models how probable that Î¾ generates x. This leads to a
rigorous definition of the context-dependent correct retrieval:
Definition 3: Correct retrieval
A query x is said to be correctly retrieved under V(Îž), if the retrieval result y satisfies that:
arg min
Î¾â€²âˆˆÎž
{âˆ¥y âˆ’Î¾â€²âˆ¥2} = arg max
Î¾â€²âˆˆÎž
{pV(Î¾â€²|x)} .
(1)
In Equation 1, the left-hand side identifies the closest memory pattern to the retrieval result y (given
by the memory system), while the right-hand side is ground truth (the most probable origin of query
x given by variant distribution V(Îž)). Thus, intuitively, correct retrieval requires that the closest
memory pattern coincides with ground truth. With further derivation,
arg max
Î¾â€²âˆˆÎž
{pV(Î¾â€²|x)} = arg max
Î¾â€²âˆˆÎž

pV(x|Î¾â€²) Â· pV(Î¾â€²)
pV(x)

= arg max
Î¾â€²âˆˆÎž
{pV(x|Î¾â€²) Â· pV(Î¾â€²)}.
(2)
This reformulation is necessary as modeling the likelihood pV(x|Î¾) is more tractable than directly
estimating the posterior pV(Î¾|x). The likelihood pV(x|Î¾) is conditioned on a single, finite, known
memory Î¾, while the posterior pV(Î¾|x) requires estimating a complex function that maps the entire
query space Rd to a discrete distribution over Îž. Given that the prior pV(Î¾) is typically uniform or
can be easily estimated from samples, the central challenge of achieving correct retrieval reduces to
accurately modeling pV(x|Î¾), in other words, how probable does x generate Î¾ under V(Îž)? With
this in hand, it is possible to model three canonical and common variant types rigorously:
Definition 4: Noisy variant
A query x is a noisy variant if it is generated by adding Gaussian noise to a certain memory
pattern Î¾ âˆˆÎž. Formally, (x âˆ’Î¾) âˆ¼N(0, diag(Ïƒ)) holds for (Î¾, x) âˆ¼Vnoisy(Îž), where diag(v)
transform vector bv to a diagonal matrix. The likelihood of noisy variant is:
pVnoisy(x|Î¾) =
1
(2Ï€)d/2|diag(Ïƒ)|1/2 exp

âˆ’1
2(x âˆ’Î¾)âŠ¤diag(Ïƒ)âˆ’1(x âˆ’Î¾)

.
Noisy variants have been widely studied (Krotov & Hopfield, 2016; Hu et al., 2023; Wu et al.,
2024a), and it occurs in scenarios such as sensor noise. Specially, under isotropy Ïƒ = Ïƒ1, the
respective likelihood reduces to: pVnoisy(x|Î¾) = (2Ï€Ïƒ)âˆ’d/2 exp(âˆ’âˆ¥x âˆ’Î¾âˆ¥2
2 / 2Ïƒ).
Definition 5: Masked variant
A masked variant of a memory pattern Î¾ âˆˆÎž is obtained by changing values in each dimension
with probability pmasked to numbers generated by G. The likelihood of masked variant is:
pVmasked(x|Î¾) = exp
 
ln pmasked Â·
d
X
i=1
[1 âˆ’Î´(xi âˆ’Î¾i)]
!
Ã—
d
Y
i=1
pG(xi)[1âˆ’Î´(xiâˆ’Î¾i)].
Masked variants arise in real-world scenarios such as information loss during transmission, the same
object appearing in different background, and more.
Definition 6: Biased variant
Adding a global bias to memory patterns gives the biased variant. Formally, x âˆ’Î¾ = d holds for
(Î¾, x) âˆ¼Vbiased(Îž) and a constant vector d âˆˆRd. The likelihood of biased variant:
pVbiased(x|Î¾) = Î´
"
d âˆ’
d
X
i=1
Î´(xi âˆ’Î¾i âˆ’di)
#
,
where Î´(x) =
1
x = 0
0
otherwise .
Biased variants occur as a systematic difference, such as changes in light conditions or use of filters.
We visualize the conditional probability density function pV(x|Î¾) in Fig. 1, providing intuition akin
to an electron cloud, with a memory pattern Î¾ as the atom nucleus and its variants as orbiting
electrons. A direct observation is that pV(x|Î¾) varies significantly across contexts, and may be an-
alytically intractable. For instance, even though visualizing the noisy + masked variant (Fig. 1 (d))
is possible by composing these two operations, deriving its likelihood pV(x|Î¾) is analytically cum-
5

Under review as a conference paper
noisy
masked
biased
noisy + masked
(a)
(b)
(c)
(d)
Figure 1: Visualization of probability density function pV(x|Î¾) for noisy, masked, biased, and noisy
+ masked variants. Darker regions indicate larger pV(x|Î¾), and the central dark point represents Î¾.
bersome. Consequently, although V(Îž) is a principled tool to link queries with memory patterns,
it poses two challenges for correct retrieval: (1) the underlying variant type is generally unknown a
priori; and (2) the resulting variant distribution V(Îž) can be too complex to model explicitly.
3.2
SIMILARITY FOOTPRINT
In the previous section, we established correct retrieval as selecting the pattern that maximizes
pV(x|Î¾). While pV(x|Î¾) constitutes the ideal similarity metric (guiding the memory system to
return arg maxÎ¾âˆˆÎž{pV(Î¾|x)} as per Eq. 2), it is often analytically intractable or unknown in real-
world scenarios. To address this, we propose mining richer evidence from observable quantities
to construct a tractable surrogate that mimics the selectivity of the true likelihood pV(x|Î¾). We
introduce similarity footprint, a multi-scale descriptor capturing the relation between query x and
memory patterns Îž in multiple subspaces. By replacing proximity-based scalar similarity measure
(e.g., Î¾âŠ¤x) with a structured descriptor (the footprint, a d-dimensional vector), the model could form
a accurate and robust decision based on the comprehensive subspatial evidence.
We begin with a base similarity measure (e.g., dot product Î¾âŠ¤x or negative squared Euclidean
distance âˆ’âˆ¥Î¾ âˆ’xâˆ¥2
2), and define the k-optimal similarity between Î¾ and x:
sim(k)(Î¾, x) â‰œ
max
DâŠ†[d],|D|=k {sim(Î¾D, xD)} ,
where vD â‰œ
vD1, vD2, Â· Â· Â· , vD|D|
âŠ¤.
Here, vD is a sub-vector of v containing only the elements corresponding to indices in D. In-
tuitively, simk(Î¾, x) quantifies the largest alignment between x and Î¾ within their most consistent
k-dimensional subspace. This formulation inherently filters out corruption or pure noise by focusing
on the most informative dimensions while ignoring outliers. Consequently, we define the similarity
footprint as the vector aggregating all k-optimal similarity sim(k)(Â·, Â·) across all dimensionalities:
ftptsim(Î¾, x) â‰œ

sim(1)(Î¾, x), sim(2)(Î¾, x), Â· Â· Â· , sim(d)(Î¾, x)
âŠ¤
This vector serves as the rich multi-scale descriptor of the subspatial relation between Î¾ and x,
offering more evidence for measuring similarity. While a naÂ¨Ä±ve computation of the footprint requires
evaluating all 2d âˆ’1 subspaces (impractical), an efficient computation is possible for decomposable
similarity measures (Eq. 6). For such measures, whose result is the aggregation of similaritiy in
each dimension, the footprint computation reduces to O(d log d) via sorting, and the procedure is
visualized in Fig. 2 bottom part. Let q be the dimension-wise similarity vector (Fig. 2 (1)), where
qi = sim(Î¾i, xi) for i âˆˆ[d], and let Ëœq be the vector q sorted in decreasing order (Fig. 2 (2)). Then,
the similarity footprint can be calculated as (Fig.2 (3)):
ftptsim(Î¾, x) = UËœq.
(3)
where U is the lower-left triangular matrix of 1dÃ—d, (i.e., Ui,j = 1 if 1 â‰¤j â‰¤i â‰¤d, and Ui,j = 0
otherwise). Such simplication is valid because sim(k)(Î¾, x) = Pk
i=1 Ëœqi holds for decomposable
similarity measures, and a rigorous proof is provided in Appendix A.2 Theorem 3.
3.3
ADAPTIVE SIMILARITY AND ADAPTIVE HOPFIELD NETWORK
The similarity footprint provides a structured, multi-scale descriptor of the association between a
query x and a memory pattern Î¾. To exploit these subspatial evidences and construct a similarity
6

Under review as a conference paper
Adaptive Similarity
Stored Patterns
Separation
Input
Output
Adaptive Similarity
Separation
Input
Pattern
Loss
Generator
Pattern
Input
Similarity Score
Adaptive Hopï¬eld Network
Adaptive Similarity Training
Adaptive
Similarity
Footprint
Footprint
Sort
Sort
Pointwise Similarity
Pointwise Similarity
Adaptive Similarity
Proximity-Based Similarity
Input/Query
Stored Pattern
Pointwise Similarity
Sort
(decreasing)
Footprint
Similarity Score
(Ours)
Similarity Score
(Conventional)
Stored Pattern
Vector Form
Values all positive
Values all negative
value
dimension
value
dimension
(1)
(2)
(3)
(4)
(a)
(b)
(c)
(d)
(e)
Figure 2: Top: The architecture of adaptive Hopfield network (a), the training procedure for adap-
tive similarity (b) and its design choice (c). Bottom: An illustrative example of memory retrieval
procedure involving two 16 Ã— 16 image patterns and one query. The conventional proximitiy-based
similarity (d) fails to retrieve the correct pattern while the adaptive similarity (e) succeeds.
measure that adapts to the underlying variant distribution V(Îž), we introduce adaptive similarity as
a learnable linear combination of the footprint elements: ssim(Î¾, x) = wâŠ¤ftptsim(Î¾, x) = wâŠ¤UËœq
(Fig. 2 (4)), for some weight vector w âˆˆRd. This formulation enables the model to learn the relative
importance of subspaces with varing sizes and pay extra attention on more informative subspaces.
As an example, for masked variants, the model could assign larger weights to the first m terms in the
footprint, effectively ignoring the â€œtailâ€ of the footprint containing corrupted dimensions, whereas
for noisy variants, it might distribute weights to larger subspaces for a global view.
To further enhance the modelâ€™s expressiveness, we combine footprints of multiple base similarities
(Fig. 2 (c)) and derive the final similarity function s(Î¾, x) and its vectorized form s(Î¾, x):
s(Î¾, x) =
B
X
k=1
Î²k Â· wâŠ¤
k ftptsimk(Î¾, x)
and
s(Îž, x) = [s(Î¾1, x), s(Î¾2, x), Â· Â· Â· , s(Î¾N, x)]âŠ¤,
where Î²1Â·Â·Â·B are learnable scalars weighting B different base similarities. In this work, we adopt two
simple and common base similarities: dis(Î¾, x) = âˆ’âˆ¥Î¾âˆ’xâˆ¥2
2 and dot(Î¾, x) = Î¾âŠ¤x. Integrating this
adaptive similarity into the unified associative memory framework (Table 1) using the softmax(Â·) as
the separation function yields Adaptive Hopfield Network (A-Hop, complete achitecure illustrated
in Fig. 2 (a)(c)), and its retrieval dynamics can be formulated as:
y = T (x) = Îž sep(s(Îž, x)) = Îž softmax(Î²dis Â· sdis(Îž, x) + Î²dot Â· sdot(Îž, x)) .
(4)
The parameters wâ€™s and Î²â€™s are optimized to align the modelâ€™s behavior with the underlying variant
distribution. We construct a training set by drawing samples from the variant distribution V(Îž) and
minimize the discrepancy loss between the modelâ€™s predicted likelihood ËœpV(x|Î¾k) â‰œsep(s(Îž, x))k
and the underlying ground-truth likelihood pV(x|Î¾k) (see Fig. 2 (b) for training procedure):
L(Îž, V) = E(Î¾k,x)âˆ¼V(Îž) [âˆ’log ËœpV(x|Î¾k)] .
(5)
For an intuitive understanding of adaptive similarity, we demonstrates a comprehensive procedure
of calculating the adaptive similarity score between Î¾1, Î¾2 (pixel art of two chicken) and the query
7

Under review as a conference paper
x (a noisy + masked variant of Î¾1) in the botton part of Fig. 2. While rigorously, Adaptive Hopfield
Network satisfies the following theoretical properties.
Definition 7: Optimal correct retrieval
We say a retrieval dynamics T (x) achieves optimal correct retrieval under V(Îž), if for any
(Î¾, x) âˆ¼V(Îž) it achieves correct retrieval for query x.
Theorem 1: A-Hopâ€™s retrieval dynamics for optimal correct retrieval
The following retrieval dynamics adopted by A-Hop achieves optimal correct retrieval for noisy,
masked, and biased variants, with a careful design of s(Îž, x):
y = T (x) = Îž sep(s(Îž, x))
First, A-Hop is theoretically capable of achieving optimal correct retrieval (its retrieval is always
correct, see Definition 7) for noisy, masked, and biased variants when weights wâ€™s and the adaptive
similarity s(Îž, x) are ideally configured. A complete walk-through and proof to Theorem 1 is pre-
sented in Appendix A.2.1. While optimal correct retrieval is not always guaranteed for continuous
and parameter-efficient adaptive similarity (e.g., s(Î¾, x) = wâŠ¤ftpt(Î¾, x)), we discuss the tradeoff
between learnability and optimality in Appendix A.3.1, and show that learnable adaptive similarity
can attain high retrieval accuracy with large probability. Together, this theoretical analysis and the
empirical results in ablation study (Appendix A.4.6) validate the design choice illustrated in Fig 2.
Theorem 2: A-Hopâ€™s decreasing, convergent, and bounded energy function
Energy E(x) will be monotonically decreasing, convergent, bounded from below, for isotropic
noisy, and biased variants, if the following energy is used:
E(x) = âˆ’lse (s(Îž, x))
Second, consist with existing associative memories, A-Hop guarantees a monotonically decreasing,
convergent energy that can be bounded from below by âˆ’ln nâˆ’âˆ¥bâˆ¥2
2/4 during the iterative retrieval
process. Notably, among the models compared in Table 1, this is the only energy function that can be
bounded from below even as âˆ¥Î¾âˆ¥2 â†’âˆž, which guarantees robust retrieval behavior for unbounded
patterns. A detailed proof to Theorem 2 is provided in Appendix A.2.2.
4
EXPERIMENTS
We evaluate the effectiveness of A-Hop on tasks including memory retrieval, tabular classification,
image classification, and multiple instance learning, demonstrating that A-Hop achieves state-of-
the-art performance on these tasks. A further ablation study validates our design choice of adap-
tive similarity (Appendix A.4.6). Due to space constraints, full descriptions of baselines, metrics,
datasets, and implementation details are moved to Appendix A.4.
4.1
MEMORY RETRIEVAL
(.8 .0 .0)
(.6 .3 .0)
1.0
0.8
0.6
0.4
0.2
(.5 .5 .0)
(.3 .6 .0)
(.0 .8 .0)
(.0 .6 .3)
(.0 .5 .5)
(.0 .5 .5)
(.0 .0 .8)
(.3 .0 .6)
(.6 .0 .3)
(.5 .0 .5)
mask
noise
bias
mask
noise
bias
A-Hop
K-Hop
M-Hop
U-Hop
MLP
2048 patterns
4096 patterns
plot settings
(a)
(b)
(c)
Figure 3: Retrieval accuracy (â†‘) of 64-dimensional synthetic memory patterns.
Prior work on Hopfield networks primarily assesses retrieval accuracy (Definition 9) under two set-
tings: (1) masking half of the dimensions and (2) adding Gaussian noise. However, real-world data
8

Under review as a conference paper
often exhibits compounded corruptions. To rigorously stress-test retrieval correctness, we intro-
duce mixed variant parameterized by a triplet (dmask, dnoise, dbias) âˆˆ[0, 1]3 that controls the intensity
(difficulty) of masking, noise, and bias, respectively (see Appendix A.4.3 for formal definitions).
We evaluate A-Hop against existing baselines on 12 mixed variant settings (listed in Fig. 3 (c)) with
64-dimensional (d = 64) random memory patterns at scales of N = 2048 (Fig. 3 (a)) and N = 4096
(Fig. 3 (b)). Furthermore, Table 2 details performance under high-intensity corruption using either
N = 2048, d = 64 synthetic vectors or N = 2048, d = 784 MNIST digits as memory patterns.
As shown in Fig. 3, existing associtive memory baselines perform poorly when masking or bias is
involved as their retrieval accuracy is less than 20% on the mask axis and less than 60% on the bias
axis while A-Hop achieve near-perfect accuracy on these settings. It is because fixed proximity-
based similarity cannot distinguish between irrelevance and missing data (masking), and have no
chance to learn the biased term added to memory pattern. In Table 2, baselines exhibit a sharp per-
formance collapse as difficulty increases, while A-Hop maintains robust retrieval gaining a 6% to
20% accuracy increment. A-Hopâ€™s adaptive similarity creates a noticable empirical gap by achiev-
ing higher retrieval accuracy and lower retrieval error in all settings. This highlights its robustness
and impressive adaptability to align similarity to the underlying variant distribution through learning.
Table 2: Retrieval accuracy (â†‘) and error (â†“) between models. Each cell contains the mean accuracy
or error with standard deviation in a smaller font. Results of the best-performing model are bolded.
Difficulty d stands for mixed variant setting with (dmask, dnoise, dbias) = (d, d, d).
Dataset
Synthetic
MNIST
Difficulty
0.4
0.5
0.6
0.7
Metrics
Accuracy
Error
Accuracy
Error
Accuracy
Error
Accuracy
Error
M-Hop
.520Â±.02
.176Â±.01
.195Â±.03
.300Â±.01
.875Â±.01
.013Â±.00
.661Â±.02
.068Â±.00
U-Hop
.260Â±.04
.417Â±.02
.059Â±.01
.554Â±.01
.540Â±.03
.143Â±.01
.176Â±.02
.347Â±.01
K-Hop
.487Â±.03
.295Â±.02
.177Â±.02
.764Â±.02
.764Â±.02
.064Â±.01
.526Â±.02
.164Â±.01
K2-Hop1
.521Â±.02
.176Â±.01
.195Â±.02
.298Â±.01
.878Â±.01
.013Â±.00
.660Â±.01
.068Â±.01
A-Hop
.724Â±.02
.106Â±.01
.360Â±.02
.227Â±.01
.939Â±.01
.005Â±.00
.849Â±.01
.015Â±.01
4.2
TABULAR CLASSIFICATION
Table 3: Predictive performance (â†‘) between models on tabular data. Each cell contains mean accu-
racy or AUC-ROC score with standard deviation in a smaller font. Results of the best-performing
associative memory are bolded, and the best other model is underlined.
Model
Adult
Bank
Vaccine
Purchase
Heart
M-Hop
.8080Â±.001
.9085Â±.003
.7975Â±.001
.8822Â±.001
.6325Â±.002
K2-Hop
.8172Â±.003
.9092Â±.002
.7971Â±.003
.8825Â±.002
.6473Â±.002
A-Hop
.8634Â±.002
.9139Â±.002
.8042Â±.002
.9007Â±.001
.7315Â±.002
Extra Trees
.8595Â±.004
.9098Â±.003
.7932Â±.002
.8916Â±.002
.7175Â±.003
Random Forest
.8592Â±.002
.9132Â±.003
.7918Â±.003
.9002Â±.001
.7254Â±.002
AdaBoost
.8597Â±.003
.9094Â±.001
.8011Â±.002
.8865Â±.001
.7294Â±.001
XGBoost
.8640Â±.002
.9152Â±.003
.8034Â±.002
.9032Â±.003
.7370Â±.003
We investigate the utility of A-Hop as a memory-based classifier for tabular data (setup details
in Appendix A.4.4). Unlike image data, tabular data is often heterogeneous, containing a mix of
continuous and categorical features with varying scales and sparsity.
A-Hop consistently outperforms all competing associative memories (M-Hop and K2-Hop),
demonstrating that adaptive similarity is crucial for handling the mixed feature types inherent in
tabular data. Furthermore, A-Hop surpasses standard deep learning baselines (Extra Trees, Ran-
dom Forest, AdaBoost) in all datasets. While the gradient-boosting method XGBoost (Chen &
Guestrin, 2016) retains a slight edge on 4 out of 5 datasets, A-Hop significantly narrows the gap
1K2-Hop is K-Hop whose kernel is optmized by Equation 5, rather than the original separation loss.
9

Under review as a conference paper
compared to prior approaches, offering a differentiable alternative to tree ensembles. Significantly,
the performance gap between A-Hop and other memory models is widest on the Adult (about 5%)
and Heart (about 10%) datasets. We hypothesize that these datasets contain more subtle and com-
plicated variant distribution where the â€œinformativenessâ€ of dimensions varies per query, but the
results suggest that A-Hop can find the most informative subspace more accurately and can retrieve
relevant prototypes even when the distance in the original feature space is misleading.
4.3
IMAGE CLASSIFICATION AND MULTIPLE INSTANCE LEARNING
Table 4: Classification accuracy (â†‘) of each model on images, and AUC-ROC score (â†‘) of each
model in multiple instance learning task. Each cell contains accuracy or AUC-ROC score with
standard deviation in a smaller font. Results of the best-performing associative memory are bolded.
Image Classification
Multiple Instance Learning
Dataset
CIFAR10
CIFAR100
Tiny
ImageNet
Dataset
Tiger
Fox
Elephant
UCSB
M-Hop
.5123Â±.003
.2464Â±.003
.1095Â±.002
M-Hop
.8924Â±.005
.6327Â±.013
.9344Â±.009
.8815Â±.022
K-Hop
.5489Â±.002
.2877Â±.002
.1164Â±.002
S-Hop
.8923Â±.006
.6433Â±.015
.9365Â±.002
.8794Â±.024
A-Hop
.5637Â±.003
.2904Â±.002
.1213Â±.002
A-Hop
.9030Â±.007
.6753Â±.013
.9451Â±.004
.8935Â±.022
Following established protocols, we evaluate A-Hop on image classification (Wu et al., 2024a) and
multiple instance learning (Ramsauer et al., 2021; Hu et al., 2023) by integrating it as a component
within larger and more complicated deep neural network architectures (i.e., HopfieldLayer,
HopfieldPooling (Ramsauer et al., 2021)).
As shown in Table 4, A-Hop consistently achieves the highest accuracy and AUC-ROC scores
among all Hopfield variants. This demonstrates that the benefits of adaptive similarity extend to
complex, high-dimensional data and can enhance the performance of sophisticated models like the
Vision Transformer. While the absolute gains in these complex architectures are naturally smaller
than in isolated retrieval tasks (as the associative memory is not the core mechanism), the consis-
tent superiority of A-Hop validates its role as a general-purpose, robust associative memory layer.
Nevertheless, the consistent improvement confirms that optimizing the similarity measure remains
a valuable factor for enhancing performance in complex deep learning systems.
4.4
ABLATION STUDY
Due to page limit, the ablation study is moved to Appendix A.4.6.
5
CONCLUSION
We reframe associative memory retrieval as a problem of correct retrieval under a task- and context-
dependent variant distribution, motivating a similarity measure that approximates the likelihood that
a stored pattern generated the query. Building on this principle, we propose adaptive similarity,
prove its optimality for three canonical variant families (noisy, masked, biased), and instantiate
it in a new adaptive Hopfield network, A-Hop. This perspective clarifies why fixed, pre-defined
similarities are inherently limited: they cannot align to the prevailing variant distribution and thus
struggle to guarantee correctness, whereas adaptivity enables the model to capture the underlying
variant distribution through samples, shifting towards correctness.
Empirically, A-Hop establishes state-of-the-art performance among Hopfield networks across mem-
ory retrieval, tabular classification, image classification, and multiple instance learning. The gains
are most pronounced under mixed variant settings where adaptive similarity maintains impressively
high retrieval accuracy and low error. In downstream tasks, A-Hop consistently improves over
prior Hopfield variants. Ultimately, adaptive similarity is a key principle for advancing associative
memories, paving the way for more powerful and resilient memory systems.
10

Under review as a conference paper
6
ETHICS STATEMENT
This work adheres to the ICLR Code of Ethics. In this study, no human subjects or animal ex-
perimentation was involved. All datasets used were sourced in compliance with relevant usage
guidelines, ensuring no violation of privacy. We have taken care to avoid any biases or discrimi-
natory outcomes in our research process. No personally identifiable information was used, and no
experiments were conducted that could raise privacy or security concerns. We are committed to
maintaining transparency and integrity throughout the research process.
7
REPRODUCIBILITY STATEMENT
Code We provide code to help understand this work, and is publicly available at: https://
anonymous.4open.science/r/Adaptive-Hopfield-Network-C137/.
Datasets All datasets are either included in the repo, or a description for how to download and
preprocess the dataset is provided. All datasets are public and raise no ethical concerns.
Hyperparameters All parameters of our proposed framework are in Appendix A.4.
Environment Details of our experimental setups are provided in Appendix A.4.
Random Seed we do not set a random seed specifically for all random behavior, with the random
seed determined PyTorch.
8
LLM USAGE
Large Language Models (LLMs) were used to aid polishing of the manuscript. Specifically, we used
an LLM to assist in refining the language, improving readability, and ensuring clarity in various
sections of the paper. The model helped with tasks such as sentence rephrasing, grammar checking,
and enhancing the overall flow of the text.
It is important to note that the LLM was not involved in the ideation, research methodology, or
experimental design. All research concepts, ideas, and analyses were developed and conducted by
the authors. The contributions of the LLM were solely focused on improving the linguistic quality
of the paper, with no involvement in the scientific content or data analysis.
REFERENCES
Barry Becker and Ronny Kohavi.
Adult.
UCI Machine Learning Repository, 1996.
DOI:
https://doi.org/10.24432/C5XW20.
Leo Breiman. Random Forests. Machine Learning, 45(1):5â€“32, October 2001. ISSN 1573-0565.
doi: 10.1023/A:1010933404324. URL https://link.springer.com/article/10.
1023/A:1010933404324. Company: Springer Distributor: Springer Institution: Springer
Label: Springer Number: 1 Publisher: Kluwer Academic Publishers.
P. Bull, I. Slavitt, and G. Lipstein. Harnessing the power of the crowd to increase capacity for data
science in the social sector. In ICML #Data4Good Workshop, 2016.
Tianqi Chen and Carlos Guestrin.
Xgboost: A scalable tree boosting system.
In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Min-
ing, KDD â€™16, pp. 785â€“794, New York, NY, USA, 2016. Association for Computing Machin-
ery. ISBN 9781450342322. doi: 10.1145/2939672.2939785. URL https://doi.org/10.
1145/2939672.2939785.
Mete Demircigil, Judith Heusel, Matthias LÂ¨owe, Sven Upgang, and Franck Vermet. On a Model of
Associative Memory with Huge Storage Capacity. Journal of Statistical Physics, 168(2):288â€“
299, July 2017. ISSN 1572-9613. doi: 10.1007/s10955-017-1806-y. URL https://doi.
org/10.1007/s10955-017-1806-y.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248â€“255. Ieee, 2009.
11

Under review as a conference paper
DrivenData.
Flu shot learning:
Predict h1n1 and seasonal flu vaccines.
https://www.
drivendata.org/competitions/66/flu-shot-learning/data/, 2019.
Yoav Freund and Robert E. Schapire.
A Decision-Theoretic Generalization of On-Line Learn-
ing and an Application to Boosting.
Journal of Computer and System Sciences, 55(1):119â€“
139, 1997.
ISSN 0022-0000.
doi: https://doi.org/10.1006/jcss.1997.1504.
URL https:
//www.sciencedirect.com/science/article/pii/S002200009791504X.
Elisa Drelie Gelasca, Jiyun Byun, Boguslaw Obara, and B.S. Manjunath. Evaluation and benchmark
for biological image segmentation. In IEEE International Conference on Image Processing, Oct
2008. URL http://vision.ece.ucsb.edu/publications/elisa_ICIP08.pdf.
Pierre Geurts, Damien Ernst, and Louis Wehenkel. Extremely randomized trees. Machine Learning,
63(1):3â€“42, April 2006. ISSN 1573-0565. doi: 10.1007/s10994-006-6226-1. URL https:
//doi.org/10.1007/s10994-006-6226-1.
J. J. Hopfield. Neural networks and physical systems with emergent collective computational abil-
ities. Proceedings of the National Academy of Sciences, 79(8):2554â€“2558, April 1982. doi:
10.1073/pnas.79.8.2554.
URL https://www.pnas.org/doi/abs/10.1073/pnas.
79.8.2554. Publisher: Proceedings of the National Academy of Sciences.
Jerry Yao-Chieh Hu, Donglin Yang, Dennis Wu, Chenwei Xu, Bo-Yu Chen, and Han Liu. On Sparse
Modern Hopfield Model. Advances in Neural Information Processing Systems, 36:27594â€“27608,
December 2023. URL https://proceedings.neurips.cc/paper_files/paper/
2023/hash/57bc0a850255e2041341bf74c7e2b9fa-Abstract-Conference.
html.
Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu.
Provably Optimal Memory Capacity for
Modern Hopfield
Models: Transformer-Compatible Dense Associative Memories as Spher-
ical Codes.
Advances in Neural Information Processing Systems, 37:70693â€“70729, Decem-
ber 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/
hash/82846e19e6d42ebfd4ace4361def29ae-Abstract-Conference.html.
Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, and Han Liu. Nonparametric modern
hopfield models. In Forty-second International Conference on Machine Learning, 2025. URL
https://openreview.net/forum?id=xkV3uCQtJm.
Kaggle.
Cardiovascular disease dataset.
https://www.kaggle.com/datasets/
sulianova/cardiovascular-disease-dataset, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Dmitry Krotov and John J. Hopfield.
Dense Associative Memory for Pattern Recognition.
In Advances in Neural Information Processing Systems, volume 29. Curran Associates,
Inc., 2016.
URL https://papers.nips.cc/paper_files/paper/2016/hash/
eaae339c4d89fc102edd9dbdb6a28915-Abstract.html.
Dmitry Krotov, Benjamin Hoover, Parikshit Ram, and Bao Pham. Modern Methods in Associative
Memory, July 2025. URL http://arxiv.org/abs/2507.06211. arXiv:2507.06211 [cs].
Gert Lanckriet and Bharath K. Sriperumbudur.
On the Convergence of the Concave-Convex
Procedure.
In Advances in Neural Information Processing Systems, volume 22. Curran As-
sociates, Inc., 2009. URL https://papers.nips.cc/paper_files/paper/2009/
hash/8b5040a8a5baf3e0e67386c2e3a9b903-Abstract.html.
Yann Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.
Beren Millidge, Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, and Rafal Bogacz. Uni-
versal Hopfield Networks: A General Framework for Single-Shot Associative Memory Models.
In Proceedings of the 39th International Conference on Machine Learning, pp. 15561â€“15583.
PMLR, June 2022. URL https://proceedings.mlr.press/v162/millidge22a.
html. ISSN: 2640-3498.
12

Under review as a conference paper
Yasushi Miyashita. Neuronal correlate of visual associative long-term memory in the primate tempo-
ral cortex. Nature, 335(6193):817â€“820, October 1988. ISSN 1476-4687. doi: 10.1038/335817a0.
URL https://www.nature.com/articles/335817a0. Publisher: Nature Publishing
Group.
SÂ´ergio Moro, Paulo Cortez, and Paulo Rita. A data-driven approach to predict the success of bank
telemarketing. Decision Support Systems, 62:22â€“31, 2014. ISSN 0167-9236. doi: https://doi.
org/10.1016/j.dss.2014.03.001.
URL https://www.sciencedirect.com/science/
article/pii/S016792361400061X.
John M. Pearce and Mark E. Bouton. Theories of Associative Learning in Animals. Annual Review
of Psychology, 52(Volume 52, 2001):111â€“139, February 2001. ISSN 0066-4308, 1545-2085. doi:
10.1146/annurev.psych.52.1.111. URL https://www.annualreviews.org/content/
journals/10.1146/annurev.psych.52.1.111. Publisher: Annual Reviews.
Hubert Ramsauer, Bernhard SchÂ¨afl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gru-
ber, Markus Holzleitner, Thomas Adler, David Kreil, Michael K Kopp, GÂ¨unter Klambauer, Jo-
hannes Brandstetter, and Sepp Hochreiter. Hopfield networks is all you need. In International
Conference on Learning Representations, 2021. URL https://openreview.net/forum?
id=tL89RnzIiCd.
Bishwajit Saha, Dmitry Krotov, Mohammed J Zaki, and Parikshit Ram. End-to-end differentiable
clustering with associative memories. In Andreas Krause, Emma Brunskill, Kyunghyun Cho,
Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th Inter-
national Conference on Machine Learning, volume 202 of Proceedings of Machine Learning
Research, pp. 29649â€“29670. PMLR, 23â€“29 Jul 2023. URL https://proceedings.mlr.
press/v202/saha23a.html.
C. Sakar and Yomi Kastro. Online Shoppers Purchasing Intention Dataset. UCI Machine Learning
Repository, 2018. DOI: https://doi.org/10.24432/C5F88Q.
Ashish Vaswani,
Noam Shazeer,
Niki Parmar,
Jakob Uszkoreit,
Llion Jones,
Aidan N
Gomez, Å ukasz Kaiser, and Illia Polosukhin.
Attention is All you Need.
In Ad-
vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,
2017.
URL https://proceedings.neurips.cc/paper_files/paper/2017/
hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
Jane X. Wang, Lynn M. Rogers, Evan Z. Gross, Anthony J. Ryals, Mehmet E. Dokucu, Kelly L.
Brandstatt, Molly S. Hermiller, and Joel L. Voss. Targeted enhancement of cortical-hippocampal
brain networks and associative memory.
Science, 345(6200):1054â€“1057, August 2014.
doi:
10.1126/science.1252900.
URL https://www.science.org/doi/full/10.1126/
science.1252900. Publisher: American Association for the Advancement of Science.
Shurong Wang, Zhuoyang Shen, Xinbao Qiao, Tongning Zhang, and Meng Zhang. Dynfrs: An
efficient framework for machine unlearning in random forest. In Y. Yue, A. Garg, N. Peng, F. Sha,
and R. Yu (eds.), International Conference on Representation Learning, volume 2025, pp. 10636â€“
10657, 2025. URL https://proceedings.iclr.cc/paper_files/paper/2025/
hash/1caf09c9f4e6b0150b06a07e77f2710c-Abstract-Conference.html.
Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, and Han Liu.
Uniform Memory Retrieval
with Larger Capacity for Modern Hopfield Models. In Proceedings of the 41st International
Conference on
Machine Learning, pp. 53471â€“53514. PMLR, July 2024a.
URL https:
//proceedings.mlr.press/v235/wu24i.html. ISSN: 2640-3498.
Yu-Hsuan Wu,
Jerry Yao-Chieh Hu,
Weijian Li,
Bo-Yu Chen,
and Han Liu.
STan-
Hop:
Sparse
Tandem
Hopfield
Model
for
Memory-
Enhanced
Time
Series
Pre-
diction.
International
Conference
on
Representation
Learning,
2024:30886â€“30925,
May 2024b. URL https://proceedings.iclr.cc/paper_files/paper/2024/
hash/832b20b65f655587e9c0447860406a82-Abstract-Conference.html.
13

Under review as a conference paper
A
APPENDIX
APPENDIX CONTENTS
A.1
Notations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
A.2
Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
A.2.1
Optimal Correct Retrieval
. . . . . . . . . . . . . . . . . . . . . . . . . .
17
A.2.2
Unified Adaptive Similarity and Energy Function . . . . . . . . . . . . . .
20
A.3
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
A.3.1
On Good Design Choice of Adaptive Similarity . . . . . . . . . . . . . . .
23
A.3.2
On More Complicated Variant Distribution . . . . . . . . . . . . . . . . .
25
A.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
A.4.1
Baselines and Metrics
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
A.4.2
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
A.4.3
Memory Retrieval
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
A.4.4
Tabular Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
A.4.5
Image Classification and Multiple Instance Learning . . . . . . . . . . . .
28
A.4.6
Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
14

Under review as a conference paper
A.1
NOTATIONS
Table 5: Notations and symbolds used in this work.
Symbol
Description
Î¾, Î¾k
A specific memory pattern (d Ã— 1) (or memory, stored memory pattern, stored pattern).
Îž
The d Ã— N memory matrix, with each memory pattern being its column vector.
d
The dimensionality of memory patterns.
N
The number of stored memory patterns.
sim(Î¾, x)
The similarity function that measures how strong the association are between the inputs (or
similarity, similarity measure, measure, association).
sep(s)
The separation function, turning the output of sim(Â·, Â·) (logits) to a probability distribution.
mod(Îž)
The modulation function that governs how memory patterns are stored and learned.
E(x)
The energy landscape, defined on the same vector space as memory patterns.
T (x)
The retrieval dynamics, defined on the same vector space as memory patterns.
p
The probability distribution vector produced by sep(Â·).
s
The similarity score vector produced by sim(Â·, Â·).
x
The query vector. Also, the input to the associative memory
y
The retrieval result vector. Also, the output of the associative memory.
V(Îž)
The variant distribution on memory matrix Îž, governs how queries are generated. Each
query x is sampled from this distribution together with its origin memory pattern Î¾.
pV(Î¾, x)
The joint probability density function that measures the likelihood that Î¾ and x are
observed together.
pV(Î¾|x)
The conditional probability density function (posterior) that measures the likelihood that x
originates from x when observed x.
pV(x|Î¾)
The joint probability density function (likelihood) that measures the likelihood that Î¾
generates x when observed Î¾.
q
The dimension-wise similarity vector whose value of the i-th index measures the similarity
between the value of i-th index in x and Î¾.
Ëœq
The sorted version of q (sorted in ascending order).
U
The upper right triangle matrix of ones.
dis(Î¾, x)
The (negative and squared) Euclidean distance similarity âˆ’âˆ¥x âˆ’Î¾âˆ¥2
2.
dot(Î¾, x)
The dot product similarity xâŠ¤Î¾.
sim(k)(Î¾, x)
The k-optimal similarity function that finds a k-dimensional subspace that maximizes the
similarity sim(Â·, Â·) of the inputs within that subspace.
ftptsim(Î¾, x)
The similarity footprint function that generates the rich descriptor between Î¾ and x with
sim(Â·, Â·) being the base similarity (or footprint).
ssim(Î¾, x)
The adaptive similarity function adopting ftptsim(Â·, Â·) with sim(Â·, Â·) as the base similarity.
ssim(Îž, x)
The vectorized form of the adaptive similarity function ssim(Â·, Â·), and returns a vector that
measures the adaptive similarity between Î¾i and x for i âˆˆ[N].
s(Î¾, x)
The final adaptive similarity function that aggregate multiple ssimk(Â·, Â·) for different base
similarity sim(Â·, Â·) / footprint.
s(Îž, x)
The vectorized form of the final adaptive similarity function s(Â·, Â·), and returns a vector
that measures the adaptive similarity between Î¾i and x for i âˆˆ[N].
w
The weight vector that turns the footprint into a scalar, which is designed to extract
information from the rich descriptor.
Î²
Scalar used to aggregate different adaptive similarities ssim(Â·, Â·).
L(Î¾, V)
The loss function used to optimize wâ€™s and Î²â€™s
[n]
The set of integers less than or equal to n.
sgn(x)
Return the sign (âˆ’1 or +1) of the input.
Î´(x)
The Dirac delta that returns 1 when the input is 0 and returns 0 otherwise.
vâŠ¤
Transpose of a vector / matrix.
diag(v)
Transform vector v to a diagonal matrix.
vD
A sub-vector of v containing only the elements corresponding to indices in D.
âˆ¥vâˆ¥p
The â„“p norm.
lse(v)
The log-sum-exp function.
15

Under review as a conference paper
A.2
THEOREMS
We define the retrieval accuracy that estimates the retrieval performance of an associative memory
under a certain variant distribution.
Definition 8: Retrieval accuracy
Retrieval accuracy for an associative memory with retrieval dynamics T (Â·) is the probability that
correct retrieval is met:
E(Î¾,x)âˆ¼V(Îž)
"
Î´

arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} âˆ’arg max
Î¾â€²âˆˆÎž
{pV(Î¾â€²|x)}
#
=
Pr
(Î¾,x)âˆ¼V(Îž)
"
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = arg max
Î¾â€²âˆˆÎž
{pV(Î¾â€²|x)}
#
However, Definition 8 is usually intractable as pV(x|Î¾) is unknown and complicated. Therefore, we
define empirical retrieval accuracy based on samples drawn from V(Îž), which is computable, and
used in our experiments (Section 4).
Definition 9: Empirical retrieval accuracy
Empirical retrieval accuracy for an associative memory with retrieval dynamics T (Â·) can be esti-
mated by performing abundant retrieval tests:
E(Î¾,x)âˆ¼V(Îž)
"
Î´

arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} âˆ’Î¾
#
=
Pr
(Î¾,x)âˆ¼V(Îž)
"
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = Î¾
#
Theorem 3: Equivalence form for decomposable base similarity
For decomposable similarity measure sim(Â·, Â·) satisfying:
sim(Î¾, x) =
d
X
i=1
sim(Î¾i, xi)
(6)
Let qi = sim(Î¾i, xi) (1 â‰¤i â‰¤d), and Ëœqj (1 â‰¤j â‰¤d) be the j-th largest element in q. We have:
sim(k)(Î¾, x) =
k
X
j=1
Ëœqj
Proof. Let D(k) â‰œarg maxDâŠ†[d],|D|=k{sim(Î¾D, xD)}, and define Q(k) â‰œ{Ëœqi | i âˆˆD(k)}, then
we know:
sim(k)(Î¾, x) =
X
iâˆˆD(k)
qi =
X
qâˆˆQ(k)
q.
Then, we try to prove by induction that Q(k) = Q(kâˆ’1) âˆª{Ëœqk} for 1 â‰¤k â‰¤d, and we let Q(0) = âˆ….
For, k = 1, we can see that Q(1) = arg maxiâˆˆ[d]{qi} = Ëœqi satisfying the induction hypothesis.
Next, for 1 < k â‰¤d, we assume that the hypothesis holds for all 1 â‰¤kâ€² < k, we can see that
Q(kâˆ’1) = Q(kâˆ’2) âˆª{Ëœqkâˆ’1} = Q(kâˆ’3) âˆª{Ëœqkâˆ’1, Ëœqkâˆ’2} =
kâˆ’1
[
j=1
{Ëœqj}
Now, suppose the followings:
(1) We can find a âˆˆq1Â·Â·Â·d s.t. a > Ëœqkâˆ’1, Q(k) = Q(kâˆ’1) âˆª{a}. This not possible because
{Ëœq1, Â· Â· Â· , Ëœqkâˆ’2, a} would be a better choice for Q(kâˆ’1), and this contradicts with the assumption.
(2) We can find a âˆˆq1Â·Â·Â·d s.t. Ëœqkâˆ’1 â‰¥a > Ëœqk, Q(k) = Q(kâˆ’1) âˆª{a}. This is not possible because
16

Under review as a conference paper
there would be k elements in q1Â·Â·Â·d that is larger than Ëœqk, so that Ëœqk is the (k + 1)-largest element,
which contradicts with the definition of qk.
(3) We can find a âˆˆq1Â·Â·Â·d s.t. Ëœqk > a, Q(k) = Q(kâˆ’1) âˆª{a}. This is not possible because
{Ëœq1, Â· Â· Â· , Ëœqkâˆ’1, Ëœqk} is a better choice for Q(k) compared to {Ëœq1, Â· Â· Â· , Ëœqkâˆ’1, a} because Ëœqk > a.
Therefore, by contradiction, we can see that D(k) = D(kâˆ’1) âˆª{Ëœq(k)}, and the hypothesis holds due
to induction. Consequently,
Q(k) =
k[
j=1
Ëœqj
=â‡’
sim(k)(Î¾, x) =
X
qâˆˆQ(k)
q =
k
X
j=1
Ëœqj.
A.2.1
OPTIMAL CORRECT RETRIEVAL
We now start to prove Theorem 1.
Let us begin with a simple variant â€” the isotropic noisy variant.
Lemma 1: Optimal correct retrieval for isotropic noisy variant
A-Hop achieves optimal correct retrieval (Definition 7) for isotrophic noisy variant Vnoisy(Îž)
(Definition 4, (x âˆ’Î¾) âˆ¼N(0, ÏƒI) for (Î¾, x) âˆ¼Vnoisy(Îž) some Ïƒ âˆˆR) for arbitrary memory
matrix Îž âˆˆRdÃ—N.
Proof. We claim that the optimal correct retrieval is achieved when using sep(Â·) = arg max(Â·), and
ftptdis(Î¾, x) only (i.e., Î²1 = 1 and Î²2 = 0 in Equation 4). That is, the retrieval dynamics should be:
T (x) = arg max
Î¾â€²âˆˆÎž

wâŠ¤ftptdis(Î¾â€², x)
	
= arg max
Î¾â€²âˆˆÎž

âˆ’âˆ¥Î¾â€² âˆ’xâˆ¥2
2
	
This step can be satisfied by setting w1 = 1, and wi = 0 for 2 â‰¤i â‰¤d. Then, optimal retrieval is
achieved only when Equation 1 is met. We first estimate the right-hand side of Equation 1:
arg max
Î¾â€²âˆˆÎž

pVnoisy(Î¾|x)
	
= arg max
Î¾â€²âˆˆÎž

ln pVnoisy(x|Î¾)
	
= arg max
Î¾â€²âˆˆÎž

âˆ’d
2 ln 2Ï€Ïƒ âˆ’1
2Ïƒ âˆ¥x âˆ’Î¾â€²âˆ¥2
2

= arg max
Î¾â€²âˆˆÎž

âˆ’âˆ¥Î¾â€² âˆ’xâˆ¥2
2
	
The first step comes from Equation 2, and assuming that the prior p(Î¾) is uniform (which is often
the case for memory retrieval) or can be easily obtained from samples. And the second step comes
from Definition 4. We can see that the derived results coincide with the retrieval dynamics derived
before. Therefore, plugging the retrieval dynamics to the left-hand side of Equation 1 gives:
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = arg min
Î¾â€²âˆˆÎž
(
 arg max
Î¾â€²â€²

âˆ’âˆ¥Î¾â€²â€² âˆ’xâˆ¥2
2
	
âˆ’Î¾â€²
2
)
=
N
X
k=1
Î´
 
 arg max
Î¾â€²â€²

âˆ’âˆ¥Î¾â€²â€² âˆ’xâˆ¥2
2
	
âˆ’Î¾k

2
!
Â· Î¾k
=
N
X
k=1
Î´

max
Î¾â€²â€²

âˆ’âˆ¥Î¾â€²â€² âˆ’xâˆ¥2
2
	
âˆ’

âˆ’âˆ¥Î¾k âˆ’xâˆ¥2
2

Â· Î¾k
= arg max
Î¾kâˆˆÎž
{âˆ’âˆ¥Î¾k âˆ’xâˆ¥2
2}
The second step holds as there always exists a Î¾â€² âˆˆÎž that let âˆ¥arg maxÎ¾â€²â€²

âˆ’âˆ¥Î¾â€²â€² âˆ’xâˆ¥2
2
	
âˆ’Î¾â€²âˆ¥2 =
0, since the resulting vector of the arg max(Â·) âˆˆÎž, and Î¾â€² iterates every column vector of Îž, thus
must have coincided with resulting vector, and the thrid step holds for a similar reason.
17

Under review as a conference paper
Therefore, we show that the left-hand side and right-hand side of Equation 1 are the same
(arg maxÎ¾â€²âˆˆÎž{âˆ’âˆ¥Î¾k âˆ’xâˆ¥2
2}). Thus, the requirement for correct retrieval is met for all (Î¾, x) âˆ¼
V(Îž), yielding optimal correct retrieval.
If we adopt a footprint that does not sort the dimension-wise similarity vector q by substituting Ëœq
in Equation 3 to q and gives ftptdisâ€²(Î¾, x) = Uq, we can prove the optimality for the standard
noisy variant defined in Definition 4, which is more general than Lemma 1. However, the footprint
ftptdis(Î¾, x) = UËœq achieves high empirical retrieval accuracy, but it is harder to estimate analyti-
cally.
Lemma 2: Optimal retrieval for noisy variant
A-Hop achieves optimal correct retrieval (Definition 7) for noisy variant Vnoisy(Îž) (Definition 4
for arbitrary memory matrix Îž âˆˆRdÃ—N.
Proof. Following the same spirit in the proof of Lemma 1. One can see that the right-hand side
(RHS) of Equation 1 is (similar to Lemma 1):
arg max
Î¾â€²âˆˆÎž
{pVnoise(Î¾â€²|x)} = arg max
Î¾â€²âˆˆÎž

âˆ’1
2 ln

(2Ï€)d|diag(Ïƒ)|

âˆ’1
2(x âˆ’Î¾â€²)âŠ¤diag(Ïƒ)âˆ’1(x âˆ’Î¾â€²)

= arg max
Î¾â€²âˆˆÎž
(
âˆ’
d
X
i=1
(Î¾â€²
i âˆ’x)2
Ïƒi
)
While the left-hand side (LHS) of Equation 1 is (step 1 follows how RHS is resolved in Lemma 1):
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = arg max
Î¾kâˆˆÎž

wâŠ¤U (Î¾k âˆ’x)2	
= arg max
Î¾kâˆˆÎž

uâŠ¤(Î¾k âˆ’x)2	
= arg max
Î¾kâˆˆÎž
( d
X
i=1
ui(Î¾k,i âˆ’xi)2
)
Here, v2 âˆˆRd denotes a dimension-wise square operation over v âˆˆRd, so that (Î¾k âˆ’x)2
i =
(Î¾k,i âˆ’xi)2. Also, we let uâŠ¤= wâŠ¤U for simplicity. One can see that LHS equals RHS when:
âˆ€i, i âˆˆ[d], ui = âˆ’1
Ïƒi
Since U is a full-rank matrix, so that wâŠ¤= uâŠ¤Uâˆ’1 holds. When we set w in the following way:
wi =
âˆ’Ïƒâˆ’1
1
i = 1
Ïƒâˆ’1
iâˆ’1 âˆ’Ïƒâˆ’1
i
2 â‰¤i â‰¤d
LHS and RHS of Equation 1 are the same, satisfying the requirement for correct retrieval. Further-
more, we can tell that Lemma 1 is a special case of this lemma.
Lemma 3: Optimal retrieval for masked variant
A-Hop achieves optimal correct retrieval (Definition 7) for masked variant Vmasked(Îž) (Defi-
nition 5 for arbitrary memory matrix Îž âˆˆRdÃ—N, and for a uniform generator G (pG(Â·) is a
constant).
Proof. As in Lemma 1, we first reformulate the RHS of Equation 1, and find the suitable choice for
w to make LHS of Equation 1 equal RHS. We let q be the dimension-wise similarity vector with
18

Under review as a conference paper
qi = âˆ’(Î¾i âˆ’xi)2, and Ëœq the vector that sort q in ascending order. The RHS can be expanded as:
arg max
Î¾â€²âˆˆÎž
{pVmasked(Î¾â€²|x)}
= arg max
Î¾â€²âˆˆÎž
(
ln pmasked Â·
d
X
i=1
[1 âˆ’Î´(xi âˆ’Î¾â€²
i)] +
d
X
i=1
[1 âˆ’Î´(xi âˆ’Î¾i)] ln pG(xâ€²
i)
)
= arg max
Î¾â€²âˆˆÎž
(
(ln pmasked + ln pG) Â·
d
X
i=1
[1 âˆ’Î´(xi âˆ’Î¾â€²
i)]
)
= arg max
Î¾â€²âˆˆÎž
(
âˆ’d +
d
X
i=1
Î´(xi âˆ’Î¾â€²
i)
)
= arg max
Î¾â€²âˆˆÎž
( d
X
i=1
Î´(Ëœqi)
)
Here, the second step is valid as pG is a constant, and we term this constant pG. As pG is a constant,
it must be less than or equal to 1 to make pG(Â·) a valid probability density function. Additionally, we
know 0 â‰¤pmasked < 1 from definition, and therefore, ln pmasked + ln pG < 0, and this explains why a
sign change occurs in step three. The derived RHS suggests designing a discrete adaptive similarity:
sdis(Î¾, x) = wâŠ¤Î´(Ëœqi) =
d
X
i=1
wiÎ´(Ëœqi)
Then, the LHS would be (first step following that of Lemma 1, and recall we use arg max(Â·) as
separation function):
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = arg max
Î¾kâˆˆÎž

wâŠ¤Î´(Ëœqi)
	
= arg max
Î¾kâˆˆÎž
( d
X
i=1
wi Â· Î´(Ëœqi)
)
Setting w = 1 concludes that LHS equals RHS for all (Î¾, x) âˆ¼V(Îž), and thus, the optimal correct
retrieval is achieved for this concrete adaptive similarity sdis(Î¾, x).
It can be shown that it is impossible to find a continuous sdis(Î¾, x) for masked variantâ€™s optimal
correct retrieval, unless more constraints on Î¾ and x are made. Typically, such a continuous function
is possible if âˆ¥Î¾ âˆ’xâˆ¥2 â‰¥Îµ (can be bounded from below) for Îµ > 0.
Lemma 4: Optimal retrieval for biased variant
A-Hop achieves optimal correct retrieval (Definition 7) for biased variant Vbiased(Îž) (Definition 5
for arbitrary memory matrix Îž âˆˆRdÃ—N, and an arbitrary difference vector d âˆˆRd.
Proof. Following the proof to Lemma 1, the LHS of Equation 1 is:
arg max
Î¾â€²âˆˆÎž
{pVbiased(Î¾â€²|x)} = arg max
Î¾â€²âˆˆÎž
(
Î´
"
d âˆ’
d
X
i=1
Î´(xi âˆ’Î¾â€²
i âˆ’di)
#)
= arg max
Î¾â€²âˆˆÎž
(
âˆ’d +
d
X
i=1
Î´(xi âˆ’Î¾â€²
i âˆ’di)
)
= arg max
Î¾â€²âˆˆÎž

âˆ’âˆ¥x âˆ’Î¾â€² âˆ’dâˆ¥2
2
	
The last step follows that the maximum score is both 0 before and after the transform, and the goal
is to assign a high score (0) when x âˆ’Î¾ = d. Here, we use a similar continuous adaptive similarity
defined in the main text:
sdis(Î¾, x) = wâŠ¤Uq âˆ’qâŠ¤q
19

Under review as a conference paper
with q = x âˆ’Î¾, and set uâŠ¤= wâŠ¤U with u = 2dâŠ¤. Then, the RHS of Equation 1 is:
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = arg max
Î¾kâˆˆÎž

uâŠ¤q âˆ’qâŠ¤q
	
= arg max
Î¾kâˆˆÎž

2dâŠ¤q âˆ’qâŠ¤q âˆ’dâŠ¤d
	
= arg max
Î¾kâˆˆÎž

âˆ’(q âˆ’d)âŠ¤(q âˆ’d)
	
= arg max
Î¾kâˆˆÎž

âˆ’âˆ¥q âˆ’dâˆ¥2
2
	
= arg max
Î¾kâˆˆÎž

âˆ’âˆ¥x âˆ’Î¾k âˆ’dâˆ¥2
2
	
This follows immediately that LHS equals RHS, and the optimal correct retrieval is achieved when:
wi =
2d1
i = 1
2di âˆ’2diâˆ’1
2 â‰¤i â‰¤d
It finally comes down to Theorem 1.
Theorem 1: A-Hop retrieval dynamics
The following retrieval dynamics adopted by A-Hop achieves optimal correct retrieval for noisy,
masked, and biased variants, with a careful design of s(Îž, x):
y = T (x) = Îž sep(s(Îž, x))
Proof. First of all, sep(Â·) = arg max(Â·) is crucial for achieving optimal correct retrieval, as it
transforms the left-hand side of Equation 1 as (see Lemma 1):
arg min
Î¾â€²âˆˆÎž
{âˆ¥T (x) âˆ’Î¾â€²âˆ¥2} = arg max
Î¾kâˆˆÎž
{sdis(Î¾k, x)}
In Lemma 2, we see that using the following adaptive similarity achieves optimal correct retrieval:
sdis(Î¾, x) = wâŠ¤Uq
with wi =
âˆ’Ïƒâˆ’1
1
i = 1
Ïƒâˆ’1
iâˆ’1 âˆ’Ïƒâˆ’1
i
2 â‰¤i â‰¤d
In Lemma 3, we see that using the following adaptive similarity achieves optimal correct retrieval:
sdis(Î¾, x) = wâŠ¤Î´(Ëœq)
with wi = 1 for i âˆˆ[d]
In Lemma 4, we see that using the following adaptive similarity achieves optimal correct retrieval:
sdis(Î¾, x) = wâŠ¤U (x âˆ’Î¾) âˆ’(x âˆ’Î¾)âŠ¤(x âˆ’Î¾)
with wi =
2d1
i = 1
2di âˆ’2diâˆ’1
2 â‰¤i â‰¤d
One can see that achieving optimal correct retrieval is not easy, and it requires the sacrifice of
continunity. However, we can build a continuous adaptive similarity inspired from the proof of
Theorem 1 that achieve high retrieval accuracy (at least, empirically). For more discussion on this
topic, please read Appendix A.3.1.
A.2.2
UNIFIED ADAPTIVE SIMILARITY AND ENERGY FUNCTION
We can find that the adaptive similarity in Lemma 1 has the form:
s(Î¾, x) = âˆ’(x âˆ’Î¾)âŠ¤(x âˆ’Î¾)
while that of Lemma 2 has the form:
s(Î¾, x) = âˆ’(x âˆ’Î¾)âŠ¤diag(a)(x âˆ’Î¾)
20

Under review as a conference paper
for some diagonal matrix diag(a), and ai > 0 for all i âˆˆ[d]. Meanwhile, for Lemma 4 has the
form:
s(Î¾, x) = âˆ’(x âˆ’Î¾)âŠ¤(x âˆ’Î¾) + bâŠ¤(x âˆ’Î¾)
for some real vector b. That is being said that we can unifies these three adaptive similarity by:
sunify(Î¾, x) = âˆ’(x âˆ’Î¾)âŠ¤diag(a)(x âˆ’Î¾) + bâŠ¤(x âˆ’Î¾)
However, this similarity is too tough, we can analysis a simpler one:
sunify(Î¾, x) = âˆ’(x âˆ’Î¾)âŠ¤(x âˆ’Î¾) + bâŠ¤(x âˆ’Î¾)
If we use a softmax(Â·) function as the separation function and sunify(Î¾, x) as the similarity function,
and construct an energy function (with sunify(Îž, x) being the vectorized form of sunify(Î¾, x)):
E(x) = âˆ’lse(sunify(Îž, x))
(7)
whose gradient is:
âˆ‡xE(x) = âˆ’softmax(sunify(Îž, x))âŠ¤âˆ‡xsunify
Letting pi(x) â‰œsoftmax(sunify(Îž, x))i:
âˆ‡xE(x) = âˆ’
N
X
i=1
pi(x)âˆ‡xsunify(Î¾i, x)
= âˆ’
N
X
i=1
pi(x) Â· (âˆ’2x + 2Î¾i + b)
= 2x âˆ’b âˆ’2
N
X
i=1
pi(x) Â· Î¾i
Retrieval on the gradient flow gives:
dx
dt = âˆ’âˆ‡xE(x) = âˆ’2x + b + 2
N
X
i=1
pi(x) Â· Î¾i
Then, consider using gradient descent with step Î·, where Î· > 0 for discrete-time retrieval:
x(t+1) = x(t) âˆ’Î·âˆ‡xE(x(t))
= (1 âˆ’2Î·) Â· x(t) + Î·b + 2Î·
N
X
i=1
pi(x(t)) Â· Î¾i
By setting Î· = 1
2 that would cancels the x term on the RHS and remove the coefficient before the
summation, which is wonderful:
x(t+1) = 1
2b + Îž softmax(sunify(Îž, x))
From Lemma 4, we know that the setting b = 2d is optimal for noisy variant, plugging this in gives:
x(t+1) âˆ’d = T (x(t)) = Îž softmax(sunify(Îž, x))
(8)
suggesting that we add a new de-bias term âˆ’d for biased variants, which coincidentally, remove the
bias vector d. However, when there is no bias, Equation 8 reduce to the simple retrieval dynamics
we are familiar with.
We then further analysis the behavior of the energy (Equation 7) retrieval dynamics Equation 8:
Lemma 5: Rewriting the energy
Let the energy function be E(x) = âˆ’lse(s(Îž, x)), where s(Î¾i, x) = âˆ’(x âˆ’Î¾i)âŠ¤(x âˆ’Î¾i) +
bâŠ¤(x âˆ’Î¾i). Then, the energy can be written as
E(x) = âˆ¥xâˆ¥2
2 âˆ’lse(Ax + c)
for some matrix A âˆˆRNÃ—d and vector c âˆˆRN.
21

Under review as a conference paper
Proof.
E(x) = âˆ’lse (s(Îž, x))
= âˆ’ln
N
X
i=1
exp

âˆ’âˆ¥xâˆ¥2
2 âˆ’âˆ¥Î¾iâˆ¥2
2 + (2Î¾i + b)âŠ¤x + Î¾âŠ¤
i b

= âˆ’ln
n
X
i=1
exp(âˆ’âˆ¥xâˆ¥2
2) + exp

(2Î¾i + b)âŠ¤x + Î¾âŠ¤
i b âˆ’âˆ¥Î¾iâˆ¥2
2

= ln n + âˆ¥xâˆ¥2
2 âˆ’lse (Ax + c)
for AâŠ¤
i = 2Î¾i + b and ci = Î¾âŠ¤
i b âˆ’âˆ¥Î¾iâˆ¥2
2. Also, we can omit the term ln n as it is a constant.
Therefore, we have decomposed the energy function E(x) into a convex function g(x) = âˆ¥xâˆ¥2
2, and
a concave function âˆ’h(x) = âˆ’lse(Ax + c).
Lemma 6: Decreasing energy
Energy function E(x) would be monotonically decreasing using the retrieval dynamics:
x(t+1) âˆ’d = T (x(t)) = Îž softmax(s(Îž, x))
for s(Î¾i, x) = âˆ’(xâˆ’Î¾i)âŠ¤(xâˆ’Î¾i)+bâŠ¤(xâˆ’Î¾i)
Proof. Using the concave convex procedure (Lanckriet & Sriperumbudur, 2009), we construct a
convex surrogate function Ut(x) for each iteration t by linearizing the concave function âˆ’h(x)
around the current x(t):
Ut(x) = g(x) âˆ’
h
h(x(t)) + âˆ‡xh(x(t))âŠ¤ x âˆ’x(t)i
= âˆ¥xâˆ¥2
2 âˆ’âˆ‡xh(x(t))âŠ¤x +
 âˆ‡xh(x(t))âŠ¤x(t) âˆ’h(x(t))

The next x(t+1) is the minimizer of Ut(x), i.e., x(t+1) = arg minxâˆˆRd{Ut(x)}, and we can find it
by setting its gradient to zero:
âˆ‡xUt(x) = 2x âˆ’âˆ‡xh(x(t)) = 0
=â‡’
x(t+1) = 1
2âˆ‡xh(x(t)) = 1
2AâŠ¤softmax(Ax(t) + c)
We can add the term âˆ’âˆ¥xâˆ¥2
2 back to softmax(Â·) as it is independent of index i. Thus, by denoting
pi(xi) = softmax(Ax(t) + c) = softmax(s(Îž, x)) (follows Lemma 5), we have:
x(t+1) = 1
2
N
X
i=1
Ai Â· pi(x(t))
=
N
X
i=1
Î¾i Â· pi(x(t)) âˆ’1
2b
N
X
i=1
pi(x(t))
= Îž softmax(s(Îž, x(t))) âˆ’1
2b
and this agrees with what we have derived before (Equation 8).
Then, by convexity of h(x) (recall âˆ’h(x) is concave), we have the following inequality:
h(x) â‰¥h(x(t)) + âˆ‡xh(x(t))âŠ¤(x âˆ’x(t))
=â‡’
g(x) âˆ’h(x) â‰¤g(x) âˆ’h(x(t)) + âˆ‡xh(x(t))âŠ¤(x âˆ’x(t)) = Ut(x)
with the equality holds iff x = x(t), and recall that x(t+1) is the minimum value of Ut(x), we have:
E(x(t+1)) â‰¤Ut(x(t+1)) â‰¤Ut(x(t)) = E(x(t))
(9)
with equality holds when x(t) = x(t+1). Thus, E(x(t+1)) â‰¤E(x(t)) completes the proof.
22

Under review as a conference paper
We can see that E(x) = âˆ¥xâˆ¥2
2 âˆ’O(âˆ¥xâˆ¥2), so that E(x) â†’+âˆžas âˆ¥xâˆ¥2 â†’+âˆž, so E(x) is coer-
cive, meaning its level sets are compact. Additionally, Ut(x) is 2-strongly convex as âˆ‡2
xUt(x) = 2I,
therefore,
Ut(x(t)) âˆ’Ut(x(t+1)) â‰¥âˆ¥x(t) âˆ’x(t+1)âˆ¥2
2
Along with Inequality 9:
E(x(t)) âˆ’E(x(t+1)) â‰¥Ut(x(t)) âˆ’Ut(x(t+1)) â‰¥âˆ¥x(t) âˆ’x(t+1)âˆ¥2
2
This yields that
E(x(0)) âˆ’E(x(t)) â‰¥
tâˆ’1
X
Ï„=0
âˆ¥x(Ï„+1) âˆ’x(Ï„)âˆ¥2
2
This mean that E(x(t)) is bounded from above. We can see that the sequence of x: {x(t)} must
remain within the level set {x | E(x) â‰¤E(x(0))} as the sequence E(x(0)) is non-increasing.
Therefore, {x(t)} must remains in the compact set {x | E(x) â‰¤E(x(0))}, and be bounded.
Theorem 2: A-Hop energy landscape
energy Energy E(x) will be monotonically decreasing and its value could be bounded for
isotropic noisy, and biased variants, if the following energy is used:
E(x) = âˆ’lse (s(Îž, x))
Proof. In Lemma 6, we have proven that the energy function will be monotonically decreasing.
Also, from above analysis we can see that E(x(t)) could be bounded by:
E(x(t)) â‰¤E(x(0)) âˆ’
tâˆ’1
X
Ï„=0
âˆ¥x(Ï„+1) âˆ’x(Ï„)âˆ¥2
2
On the other hand, we can try to bound E(x(t)) from below:
E(x(t)) = âˆ’ln
N
X
i=1
exp
 âˆ’âˆ¥x âˆ’Î¾iâˆ¥2
2 + bâŠ¤(x âˆ’Î¾i)

â‰¥âˆ’ln
N
X
i=1
exp
 âˆ’âˆ¥x âˆ’Î¾iâˆ¥2
2 + âˆ¥bâˆ¥2 Â· âˆ¥x âˆ’Î¾iâˆ¥2

â‰¥âˆ’ln
N
X
i=1
exp
"
âˆ’
âˆ¥bâˆ¥2
2
2
+ âˆ¥bâˆ¥2 Â· âˆ¥bâˆ¥2
2
#
= âˆ’ln N âˆ’âˆ¥bâˆ¥2
2
4
As âˆ¥bâˆ¥2 is bounded, we can see that E(x(t)) is bounded from below.
We try to find the property for a very different energy landscape and a different retrieval dynamics,
also these retrieval dynamics can guaratee optimal correct retrieval, for iostropic noisy, and biased
variant, when the separation function is arg max(Â·) and has weight b choosen ideally. Theorem 2
tries to connect the noval correct retrieval and the traditional energy analysis.
A.3
DISCUSSION
A.3.1
ON GOOD DESIGN CHOICE OF ADAPTIVE SIMILARITY
Achieving optimal correct retrieval is costly, as it requires designing â€œweirdâ€ similarity function for
corner cases or use discrete function that makes the model unlearnable. For instance, it is impossible
for a continuous similarity function to achieve optimal correct retrieval for masked variants, as they
23

Under review as a conference paper
have a hard time distinguishing |Î¾iâˆ’xi| = 0 and |Î¾iâˆ’Î¾i| = Ïµ for some arbitrary small Ïµ. Also, using
softmax(Â·) as the separation function make it hard to prove whether optimal retrieval is achieved or
not, as we cannot exclude the effect of other memory patherns from the one receive largest similarity
score, while using softmax(Â·) is crucial for learnable adpative similarity.
Therefore, achieving optimal correct retrieval for a certain variant distribution is not the only golden
criteria for a similarity measure. More generally, we think good (adaptive) simliarty measures should
be assessed from the following aspects:
Adaptivity Can the similarity measure adapt to a wide range of variant distribution? This criteria
measures the breadth of the similarity.
Optimality How accurate can the similarity measure predicts the variant distribution? This criteria
measures the depth of the similarity.
Learnability Can this similarity measure be learned easily when some samples from the variant dis-
tribution is provided? How many learnable parameters it requires? This criteria measures
the scale of the similarity.
Efficiency What is the time complexity of calculating the similarity score? This criteria measures
the speed of the similarity.
It is ideal but not likely that we can find a perfect adaptive similarity that meets all above mentioned
criteria, because there are tradeoffs between these aspects. For example, for the adaptive similarity
introduced in the proofs to Lemma 2, 3 and 4, they only achieves optimality in noisy, masked and
biased variants but would have poor performance in other variants (i.e., poor adaptivity). This is
because adaptivity is sacrified when every corner case (cases happen in a very low probability but
need extra care) is handled carefully. On the other side, optimality would be limited for similarity
with strong adaptivity because the outcome for some corner cases could be contradicting for differ-
ent scenarios. Moreover, learnable adaptivity similarity might not often achieve optimality as the
learning alogithm cannot always guarantee convergence of weights to the optimum state (e.g., min-
imum loss). Also, large-scale similarity measure with plenty of learnable parameters are not likely
to have good efficiency, as their computation procedure could be complicated.
The design of the adaptive similarity s(Î¾, x) = Î²disftptdis(Î¾, x) + Î²dotftptdot(Î¾, x) in Eq. 4 (il-
lustrated in Fig. 2) is a balanced result of considering all four aspects â€” adaptivity, optimality,
learnability, and efficiency. We will explain the design choice of this adaptive similarity from the
perspectives of these four criteria.
The similarity footprint
The footprint is the core mechanism of adaptive similarity (Eq. 4 and
Fig. 2). Sorting and cumulative sum is not the essential part of it, while the idea of exploiting
subspaces is. The idea is that considering subspatial information will give extra benefits compared
to considering the whole Rd itself. The footprint picks d most useful subspaces from all 2d âˆ’
1 subspaces for much better efficiency (from computationally impossible to possible). However,
sorting and cumulative sum happen to be the right algorithm for computing the similarity score
efficiently for decomposable similarity measure (Eq. 6), it doesnâ€™t mean that the footprint is about
sorting dimensions. Regard the footprint as mining the most valuable subspatial information from
all 2d âˆ’1 subspaces, and it provides useful evidence for guiding the model to make decision.
Recall that ftptsim(Î¾, x)k = sim(k)(Î¾, x), and sim(k)(Î¾, x) is the k-optimal similarity between Î¾
and x, which finds the optimal k-dimensional subspace such that Î¾ and x is most associated (closest)
on this subspace. That is to say, the footprint finds a representative for each dimensionality k âˆˆ[d]
and regards the one that minimize the distance between Î¾ and x as the most useful k-dimensional
subspace. But the positional information of each subspaces is not used (i.e., which dimensions
contributes to sim(k)(Î¾, x) is unkonwn) this would harm the optimality compared to a similarity
measure that exploits all 2d âˆ’1 subspaces. Although finding the positional information of each
subspaces is possible (which would benefits optimality), it would need O(d2) time for computation
(which is not worthy).
However, compared to fixed proximity-based similarity (e.g., Î¾âŠ¤x), the footprint offers a multi-scale
descriptor to the relation between Î¾ and x (the evidence for deciding whether Î¾ generates x). The k-
optimal similarity sim(k)(Â·, Â·) ignores d âˆ’k non-informative dimensions, which make it inherently
24

Under review as a conference paper
suitable for handling masked dimensions and heavily noisy dimensions (by simply ignore them).
Therefore, the footprint enables better optimality against fixed proximity-based similarity as it offer
richer evidence (d v.s. 1) for models to make the right decision.
Linear combination of footprint elements
The proposed adaptive similarity calculate the sim-
ilarity score as the dot product of ftptdis(Î¾, x) (a d-dimensional vector) and learnable weight w
(also a d-dimensional vector). This allows the model to behave differently (by choosing differ-
ent weight) when facing different variant distribution.
Also, one can see that when wd = 1
and wk = 0 for 1 â‰¤k < d, the adaptive similarity degenerates to its base similarity as
wâŠ¤ftptsim(Î¾, x) = sim(Î¾, x). That is to say, the adaptive similarity has better expressiveness
(better adaptivity) and is at least as good as the fixed proximity-based similarity.
However, one can use a more complicated method (e.g., train a neural network to let s(Î¾, x) =
MLP(Î¾, x, ftptsim(Î¾, x))) to extract information from footprints and gains better optimality and
adaptivity. But this also brings more computation cost and may make the model harder to train.
Using a linear map is the simplest fastest way to exploit the footprint, and empirical results demon-
strates that it can largely improve existing associative memory models.
Aggregating among different base similarities
The proposed adaptive similarity aggregate
the similarity footprint with base similarity dis(Î¾, x) = âˆ’âˆ¥Î¾ âˆ’xâˆ¥2
2 and dot(Î¾, x) = Î¾âŠ¤x.
Aggregating adaptive similarities with different base similarity would improve modelâ€™s adaptivity
and optimality but reduce its efficiency (scale up the runtime by a constant). Think of base similarity
as base vectors in a linear space, and the aggregated model resembles the span of these basis.
The learnable parameter Î²1Â·Â·Â·B controls how each base similarity is used in different scenarios.
Next, we show that the proposed adaptive similarity (Eq. 4) is suitable (has high optimality) for
noisy + masked variant.
However, we propose choosing the value of w wisely for adaptive similarity s(Î¾, x) = wâŠ¤UËœq to
achieve â€œsub-optimalâ€ correct retrieval (well, it is hard to rigorously define sub-optimality).
[TODO: Probably Approximately Correct (PAC) form here]
For noisy variant, a choice that setting w1 = 1 and other values to 0 is suggested. This degenerates
the model to the one defined in the proof to Lemma 1 and enable the model to have global view
(mauniplate similarity in the largest subspace), and its electron cloud would look like a shpere as
illustrated in Fig. 1 (a).
Next, for masked variant, we suggest using wi = iC for some large constant C > 2d2, because this
punishes patterns that has very limited dimension where Î¾i = xi, and stress importance on small
subspaces. By doing so the similarity function will look like the likelihood electron cloud in Fig. 1b.
Finally, for biased variant, the best way to set w is to set uâŠ¤= wâŠ¤U to Ëœd, the sorted bias vector,
so that they similarity function can focus on how comparing the difference of x âˆ’Î¾ with a unkown
but almost known bias.
The main point of this section is that the optimality of correct retrieval is too strict so that achieving
so force us to abandon good properties. Also, in most cases, some very corner cases set very high
difficulty for achieve optimal. However, correct retrieval itself is a good property, but making every
retrieval correct is too strict. Therefore, an open question is that how to define a sub-optimal correct
retrieval standard so that it guarantees great memory retreival performance and leave us freedom.
A.3.2
ON MORE COMPLICATED VARIANT DISTRIBUTION
Variant distributions discussed so far are actually simple. In previous analysis, we assumes that all
memory variants follows a similar distribution. For example, (Î¾, x) âˆ¼Vnoisy(Îž) ensures that all
memory patterns generates a noisy variants, sharing a similar pV(x|Î¾). We say a variant distribution
general if knowing pV(x|Î¾i) is equivalent to knowing pV(x|Î¾j) for arbitrary i, j âˆˆ[N] and i Ì¸= j.
In other words, we can obtain pV(x|Î¾j) by substituting xj with xi. However, there could be cases
where each memory patterns generates variants quite differently. Intuitively, we say each pattern is
25

Under review as a conference paper
generates on their own, meaning that pV(x|Î¾) has completely different form for different memory
patterns, and we call such memory variant isolated.
By looking closely to the adaptive similarity function:
s(Î¾, x) = wâŠ¤UËœq
One can see that it uses a universal weight w for all pairs of (Î¾, x) âˆ¼V(Îž), assuming that the
variant distribution it is trying to model is general. However, such limiltaion can be easily broken
by introducing more weights. For instance, we use separate weights for different memory patterns,
i.e., for N memory patterns, we spare weight wk to memory pattern Î¾k. Therefore, the adaptive
similarity that can suit isolated variant distribution look like:
s(Î¾k, x) = wâŠ¤
k UËœq
This can fit each isolated likelihood pV(x|Î¾k) as the weights are no longer shared. But this rises
more problem: (1) it requires more samples, and it might be impossible in real-world scenarios. (2)
it requires samples generated by each memory pattern Î¾k, as each individual weight wk is optimized
by samples involving Î¾k.
Additionally, the adaptive similarities are a family of similarity measures that can fit to the variant
distribution through sampling, it is not solely Equation 4. When proving the optimal correct retrieval
for noisy, masked, and biased variants, we propose more adaptive similarity as theoretical tools. We
wrote Equation 4 in the main text simply because it is the most effective ones for retrieving under
noisy, masked, biased, and mixed settings, and it requires minimum trainable weight, as examined
in ablation study (Appendix A.4.6).
One more thing is that the priori pV(Î¾|x) are often ignored in this work. Well, it should not be
ignored in all cases. However, we can use a bias term b âˆˆRN and use bk capture the occurrance
of pattern Î¾k. By assuming similarity is a logits of the posteriori pV(Î¾|x) (e.g., log pV(Î¾|x)), then
we can see that arg maxÎ¾â€²âˆˆÎž{log pV(Î¾|x)} = arg maxÎ¾â€²âˆˆÎž{log pV(x|Î¾â€²) + log pV(Î¾â€²)}, and we
set pass s(Îž, x) + b to the separation function so that b handles the term log pV(Î¾).
A.4
EXPERIMENTS
A.4.1
BASELINES AND METRICS
For memory retrieval test, we compare our Adaptive Hopfield network A-Hop against: Modern
Hopfield network M-Hop (Ramsauer et al., 2021); Universal Hopfield network U-Hop (Millidge
et al., 2022) with sim(Î¾, x) = âˆ’âˆ¥Î¾ âˆ’xâˆ¥1 and sep(Â·) = arg max(Â·) as they report a leading
performance for such configuration when masking out half of the dimensions in memory pat-
terns (similar to masked variant, Definition 5); Kernelized Hopfield network K-Hop (Wu et al.,
2024a) with the kernel optimized by separation loss proposed by Wu et al. (2024a); Kernelized
Hopfield network K2-Hop with the kernel optimized by loss defined by variant distribution (Equa-
tion 5) rather than the separation loss; and Multi-Layer Perceptrons MLP that has 4 layers and a
input dimensionality d, output dimensionality N, trying to fit pV(Î¾|x) but unsatisfactory. We es-
timate the empirical retrieval accuracy of each model (Definition 9), and the mean squared error
E(Î¾,x)âˆ¼V(Îž)

(T (x) âˆ’Î¾)âŠ¤(T (x) âˆ’Î¾)

. We wrote a generator that can generate noisy, masked,
biased, and mixed variants.
For tabular classification test, we compare the A-Hop with a memory-based classifier (Ap-
pendix A.4.4) with: M-Hop uses the same classifier framework but the unlearnable similarity func-
tion; K2-Hop that uses the same classifier framework but a different similarity function, and its
kernel function is optimized by the classification loss; Extremely Randomized Trees (Geurts et al.,
2006), or Extra Trees, a tree-based classifier; Random Forest (Breiman, 2001), yes, the famous
Random Forest classifier; AdaBoost (Freund & Schapire, 1997), a classic boosting classifier; and
XGBoost (Chen & Guestrin, 2016), an enhanced Gradient Boosting Decision Tree. All models are
tuned with a 5-fold cross-validation on the training set. We measure the test accuracy for the dataset
with a positive sample rate 0.2 < % pos < 0.8, and the ROC-AUC score otherwise, following the
settings in Wang et al. (2025).
For image classification task, we follow the settings in Wu et al. (2024a), where they replaced the
attention component with a HopfieldLayer (Ramsauer et al., 2021). We experiment by inte-
grating A-Hop, M-Hop, and K-Hop into the HopfieldLayer. Similarly, we follow the settings
26

Under review as a conference paper
in Ramsauer et al. (2021); Hu et al. (2023), where they use HopfieldPooling for multiple in-
stance learning. We integrate A-Hop, M-Hop, and K-Hop to HopfieldPooling, and run a
5-fold cross-validation to report the mean ROC-AUC of all folds as the result.
For all experiments, we report the results with the mean and standard deviation of five runs.
A.4.2
DATASETS
We used a total of 12 datasets to assess the performance of A-Hop on tasks including tabular clas-
sification (Adult, Bank, Vaccine, Purchase, and Heart), and image classification (CIFAR 10, CIFAR
100, and Tiny ImageNet), and multiple instance learning (Tiger, Fox, Elephant, UCSB).
Adult (Becker & Kohavi, 1996) The prediction task for this dataset is to classify individualsâ€™ in-
come levels as either above or below $50,000 annually. The data was extracted by Barry
Becker from the 1994 Census database.
Bank (Moro et al., 2014) To predict the success of a term deposit subscription, this dataset records
the outcomes of telemarketing campaigns from a banking institution in Portugal.
Vaccine (Bull et al., 2016; DrivenData, 2019) We use this dataset from a DrivenData competition
to predict if a person received a seasonal flu vaccine. The data consists of 26,707 survey
responses detailing 36 behavioral and personal attributes.
Purchase (Sakar & Kastro, 2018) The objective with this dataset is to forecast the online shopping
intentions of visitors to an e-commerce website, determining whether a user will proceed
with a purchase.
Heart (Kaggle, 2018) This dataset facilitates the prediction of cardiovascular disease presence. It
contains health-related data from 70,000 patients, as provided by Ulianova.
CIFAR 10 (Krizhevsky et al., 2009) is a classic image recognition dataset consisting of 60,000
32 Ã— 32 color images in 10 classes, with 6,000 images per class.
CIFAR 100 (Krizhevsky et al., 2009) is a more challenging version of CIFAR 10, containing the
same number of images but split into 100 fine-grained classes.
Tiny ImageNet (Le & Yang, 2015) is a subset of the ImageNet dataset designed for educational
purposes. It contains 100,000 images from 200 classes, downsized to 64 Ã— 64 pixels.
Tiger, Fox, Elephant (Deng et al., 2009) These are specific class subsets extracted from the large-
scale ImageNet database, often used for fine-grained image classification tasks.
UCSB (Gelasca et al., 2008) This dataset is composed of 58 breast cancer histopathology images
stained with Hematoxylin and Eosin (H&E). The primary challenge it presents is the ac-
curate segmentation of individual cells from the complex tissue background, which is a
critical precursor to classifying cells as benign or malignant.
A.4.3
MEMORY RETRIEVAL
We first introduce the intensity of variant setting. A triplet (dmask, dnoise, dbias) âˆˆ[0, 1]3 is used to
describe a variant setting. This mean that we will first add a Gaussian noise n âˆ¼N(0, dnoiseI) to a
certain memory pattern Î¾ âˆˆÎž obtaining x â†Î¾ + n. Then, we will choose d Â· dmask indices, and set
these indices of x to random numbers choosen uniformly from [âˆ’1, 1]. Finally, we will add a bias
d to x â†x + d with di = si Â· dbias, where si is a random sign sampled uniformly from {âˆ’1, +1}.
Then, the generator return (Î¾, x) as a sample of V(Îž). Therefore, different triplet describle different
variant settings, and thus, corresponds to different variant distribution V(Îž).
In the memory retrieval task, we use the retrieval dynamics written in Equation 4. To learn the
weight wâ€™s and Î²â€™s we use optimizer Adam for 200 epoches, and use a learning rate 0.1 in all
settings. However, we argue that number of epoches (N epoch) and learning rate (lr) should be
tuned when applying A-Hop to other memory retrieval settings.
For K-Hop and K2-Hop, we tuned them carefully. For the original K-Hop, it optimize a separation
loss, and we try to make it as small as possible. However, the retrieval accuracy of K-Hop is not
satisfactory, and it is reasonable since it is not optimized for correct retrieval, but for Ïµ-retrieval. We
found that the domain of the memory pattern in their experiments is [0, 1]d, and it is harmful to dot
27

Under review as a conference paper
product-based methods (think of using only 2âˆ’d of the space) as dot product highly relies on signs.
Therefore, for fair comparision, we sample the random vectors uniformly from [âˆ’1, 1]d and change
the domain of pixels in MNIST images to [âˆ’1, 1].
A.4.4
TABULAR CLASSIFICATION
We develop a memory-based model for tabular classification that takes advantage of the excellent
memory retrieval effectiveness of associative memories. The insight is that classification is hierar-
chical, and we divide instances into cases, and further classify cases into the final class. For instance,
there are type I diabetes and type II diabetes, where each type here resembles the idea of cases. An-
other samples is that cats has plenty of breeds, and associative memory can capture specific idea of
orange cat, or blue cat, while they have a hard time figuring out the generalize idea of cats. So, we
let associative memory match instances into cases by choosing some instances in the training set as
the representatives of cases, or use K-means cluster to produce such representatives, and pass the
retrieval probability to a multi-layer perceptron (MLP) for classifying cases. That is, the model can
be represented as y = MLP(LayerNorm(sim(Îž, x))), and this can be trained on a conventional
machine learning fashion by minimizing classification loss:
L(D) = E(x,y)âˆ¼D[(y âˆ’Ë†y)2]
(10)
We no longer tune weights in adaptive memories using loss defined in Equation 5, as they can be
tuned using the classification loss when participanting into going forward in the network.
We tuned the hyperparameters by grid searching on the training data:
Table 6: Hyperparameters tuned in tabular classification
Name
Domain
N epoch
{50, 100, 150, 200, 250}
batch size
{100, 1000}
lr
{2 Â· 10âˆ’3, 10âˆ’3, 5 Â· 10âˆ’4, 2 Â· 10âˆ’4, 10âˆ’4}
init
Cluster, No cluster
A.4.5
IMAGE CLASSIFICATION AND MULTIPLE INSTANCE LEARNING
For image classification, we follow the settings in Wu et al. (2024a), and place the adaptive similarity
to the Hopfield layer inside a image Transformer. For M-Hop and K-Hop, we use the hyperparame-
ter suggested in Wu et al. (2024a), and find the optimal number of epoch and learning rate for A-Hop
via grid search. We run five iterations of separate loss optimization for A-Hop before testing.
Table 7: Hyperparameters tuned in image classification
Name
Domain
N epoch
{25, 40, 50}
lr
{10âˆ’3, 10âˆ’4}
For multiple instance learning,
we follow the settings in Hu et al. (2023),
and use
HopfieldPooling as the backbone. Similarly, we place the adaptive similarity in the core Hop-
field component replacing the fixed dot product. All experiments are run on a 5-fold validation, and
the ROC-AUC scored is taken as the mean of all folds.
A.4.6
ABLATION STUDY
We conduct four different ablation studies to see the effective of different components.
In the first experiment (Table 9), we tested if sorting the dimension-wise similarity vector q and the
upper-right triangle matrix U is needed. The results shows that both of them are necessay for high
retrieval accuracy.
28

Under review as a conference paper
Table 8: Hyperparameters tuned in multiple instance learning
Name
Domain
N epoch
{10, 20, 40}
lr
{10âˆ’3, 10âˆ’4, 10âˆ’5}
lr decay
{0.9, 0.75}
Table 9: Retrieval accuracy (â†‘) and error (â†“) between unsorted and sorted q. Each cell contains the
mean accuracy or error with standard deviation in a smaller font. Results of the best-performing
model are bolded.
Conditions
Synthetic (d = 0.4)
Synthetic (d = 0.5)
q sorted?
use U?
Accuracy
Error
Accuracy
Error
âœ—
âœ—
.5172Â±.034
.1900Â±.017
.2094Â±.022
.2658Â±.005
âœ—
âœ“
.5444Â±.007
.1665Â±.007
.1888Â±.015
.2738Â±.003
âœ“
âœ—
.6928Â±.034
.1173Â±.010
.3374Â±.025
.2324Â±.006
âœ“
âœ“
.7280Â±.034
.1033Â±.011
.3634Â±.040
.2207Â±.011
Next, we look for a better matrix U (Table 10), which is the core of similarity measure. Our results
show that the upper-right triangle structure of U is optimal, while making U learnable and initialize
it with the upper-right triangle yields the best result, but we does not adopt learnable U in other
experiments to keep minimum learnable parameters. Another finding is that a randomized matrix U
is better than not having U (when U = I). Therefore, along with Table 9, we find that the footprint
structure is essential to adaptive similarities.
Table 10: Retrieval accuracy (â†‘) and error (â†“) between different configuration of U. Each cell
contains the mean accuracy or error with standard deviation in a smaller font. Results of the best-
performing model are bolded, and the second are underlined.
Conditions
Synthetic (d = 0.4)
Synthetic (d = 0.5)
U learnable?
initialization?
Accuracy
Error
Accuracy
Error
âœ—
Random
.6928Â±.015
.1147Â±.005
.3340Â±.019
.2305Â±.005
âœ—
I
.6802Â±.013
.1194Â±.004
.3458Â±.025
.2298Â±.008
âœ—
U
.7242Â±.016
.1056Â±.007
.3600Â±.024
.2272Â±.005
âœ“
Random
.7176Â±.027
.1087Â±.007
.3414Â±.023
.2292Â±.006
âœ“
I
.7114Â±.019
.1096Â±.006
.3528Â±.021
.2270Â±.005
âœ“
U
.7280Â±.034
.1033Â±.011
.3634Â±.040
.2207Â±.011
We also tested the effect of different footprint with different base similarity (see Table 11). Results
shows that two footprints (dis and dot) is always better than one alone, and we found that ftptdis
performs better than ftptdot in this setting. However, even though retrieval accuracy degrades by re-
moving one of the footprint, using ftptdis or ftptdot only is still better than other Hopfield networks.
Table 11: Retrieval accuracy (â†‘) and error (â†“) between different usage of footprint. Each cell con-
tains the mean accuracy or error with standard deviation in a smaller font. Results of the best-
performing model are bolded.
Conditions
Synthetic (d = 0.4)
Synthetic (d = 0.5)
use ftptdis?
use ftptdot?
Accuracy
Error
Accuracy
Error
âœ—
âœ“
.5926Â±.016
.1517Â±.005
.2520Â±.027
.2515Â±.004
âœ“
âœ—
.6458Â±.042
.1317Â±.011
.3286Â±.023
.2326Â±.007
âœ“
âœ“
.7242Â±.016
.1056Â±.007
.3600Â±.024
.2272Â±.005
29

Under review as a conference paper
Finally, we tested the number of samples needed for adaptive similarity to mimic the variant distri-
bution. It looks like training on only 512 samples provides a good enough adaptive similarity for
2048 64-dimension memory patterns.
Table 12: Retrieval accuracy (â†‘) and error (â†“) between different number of samples provided for
learning. Each cell contains the mean accuracy or error with standard deviation in a smaller font.
Results of the best-performing model are bolded.
Conditions
Synthetic (d = 0.4)
Synthetic (d = 0.5)
number of samples?
Accuracy
Error
Accuracy
Error
512
.7204Â±.029
.1077Â±.010
.3541Â±.028
.2311Â±.004
âˆž
.7242Â±.016
.1056Â±.007
.3600Â±.024
.2272Â±.005
Table 13: Retrieval accuracy (â†‘) between models in multi-modal memory retrieval task. Each cell
contains the mean accuracy or error with standard deviation in a smaller font. Results of the best-
performing model are bolded.
Model
Number of concepts
Phrase 1
Phrase 2
128
256
512
1024
M-Hop
M-Hop
.173Â±.02
.092Â±.01
.044Â±.01
.021Â±.00
K2-Hop
K2-Hop
.574Â±.04
.349Â±.02
.146Â±.02
.081Â±.02
K2-Hop
A-Hop
.636Â±.03
.465Â±.02
.276Â±.03
.183Â±.02
A-Hop
K2-Hop
.904Â±.02
.709Â±.03
.492Â±.04
.328Â±.02
A-Hop
A-Hop
.997Â±.00
.998Â±.00
.993Â±.00
.988Â±.00
Table 14: Training time (â†“) of A-Hop with and without sorting.
sort?
Scaling dimensionality d
Scaling number of patterns N
d
64
128
256
64
64
64
N
2048
2048
2048
2048
4096
8192
Training time (ms, â†“)
âœ“
840
1573
3609
840
1531
2949
âœ—
2093
3059
8224
2093
4011
7983
ratio
2.49Ã—
1.95Ã—
2.27Ã—
2.49Ã—
2.62Ã—
2.71Ã—
Inference time (ms, â†“)
âœ“
.355
.368
.414
.355
.446
.570
âœ—
.287
.318
.326
.287
.364
.467
ratio
1.23Ã—
1.15Ã—
1.27Ã—
1.23Ã—
1.23Ã—
1.22Ã—
30
