Variational bagging: a robust approach for
Bayesian uncertainty quantification
Shitao Fan, Ilsang Ohn, David Dunson and Lizhen Lin
Abstract: Variational Bayes methods are popular due to computa-
tional efficiency and adaptivity to diverse applications. In specifying the
variational family, mean-field classes are commonly chosen, which en-
ables efficient algorithms such as coordinate ascent variational inference
(CAVI), but fails to capture parameter dependence and typically under-
estimates uncertainty. In this work, we introduce a variational bagging
approach that integrates a bagging procedure with variational Bayes,
resulting in a bagged variational posterior for improved inference. We
establish strong theoretical guarantees, including posterior contraction
rates for general models and a Bernsteinâ€“von Mises (BVM)-type theo-
rem that ensures valid uncertainty quantification. Notably, our results
show that even when using a mean-field variational family, our approach
can recover off-diagonal elements of the limiting covariance structure
and, crucially, provide proper uncertainty quantification. In addition,
variational bagging is robust to model misspecification, with the covari-
ance structures matching that of the target covariance. We illustrate our
variational bagging in numerical studies through applications to para-
metric models, finite mixture models, deep neural networks, and varia-
tional autoencoders (VAEs).
Keywords: Bagging; Bagged VAE; Deep neural networks; Model
misspecification; Posterior contraction rates; Robustness; Uncertainty
quantification; Variational Bayes.
1. Introduction
Variational Bayes has emerged as a powerful framework for scalable Bayesian
inference by approximating posterior distributions in complex and high-
dimensional models with simpler variational families bypassing the need for
MCMC sampling. There have been wide applications including for topic
modeling [26], graphical models [13, 2], Bayesian nonparametric modeling
[3], and high-dimensional sparse models [24]. Variational Bayes also plays an
important role in modern generative AI through the variational autoencoder
(VAE) [16, 6]. In specifying the variational family, a commonly used ap-
proximation within this framework is the mean-field family, which simplifies
computation by assuming independence among model parameters. Although
this assumption facilitates efficient algorithms like coordinate ascent varia-
1
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025
arXiv:2511.20594v1  [math.ST]  25 Nov 2025

Fan, Ohn, Dunson and Lin/Variational bagging
2
tional inference (CAVI), it can lead to poor uncertainty quantification by
ignoring parameter dependencies and systematically underestimating poste-
rior variance. Although there have been attempts to mitigate problems with
mean-field approaches [10, 27, 14, 20], most existing approaches are tailored
to specific model classes, for example Gaussian or sparse Gaussian process
models.
In this work, we propose variational bagging, a method that combines
variational Bayes with a bootstrap aggregation (bagging) procedure to pro-
duce a bagged variational posterior with improved inferential properties,
especially uncertainty quantification. We will show that variational bagging,
which yields a bagged variational posterior, comes with strong theoretical
guarantees, including posterior contraction rates for general models and a
Bernsteinâ€“von Mises (BvM)-type result that ensures valid uncertainty quan-
tification asymptotically. Remarkably, even when restricting to a mean-field
variational family, our method can recover aspects of the full covariance
structure, including off-diagonal elements.
Beyond the well-known undercoverage of variational Bayes, there is a
broader concern about robustness to model misspecification. Bayesian statis-
tics, including variational Bayes, is a model-based approach, which comes
with the implicit assumption that the likelihood is correctly specified. In
reality, however, knowledge about the true model or model class is rarely
given, and model misspecification is more often than not. For many model
classes, the posterior distribution (or their variational approximations) and
our corresponding inferences and predictions are sensitive to model mis-
specification. As sample size increases in the misspecified case, the posterior
typically concentrates around the pseudo-true parameter value correspond-
ing to the minimal Kullback-Leibler (KL) divergence from the true data-
generating model. Kleijn and van der Vaart [17] provide a Bernstein von
Mises (BvM) theory characterizing the limiting form of the posterior under
misspecification, showing that in general Bayesian credible sets are not valid
confidence sets when the model is misspecified even asymptotically. We will
show that variational bagging is robust to model misspecification, and yield
a covariance that matches the target covariance.
â€˜BayesBagâ€™ is a simple and general approach to obtain a robustified poste-
rior by averaging over posteriors defined conditionally on different bootstrap-
replicated datasets [5, 12]. This is an application of the bootstrap aggrega-
tion (bagging) approach of [4]. Let Xâˆ—
1:M = (Xâˆ—
1, . . . , Xâˆ—
M) be a bootstrapped
sample of size M from the data X = (X1, . . . , Xn) with sample size n. Let
Ï€(Î¸|Xâˆ—) be the posterior distribution conditioned on the bootstrapped copy
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
3
of the data. The bagged posterior [12] is defined as
ËœÏ€(Î¸|X1:n) =
1
nM
X
Xâˆ—
1:M
Ï€(Î¸|Xâˆ—
1:M),
(1)
which is defined over all possible bootstrap datasets. To reduce computa-
tional burden, we may use B bootstrap samples, with B â‰ªnM, to obtain
ËœÏ€(Î¸|X1:n) â‰ˆ1
B
B
X
b=1
Ï€

Î¸|Xâˆ—
(b)

,
where each Xâˆ—
(b) is a bootstrap sample of size M. Huggins and Miller [12]
studied asymptotic properties of the bagged posterior, including a BvM type
theorem, and showed good predictive and uncertainty quantification (UQ)
performance under model misspecification in simulation studies.
One potential drawback with BayesBag is its computation cost in the typ-
ical case in which Ï€ (Î¸|Xâˆ—
1:M) is intractable. Running MCMC sampling for
each bootstrap replicate is often infeasible. We propose a variational bagging
approach by first providing a variational approximation of each Ï€ (Î¸|Xâˆ—
1:M),
the collection of which is then aggregated to produce the final bagged vari-
ational posterior distribution for inference. Compared to BayesBag, varia-
tional bagging massively speeds up the computation while yielding new and
interesting theoretical results that rely on the unique properties of varia-
tional Bayes.
To summarize, we outline our theoretical results:
1. First, although variational approximations are well known to underes-
timate posterior variance in many cases, we provide a BvM theorem
showing that bootstrap aggregation not only accommodates model
misspecification but also appropriately inflates the variance so that
the bagged posterior has theoretical guarantees of accurate uncer-
tainty quantification. Remarkably, the limiting covariance structure
(with simple adjustment) matches the target covariance even if the
popular mean-field variational family is adopted. When variational in-
ference is not conducted over the latent variable Z, the covariance
coincides with the asymptotic variance of the MLE under model mis-
specification.
2. Second, we provide theoretical results on posterior contraction rates of
the resulting bagged VB posterior and conditions on when such rates
are minimax optimal. Our results encompass complex and nonpara-
metric models and required the development of new technical tools.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
4
3. Finally, we show that bagged variational posterior distributions lack
overconfident credible sets.
From the practical side, a major advantage of variational bagging is in
providing a natural mechanism for better approximating complex multi-
modal posteriors by combining many local variational approximations from
different bootstrapped data. In many modern complex models, ranging from
intricate mixture models to deep neural networks, MCMC algorithms can
fail to adequately explore the complex and multi-modal posterior landscape,
while simple variational approximations without bagging only capture local
features of this landscape. The variational bagging procedure can naturally
overcome the above issues and we demonstrate this in a rich variety of exam-
ples including finite mixture models, deep neural networks, and variational
auto-encoders.
The paper is organized as follows. Section 2 introduces notation and back-
ground on MLE, Bayes and variational Bayes under model misspecification.
In Section 3, we describe the variational bagging algorithm in general and
provide associated theory, while discussing practical aspects. Section 3.4
shows applications of variational bagging to a variety of models, providing
algorithmic details and theory support in these specific contexts. Section 4
provides an empirical evaluation and demonstration through a simulation
study and real data analyses.
2. Preliminaries
2.1. Model setup and notation
We consider a probability triple (X, F, P0) with corresponding density p0.
Let X1:n = (X1, . . . , Xn) be an i.i.d. sample from P0. We model the data
using a parametric family PÎ˜ = {PÎ¸ : Î¸ âˆˆÎ˜}, where each PÎ¸ has density
p(Â· | Î¸) indexed by a parameter Î¸.
The true distribution P0 is not assumed to belong to the parametric family
PÎ˜; that is, we explicitly allow for model misspecification. We denote by Î¸0
the pseudo-true parameter, defined as the minimizer of the Kullbackâ€“Leibler
(KL) divergence:
Î¸0 = arg min
Î¸âˆˆÎ˜ KL(PÎ¸, P0),
where KL(Â·, Â·) denotes the KL divergence
KL(P1, P2) =
Z
log
dP1
dP2

dP1,
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
5
whenever P1 is absolutely continuous with respect to P2. When the corre-
sponding densities p1 and p2 exist, we write KL(p1, p2) for the induced KL
divergence. When the model is correctly specified, we have PÎ¸0 = P0.
The maximum likelihood estimator (MLE),
Ë†Î¸mle = arg max
Î¸âˆˆÎ˜
n
X
i=1
log p(Xi | Î¸),
is asymptotically centered at Î¸0 and, under standard regularity conditions,
satisfies
âˆšn
 Ë†Î¸mle âˆ’Î¸0
 dâ†’N
 0, V (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1
,
where
V (Î¸) = âˆ’EP0

âˆ‡2 log p(X | Î¸)

,
D(Î¸) = EP0

âˆ‡log p(X | Î¸) âˆ‡log p(X | Î¸)âŠ¤
.
When the model is correctly specified, the above â€œsandwichâ€ covariance
reduces to the inverse Fisher information matrix. Under misspecification,
any unbiased estimator Ë†Î¸ of Î¸0 satisfies
Var(Ë†Î¸) â‰¥V (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1,
so that the MLE is asymptotically efficient in the usual sandwich sense.
2.2. Bernsteinâ€“von Mises (BvM) theorem for a Bayesian model
We first introduce a local asymptotic normality (LAN) condition for mis-
specified models, which will be useful in our later discussions. A model is
said to satisfy a stochastic LAN condition around Î¸0 âˆˆÎ˜ relative to a rate
Î´n â†’0 if there exists a random vector âˆ†n,Î¸0, bounded in P0-probability,
such that for every compact set K âŠ‚Rd,
sup
hâˆˆK
log
n
Y
i=1
p(Xi | Î¸0 + Î´nh)
p(Xi | Î¸0)
âˆ’hâŠ¤V (Î¸0)âˆ†n,Î¸0 âˆ’1
2hâŠ¤V (Î¸0)h

P0
âˆ’â†’0.
Under the above stochastic LAN condition, Kleijn and van der Vaart [17]
prove an asymptotic normality result for the posterior distribution. Writing
Ï‘ âˆ¼Ï€(Î¸ | X1:n) for a draw from the posterior, one has
âˆšn(Ï‘ âˆ’Î¸0) âˆ’âˆ†n,Î¸0
d
âˆ’â†’N
 0, V (Î¸0)âˆ’1
.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
6
For a misspecified model, the covariance of the limiting normal distribu-
tion differs from the sandwich covariance V (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1 introduced
above. This discrepancy implies that, under misspecification, credible sets
derived from the usual Bayesian posterior are in general not expected to
have asymptotically valid frequentist coverage.
2.3. Variational Bayes and asymptotic properties
Variational Bayes approximates the posterior distribution by a member of
a prespecified parametric family (the variational family) by minimizing the
KL divergence between the posterior distribution and distributions in this
family [7]. Variational Bayes has become one of the most popular approaches
for posterior approximation due to its simplicity, generality, and computa-
tional efficiency. There is also an emerging literature providing theoretical
guarantees for variational methods [35, 23, 1, 21].
We consider a setting in which the unknowns consist of latent variables
Z1:n = (Z1, . . . , Zn) and a global parameter Î¸ = (Î¸1, . . . , Î¸d). Variational
Bayes aims to approximate the joint posterior distribution Ï€(Î¸, Z1:n | X1:n)
by solving the optimization problem
qâˆ—(Î¸, Z1:n) = arg min
qâˆˆQ KL
 q, Ï€(Â· | X1:n)

,
where Q denotes the variational family. In this paper, we focus on the mean-
field variational family
Q =

q : q(Î¸, Z1:n) =
d
Y
j=1
qÎ¸j(Î¸j)
n
Y
i=1
qZi(Zi)

.
(2)
Recently, frequentist asymptotic properties of variational Bayes approxi-
mations have been established, including consistency, contraction rates, and
BVM [35, 23, 34, 1, 30, 29]. We briefly review the relevant results.
Definition 2.1. The variational log-likelihood is defined as
log pvb(X | Î¸) = max
qZ EqZ

log p(X, Z | Î¸) âˆ’log qZ(Z)

.
(3)
Wang and Blei [29] introduced a LAN-type condition for the variational
log-likelihood. A model is said to satisfy a stochastic variational local asymp-
totic normality (VLAN) condition around an interior point Î¸0 âˆˆÎ˜ relative
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
7
to a rate Î´n â†’0 if the following holds: there exists a random vector âˆ†n,Î¸0,
bounded in P0-probability, such that for every compact set K âŠ‚Rd,
sup
hâˆˆK
log
n
Y
i=1
pvb(Xi | Î¸0 + Î´nh)
pvb(Xi | Î¸0)
âˆ’hâŠ¤Vvb(Î¸0)âˆ†n,Î¸0 âˆ’1
2hâŠ¤Vvb(Î¸0)h

P0
âˆ’â†’0,
where
Vvb(Î¸) = âˆ’EP0

âˆ‡2 log pvb(X | Î¸)

,
(4)
Dvb(Î¸) = EP0

âˆ‡log pvb(X | Î¸) âˆ‡log pvb(X | Î¸)âŠ¤
.
(5)
Under the VLAN condition (together with additional regularity condi-
tions), Wang and Blei [29] show that the limiting distribution of Î¸ under the
variational posterior is Gaussian:
âˆšn(Ï‘ âˆ’Î¸0) âˆ’âˆ†n,Î¸0
d
âˆ’â†’N
 0, ( ËœV 0
vb)âˆ’1
,
(6)
for Ï‘ âˆ¼
R
qâˆ—(Î¸, Z) dZ, where ËœV 0
vb is the diagonal matrix that has the same
diagonal entries as Vvb(Î¸0). As is well-known and as shown in (6), the VB
covariance with a mean-field class is only diagonal.
2.4. Asymptotic properties of BayesBag under model
misspecification
Huggins and Miller [12] show that, under suitable regularity conditions on
the log density log p(x | Î¸), the BayesBag posterior ËœÏ€(Î¸ | X1:n) in (1) satisfies
the following asymptotic normality: for Ï‘ âˆ¼ËœÏ€(Î¸ | X1:n),
âˆšn(Ï‘ âˆ’Î¸0) âˆ’âˆ†n
 X1:n
d
âˆ’â†’N

0, 1
cV (Î¸0)âˆ’1 + 1
cV (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1

,
for some random vector âˆ†n that is bounded in P0-probability, where c =
limnâ†’âˆM/n and M is the bootstrap sample size.
In contrast to the original Bayesian posterior, whose limiting covariance
under misspecification is V (Î¸0)âˆ’1 (see BvM result in Section 2), the Bayes-
Bag posterior has asymptotic covariance 1
cV (Î¸0)âˆ’1+ 1
cV (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1,
which is closer to the sandwich form V (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1 and therefore
yields better-calibrated credible sets under model misspecification. In par-
ticular, taking c = 1 already leads to conservative uncertainty quantifica-
tion. Moreover, as discussed in Section 4.1, one can select c in a princi-
pled way so that the resulting covariance approximates the sandwich form
V (Î¸0)âˆ’1D(Î¸0)V (Î¸0)âˆ’1.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
8
3. Variational bagging: bagged variational posterior
In this section, we introduce our variational bagging approach. For a boot-
strap sample Xâˆ—
1:M = (Xâˆ—
1, . . . , Xâˆ—
M) generated from the original data X1:n =
(X1, . . . , Xn), define
qâˆ—(Î¸, Zâˆ—
1:M | Xâˆ—
1:M) = arg min
qâˆˆQ KL
 q(Î¸, Zâˆ—
1:M), Ï€(Î¸, Zâˆ—
1:M | Xâˆ—
1:M)

as the joint variational approximation to the posterior given this bootstrap
dataset. We focus on inference for Î¸ via the marginal
qâˆ—(Î¸ | Xâˆ—
1:M) =
Z
qâˆ—(Î¸, Zâˆ—
1:M | Xâˆ—
1:M) dZâˆ—
1:M.
When Q is a mean-field family, qâˆ—(Î¸ | Xâˆ—
1:M) is easily obtained using the
factorization structure.
We define the bagged variational posterior as the average of the variational
posteriors obtained from B bootstrap samples:
qbvB(Î¸ | X1:n) = 1
B
B
X
b=1
qâˆ—(Î¸ | Xâˆ—
(b)),
where each Xâˆ—
(b) denotes a bootstrap sample of size M. In our theoretical
analysis of the bagged variational posterior, we focus on the mean-field vari-
ational family.
3.1. Robust uncertainty quantification
In this section, we derive the asymptotic distribution of the bagged varia-
tional posterior and discuss implications for uncertainty quantification. Re-
call the definitions of Vvb(Â·) and Dvb(Â·) from (4) and (5), that is,
Vvb(Î¸) = âˆ’EP0

âˆ‡2 log pvb(X | Î¸)

,
Dvb(Î¸) = EP0

âˆ‡log pvb(X | Î¸) âˆ‡log pvb(X | Î¸)âŠ¤
.
Notably, we show that under model misspecification, the asymptotic co-
variance of the bagged variational posterior contains an additional â€œsand-
wichâ€ term
(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1,
V 0
vb = Vvb(Î¸0), D0
vb = Dvb(Î¸0),
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
9
on top of the covariance ( ËœV 0
vb)âˆ’1 arising from the usual variational posterior,
where ËœV 0
vb is the diagonal matrix formed from the diagonal entries of V 0
vb. In
particular, ( ËœV 0
vb)âˆ’1 corresponds to the covariance under the standard mean-
field VB procedure, which only captures marginal variances.
Theorem 3.1 (Bernsteinâ€“von Mises theorem for the bagged variational
posterior). Let â„“Î¸(x) = log pvb(x | Î¸), and assume the following conditions
hold:
1. The map Î¸ 7â†’â„“Î¸(X1) is differentiable at Î¸0 in probability.
2. There exists an open neighborhood U of Î¸0 and a function mÎ¸0 : X â†’R
such that EP0
 m3
Î¸0

< âˆ, and for all Î¸, Î¸â€² âˆˆU,
â„“Î¸ âˆ’â„“Î¸â€²
 â‰¤mÎ¸0 âˆ¥Î¸ âˆ’Î¸â€²âˆ¥2
a.s. [P0].
3. As Î¸ â†’Î¸0,
âˆ’EP0
 â„“Î¸ âˆ’â„“Î¸0

= 1
2(Î¸ âˆ’Î¸0)âŠ¤V 0
vb(Î¸ âˆ’Î¸0) + o
 âˆ¥Î¸ âˆ’Î¸0âˆ¥2
2

.
4. V 0
vb is invertible.
5. For every Ïµ > 0, there exists a sequence of tests Ï•n such that
Z
Ï•n(x1, . . . , xn)
n
Y
i=1
p0(xi) dxi â†’0,
sup
Î¸: âˆ¥Î¸âˆ’Î¸0âˆ¥>Ïµ
Z 
1 âˆ’Ï•n(x1, . . . , xn)
	 n
Y
i=1
pvb(xi | Î¸)
pvb(xi | Î¸0) p0(xi) dxi â†’0.
6. c = limnâ†’âˆM/n âˆˆ(0, âˆ), where M is the bootstrap sample size.
Then, for Ï‘â€  âˆ¼qbvB(Î¸ | X1:n), we have
âˆšn(Ï‘â€  âˆ’Î¸0) âˆ’âˆ†n
 X1:n
d
âˆ’â†’N

0, 1
c( ËœV 0
vb)âˆ’1 + 1
c(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1

,
(7)
where
âˆ†n = n1/2(V 0
vb)âˆ’1(Pn âˆ’P0) Ë™â„“Î¸0,
Pn = nâˆ’1
n
X
i=1
Î´Xi,
and ËœV 0
vb is the diagonal matrix with the same diagonal entries as V 0
vb.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
10
Remark 1. The sandwich term (V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1 in Theorem 3.1 can be
viewed as the target covariance: it corresponds to the covariance of the MLE
defined via the variational log-likelihood (3). This term is typically non-
diagonal, even when a mean-field variational family is used. It is precisely
this additional sandwich term that drives robustness and more accurate un-
certainty quantification of VB bagging: even under model misspecification,
variational bagging attempts to recover the â€œbestâ€ covariance structure al-
lowed by the model and the variational approximation.
When the model is specified correctly, we have diag
 (V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1
=
diag
 ( ËœV 0
vb)âˆ’1
. That is, these two terms share the same diagonal covariance.
The covariance in (7) can be decomposed as
1
c

(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1 âˆ’( ËœV 0
vb)âˆ’1
+ 2
c( ËœV 0
vb)âˆ’1.
The first term above contains only off-diagonal entries, while the second
term is purely diagonal. In this case, one can recover the off-diagonal entries
of the target covariance (V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1 from the bagged VB posterior
(e.g., with c = 1), and for the diagonal term we need to make the simple
adjustment by multiplying them by 1/2. Combining these two pieces yields
an estimator of the full target covariance.
When the model is misspecified, we can still recover the off-diagonal struc-
ture by setting c = 1 and extracting only the off-diagonal entries from the
bagged VB covariance. For the diagonal entries, we can use the choice of M
described in Section 4.1 to match the desired marginal variances.
Remark 2. If we instead use a variational family of Gaussian distributions
with general covariance matrices, Q = {q : q(Î¸) = N(Âµ, Î£)}, i.e., without a
mean-field restriction, then by inspecting the proof of Theorem 3.1 one can
see that the limiting distribution of the bagged variational posterior becomes
âˆšn(Ï‘â€  âˆ’Î¸0) âˆ’âˆ†n
 X1:n
d
âˆ’â†’N

0, 1
c(V 0
vb)âˆ’1 + 1
c(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1

.
The next corollary is a special case of Theorem 3.1 when variational in-
ference is not conducted over a latent variable Z (that is, we marginalize Z
in the model), so that pvb(x | Î¸) = p(x | Î¸).
Corollary 3.2 (BvM theorem without latent variables). Assume that pvb(x |
Î¸) = p(x | Î¸). Let V 0 = V (Î¸0) and D0 = D(Î¸0), and let ËœV 0 be the diagonal
matrix with the same diagonal entries as V 0. Then, under the same condi-
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
11
tions as in Theorem 3.1, for Ï‘â€  âˆ¼qbvB(Î¸ | X1:n),
âˆšn(Ï‘â€  âˆ’Î¸0) âˆ’âˆ†n
 X1:n
d
âˆ’â†’N

0, 1
c( ËœV 0)âˆ’1 + 1
c(V 0)âˆ’1D0(V 0)âˆ’1

,
where
âˆ†n = n1/2(V 0)âˆ’1(Pn âˆ’P0) Ë™â„“Î¸0.
In Theorem 3.2, the sandwich term (V 0)âˆ’1D0(V 0)âˆ’1 does not depend
on the choice of variational family and coincides with the usual sandwich
covariance for the MLE. Moreover, the full asymptotic covariance of the
bagged variational posterior is larger than that of the MLE, with the dif-
ference given by ( ËœV 0)âˆ’1/c. Thus, the bagged variational posterior yields
conservative uncertainty quantification: asymptotically, it is never overcon-
fident. This is rigorously formalized in the following corollary.
Corollary 3.3 (No overconfident credible sets). Assume the same condi-
tions as in Theorem 3.2 and take M = n (so that c = 1). Consider the
ellipsoid
C(r) =
n
Î¸ âˆˆÎ˜ : n(Î¸ âˆ’Ë†Î¸mle)âŠ¤bÎ£âˆ’1(Î¸ âˆ’Ë†Î¸mle) â‰¤r2o
with radius r > 0, where Ë†Î¸mle is the MLE of Î¸0 and bÎ£ is a consistent esti-
mator of the asymptotic covariance
Î£0 = ( ËœV 0)âˆ’1 + (V 0)âˆ’1D0(V 0)âˆ’1
of the bagged variational posterior. For Î± âˆˆ(0, 1), let rn,1âˆ’Î± be such that
C(rn,1âˆ’Î±) is a (1 âˆ’Î±)-credible set for Î¸0 under the bagged variational poste-
rior, i.e.,
QbvB Ï‘â€  âˆˆC(rn,1âˆ’Î±)

= 1 âˆ’Î±.
Then
lim
nâ†’âˆP0
 Î¸0 âˆˆC(rn,1âˆ’Î±)

â‰¥1 âˆ’Î±.
3.2. Valid variational Bayes uncertainty quantification
Mean-field variational Bayes is well known for providing a fast and tractable
approximation of the Bayesian posterior. As shown by Wang and Blei [30]
(see Equation (6)), mean-field approximations are asymptotically normal
under standard conditions, but they only approximate the diagonal terms of
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
12
the covariance structure, ignoring dependence among parameters. For this
reason, although variational Bayes often delivers accurate first-order infer-
ence (e.g., point estimates), it is generally unsuitable for second-order in-
ference such as uncertainty quantification, even when the model is correctly
specified.
An interesting consequence of Theorem 3.1 is that the variational bag-
ging procedure can address this limitation by recovering the off-diagonal
elements of the covariance structure that are wiped out in the standard
mean-field variational approximation. In particular, when the model is cor-
rectly specified, variational bagging yields asymptotically valid uncertainty
quantification, comparable to that of the standard Bayesian posterior. We
briefly discussed this in Remark 1 for the case of a well-specified model with
the presence of latent variables, and the following deals with the case when
there is no latent variable.
Corollary 3.4 (BvM theorem when the model is correctly specified). Let
â„“Î¸(x) = log pvb(x | Î¸) and assume Assumptions 1â€“5 in Theorem 3.1 hold,
with M = n. In this case Î¸0 is the true parameter so that P0 = PÎ¸0. Then,
for Ï‘â€  âˆ¼qbvB(Î¸ | X1:n), we have
âˆšn(Ï‘â€  âˆ’Î¸0) âˆ’âˆ†n
 X1:n
d
âˆ’â†’N
 0, ( ËœV 0
vb)âˆ’1 + (V 0
vb)âˆ’1
,
where V 0
vb = Vvb(Î¸0) and ËœV 0
vb is the diagonal matrix with the same diagonal
entries as V 0
vb.
When the model is correctly specified and the usual regularity conditions
hold, the Fisher information structure implies that
diag
 (V 0
vb)âˆ’1
= diag
 ( ËœV 0
vb)âˆ’1
,
so that the limiting covariance in Corollary 3.4 can be decomposed as
( ËœV 0
vb)âˆ’1 + (V 0
vb)âˆ’1 = 2 diag
 (V 0
vb)âˆ’1
+ offdiag
 (V 0
vb)âˆ’1
,
where offdiag(A) denotes the matrix obtained from A by zeroing out its
diagonal. The first term above only has nonzero diagonal entries, while the
second term only contains off-diagonal entries. Thus:
â€¢ the off-diagonal part of the target covariance (V 0
vb)âˆ’1 is recovered by
the bagged variational posterior, and
â€¢ the diagonal part is inflated by a factor of 2 relative to the target
covariance.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
13
To recover the same covariance as in the standard Bayesian BvM result,
one can simply rescale the diagonal entries of the bagged VB covariance
by 1/2, keeping the off-diagonal entries unchanged. In this way, variational
bagging can serve as a general tool to improve and calibrate uncertainty
quantification based on mean-field variational Bayes, restoring both the cor-
rect marginal variances (after a simple adjustment) and the dependence
structure.
3.2.1. Toy example illustration
We illustrate the implication of Corollary 3.4 with a simple toy example.
Consider estimation of the mean vector Âµ for two-dimensional Gaussian
data X1, . . . , X500, where
Xi âˆ¼N(Âµ, Î£),
Âµ = (âˆ’1, 1)âŠ¤,
Î£ =
 1
0.5
0.5
1

.
We place a conjugate prior on Âµ and compare three posterior approxima-
tions: (i) Hamiltonian Monte Carlo (HMC), (ii) a mean-field variational
approximation, and (iii) our variational bagging approach.
Figure 1 shows the resulting 95% posterior credible regions. The mean-
field variational posterior provides a reasonable approximation of the poste-
rior mean of Âµ, but it fails to capture the correlation structure between the
components of Âµ and yields an elliptical region aligned with the coordinate
axes. In contrast, the bagged variational posterior closely matches the full
Bayesian posterior obtained by HMC, successfully recovering the correla-
tion and orientation of the credible region. This illustrates how variational
bagging can substantially improve uncertainty quantification over standard
mean-field variational Bayes while retaining its computational benefits.
3.3. Contraction rates of the bagged variational posterior
Our BvM theorem (Theorem 3.1) implies that the bagged variational pos-
terior contracts to Î¸0 at the parametric rate nâˆ’1/2 for finite-dimensional
parametric models satisfying its assumptions. In this section, we extend this
result to a general setup that encompasses nonparametric models, and we
consider convergence in Hellinger distance. Let
H(P1, P2) =
Z  p
dP1/dP2 âˆ’1
2 dP2
1/2
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
14
Fig 1: 95% posterior credible regions for a two-dimensional Gaussian mean:
comparison of HMC, mean-field VB, and variational bagging.
denote the Hellinger distance between two probability measures P1 and P2.
To quantify the corresponding convergence rate, we use the notion of brack-
eting Hellinger metric entropy.
We say that a finite collection of pairs of functions {(fL
i , fU
i ) : i =
1, . . . , N} is a Î´-bracketing of a function space F if
(fU
i )1/2 âˆ’(fL
i )1/2
2 â‰¤Î´,
i = 1, . . . , N,
and for any f âˆˆF there exists i âˆˆ{1, . . . , N} such that fL
i â‰¤f â‰¤fU
i . The
Î´-bracketing Hellinger metric entropy of F, denoted by HB(Î´, F), is defined
as the logarithm of the cardinality of a minimal Î´-bracketing.
Assumption 3.1. Let (Ïµn)n=1,2,..., be a positive sequence such that Ïµn â†’0
and nÏµ2
n â†’âˆas n â†’âˆ. Assume the following:
(1) (Prior mass) There exists a constant C1 > 0 such that
Î 

Î¸ âˆˆÎ˜ : KL(P0, PÎ¸) â‰¤Ïµ2
n, KLV(P0, PÎ¸) â‰¤Ïµ2
n

â‰¥exp(âˆ’C1nÏµ2
n),
where KLV(P0, PÎ¸) =
R  log(dPÎ¸/dP0)
2 dP0.
(2) (Sieve and complexity) There exists a subset Î˜n âŠ‚Î˜ such that
Î (Î˜ \ Î˜n) â‰¤exp
 âˆ’(C1 + 4)nÏµ2
n

imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
15
and, for some constants C2 > 0 and C3 > 0,
Z âˆš
2 Ïµ
Ïµ2/28 H1/2
B
 u/C2, {p(Â· | Î¸) : Î¸ âˆˆÎ˜n}

du â‰¤C3
âˆšn Ïµ2
for any Ïµ > Ïµn.
(3) (Variational family) There exists a constant C4 > 0 such that
inf
qâˆˆQ

n
Z
KL(PÎ¸, P0) q(Î¸) dÎ¸ + KL(q, Ï€)

â‰¤C4nÏµ2
n.
(4) (Bootstrap sample size) M = n.
The â€œprior mass and testingâ€ method, first developed in the seminal pa-
per Ghosal et al. [9], is a powerful and general tool for deriving contraction
rates of posterior distributions. We retain a prior mass condition in As-
sumption 3.1(1), which plays the same role as in the prior mass and testing
framework. However, in our setting it is not straightforward to construct
suitable tests based on bootstrapped samples. To the best of our knowledge,
there is no principled general approach for test construction for Bayesian
procedures involving bootstrap-based posteriors.
Instead, we follow the strategy of Shen and Wasserman [25] with a suitable
modification to handle bootstrap-weighted likelihoods. Han and Yang [11]
adopt a similar idea, but their result is restricted to parametric models. In
our nonparametric setting, we use the complexity condition on the Hellinger
metric entropy in Assumption 3.1(2), adapted from Shen and Wasserman
[25], to control the behavior of the empirical process of the â€œbootstrap-
weightedâ€ log-likelihood ratio. The third assumption on the variational fam-
ily, which has been employed in several related works such as Ohn and Lin
[21] and Zhang and Gao [35], controls the variational approximation error
between the variational posterior and the original posterior distribution. We
show that this assumption, together with the bootstrap sample size condition
in Assumption 3.1(4), can still be used to bound the variational approxima-
tion error even when a bootstrapped sample is used.
Theorem 3.5 (Contraction rate). Suppose that Assumption 3.1 holds. Then
the bagged variational posterior satisfies
E
h
QbvB H2(PÎ¸, P0) â‰¥MnÏµ2
n log n
i
âˆ’â†’0,
as n â†’âˆfor any diverging sequence (Mn) with Mn â†’âˆ, where the
expectation is taken with respect to P âŠ—n
0
.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
16
3.4. Illustrating examples
In this section, we demonstrate our theory by considering two simple exam-
ples: a multivariate Gaussian example and a two-component finite mixture
model.
3.4.1. Multivariate Gaussian mean
Consider again the toy example in Section 3.2.1. That is, X1:n âˆˆR2 with
Xi âˆ¼N(Âµ0, Î›âˆ’1
0 ),
where Î›0 is known and we are interested in estimating the mean vector Âµ0.
For the posterior distribution Ï€(Âµ | X1:n), the Bernsteinâ€“von Mises theorem
yields
âˆšn(Âµ âˆ’Â¯Xn)
d
âˆ’â†’N(0, Î›âˆ’1
0 ),
for Âµ âˆ¼Ï€(Âµ | X1:n), where Â¯Xn is the sample mean.
For variational Bayes, we consider the mean-field variational family, which
assumes q(Âµ) = q1(Âµ1) q2(Âµ2). By the BvM theorem for the variational pos-
terior in Wang and Blei [29], we have
âˆšn(Âµ âˆ’Â¯Xn)
d
âˆ’â†’N
 
0,
Î›âˆ’1
022
0
0
Î›âˆ’1
011

/det(Î›0)
!
,
for Âµ drawn from the mean-field variational posterior qâˆ—(Âµ), where we de-
compose the true precision matrix as
Î›0 =
Î›011
Î›012
Î›021
Î›022

.
Compared with the asymptotic distribution of the exact posterior, we see
that variational Bayes ignores the correlation between Âµ1 and Âµ2.
Now, applying Corollary 3.4, we obtain
âˆšn(Âµ âˆ’Â¯Xn)
 X1:n
d
âˆ’â†’N
 
0,
Î›âˆ’1
022
0
0
Î›âˆ’1
011

/det(Î›0) + Î›âˆ’1
0
!
,
for Âµ drawn from the bagged variational posterior qbvB(Âµ | X1:n).
Therefore, using variational bagging, we recover the off-diagonal (corre-
lation) structure of the true posterior covariance matrix via the Î›âˆ’1
0
term,
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
17
while the diagonal terms are inflated relative to the target covariance. As
discussed in Section 3.2, a simple correction of rescaling the diagonal entries
by a factor of 1/2 recovers the full target covariance, providing a concrete
illustration of how variational bagging improves uncertainty quantification
over standard mean-field variational Bayes in this simple Gaussian setting.
3.4.2. Bayesian mixture models
In this example, we consider model misspecification in a finite mixture
model. For technical simplicity, we assume that our working inference model
is a symmetric two-component Gaussian mixture with unit variance,
p(x | Î¸) = 1
2N(x; Î¸, 1) + 1
2N(x; âˆ’Î¸, 1),
with Î¸ > 0, while the true data-generating distribution P0 is not necessarily
in this model class.
The above mixture model can be equivalently written in hierarchical form
as
Xi | Zi âˆ¼N
 (2Zi âˆ’1)Î¸, 1

,
Zi âˆ¼Bernoulli(1/2),
so we can conduct variational inference jointly over the parameter Î¸ and
latent variables Z1, . . . , Zn. We consider a mean-field variational family in
which each Zi has a degenerate (point-mass) distribution at either 0 or 1,
which is analogous to a hard clustering procedure.
In this case, it is straightforward to see that the variational log-likelihood
is given by
â„“(Î¸) = log pvb(X | Î¸) = sup
q(Z)
Z
q(Z) log p(X, Z | Î¸)
q(Z)
dZ
= max
zâˆˆ{0,1} log p(X, z | Î¸)
= 1
2 max

âˆ’(X âˆ’Î¸)2, âˆ’(X + Î¸)2	
âˆ’1
2 log(2Ï€)
= âˆ’1
2
 X âˆ’sign(X)Î¸
2 âˆ’1
2 log(2Ï€).
Differentiating twice in Î¸ gives
Vvb(Î¸) = âˆ’EP0
h d2
dÎ¸2 â„“(Î¸)
i
= 1.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
18
It is then easy to verify that the first through fourth conditions of Theo-
rem 3.1 hold for this model. The fifth condition (existence of suitable tests)
follows from Theorem 1 of Westling and McCormick [32], which establishes
consistency of the maximum variational likelihood estimator in this setting.
In our case, the maximum â€œvariationalâ€ likelihood estimator is
Ë†Î¸n = 1
n
n
X
i=1
Xi sign(Xi),
and this estimator is consistent for the pseudo-true parameter Î¸0 under
model misspecification.
Applying Theorem 3.1, we obtain, for Ï‘â€  âˆ¼qbvB(Î¸ | X1:n),
âˆšn(Ï‘â€  âˆ’Ë†Î¸n)
 X1:n
d
âˆ’â†’N

0, 1
c

1 + EP0

(X âˆ’sign(X)Î¸0)2	
,
where c = limnâ†’âˆM/n is the limiting ratio of bootstrap sample size to the
original sample size. This illustrates how, even under model misspecification
in a mixture setting, the bagged variational posterior enjoys a well-defined
asymptotic distribution with a variance that incorporates a sandwich-type
correction term.
4. Simulation study
In this section, we conduct a simulation study in which we apply variational
bagging to several examples, including the multivariate Gaussian model, a
finite Gaussian mixture model, sparse linear regression, regression based on
feedforward deep neural networks, and a bagged VAE (variational autoen-
coder). We first discuss some practical aspects of variational bagging, such
as the choice of bootstrap sample size and the number of bootstrap repli-
cates.
4.1. Bootstrap sample size and number of bootstrap samples
If we have strong confidence in our model specification, we may set M = n
and use mean-field VB to learn the off-diagonal terms of the covariance and
take 1/2 of the diagonal terms. Under model misspecification, however, as
illustrated in the previous examples, choosing M = n (i.e., c = 1) may not
yield the desired robust, sandwich-type covariance.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
19
Let Ëœvn and Ëœvâˆ—
n denote, respectively, the standard and bagged variational
posterior variances of a real-valued functional f(Î¸), where the bagged vari-
ational posterior is computed with M = n. In the asymptotic BvM setting,
the bagged variational covariance (with a general c) behaves like
1
c(V 0
vb)âˆ’1 + 1
c(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1,
while the â€œtargetâ€ (sandwich) covariance is
(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1.
At the level of a scalar functional f(Î¸), this corresponds to finding c such
that
1
c Ëœvn + 1
c
 Ëœvâˆ—
n âˆ’Ëœvn

â‰ˆËœvâˆ—
n âˆ’Ëœvn,
which yields
1
c Ëœvâˆ—
n = Ëœvâˆ—
n âˆ’Ëœvn
=â‡’
c =
Ëœvâˆ—
n
Ëœvâˆ—n âˆ’Ëœvn
.
Hence the corresponding optimal bootstrap sample size is
Mâˆ—= c Â· n =
Ëœvâˆ—
n
Ëœvâˆ—n âˆ’Ëœvn
n.
We therefore suggest the plug-in estimator
Ë†
Mâˆ—=
Ëœvâˆ—
n
Ëœvâˆ—n âˆ’Ëœvn
n.
(8)
In finite-sample settings, we also need to account for prior influence when
choosing a suitable bootstrap sample size. Following Huggins and Miller
[12], we define a finite-sample version of the optimal bootstrap size, denoted
Ë†
Mâˆ—
fs,opt.
Let v0 denote the prior variance of f(Î¸) and define
Ë†Ïƒ2
â—¦:= n v0Ëœvn
 v0 âˆ’Ëœvn

,
and
Ë†s2
â—¦:=
v2
0
 v0 âˆ’Ëœvn
2
 Ëœvâˆ—
n âˆ’Ëœvn

n.
(9)
Then the finite-sample optimal bootstrap size is estimated by
Ë†
Mfs,opt := n
2 + nË†Ïƒ2
â—¦
2Ë†s2â—¦
âˆ’Ë†Ïƒ2
â—¦
v0
+
(n
2 + nË†Ïƒ2
â—¦
2Ë†s2â—¦
2
âˆ’nË†Ïƒ2
â—¦
v0
)1/2
.
(10)
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
20
For the derivation of (10), we refer to Appendix E of Huggins and Miller
[12].
Regarding the number of bootstrap samples B, Huggins and Miller [12]
recommend B â‰ˆ50 to 100 for BayesBag due to the computational cost
of repeated MCMC. In variational bagging, by contrast, computing each
variational posterior is typically much faster than running MCMC, allowing
substantially larger B (e.g., B â‰ˆ200). In our experiments, however, we find
that even a relatively small number of bootstrap samples, such as B = 5, is
often sufficient to obtain robust and well-calibrated uncertainty quantifica-
tion.
4.2. Uncertainty quantification for a multivariate Gaussian
Using the same toy example as in Section 3.2.1, we generate two-dimensional
Gaussian data X1, . . . , Xn, where
Xi âˆ¼N(Âµ, Î£),
Âµ = (âˆ’1, 1)âŠ¤,
Î£ =
 1
0.5
0.5
1

.
We vary the number of bootstrap replicates B and the sample size n to
assess the effectiveness of uncertainty quantification under variational bag-
ging. Specifically, we take
B âˆˆ{5, 10, 20, 30, 50},
n âˆˆ{50, 100, 200, 300, 500, 1000}.
Because Corollary 3.4 is an asymptotic result, our goal here is to identify
practically reasonable choices of B and n for which the asymptotic behavior
is already well approximated.
We place a conjugate prior on Âµ and approximate the posterior using three
methods: (i) Hamiltonian Monte Carlo (HMC) with 2 chains and 2000 pos-
terior draws, (ii) mean-field variational Bayes (MFVB), and (iii) variational
bagging based on MFVB. For variational bagging, we compute MFVB for
each bootstrap dataset and average the corresponding variational posteriors;
runs in which the MFVB algorithm fails to converge are discarded.
From Figure 2, we observe that when the number of bootstrap replicates B
is at least 30, the credible regions obtained from variational bagging closely
match those from the full Bayesian posterior, even for relatively small sample
sizes. In particular, the results suggest that our theoretical guarantees are
already informative for sample sizes as small as n = 50, and that a moderate
number of bootstrap replicates (around B â‰¥30) suffices to capture the
improved uncertainty quantification predicted by our theory.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
21
Fig 2: 95% posterior credible regions for the Gaussian mean under HMC,
MFVB, and variational bagging, for varying sample size n and number of
bootstrap replicates B.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
22
4.3. Gaussian mixture model
This simulation study considers the Gaussian mixture model
Xi âˆ¼
K
X
k=1
Ï€kN(Âµk, Ïƒ2
k),
i = 1, . . . , n.
For the prior, we use
Ï€ âˆ¼Dirichlet(Î±),
Âµk | Ïƒ2
k âˆ¼N(0, Î½0Ïƒ2
k),
Ïƒ2
k âˆ¼IG(a, b).
We generate data from heavy-tailed mixture distributions such as t mix-
tures and double-exponential mixtures, but fit a Gaussian mixture model to
these data. We also fit the same data under the correctly specified (true)
model to evaluate the performance of variational bagging.
For the full Bayesian posterior, we use Stan (R interface), based on Hamil-
tonian Monte Carlo (HMC), with 2000 iterations and 1000 burn-in per chain.
For variational Bayes, we implement a CAVI algorithm. For bagging, we
choose B = 50 bootstrap replicates and set the bootstrap sample size to be
Ë†
Mâˆ—from (8). To accelerate computation, variational fits for different boot-
strap samples are run in parallel.
(a) Data
(b) Âµ1
(c) Âµ2
Fig 3: Gaussian mixture fit to t-mixture data.
Figure 3a shows an example where the data are generated from a t-mixture
distribution but fitted with a Gaussian mixture model, leading to heavier
tails in the data than assumed by the working model. Figures 3b and 3c
show results for the component means Âµ1 and Âµ2 under several approaches,
including variational bagging.
When using standard Bayesian methods and mean-field variational Bayes,
the posteriors tend to be reasonably centered around the true means but ex-
hibit poorly calibrated uncertainty, with credible intervals that do not match
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
23
(a) Data
(b) Âµ1
(c) Âµ2
Fig 4: Gaussian mixture fit to data from a double-exponential mixture
model.
(a) Data
(b) Âµ1
(c) Âµ2
Fig 5: Gaussian mixture fit to double-exponential mixture data with larger
variance.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
24
the empirical variability induced by the heavy-tailed data. In contrast, our
Variational Bagging procedure corrects for this misspecification effect and
yields posterior coverage much closer to the empirical coverage. Here, â€œro-
bustnessâ€ means that, relative to standard Bayesian or VB fits, the bagged
procedures produce uncertainty statements that better reflect the true dis-
tribution of the estimators under model misspecification.
This is visually summarized in Figures 3b and 3c. The green boxes repre-
sent the empirical interquartile range (Q1 to Q3) of the true sampling distri-
bution. The standard methods (olive and blue boxes) produce interquartile
ranges that are noticeably misaligned with this target, either too wide or too
narrow depending on the setting. By contrast, the bagging procedures (red
boxes for BayesBag and purple boxes for variational bagging) yields Q1â€“Q3
ranges more closely aligned with the green boxes, demonstrating improved
accuracy and robustness in the presence of model misspecification.
A similar phenomenon is observed when the data are generated from
double-exponential mixture distributions, as shown in Figures 4 and 5. In
these settings as well, the bagging-based procedures effectively mitigate the
impact of tail misspecification and produce uncertainty quantification that
more faithfully reflects the underlying data-generating process.
Consistent with our theoretical results, the outcomes of variational bag-
ging closely mirror those of Bayesian bagging in these simulations. This
indicates that variational bagging can inherit the robustness properties of
BayesBag while being substantially more computationally efficient, thereby
offering a practical and effective tool for robust uncertainty quantification
in mixture models and beyond.
4.4. Sparse linear regression model
In this section, we consider a sparse linear regression model with a spike-
and-slab prior, a popular and effective prior for variable selection. The model
is
Y | X, Î“, Î², Ïƒ2 âˆ¼N(XÎ“Î², Ïƒ2In),
Î² âˆ¼N(0, Ïƒ2
Î²Iq),
Ïƒ2 âˆ¼InvGamma(A, B),
Î³i
i.i.d.
âˆ¼Bernoulli(p),
Î“ = diag(Î³1, . . . , Î³q).
We simulate data from several models and fit each using this sparse linear
regression specification. Unless otherwise noted, the general settings are:
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
25
sample size n = 1000, hyperparameters A = B = 0.1, Ïƒ2
Î² = 10, and p = 0.5.
For standard Bayesian inference, we obtain 2000 MCMC samples via Gibbs
sampling. For variational inference, we use Algorithm 1 of Ormerod et al.
[22]. For bagging, we take B = 50 bootstrap samples and set the bootstrap
sample size to M = Ë†
Mâˆin (8)).
To compare methods, we focus on estimation accuracy for the regression
coefficients. Let Î²OLS be the ordinary least squares estimator (when defined),
and let Ë†Î² denote a point estimate from each method (e.g., posterior mean).
We compute the relative squared error (RSE)
RSE = âˆ¥Ë†Î² âˆ’Î²OLSâˆ¥2
âˆ¥Î²OLSâˆ¥2
,
and compare RSEs across standard Bayes, BayesBag, variational Bayes
(VB), and Variational Bagging.
We consider the following four scenarios:
â€¢ Scenario 1 (S1): 10 covariates, n = 1000.
â€¢ Scenario 2 (S2): 10 covariates, n = 2000.
â€¢ Scenario 3 (S3): 20 covariates, n = 1000.
â€¢ Scenario 4 (S4): 20 covariates, n = 2000.
Tables 1 and 2 report RSEs under Gaussian and Student-t errors, respec-
tively.
Scenario
Bayes
BayesBag
VB
VB Bagging
S1
3.16e-4
2.17e-4
1.24e-4
9.67e-5
S2
3.04e-4
2.15e-4
4.09e-4
3.63e-4
S3
8.82e-4
3.33e-4
1.65e-4
1.39e-4
S4
2.50e-5
2.37e-5
3.42e-5
2.15e-5
Table 1
RSEs of four methods for sparse linear regression with Gaussian errors.
Scenario
Bayes
BayesBag
VB
VB bagging
S1
6.55e-4
3.34e-4
5.16e-4
4.14e-4
S2
9.73e-4
5.10e-4
1.97e-3
1.54e-3
S3
1.35e-3
7.80e-4
2.81e-4
2.41e-4
S4
6.68e-4
7.48e-4
1.15e-3
7.57e-4
Table 2
RSEs of four methods for sparse linear regression with Student-t errors.
Across all scenarios, the bagging regimes (BayesBag and Variational Bag-
ging) generally achieve lower RSEs than their non-bagged counterparts,
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
26
highlighting the robustness benefits of bagging. Under Gaussian errors, VB
bagging often produces the most accurate results, slightly improving upon
both Bayes and standard VB. Under Student-t errors, where the likelihood
is misspecified, the gains from bagging are even more pronounced: Bayes-
Bag consistently improves on Bayes, and Variational Bagging improves on
standard VB in three of the four scenarios.
We also observe that VB can occasionally outperform standard Bayesian
inference. This is likely due to Monte Carlo error and limited MCMC ex-
ploration in the Gibbs sampler, especially in higher-dimensional settings.
Overall, these results indicate that bagging can enhance the accuracy and
stability of both Bayesian and variational approaches for sparse linear re-
gression, while retaining the computational advantages of VB.
4.5. Deep learning model for prediction
We implement variational bagging for prediction via
qbvB(Xnew | X1:n) = 1
B
B
X
b=1
q
 Xnew | Xâˆ—(b)
1:M

,
where Xâˆ—(b)
1:M denotes the b-th bootstrap sample. From Section 4.1, the re-
sulting asymptotic covariance for the bagged variational posterior has the
form
1
2

( ËœV 0
vb)âˆ’1 + (V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1
,
which can be viewed as a compromise between the model-based covariance
( ËœV 0
vb)âˆ’1 and the sandwich covariance (V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1. Even with M =
2n, we still recover half of the off-diagonal (sandwich) contribution through
the term 1
2(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1.
We apply fully-connected deep neural networks (DNNs) for regression
and assess predictive uncertainty. Motivated by the fact that such DNNs
can achieve near-optimal nonparametric rates [21, 18], we investigate three
regression settings:
â€¢ simple linear regression,
â€¢ nonlinear regression,
â€¢ multivariate linear regression.
For each setting, data is generated with error terms (Ïµ) drawn either
from a Student-t distribution or a Laplace distribution, but the working
regression model assumes Gaussian errors, thereby introducing deliberate
model misspecification.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
27
Fig 6: Regression data from a linear model with t and Laplace errors.
Fig 7: Regression data from a nonlinear model with t and Laplace errors.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
28
â€¢ Simple linear regression. Data are generated as
Y = 2.5 + 1.8X + Ïµ,
as illustrated in Figure 6. For this setting, we use a DNN with archi-
tecture [1, 10, 1] (one input, one hidden layer with 10 units, and one
output), which is sufficient to capture the underlying linear relation-
ship.
â€¢ Nonlinear regression. Data follow
Y = sin(X)
1 + X2 + Ïµ,
as shown in Figure 7. To capture the nonlinear structure, we employ a
deeper and wider DNN with architecture [1, 32, 64, 1], which provides
increased capacity to approximate the nonlinear regression function.
â€¢ Multivariate linear regression. We generate responses via
Y = XÎ² + Ïµ,
where Î²i
i.i.d.
âˆ¼N(0, 1) and there are 7 predictors (so X âˆˆRnÃ—7). For
this model, we use a larger DNN with architecture [7, 128, 64, 32, 1] to
handle the increased input dimension and capture interactions among
predictors.
Our primary evaluation metric is the empirical coverage of 95% predictive
intervals on the original data. Specifically, we examine whether the nomi-
nal 95% predictive intervals contain approximately 95% of the observed
responses, which serves as a diagnostic for the calibration of predictive un-
certainty. In the simulation study:
â€¢ the number of bootstrap replicates is B = 10;
â€¢ the bootstrap sample size is set to M = 2n.
Model misspecification (Gaussian working errors vs. heavy-tailed true er-
rors) tends to produce predictive intervals that are too narrow, leading to
under-coverage of the nominal 95% predictive intervals. This effect is vis-
ible in the variational Bayes (VB) results. However, as shown in Table 3,
variational bagging (VB bagging) substantially corrects this under-coverage
and yields predictive intervals whose empirical coverage is much closer to
the nominal level.
Across all regression settings and both error distributions, VB alone yields
slightly under-covered predictive intervals (around 93% rather than 95%),
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
29
Data setting
VB
VB bagging
linear reg + t errors
93.8
94.6
linear reg + Laplace errors
93.8
94.6
nonlinear reg + t errors
93.2
95.3
nonlinear reg + Laplace errors
92.9
95.7
multivariate reg + t errors
93.0
94.8
multivariate reg + Laplace errors
93.5
95.0
Table 3
Empirical coverage (%) of nominal 95% predictive intervals for DNN-based regression
under VB and variational bagging.
reflecting the combination of model misspecification and variational under-
dispersion. In contrast, variational bagging systematically improves cover-
age, bringing it close to the nominal 95% level in all cases. These results
highlight the robustness of variationla bagging in restoring calibrated pre-
dictive uncertainty for deep neural network models, even when the error
distribution is misspecified and the underlying regression function is nonlin-
ear or high-dimensional.
4.6. Bagged variational autoencoder (BVAE)
In this simulation study, we consider the application of variational bagging
to a variational autoencoder (VAE) model [15], which we refer to as the
bagged variational autoencoder. Recall in a deep generative model where
a D-dimensional random variable X is modeled by X = f(Z) + Ïµ, where
Z is some latent variable of dimension d, Ïµ represents the error and f is
the generator typically parametrized by a deep neural network. VAE is one
of the primary likelihood based training methods for estimating the deep
generative model, where P(Z | X) is parameterized by another deep neural
network, the so-called encoder network. For implementation, we adopt the
same algorithm described in [6]. To model data as noisy realizations from a
distribution on a 1-dimensional manifold, we let X = f(Z)+Ïµ, where D = 2,
Ïƒâˆ—= 0 is the true variance of the residual, and Z is a univariate random
variable following a standard normal distribution N(0, 1). We examine three
different functions for the true generators fâˆ—= (fâˆ—1, fâˆ—2) as:
â€¢ Case 1 (Figure 8a). fâˆ—1(z) = 6(z âˆ’0.5),
fâˆ—2(z) = 0.5(z âˆ’2)z(z + 2).
â€¢ Case 2 (Figure 8b). fâˆ—1(z) = cos(2Ï€z),
fâˆ—2(z) = sin(2Ï€z).
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
30
â€¢ Case 3 (Figure 8c).
 fâˆ—1(z) = 2 cos(2Ï€z) + 1,
fâˆ—2(z) = 2 sin(2Ï€z) + 0.4,
if z > 0.5
fâˆ—1(z) = 2 cos(2Ï€z) âˆ’1,
fâˆ—2(z) = 2 sin(2Ï€z) âˆ’0.4,
otherwise
(a) case 1
(b) case 2
(c) case 3
Fig 8: Simulated data concentrated on different 1-dimensional manifolds.
These data will be analyzed with VAEs with and without bagging.
(a) case 1
(b) case 2
(c) case 3
Fig 9: Results from apply a usual (non-bagged) VAE to the one-dimensional
manifold data from the previous Figure.
Subsequently, we investigate two additional distributions: one on the Swiss
roll in Figure 11 and another featuring a uniform distribution on a sphere
in Figure 12. Both distributions are supported on 2-dimensional manifolds
within the ambient space R3. The Swiss roll distribution represents the dis-
tribution of f(Z), where Z follows a uniform distribution on (0, 1)2, and the
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
31
(a) case 1
(b) case 2
(c) case 3
Fig 10: Results of bagged VAE on the simulated one-dimensional manifold
data; averaging over bootstrap dataset produces smoother and more accu-
rate estimates
true generator fâˆ—= (fâˆ—1, fâˆ—2, fâˆ—3) : (0, 1)2 â†’R3 is defined as:
t1 = 1.5Ï€ (1 + 2z1) , t2 = 21z2,
fâˆ—1 (z1, z2) = t1 cos (t1) ,
fâˆ—2 (z1, z2) = t2,
fâˆ—3 (z1, z2) = t1 sin (t1) .
We first use the standard VAE with 10 epochs for training. We generate
the figures by sampling from the posterior predictive distribution. We also
apply our proposed bagged VAE approach. In all experiments, we set the
validation and test sample sizes to 1000, while the training sample is set to
5000.
(a) data
(b) VAE
(c) BVAE
Fig 11: Simulated data on a 2d swiss roll embedded in 3d are shown in panel
(a) with the results of VAE in panel (b) and those of BVAE in panel (c).
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
32
(a) data
(b) VAE
(c) BVAE
Fig 12: Sphere data
In our study, we have observed distinct differences in the performance
of the bagged Variational Autoencoder (BVAE) compared to the standard
VAE across various data sets, especially those representing 1-dimensional
manifolds. The findings can be summarized as follows:
â€¢ General Observation for 1-D Manifolds in Figure 9 and 10: Across
all the data sets representing 1-dimensional manifolds, the bagged
VAE consistently demonstrates superior capability in reconstructing
the generator. This suggests that the bagging technique enhances the
VAEâ€™s ability to capture and replicate the underlying structure of the
data more effectively than the standard VAE.
â€¢ Swiss Roll Dataset in Figure 11: In the specific case of the Swiss roll
data set, the bagged VAE again shows a notable improvement over the
standard VAE. It demonstrates its ability to reconstruct the manifold
structure with greater fidelity, indicating its enhanced modeling capa-
bility in this more complex data set.
â€¢ Sphere Dataset in Figure 12: The sphere example presents a slightly
different scenario. Here, the difference in performance between bagged
VAE and standard VAE is not as pronounced. While the bagged VAE
still performs marginally better in reconstructing the sphere, the im-
provement is more subtle then in the other cases.
These observations suggest that the bagging technique, when applied to
a VAE, generally enhances the modelâ€™s ability to accurately reconstruct
various types of data structures, particularly in the context of 1-dimensional
manifolds. The enhanced performance of the bagged VAE in most scenarios
indicates its potential as a more robust and effective approach in the field
of generative modeling. However, the slight improvement seen in the sphere
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
33
data set also highlights that the effectiveness of bagging can vary depending
on the specific characteristics of the data set being modeled.
5. Real data analysis: Bagged VAE for the MNIST and
Omniglot datasets
We first consider the well-known MNIST dataset [19]. MNIST consists of
grayscale images of handwritten digits of size 28 Ã— 28, with a training set of
60,000 images and a test set of 10,000 images. We randomly sample 10,000
images from the training set to form a validation set.
In our VAE architecture for images, we utilize convolutional Flipout lay-
ers [31] as variational counterparts of standard convolutional layers, and
transposed convolutional Flipout layers as variational analogs of transposed
convolutional layers in the decoder.
(a) Original
(b) VAE
(c) Bagged VAE (BVAE)
Fig 13: MNIST: original images, standard VAE reconstructions, and bagged
VAE (BVAE) reconstructions.
We compare the standard VAE and the bagged VAE (BVAE) using the
same protocol as in our earlier VAE simulation study. As illustrated in Fig-
ure 13, the bagged VAE reconstructions appear visually cleaner and more
representative of the underlying digit structure. In particular, the BVAE
tends to denoise the images more effectively and emphasize the main strokes
and shapes of each digit, while the standard VAE produces blurrier recon-
structions.
Next, we consider the Omniglot dataset, which consists of handwritten
character images from 50 different alphabets, each of size 28Ã—28. The dataset
contains 24,345 training samples and 8,070 test samples. As with MNIST,
we split the training set into 20,000 images for training and 4,345 images for
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
34
validation. We apply the same VAE and BVAE architectures and training
procedures as in the MNIST experiment.
(a) Original
(b) VAE
(c) BVAE
Fig 14: Omniglot: original images, standard VAE reconstructions, and
bagged VAE (BVAE) reconstructions.
Figure 14 shows that the qualitative behavior observed on MNIST carries
over to Omniglot. The bagged VAE reconstructions more faithfully preserve
fine-grained structural details of the characters and better capture their dis-
tinctive features compared to the standard VAE. Overall, these experiments
suggest that variational bagging can substantially enhance the quality and
robustness of VAE-based generative modeling for image data, resulting in
reconstructions that are both cleaner and more representative of the original
inputs.
Appendix A:
Proofs
A.1. Proof of Theorem 3.1
Define Pn = nâˆ’1 Pn
i=1 Î´Xi and Gn = n1/2(Pn âˆ’P0). Also, define their boot-
strapped versions
Pâˆ—
n = Mâˆ’1
n
X
i=1
KiÎ´Xi
Gâˆ—
n = M1/2(Pâˆ—
n âˆ’Pn)
where (K1, . . . , Kn) âˆ¼Multi(M, 1/n). Under the conditions 1 to 4, we can
apply Lemma 19.31 of [28] with â„“Î¸(x) = log pvb(x|Î¸) to have
Gn
âˆšn(â„“Î¸0+hn/âˆšn âˆ’â„“Î¸0) âˆ’hâŠ¤
n Ë™â„“Î¸0
 P0
â†’0
(11)
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
35
for any sequence hn bounded in P0-probability. Moreover, by Theorem 23.7
of [28], conditionally given X1, X2, . . . , the bootstrap empirical process Gâˆ—
n
and the empirical process Gn converge weakly to the same limiting random
process, thus we also have
Gâˆ—
n
âˆšn(â„“Î¸0+hn/âˆšn âˆ’â„“Î¸0) âˆ’hâŠ¤
n Ë™â„“Î¸0
 P0
â†’0.
This implies that
(nM)1/2Pâˆ—
n(â„“Î¸0+hn/âˆšnâˆ’â„“Î¸0) âˆ’âˆšnhâŠ¤
n (Pâˆ—
n âˆ’Pn) Ë™â„“Î¸0
âˆ’nPn(â„“Î¸0+hn/âˆšn âˆ’â„“Î¸0) P0
â†’0,
where, from (11), the third term of the above display satisfies
nPn(â„“Î¸0+hn/âˆšnâˆ’â„“Î¸0) âˆ’âˆšnhâŠ¤
n (Pn âˆ’P0) Ë™â„“Î¸0
âˆ’nP0(â„“Î¸0+hn/âˆšn âˆ’â„“Î¸0) P0
â†’0
Moreover, by the second-order Taylar expansion in the condition 3, we have
âˆ’P0(â„“Î¸0+hn/âˆšn âˆ’â„“Î¸0) âˆ’1
2nhâŠ¤
n V 0
vbhn = o(1).
Therefore, letting
âˆ†n = n1/2(V 0
vb)âˆ’1 (Pn âˆ’P0) Ë™â„“Î¸0
âˆ†âˆ—
n = n1/2(V 0
vb)âˆ’1 (Pâˆ—
n âˆ’Pn) Ë™â„“Î¸0,
we have for every compact K âŠ‚Î˜,
sup
hâˆˆK
MPâˆ—
n(â„“Î¸0+hn/âˆšn âˆ’â„“Î¸0) âˆ’hâŠ¤ cV 0
vb

(âˆ†n + âˆ†âˆ—
n) âˆ’1
2hâŠ¤ cV 0
vb

h

P+
â†’0.
(12)
Then we can apply the second result of Theorem 3 of [29] with Xâˆ—
1:M in
place of X1:n, Pâˆ—
n in place of Pn, cV 0
vb in place of VÎ¸0, and âˆ†n + âˆ†âˆ—
n in place
of âˆ†n,Î¸0, to obtain that
L(âˆšn(Ï‘â€² âˆ’Î¸0)|Xâˆ—
1:M) âˆ’N(âˆ†n + âˆ†âˆ—
n, (c ËœV 0
vb)âˆ’1)

TV
P0
â†’0
(13)
for Ï‘â€² âˆ¼q(Î¸|Xâˆ—
1:M), where L(âˆšn(Ï‘â€² âˆ’Î¸0)|Xâˆ—
1:M) denotes the conditional raw
of âˆšn(Ï‘â€² âˆ’Î¸0) given Xâˆ—
1:M. Hence, the characteristic function of âˆšn(Ï‘â€  âˆ’
Î¸0) âˆ’âˆ†n | X1:n for Ï‘â€  âˆ¼qbvB(Î¸|X1:n) evaluated at t âˆˆRd can be written as
E

exp

i(âˆ†âˆ—
n)âŠ¤t âˆ’1
2ctâŠ¤( ËœV 0
vb)âˆ’1t

| X1:n

+ Ïµn(t)
(14)
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
36
for some function Ïµn(t) such that lim supnâ†’âˆsupt Ïµn(t) = 0. We can expand
(14) as
E
h
exp
n
in1/2Pâˆ—
n Ë™â„“Î¸0(V 0
vb)âˆ’1t
o
|X1:n
i
exp
n
âˆ’in1/2Pn Ë™â„“Î¸0(V 0
vb)âˆ’1t
o
Ã— exp
n
âˆ’tâŠ¤( ËœV 0
vb)âˆ’1t/2c
o
+ Ïµn(t)
The first line can be written as:
E
ï£®
ï£°exp
ï£±
ï£²
ï£³in1/2Mâˆ’1
n
X
j=1
Kj Ë™â„“Î¸0(Xj)âŠ¤(V 0
vb)âˆ’1t
ï£¼
ï£½
ï£¾|X1:n
ï£¹
ï£»
Ã— exp
n
âˆ’in1/2Pn Ë™â„“Î¸0(V 0
vb)âˆ’1t
o
=
ï£®
ï£°1
n
n
X
j=1
exp
(
in1/2 Ë™â„“Î¸0(Xj)âŠ¤(V 0
vb)âˆ’1t
M
)ï£¹
ï£»
M
exp
n
âˆ’in1/2Pn Ë™â„“Î¸0(V 0
vb)âˆ’1t
o
=
ï£®
ï£°1
n
n
X
j=1
exp
(
in1/2Î´ Ë™â„“Î¸0(Xj)âŠ¤(V 0
vb)âˆ’1t
M
)ï£¹
ï£»
M
where we let Î´ Ë™â„“Î¸0(Xj) = Ë™â„“Î¸0(Xj)âˆ’Pn Ë™â„“Î¸0. By the second-order Taylor expan-
sion, the last display can be further written as
"
1 âˆ’nPn[(Î´ Ë™â„“Î¸0)(Î´ Ë™â„“Î¸0)âŠ¤](V 0
vb)âˆ’1t
2M2
+ Rn
#M
,
where Rn = OP (1/M3) denotes the remainder term. Then since
Pn[(Î´ Ë™â„“Î¸0)(Î´ Ë™â„“Î¸0)âŠ¤] â†’D0
vb
almost surely and M/n â†’c by assumption, the last display converges to
exp

âˆ’1
2ctâŠ¤( ËœV 0
vb)âˆ’1t âˆ’1
2ctâŠ¤(V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1t

(15)
This implies the desired
âˆšn(Ï‘â€  âˆ’Î¸0) âˆ’âˆ†n | X1:n
dâ†’N(0, ( ËœV 0
vb)âˆ’1/c + (V 0
vb)âˆ’1D0
vb(V 0
vb)âˆ’1/c)
by Levyâ€™s continuity theorem.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
37
A.2. Proof of Theorem 3.3
Since Ë†Î¸mle â†’Î¸0 and âˆ†n â†’0 in P0-prabability as well as bÎ£ â†’Î£0 in P0-
prabability by assumption, by Theorem 3.2, we have
PN(0,Î£0)(C(rn,1âˆ’Î±)) = QbvB(Ï‘â€  âˆˆC(rn,1âˆ’Î±)) + oP0(1)
= 1 âˆ’Î± + oP0(1),
where we denote by PN(0,Î£) the probability measure under the normal dis-
tribution N(0, Î£). Therefore by the continuous mapping theorem, r2
n,1âˆ’Î± â†’
Ï‡2
d,1âˆ’Î± in P0-probability, where Ï‡2
d,1âˆ’Î± denotes the 1âˆ’Î± quntitle of the Ï‡2(d)
distribution and d denotes the dimension of the parameter Î¸. We use this
fact to get, letting S0 = (V 0)âˆ’1D0(V 0)âˆ’1 for notational simplicity,
P0(Î¸0 âˆˆC(rn,1âˆ’Î±)) = P0(âˆšn(Ë†Î¸mle âˆ’Î¸0) âˆˆ{u : uâŠ¤Î£âˆ’1
0 u â‰¤r2
n,1âˆ’Î±}) + o(1)
= P0(âˆšn(Ë†Î¸mle âˆ’Î¸0) âˆˆ{u : uâŠ¤Î£âˆ’1
0 u â‰¤Ï‡2
d,1âˆ’Î±}) + o(1)
= PUâˆ¼N(0,S0)(UâŠ¤Î£âˆ’1
0 U â‰¤Ï‡2
d,1âˆ’Î±) + o(1)
where the last equality holds due to
âˆšn(Ë†Î¸mle âˆ’Î¸0) dâ†’N(0, S0).
Therefore, since Î£0 âˆ’S0 = ( ËœV 0)âˆ’1 is non-negative definite, so is Sâˆ’1
0
âˆ’Î£âˆ’1
0 ,
we have
PUâˆ¼N(0,S0)(UâŠ¤Î£âˆ’1
0 U â‰¤Ï‡2
d,1âˆ’Î±)
â‰¥PUâˆ¼N(0,S0)(UâŠ¤Sâˆ’1
0 U â‰¤Ï‡2
d,1âˆ’Î±) = 1 âˆ’Î±,
which completes the proof.
A.3. Proof of Theorem 3.5
Before giving the proof, we introduce additional notations.
Let pÎ¸,K1:n(X1:n) = Qn
i=1 p(Xi|Î¸)Ki and p0,K1:n(X1:n) = Qn
i=1 p(Xi|Î¸)Ki
be denote the density of the bootstrapped sample Xâˆ—
1:n with the â€œboot-
strap weightâ€ K1:n = (K1, . . . , Kn) âˆ¼Multi(n, 1/n), under PÎ¸ and P0, re-
spectively. Then the posterior given the bootstrapped sample can be writ-
ten as Ï€(Î¸|Xâˆ—
1:n) = pÎ¸,K1:n(X1:n)Ï€(Î¸)/pÎ ,K1:n(X1:n) with pÎ ,K1:n(X1:n) =
R Qn
i=1 p(Xi|Î¸)KiÏ€(Î¸)dÎ¸. In what follows, the probability measure P and
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
38
the expectation operator E are taken on the randomness of both the sample
X1:n and the bootstrap weights K1:n.
Let Qâˆ—be the variational posterior obtained with a bootstrapped sample
Xâˆ—
1:n. The proof is done if we show that the contraction rate of Qâˆ—is given
by Ïµn
âˆšlog n. We start with observing that, by Donsker and Varadhanâ€™s
variational inequality (e.g., Lemma J.1 of [21]),
Q(A) â‰¤1
t {KL(Q, Î (Â·|Xâˆ—
1:n)) + etÎ (A|Xâˆ—
1:n)}
(16)
for any distribution Q on Î˜, any event A âŠ‚Î˜ and any positive number t. We
apply the above display to Q = Qâˆ—and A = An = {Î¸ âˆˆÎ˜ : H2(PÎ¸, P0) â‰¥Î·2
n}
with Î·2
n = MnÏµ2
n log n.
Next, we define the event
Gn =
Z pÎ¸,K1:n(X1:n)
p0,K1:n(X1:n)Ï€(Î¸)dÎ¸ â‰¥exp(âˆ’(C1 + 2)nÏµ2
n)

.
We have
E

log pÎ¸,K1:n(X1:n)
p0,K1:n(X1:n)

= nKL(P0, PÎ¸)
and
V ar

log pÎ¸,K1:n(X1:n)
p0,K1:n(X1:n)

â‰¤
n
X
i=1
E
"
Ki log p(Xi|Î¸)
p0(Xi)
2#
=
n
X
i=1
E(K2
i )KLV(P0, PÎ¸)
â‰¤2nKLV(P0, PÎ¸),
where the equality follows from the independence of Ki and Xi. Hence, by
a standard argument for bounding the probability of Gn under the prior
mass condition (e.g., Lemma 8.10 of [8]), we have P(Gc
n) â‰¤2/(nÏµ2
n) â†’0.
Therefore, it suffices to bound the quantity E(Qâˆ—(An)I(Gn)), which, from
the inequality in (16), is further bounded by
E(Qâˆ—(An)I(Gn))
â‰¤1
t {E[KL(Qâˆ—, Î (Â·|Xâˆ—
1:n))I(Gn)] + etE[Î (An|Xâˆ—
1:n)I(Gn)]}.
(17)
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
39
For the first term in (17), we have
E[KL(Qâˆ—, Î (Â·|Xâˆ—
1:n))I(Gn)] â‰¤E[KL(Qâˆ—, Î (Â·|Xâˆ—
1:n)]
= E
Z 
log qâˆ—(Î¸)
Ï€(Î¸) + log p0,K1:n(X1:n)
pÎ¸,K1:n(X1:n)

dQâˆ—(Î¸) + log pÎ ,K1:n(X1:n)
p0,K1:n(X1:n)

â‰¤E
Z 
log q(Î¸)
Ï€(Î¸) + log p0,K1:n(X1:n)
pÎ¸,K1:n(X1:n)

dQ(Î¸) + log pÎ ,K1:n(X1:n)
p0,K1:n(X1:n)

= KL(Q, Î ) + n
Z
KL(PÎ¸, P0)dQ(Î¸) + E

log pÎ ,K1:n(X1:n)
p0,K1:n(X1:n)

for any Q âˆˆQ, where we use the optimization optimality of Qâˆ—in the third
line and use the assumption Pn
i=1 Ki = n in the last line. Moreover, by
Jensenâ€™s inequality together with that Pn
i=1 Ki = n, we have
E

log pÎ ,K1:n(X1:n)
p0,K1:n(X1:n)

= E

log pÎ ,K1:n(X1:n)
Qn
i=1 p0(Xi) + log
Qn
i=1 p0(Xi)
p0,K1:n(X1:n)

â‰¤E

log
Qn
i=1 p0(Xi)
p0,K1:n(X1:n)

= E
" n
X
i=1
(1 âˆ’Ki) log p0(Xi)
#
= 0.
Therefore, by our variational family assumption,
E[KL(Qâˆ—, Î (Â·|Xâˆ—
1:n)] â‰¤Câ€²
1nÏµ2
n
(18)
for some constant Câ€²
1 > 0.
Now we focus on the second term in (17). We can bound it as
Î (An|Xâˆ—
1:n)I(Gn) â‰¤Î (An âˆ©Î˜n|Xâˆ—
1:n)I(Gn) + Î (Î˜c
n|Xâˆ—
1:n)I(Gn).
The expectation of the second term in the above display satisfies
E[Î (Î˜c
n|Xâˆ—
1:n)I(Gn)] â‰¤e(2+C1)nÏµ2
nÎ (Î˜c
n) â‰¤eâˆ’2nÏµ2
n â†’0
by the second sieve assumption. On the other hand, for the first term, we
need the following technical result on the bootstrap weight. Since Ki âˆ¼
Binom(n, 1/n), by Markovâ€™s inequality
P(Ki > 2 log n) â‰¤eâˆ’2 log nE(exp(Ki))
= eâˆ’2 log n(1 âˆ’nâˆ’1 + nâˆ’1e)n
â‰¤eâˆ’2 log neeâˆ’1 = eeâˆ’1/n2,
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
40
where the last inequality follows from the inequality 1 + x â‰¤ex for any
x âˆˆR. Thus, by the union bound, we have
P( max
1â‰¤iâ‰¤n Ki > 2 log n) â‰¤eeâˆ’1
n
(19)
which tends to zero as n â†’âˆ. Then, following the proof strategy of [11],
we define
UÎ¸(X) = max

log p(X|Î¸)
p0(X) , âˆ’Ï„

for Ï„ > 0. Then we consider the weighted likelihood ratio empirical process
defined as
Î½n(Î¸) =
1
âˆšn
n
X
i=1
[KiUÎ¸(Xi) âˆ’E(KiUÎ¸(Xi))]
Then by Lemma 12 of [11] together with the fact given in (19) and our
complexity assumption, we have
P
 
sup
Î¸âˆˆÎ˜n:H(PÎ¸,P0)â‰¤
âˆš
2Ïµ
Î½n(Î¸) â‰¥k
2
âˆšnÏµ2
!
â‰¤3 exp

âˆ’Câ€²
2
nÏµ2
1 + 2 log n

(20)
for any Ïµ > Ïµn for some constants Câ€²
2 > 0 and k âˆˆ(1/2, 1). By Lemma
4 of [33], E(Î½n(Î¸)) â‰¤âˆ’(1 âˆ’Î´0)H2(PÎ¸, P0) with Î´ = 2 exp(âˆ’Ï„/2)/(1 âˆ’
exp(âˆ’Ï„/2))2. Let Î˜n(Ïµ;
âˆš
2Ïµ) = {Î¸ âˆˆÎ˜n : Ïµ â‰¤H(PÎ¸, P0) â‰¤
âˆš
2Ïµ}. Then
we have
B(Ïµ;
âˆš
2Ïµ) =
(
sup
Î¸âˆˆÎ˜n(Ïµ;
âˆš
2Ïµ)
pÎ¸,K1:n(X1:n)
p0,K1:n(X1:n) â‰¥exp(âˆ’nÏµ2(1 âˆ’Î´0 âˆ’k/2))
)
âŠ‚
(
sup
Î¸âˆˆÎ˜n(Ïµ;
âˆš
2Ïµ)
Î½n(Î¸) â‰¥k
2
âˆšnÏµ2
)
Hence, by (20), the probability of the event B(Ïµ;
âˆš
2Ïµ) is bounded above
by 3 exp

âˆ’Câ€²
1
nÏµ2
1+2 log n

. Let J be the smallest integer such that 2JÎ·2
n â‰¥4.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
41
Then by using a peeling technique, we have
P
 
sup
Î¸âˆˆÎ˜n:H(PÎ¸,P0)â‰¥Î·n
pÎ¸,K1:n(X1:n)
p0,K1:n(X1:n) â‰¥exp(âˆ’nÎ·2
n(1 âˆ’Î´0 âˆ’k/2)
!
â‰¤
J
X
j=0
P

B(
âˆš
2jÎ·n;
âˆš
2j+1Î·n)

â‰¤
J
X
j=0
3 exp

âˆ’Câ€²
2
n2jÎ·2
n
1 + 2 log n

â‰¤4 exp

âˆ’Câ€²
2
nÎ·2
n
1 + 2 log n

â‰¤4 exp

âˆ’1
3Câ€²
2MnnÏµ2
n

.
Therefore, E[Î (An âˆ©Î˜n|Xâˆ—
1:n)I(Gn)] â‰¤exp(âˆ’Câ€²
3nÎ·2
n) for some constant
Câ€²
3 > 0. Combining these derivations together, we have that by taking
t = Câ€²
4nÎ·2
n with Câ€²
4 being a positive constant less than Câ€²
3, the term (17)
converges to zero, which proves that the variational posterior Qâˆ—with a sin-
gle bootstrapped sample contracts to P0 at a rate Î·n.
References
[1] Pierre Alquier and James Ridgway. Concentration of tempered poste-
riors and of their variational approximations. The Annals of Statistics,
48(3):1475â€“1497, 2020.
[2] Matthew J. Beal and Zoubin Ghahramani. Variational Bayesian learn-
ing of directed graphical models with hidden variables. Bayesian Analy-
sis, 1(4):793 â€“ 831, 2006. . URL https://doi.org/10.1214/06-BA126.
[3] David M. Blei and Michael I. Jordan. Variational inference for Dirichlet
process mixtures.
Bayesian Analysis, 1(1):121 â€“ 143, 2006.
.
URL
https://doi.org/10.1214/06-BA104.
[4] Leo Breiman. Bagging predictors. Machine Learning, 24(2):123â€“140,
1996.
[5] Peter BÂ¨uhlmann. Discussion of big bayes stories and bayesbag. Sta-
tistical Science, 29(1):91â€“94, 2014.
ISSN 08834237, 21688745.
URL
http://www.jstor.org/stable/43288454.
[6] Minwoo Chae, Dongha Kim, Yongdai Kim, and Lizhen Lin. A likelihood
approach to nonparametric estimation of a singular distribution using
deep generative models. J. Mach. Learn. Res., 24:1â€“42, 2023.
[7] Alp Kucukelbir David M. Blei and Jon D. McAuliffe. Variational in-
ference: A review for statisticians. Journal of the American Statistical
Association, 112(518):859â€“877, 2017.
.
URL https://doi.org/10.
1080/01621459.2017.1285773.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
42
[8] Subhashis Ghosal and Aad Van der Vaart. Fundamentals of nonpara-
metric Bayesian inference, volume 44.
Cambridge University Press,
2017.
[9] Subhashis Ghosal, Jayanta K Ghosh, and Aad van der Vaart. Conver-
gence rates of posterior distributions. The Annals of Statistics, 28(2):
500â€“531, 2000.
[10] Ryan Giordano, Tamara Broderick, and Michael I. Jordan.
Covari-
ances, robustness, and variational bayes. J. Mach. Learn. Res., 19:51:1â€“
51:49, 2017.
URL https://api.semanticscholar.org/CorpusID:
53238793.
[11] Wei Han and Yun Yang. Statistical inference in mean-field variational
bayes. arXiv preprint arXiv:1911.01525, 2019.
[12] Jonathan H Huggins and Jeffrey W Miller. Robust inference and model
criticism using bagged posteriors.
arXiv preprint arXiv:1912.07104,
2019.
[13] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and
Lawrence K. Saul. An introduction to variational methods for graphical
models, page 105â€“161. MIT Press, Cambridge, MA, USA, 1999. ISBN
0262600323.
[14] Anya Katsevich and Philippe Rigollet.
On the approximation accu-
racy of gaussian variational inference. arXiv preprint arXiv:2306.00052,
2023. URL https://arxiv.org/abs/2306.00052.
[15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Representations,
2014.
[16] Diederik P Kingma and Max Welling.
Auto-Encoding Variational
Bayes. arXiv e-prints, art. arXiv:1312.6114, December 2013. .
[17] Bas JK Kleijn and Aad W van der Vaart. The bernstein-von-mises
theorem under misspecification. Electronic Journal of Statistics, 6:354â€“
381, 2012.
[18] Michael Kohler and Sophie Langer. On the rate of convergence of fully
connected deep neural network regression estimates.
The Annals of
Statistics, 49(4):2231â€“2249, 2021.
[19] Yann LeCun, LÂ´eon Bottou, Yoshua Bengio, and Patrick Haffner.
Gradient-based learning applied to document recognition. Proceedings
of the IEEE, 86(11):2278â€“2324, 1998.
[20] Dennis Nieman, Botond Szabo, and Harry van Zanten. Uncertainty
quantification for sparse spectral variational approximations in Gaus-
sian process regression. Electronic Journal of Statistics, 17(2):2250 â€“
2288, 2023. . URL https://doi.org/10.1214/23-EJS2155.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
43
[21] Ilsang Ohn and Lizhen Lin. Adaptive variational bayes: Optimality,
computation and applications. The Annals of Statistics, 52(1):335â€“363,
2024.
[22] John T Ormerod, Chong You, and Samuel MÂ¨uller. A variational bayes
approach to variable selection. 2017.
[23] Debdeep Pati, Anirban Bhattacharya, and Yun Yang. On statistical
optimality of variational Bayes. pages 1579â€“1588, 2018.
[24] Kolyan Ray and Botond SzabÂ´o and.
Variational bayes for high-
dimensional linear regression with sparse priors. Journal of the Amer-
ican Statistical Association, 117(539):1270â€“1281, 2022. . URL https:
//doi.org/10.1080/01621459.2020.1847121.
[25] Xiaotong Shen and Larry Wasserman. Rates of convergence of posterior
distributions. Annals of Statistics, pages 687â€“714, 2001.
[26] Yee Teh, David Newman, and Max Welling.
A collapsed varia-
tional bayesian inference algorithm for latent dirichlet allocation. In
B. SchÂ¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neu-
ral Information Processing Systems, volume 19. MIT Press, 2006.
URL https://proceedings.neurips.cc/paper_files/paper/2006/
file/532b7cbe070a3579f424988a040752f2-Paper.pdf.
[27] Sattar Vakili, Jonathan Scarlett, Da shan Shiu, and Alberto Bernac-
chia. Improved convergence rates for sparse approximation methods
in kernel-based learning.
In International Conference on Machine
Learning, 2022. URL https://api.semanticscholar.org/CorpusID:
246652434.
[28] Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge
university press, 2000.
[29] Yixin Wang and David Blei. Variational bayes under model misspecifi-
cation. Advances in Neural Information Processing Systems, 32, 2019.
[30] Yixin Wang and David M Blei. Frequentist consistency of variational
bayes. Journal of the American Statistical Association, 114(527):1147â€“
1161, 2019.
[31] Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
Flipout: Efficient pseudo-independent weight perturbations on mini-
batches. arXiv preprint arXiv:1803.04386, 2018.
[32] T Westling and TH McCormick. Beyond prediction: A framework for
inference with variational approximations in mixture models. Journal
of Computational and Graphical Statistics, 28(4):778â€“789, 2019.
[33] Wing Hung Wong and Xiaotong Shen. Probability inequalities for like-
lihood ratios and convergence rates of sieve mles. The Annals of Statis-
tics, pages 339â€“362, 1995.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025

Fan, Ohn, Dunson and Lin/Variational bagging
44
[34] Yun Yang, Debdeep Pati, and Anirban Bhattacharya.
Î±-variational
inference with statistical guarantees. The Annals of Statistics, 48(2):
886â€“905, 2020.
[35] Fengshuo Zhang and Chao Gao. Convergence rates of variational pos-
terior distributions. The Annals of Statistics, 48(4):2180 â€“ 2207, 2020.
imsart-generic ver. 2020/08/06 file: main-new.tex date: November 26, 2025
