SRA-CP: Spontaneous Risk-Aware Selective Cooperative
Perception
Jiaxi Liua, Chengyuan Ma*a, Hang Zhoua, Weizhe Tanga, Shixiao Lianga, Haoyang Dingb,
Xiaopeng Lia and Bin Rana
aDepartment of Civil and Environmental Engineering, University of Wisconsin-Madison, Madison, 53706, Wisconsin, United States
bSchool of Computer, Data and Information Sciences, University of Wisconsin-Madison, Madison, 53706, Wisconsin, United States
A R T I C L E I N F O
Keywords:
Cooperative perception
Vehicle-to-Everything
Object detection
Blind spot analysis
Spontaneous Risk-Aware Selective
Cooperative Perception (SRA-CP)
A B S T R A C T
Cooperative perception (CP) offers significant potential to overcome the limitations of single-
vehicle sensing by enabling information sharing among connected vehicles (CVs). However,
existing generic CP approaches need to transmit large volumes of perception data that are
irrelevant to the driving safety, exceeding available communication bandwidth. Moreover,
most CP frameworks rely on pre-defined communication partners, making them unsuitable
for dynamic traffic environments. This paper proposes a Spontaneous Risk-Aware Selective
Cooperative Perception (SRA-CP) framework to address these challenges. SRA-CP introduces
a decentralized protocol where connected agents continuously broadcast lightweight perception
coverage summaries and initiate targeted cooperation only when risk-relevant blind zones are
detected. A perceptual risk identification module enables each CV to locally assess the impact
of occlusions on its driving task and determine whether cooperation is necessary. When CP
is triggered, the ego vehicle selects appropriate peers based on shared perception coverage
and engages in selective information exchange through a fusion module that prioritizes safety-
critical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset
against several representative baselines. Results show that SRA-CP achieves less than 1% average
precision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of
the communication bandwidth. Moreover, it improves the perception performance by 15% over
existing selective CP methods that do not incorporate risk awareness.
1. Introduction
Multi-agent cooperative perception (CP) has emerged as a promising paradigm to overcome the limitations of
single-agent sensing by enabling agents to share information with each other. In road traffic environments, a single
vehicleâ€™s sensing capability is often obstructed by occlusions, resulting in blind zones that lead to hesitation in
decision-making and increased collision risk with surrounding participants. These issues are particularly pronounced
in scenarios such as unprotected left turns or pedestrians suddenly appearing from behind parked vehicles. With the
rapid advancement of connected and automated vehicle (CAV) technologies, ensuring safe and complete perception
becomes even more critical, especially for autonomous driving in complex environments [Zha et al., 2025]. In such
contexts, CP offers valuable potential to enhance safety.
However, most existing CP studies remain limited to simulations or small-scale experimental setups conducted
under ideal and controlled conditions. Achieving large-scale deployment of CP in real-world traffic still faces two major
challenges. The first challenge lies in the gap between the massive volume of sensing data generated by CAVs and the
limited bandwidth of vehicular communication networks [Hu et al., 2022]. For example, intermediate features extracted
from onboard sensors at a rate of 5â€“20 Hz can produce up to 2MB per frame, translating to a potential transmission
rate of 300 Mbps. Such data loads are far beyond what even advanced wireless systems (e.g., 5G) can support in
dense environments, especially when vehicles attempt to transmit full perception data simultaneously, as the Generic
CP shown in Figure 1 (a). Moreover, the problem is exacerbated in real-world traffic, where the number of dynamic
agents is large and constantly changing. If every pair of agents were required to maintain real-time communication,
the bandwidth burden would grow quadratically with the number of agents. In reality, most of the information being
shared is unnecessaryâ€”an individual vehicleâ€™s local perception is often sufficient for safe driving in the majority of
âˆ—Corresponding author
ORCID(s): 0009-0001-2749-6435 (J. Liu); 0000-0002-6337-0450 (C. Ma*); 0000-0003-3286-341X (H. Zhou);
0000-0002-5264-3775 (X. Li)
Jiaxi Liu: Preprint submitted to Elsevier
Page 1 of 22
arXiv:2511.17461v1  [cs.AI]  21 Nov 2025

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 1: Comparison between (a) Generic CP with full-time information exchange VS the proposed Risk-aware selective
CP activated by risky blind-spot events; and (b) Pre-arranged CP constrained by predefined communication partners VS
Spontaneous CP enabling dynamic ad-hoc cooperation in arbitrary encounter situations.
situations. Even in CP-required cases, not all detected elements need to be transmitted. A more efficient strategy is
to share only the information that is both unseen by the receiving vehicle and potentially hazardous to its driving
decisions. This observation motivates the concept of Risk-aware selective CP, where each vehicle evaluates the risk
level of objects it perceives and only transmits those that satisfy two conditions simultaneously: (i) the object lies within
another vehicleâ€™s blind zone, and (ii) the object poses a potential safety risk. For instance, in Figure 1, a parked roadside
vehicle obstructs a portion of the scene, and the oncoming vehicle provides supplementary information to complete
perception. While recent studies have explored selective CP strategies that only share blind-zone contentâ€”resulting in
significant communication reduction compared to generic CP [Qiu et al., 2025]â€”they still do not account for the traffic
risk relevance of shared content. Our previous work validated that only a small fraction (0.1%) of driving scenarios
actually require CP [Ma et al., 2025]. Thus, the insight behind risk-aware selective CP serves as the foundation for the
communication-efficient strategy proposed in this study.
The second challenge concerns how to construct communication pathways in a scalable and dynamic traffic system.
To the best of our knowledge, most existing CP studies are conducted within Pre-arranged communication zones and
among predefined partner vehicles, as illustrated in Figure 1(b). While pre-arranged communication enables stable
point-to-point connections under idealized network assumptionsâ€”and even allows for global optimization of com-
munication topologies and grouping strategies [Dong et al., 2022]â€”these setups are typically limited to experimental
testbeds with a fixed number of specified agents. They fail to generalize to open-world traffic environments, where a
small number of unfamiliar connected vehicles may encounter each other spontaneously across large spatial areas and
at unpredictable times. This limitation highlights the urgent need for a decentralized and self-organized communication
mechanism. To address this, we propose a Spontaneous CP framework that builds upon the selective CP principle.
Each connected vehicle operates independently, broadcasting minimal information about its own perception coverage.
Only when a risk-relevant blind zone is detected does it initiate a CP request. Neighboring vehicles, upon receiving
the request, respond cooperatively if they are capable of contributing, as illustrated in Figure 1(b). This mechanism
leverages a key principle: connected vehicles can identify whether they can be assisted in blind-zone completion by
evaluating other vehiclesâ€™ relative positions and their shared perception coverage. As a result, the handshake process
is realized through lightweight broadcasting during regular operation and precise, selective information exchange
triggered only when necessary.
To bridge the above two gaps, we propose a Spontaneous Risk-Aware Selective Cooperative Perception (SRA-
CP) framework. It introduces a spontaneous collaboration mechanism composed of two modes: a routine mode, where
vehicles continuously broadcast only their perception coverage maps; and a triggered mode, where a vehicle initiates
CP only when it detects a risk-relevant blind zone, and neighboring connected agents are capable of assisting. Built
upon this mechanism, we develop a risk-aware hierarchical perception fusion model that ensures efficient CP by
adaptively prioritizing critical information within any available communication bandwidth. The model consists of
four key components: a shared feature encoder, a risk-aware communication module, a dual-attention fusion network,
and a multi-task decoder. This architecture enables vehicles to selectively fuse the most important perceptual features
based on spatial occlusion and safety relevance under any given communication constraints. We evaluate the proposed
Jiaxi Liu: Preprint submitted to Elsevier
Page 2 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
framework on a public dataset by comparing it against three baselines: generic CP, a state-of-the-art selective CP
method without risk awareness, and a no-CP setup, in terms of communication cost and Average Precision (AP) for
key-object detection. Results demonstrate that SRA-CP achieves comparable or superior performance with significantly
reduced communication overhead. Specifically, compared to generic CP, SRA-CP reduces the transmission volume to
80% while incurring only a 0.1 drop in AP. Compared to the selective CP baseline without risk modeling, SRA-CP
improves AP for critical objects by 15% under the same communication budget, showcasing its potential for scalable
deployment in large-scale, dynamic traffic environments.
The main contributions of this paper are as follows:
â€¢ We address the bandwidth bottleneck in multi-agent CP by proposing a risk-aware selective CP strategy, which
prioritizes the transmission of perceptual elements based on their impact on driving safety. This approach enables
efficient use of limited communication resources under varying bandwidth constraints.
â€¢ We propose the Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework, which
supports dynamic, on-demand handshakes between agents without predefined regions or communication
partners. This design enables scalable and low-cost CP in large-scale, real-world traffic environments with self-
organizing connected agents.
â€¢ We validate the proposed framework on a public dataset and show that SRA-CP achieves comparable
performance to generic CP while using only 20% of the communication volume, with less than 1% drop in
AP. Compared to a cutting-edge selective CP baseline that does not consider driving risk, SRA-CP improves
critical object detection accuracy by 15%.
2. Related work
2.1. Cooperative Perception (CP)
CP allows multiple agents to share perceptual information to achieve a more complete understanding of their
surroundings. This paradigm addresses key limitations of single-agent perception such as occlusion and limited sensing
range [Chen et al., 2019a, Liu et al., 2020]. There could be different downstream perception tasks to be fulfilled with
CP, such as 3D object detection [Xu et al., 2023, Li and Pei, 2024, Xiang et al., 2024, Yu et al., 2023], lane detection
[Jahn et al., 2024, El Boukili et al., 2025], object tracking [Chiu et al., 2024, Zimmer et al., 2024, Zhong et al., 2025].
CP has been implemented through various fusion schemes, including early [Chen et al., 2019b, Yang et al., 2025],
intermediate [Liu et al., 2020, Wang et al., 2020, Xu et al., 2022a], and late fusion [Liu et al., 2024, Sarlak et al.,
2025]. Early fusion directly shares raw sensor data (e.g., LiDAR point clouds or images), aligns them in a common
coordinate frame, and jointly processes the fused measurements through a single perception network. Intermediate
fusion exchanges intermediate feature maps extracted by each agentâ€™s backbone. These features are spatially aligned
and combined before the detection head. Late fusion transmits only high-level perception outputs such as object boxes
or tracks, which are then associated and merged at the cooperative layer, offering low communication cost at the expense
of reduced ability to recover missed local detections.
Although these schemes achieve a certain trade-off between communication cost and perception fusion perfor-
mance, the widespread transmission of perceptual information in multi-agent scenarios remains a significant challenge.
2.2. Selective Information Sharing
A major challenge in CP is reducing the communication burden while maintaining perception quality. Recent work
such as Where2comm [Hu et al., 2022] has proposed to use spatial confidence maps to identify perceptually important
regions and selectively transmit only the features from those areas. This strategy improves the perception accuracy
under limited bandwidth by avoiding indiscriminate sharing of all information. There are also other CP works that
share the same thoughts [Yang et al., 2023a, Liu et al., 2020, Yang et al., 2023b]. However, perceptual accuracy alone
is not a sufficient criterion in driving scenarios. In practice, much of the perceptual improvement may not contribute
to driving decisions or safety [Ma et al., 2025, Van Brummelen et al., 2018, Pan et al., 2024, Gao et al., 2024]. For
example, perceiving a distant vehicle with higher precision may not change the ego vehicleâ€™s behavior. Thus, a key
limitation of current selective strategies is the lack of risk-awareness. They fail to distinguish between information that
is perceptually useful and information that is safety-critical. A risk-aware selection mechanism is needed to ensure that
perception elements that are both unseen and pose potential safety risks are shared.
Jiaxi Liu: Preprint submitted to Elsevier
Page 3 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 2: Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP)
2.3. Communication Paradigms for CP
The communication paradigm for CP is also an important topic in CP. In most cases, prior studies assume a limited
and fixed set of collaborators operating within a bounded area. These assumptions simplify the design of interaction
protocols and enable direct coordination [Feng et al., 2018, Yu et al., 2019]. Most existing CP frameworks also adopt
such settings [Chen et al., 2019a, Liu et al., 2020]. However, these conditions are difficult to satisfy in real-world
traffic environments, where agents are numerous, highly dynamic, and distributed across large spatial regions. In
addition, only a small fraction of encounters truly require cooperation, and participating agents are often unfamiliar
with one another. These limitations highlight the need for a scalable, deployable, and self-organized CP communication
paradigm. Recent work in agentic AI has begun to explore the notion of spontaneous cooperation [Wu et al., 2024,
Godhwani et al., 2025, Mirsky et al., 2022], where collaboration emerges dynamically based on local context and shared
objectives. Building on this insight, our proposed SRA-CP framework leverages the property that connected vehicles
can share their perception coverage, allowing other agents to identify opportunities to fulfill blind-zone completion
needs. This enables the spontaneous formation of cooperation links without prior coordination or global knowledge.
3. Problem Description
As illustrated in Figure 2, we consider a dynamic road network with multiple Connected Vehicle (CV) agents
indexed by ğ‘’, ğ‘—, ğ‘˜, in which ğ‘’denotes the ego vehicle. At a certain time ğ‘¡(all variables hereafter are defined at time
ğ‘¡unless otherwise specified), each vehicle has a physical state represented by its position ğ©ğ‘’= (ğ‘¥ğ‘’, ğ‘¦ğ‘’) and velocity
ğ¯ğ‘’= (ğ‘£ğ‘¥
ğ‘’, ğ‘£ğ‘¦
ğ‘’), taking ego vehicle as an example. For a given connected agent ğ‘’, its perception range from a birdâ€™s-eye
view (e.g., the spatial coverage of LiDAR) is denoted by îˆ¼ğ‘’, with the corresponding sensed information ğš½(îˆ¼ğ‘’). The
blind zoneâ€”areas not observable by the agentâ€”is denoted as îˆ»ğ‘’. In most cases, such blind zones have a negligible
impact on driving safety, as illustrated by îˆ»ğ‘—and îˆ»ğ‘˜in Figure 2. However, in some cases, such as a vehicle approaching
from the opposite direction within îˆ»ğ‘’that affects a left-turn decision, the blind zone can pose a significant risk. This
study focuses on identifying such risky blind zones and selectively completing them via CP with limited communication
bandwidth.
We design the SRA-CP framework in which the ego connected vehicle ğ‘’continuously broadcasts its own position
ğ©ğ‘’, velocity ğ¯ğ‘’, and perception coverage îˆ¼ğ‘’(requiring only low bandwidth). It then receives broadcasted data from
nearby vehicles within its communication zone î‰†ğ‘’â€”a circular area of radius ğ‘™ğ‘â€”which includes the set of neighboring
agents î‰ƒğ‘’. Based on this shared information, the agent determines whether it has a risky blind zone that can be
supplemented by any surrounding vehicle, and if so, initiates a spontaneous CP handshake and performs cooperative
Jiaxi Liu: Preprint submitted to Elsevier
Page 4 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 3: End-to-end architecture of Selective Information Sharing and Fusion. Each co-operative vehicle ğ‘–âˆˆ{ğ‘’, ğ‘˜, ğ‘—}
projects its raw point cloud ğš½(îˆ¼ğ‘–
) to the ego Birdâ€™s-Eye-View (BEV) frame and encodes it through a shared feature-encoder,
yielding ğ¹ğ‘–. Risk-aware communication (Sec. 4.3.2) attaches two light-weight masksâ€”the spatial mask ğ‘†ğ‘–and the risk
mask ğ‘…ğ‘–â€”to the feature map and broadcasts only these three tensors, avoiding transmission of raw point clouds. The ego
car receives the partner streams and performs dual-attention feature fusion (Sec. 4.3.3): a safety-focused selector prunes
partner features with (ğ‘†ğ‘–, ğ‘…ğ‘–) and a location-wise multi-head attention block aligns the surviving cells with the ego map
ğ¹ğ‘’, producing Ìƒğ¹ğ‘’. Finally, two heads operate on Ìƒğ¹ğ‘’: (i) a Risk Decoder refines a dense risk heat-map, and (ii) a Detection
Decoder outputs 3-D bounding boxes.
fusion. Specifically, the ego connected vehicle ğ‘’first evaluates the risk level of its perception blind zones using the
proposed perceptual risk identification model, resulting in a perceptual risk matrix îˆ¾ğ‘’. If no risky blind zones are
detected (e.g., as in the case of other CVs ğ‘—and ğ‘˜), the process at ğ‘¡terminates. If risky blind zones are identified,
the vehicle proceeds to select an appropriate target connected agent. Based on the shared perception coverage from
neighboring agents (e.g., îˆ¼ğ‘—and îˆ¼ğ‘˜), the vehicle determines whether any connected agent can compensate for its
occluded regions (e.g., agent ğ‘—in the illustrated case). Note that if no suitable connected agents are available to provide
blind-zone compensation, the ego vehicle relies solely on its own onboard perception for decision-makingâ€”e.g., by
stopping to continue observation. Once a candidate is selected, ego vehicle ğ‘’sends a CP request to agent ğ‘—, including
its current position ğ©ğ‘’, blind zone îˆ»ğ‘’, and the computed risk matrix îˆ¾ğ‘’. Upon receiving the request, agent ğ‘—invokes
the proposed selective information sharing model, which, under the given bandwidth constraint ğµbytes, selects and
transmits the most informative features ğ¹ğ‘—,ğ‘’to supplement agent ğ‘’â€™s perception. Finally, agent ğ‘’performs cooperative
fusion via a dual-attention feature fusion model to integrate the received features and complete its understanding of
the occluded region.
In this study, we focus on CP using LiDAR data, which provides accurate geometric structure, consistent
performance under varying illumination, and reliable spatial measurements for dynamic traffic environments. These
properties make LiDAR particularly suitable for blind-zone estimation and risk-aware perception. It is worth noting
that the proposed framework is modality-agnostic. Although our implementation uses LiDAR as the primary sensing
modality, the methodology can be extended to other perception inputs, such as video data.
3.1. Notation and Symbols
Jiaxi Liu: Preprint submitted to Elsevier
Page 5 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Table 1: Notation used throughout the paper (unified).
Symbol
Type
Meaning / Unit
Sets, indices, regions
ğ‘¡
scalar
Time index.
ğ‘–, ğ‘—, ğ‘˜
index
Generic agent indices.
ğ‘’
index
Ego agent.
ğ©ğ‘–= (ğ‘¥ğ‘–, ğ‘¦ğ‘–)
vector
2D position of agent ğ‘–in BEV/ego frame (at time ğ‘¡, unless stated otherwise).
ğ¯ğ‘–= (ğ‘£ğ‘¥
ğ‘–, ğ‘£ğ‘¦
ğ‘–)
vector
2D velocity of agent ğ‘–.
îˆ¼ğ‘–
set
Perception coverage (field of view, FoV) of agent ğ‘–.
ğš½(â‹…)
map
Region â†’perceived sensory information; e.g., ğš½(îˆ¼ğ‘–) is the perception information of
agent ğ‘–.
îˆ»ğ‘–
set
Blind zone of agent ğ‘–.
î‰†ğ‘–
set
Local communication region of agent ğ‘–(disk of radius ğ‘™ğ‘).
ğ‘™ğ‘
scalar
Communication radius defining î‰†ğ‘–(m).
î‰ƒğ‘–
set
Neighboring connected agents within î‰†ğ‘–.
SRA-CP protocol
ğœŒğ‘–,ğ‘—
scalar
Pairwise collision risk score between ğ‘–and ğ‘—.
ğœğ‘Ÿ
scalar
Threshold on ğœŒto trigger risk-aware sharing.
îˆ¾
matrix
Risk matrix collecting ğœŒğ‘–,ğ‘—.
ğ‘…ğ‘–, ğ‘…(ğ‘‘)
ğ‘–, ğ‘…(ğ‘ )
ğ‘–, ğ‘…ğ‘›
ğ‘–
scalar
Object ğ‘–â€™s total risk and distance/speed/intersection components.
Ì‚ğ‘…ğ‘–
scalar
Clipped/normalized risk of object ğ‘–in [0, 1].
ğ‘…gt
map
Ground-truth risk heatmap.
îˆ°ğ‘’
set
Potentially dangerous agents for ego ğ‘’, selected from î‰ƒğ‘’using îˆ¾.
Perceptual risk identification model
ğ®= (ğ‘¥, ğ‘¦)
vector
2D BEV grid cell center in ego coordinates.
îˆ³ğ®
grid
BEV grid cell centered at ğ®= (ğ‘¥, ğ‘¦).
ğ‘œ(ğ®) âˆˆ[0, 1]
field
Occupancy at BEV cell ğ®.
Î BEV
op
3Dâ†’BEV projection operator.
ğœ…, ğœ(â‹…)
func
Smoothing kernel; squashing function for occupancy.
ğœ—
angle
Ray azimuth (rad).
ğ‘Ÿ
scalar
Range of BEV grid cell (m).
ğ«(ğ‘ ; ğœ—)
curve
Ray parameterization along azimuth ğœ—.
ğ‘‡(ğ®)
scalar
Line-of-sight transmittance to ğ®.
ğœ†, Î”ğ‘ , ğ¾
scalars
Beerâ€“Lambert attenuation; step; number of samples along a ray.
ğœ’fov(ğ®)
gate
FoV gate {0, 1}.
ğ‘ƒocc(ğ®)
prob
Occlusion probability at ğ®.
ğœocc, ğ¾ğ‘¡, ğœğ‘¡
scalars
Occlusion threshold; number of temporal frames; temporal consensus threshold.
îˆ»e(ğ®), Ì„îˆ»e
mask
Instantaneous and stabilized blind-zone masks.
ğ“ğ‘’â†ğ‘¤
matrix
Rigid transform from world to ego frame.
ğ‘§
scalar
Vertical coordinate (height) in the ego frame.
Selective information sharing and fusion
ğ¹ğ‘–âˆˆâ„ğ¶Ã—ğ»Ã—ğ‘Š
tensor
BEV feature map of agent ğ‘–; ğ¶channels, ğ»Ã—ğ‘Šthe height/width of grid.
ğ¶ğ‘ ,ğ‘—, ğ¶ğ‘Ÿ,ğ‘—
map
Spatial- and risk-confidence maps on partner ğ‘—.
ğ‘†ğ‘—, ğ‘…ğ‘—âˆˆ{0, 1}ğ»Ã—ğ‘Š
mask
Spatial / risk masks (binary).
Ìƒğ¹ğ‘–
tensor
Masked feature patch to transmit from partner ğ‘–.
ğ¹ğ‘—,ğ‘’
tensor
Partner ğ‘—â€™s selected feature patch transmitted to ego ğ‘’.
ğ‘“enc, ğ‘“dec
net
Shared encoder; multi-task decoder.
( Ì‚ğ¶, Ì‚ğµ, Ì‚ğ‘…)
out
Class scores, 3D boxes, refined risk heatmap.
ğ¾sel
scalar
Top-ğ¾selected cells for transmission.
ğ‘”sp(ğ®), ğ‘”risk(ğ®)
score
Spatial/risk gains used for selection.
ğ‘”(ğ®), ğ›¼
score
Combined gain and its mixing weight ğ›¼âˆˆ[0, 1].
Continued on next page
Jiaxi Liu: Preprint submitted to Elsevier
Page 6 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Table 1 (continued).
Symbol
Type
Meaning / Unit
ğ‘ƒğ‘’
path
Planned trajectory used by the risk head of ego agent ğ‘’.
Training objective and evaluation
îˆ¸total
loss
Total training loss.
îˆ¸det
loss
Detection loss.
ğœ†risk, ğœ†comm
weight
Weights for risk regression and communication penalty.
ğœ™(usage; target)
penalty
Hinge-style penalty on over-usage of bytes.
ğµbytes
bytes
Target per-link byte budget.
â„hdr
bytes
Header/metadata overhead per message.
ğ‘idx, ğ‘feat, ğ‘cell
bytes
Bytes per cell index / per feature value / per cell (ğ‘cell=ğ‘idx+ğ¶ğ‘feat).
ğ‘ˆ
bytes
Actual bytes used in a batch.
ğµbatch, ğ¿ğ‘
count
Batch size; number of agents in sample ğ‘.
ğ‘€(ğ‘)
ğ‘™,ğ‘–,ğ‘—âˆˆ{0, 1}
mask
Binary selection mask for sample ğ‘.
AP, 3DAP(ğœƒ)
metric
AP and 3D AP at IoU threshold ğœƒ.
ğ‘‡ğ‘ƒ(ğœƒ), ğ¹ğ‘ƒ(ğœƒ), ğ¹ğ‘(ğœƒ)
count
True/false positives and false negatives at ğœƒ.
îˆµrisk(ğœ)
set
Subset filtered by risk threshold ğœfor Risk-AP.
Î”Risk-APâˆ•KB
metric
Î”Risk-AP per KB (Î”Risk-APâˆ•KB).
Risk label generation
ğ›¼ğ‘‘, ğ›¼ğ‘ , ğ›¼ğ‘›
scalar
Weights for distance-, speed-, and intersection-based risk components in the overall risk
score ğ‘….
ğœ†ğ‘‘, ğœ†ğ‘›
scalar
Decay rates for distance-based and intersection-based risk terms.
ğ‘š
index
index of intersections.
ğªğ‘š
vector
Center location of the ğ‘š-th intersection.
îˆ½
set
Set of all intersection center locations.
ğ‘£ğ‘–
scalar
Speed magnitude of object ğ‘–.
ğœ–
scalar
Small positive constant to avoid division by zero in the speed-based risk normalization.
4. Methodology
As mentioned in the previous section, the proposed SRA-CP framework is designed to operate in two phases: during
normal operation, each vehicle broadcasts basic perception coverage information with minimal bandwidth; when a
risk-relevant blind zone is detected, it initiates a targeted CP link and transmits only the most critical information
within the available communication bandwidth. The framework relies on a perceptual risk identification model to
assess the risk level of blind zones. Upon identifying a suitable cooperative agent, the responder employs a selective
information sharing model to determine which features to transmit under bandwidth constraints. The receiving agent
then performs cooperative fusion using a dual-attention feature fusion model to produce an enhanced perception
result. The following subsections detail each of these four key components.
4.1. SRA-CP Protocol
The core idea of the proposed SRA-CP protocol is as follows: at each time ğ‘¡, the ego vehicle ğ‘’periodically
broadcasts a compact coverage map of îˆ¼ğ‘’to all nearby agents î‰ƒğ‘’within î‰†ğ‘’. This map summarizes which BEV cells
are currently visible and which are likely occluded (Sec. 4.2), without exposing raw sensor data. Each neighbor does
the same, enabling every agent to infer who can potentially compensate for its blind zones. When a risky blind area
is detected, the ego triggers an on-demand handshake with one suitable partner and proceeds with selective sharing
and fusion under the current byte budget. In practice, a risk threshold ğœğ‘Ÿdetermines whether the detected blind-zone
risk warrants initiating the cooperative handshake. Then, based on the received coverage maps, each agent constructs
an inter-object risk matrix îˆ¾= [ ğœŒ(ğ‘’, ğ‘–) | ğ‘–âˆˆî‰ƒe ] by evaluating pairwise risks. From this vector, the potentially
dangerous set îˆ°e is identified. If an agent ğ‘–âˆˆîˆ°e also lies in îˆ»e, the partner transmits only the features covering that
region to assist perception completion.
For example, as illustrated in the intersection scenario in Figure 2, there are six vehicles, among which ğ‘–, ğ‘—, and
ğ‘˜are connected agents. In the first step, each connected agent broadcasts its local perception coverage îˆ¼ğ‘–, îˆ¼ğ‘—, and
Jiaxi Liu: Preprint submitted to Elsevier
Page 7 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
îˆ¼ğ‘˜to others. Since only coverage maps are sharedâ€”without detailed perception contentâ€”this step incurs negligible
communication overhead.
Next, each agent performs an inter-object risk estimation over the observed objects in the scene and generates a
risk vector îˆ¾to estimate whether they need external information from other agents to help with their perception. In
this scenario, agent ğ‘˜and agent ğ‘—find no risky blind spot, so they do not need further external information from other
agents. However, agent ğ‘–finds it can not see the potentially risky objects in the blind zone of the black vehicle, which
is risky to its driving intention, and agent ğ‘—finds it can help the detection of agent i. Therefore, agent ğ‘—sends the
information of the potentially risky zone to agent ğ‘–to complete its perception.
4.2. Perceptual risk identification model
The Perceptual Risk Identification Model takes the individual perception ğš½(îˆ¼ğ‘–) as input and produces a risk
matrix îˆ¾ğ‘–over the blind zone îˆ»ğ‘–, indicating the safety-critical importance of each location with respect to the ego
vehicleâ€™s driving decisions.
SRA-CP requires a light-weight, geometry-based estimate of the ego vehicleâ€™s blind zones to prioritize compen-
sation from partners. We adopt a BEV visibility model that is fast, rule-based, and admits a continuous formulation
for analysis. Let îˆ³ğ®denote a BEV grid with cell centers ğ®= (ğ‘¥, ğ‘¦) in ego coordinates, z is the vertical coordinate of
ğ®in the ego frame used for 2.5D occupancy computation, ego pose ğ“ğ‘’â†ğ‘¤(world â†’ego), and a 2.5D occupancy field
ğ‘œ(ğ®) âˆˆ[0, 1] obtained from the LiDAR sweep ğš½(îˆ¼ğ‘–
) by height-thresholding and kernel density aggregation:
ğ‘œ(ğ®) = ğœ
(
max
ğ‘§âˆˆ[ğ‘§min,ğ‘§max] ğœ…âˆ—
âˆ‘
ğ©âˆˆîˆ¸
ğ›¿(Î BEV(ğ“ğ‘’â†ğ‘¤ğ©) âˆ’(ğ®, ğ‘§))
)
,
(1)
where Î BEV projects 3D points to the BEV cell, ğœ…is a spatial smoothing kernel, and ğœis a squashing function
ensuring ğ‘œâˆˆ[0, 1] (e.g., ğœ(ğ‘) = 1 âˆ’ğ‘’âˆ’ğ‘). For a BEV direction ğœ—= atan2(ğ‘¦, ğ‘¥) and range ğ‘Ÿ= â€–ğ®â€–2, define
the ray parameterization ğ«(ğ‘ ; ğœ—) = ğ‘ [cos ğœ—, sin ğœ—]âŠ¤, ğ‘ âˆˆ[0, ğ‘Ÿ]. The line-of-sight transmittance to ğ®is modeled by a
Beerâ€“Lambert integral over occupancy:
ğ‘‡(ğ®) = exp
(
âˆ’âˆ«
ğ‘Ÿ
0
ğœ†ğ‘œ(ğ«(ğ‘ ; ğœ—)) dğ‘ 
)
,
ğœ†> 0,
(2)
with discrete approximation on grid steps Î”ğ‘ :
ğ‘‡(ğ®) â‰ˆexp
(
âˆ’ğœ†Î”ğ‘ 
ğ¾
âˆ‘
ğ‘˜=0
ğ‘œ(ğ«(ğ‘˜Î”ğ‘ ; ğœ—))
)
, ğ¾Î”ğ‘ â‰ˆğ‘Ÿ.
(3)
Cells outside the sensor field-of-view (FOV) or range are treated as fully occluded by an FOV gate ğœ’fov(ğ®) âˆˆ{0, 1};
we define the occlusion probability and the binary blind-zone mask as
ğ‘ƒocc(ğ®) = 1 âˆ’ğœ’fov(ğ®) ğ‘‡(ğ®),
îˆ»e(ğ®) = ğ•€[ğ‘ƒocc(ğ®)>ğœocc
],
(4)
with threshold ğœocc âˆˆ(0, 1). To reduce flicker, we temporally stabilize the mask by warping the last ğ¾ğ‘¡frames into the
current ego frame using odometry and taking a robust union:
Ì„îˆ»e(ğ®) = ğ•€
[
1
ğ¾ğ‘¡
ğ‘¡âˆ‘
ğ‘¡â€²=ğ‘¡âˆ’ğ¾ğ‘¡+1
îˆ»(ğ‘¡â€²)
e
(ğ“ğ‘’â†ğ‘’(ğ‘¡â€²)(ğ®)) > ğœğ‘¡
]
.
(5)
The mask Ì„îˆ»e is used as a compressed coverage summary and to increase selection gains in risky blind zones.
Specifically, let ğ‘”sp(ğ®) and ğ‘”risk(ğ®) are spatial/risk scores (Sec. 4.3.2), the budgeted gain can be
ğ‘”(ğ®) = ğ›¼ğ‘”sp(ğ®) ğ‘”risk(ğ®) + (1 âˆ’ğ›¼) Ì„îˆ»e(ğ®) ğ‘”risk(ğ®), ğ›¼âˆˆ[0, 1],
(6)
which prioritizes risky and occluded cells under a rate/byte budget.
Jiaxi Liu: Preprint submitted to Elsevier
Page 8 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
4.3. Selective Information Sharing and Fusion
The Selective Information Sharing and Fusion Model describes the full pipeline from selecting the target agent
for cooperation, to determining which features to share, and finally to integrating the received features on the ego
vehicle. The overall framework is illustrated below.
Selective Information Sharing and Fusion is the model layer that operationalizes the SRA-CP contract: given the
neighbors and a per-link budget, it learns what to communicate and how to fuse. Concretely, Selective Information
Sharing and Fusion produces lightweight spatial and risk masks, sparsifies partner features under a given communi-
cation budget, and performs risk-aware fusion on the ego agent using the fused ego map (Figure 3). This converts
communication bandwidth into safety-relevant detections by prioritizing risky Ã— occluded regions. The pipeline has
four building blocks:
1. Shared feature encoder ğ‘“enc(â‹…) that transforms each LiDAR sweep ğš½(îˆ¼ğ‘–
) into a BEV feature tensor ğ¹ğ‘–âˆˆ
â„ğ¶Ã—ğ»Ã—ğ‘Š;
2. Risk-aware communication module (Figure 4) that derives a spatial mask ğ‘†ğ‘–and a risk mask ğ‘…ğ‘–from ğ¹ğ‘–and
these masks will be used as a reference in the Dual-attention feature fusion process to decide which features
should be shared;
3. Dual-attention feature fusion module (Figure 5) that selects the features { Ìƒğ¹ğ‘—}ğ‘—â‰ ğ‘’to be shared to the ego vehicle
based on the spatial mask ğ‘†ğ‘—and the risk mask ğ‘…ğ‘—and transmits { Ìƒğ¹ğ‘—}ğ‘—â‰ ğ‘’to the ego vehicle and merges them
with the ego feature map ğ¹ğ‘’and outputs the fused representation Ìƒğ¹ğ‘’in the ego vehicleâ€™s coordinate system;
4. Multi-task decoder that predicts both 3D bounding boxes and a dense risk heat-map.
4.3.1. Feature Encoding
During the training process each CV ğ‘–encodes its LiDAR sweep ğš½(îˆ¼ğ‘–) with a shared PointPillar BEV encoderLang
et al. [2019] in the same structure, yielding ğ¹ğ‘–âˆˆâ„ğ¶Ã—ğ»Ã—ğ‘Š. Features are expressed in a common ego BEV frame using
the known pairwise poses which is transmitted with the coverage map. The backbone within the same structure feeds
two light heads to derive a spatial confidence map and a risk map. The spatial confidence map stores the confidence
score of the features from the spatial perspective, which means which feature is spatially important for perception. And
the risk confidence map stores the confidence score of the features from the traffic risk perspective, which means which
feature is essential in terms of traffic importance. Both of these two confidence maps will guide the communication
process to choose which features to communicate and the later fusion.
4.3.2. Risk-Aware Communication
The aim of this module is to reduces the communication bandwidth while preserving the balance of safety relevance
and spatial relevance. Each partner summarizes where its features are informative (spatial saliency) and where they
are safety-critical for the ego (risk), then the CVs will combine the scores together to select which features are more
important for the ego vehicle and they will send only the most important parts under a given budget.
On each partner ğ‘—, two lightweight heads process ğ¹ğ‘—(Figure 4):
â€¢ Spatial-confidence head outputs ğ¶ğ‘ ,ğ‘—.
â€¢ Risk-confidence head outputs ğ¶ğ‘Ÿ,ğ‘—.
Under a given communication budget, adaptive sampling will preform Top-K selection over non-ego grid cells based
on their spatial and risk scores separately in the scene and then it will produces binary masks ğ‘†ğ‘—, ğ‘…ğ‘—âˆˆ{0, 1}ğ»Ã—ğ‘Š.
Under a given per-link budget, SRA-CP combines the two cues (union) and serializes only cells within the mask as in
the safety-focus feature selection part in the Figure 5. In practice this masks the feature map:
Ìƒğ¹ğ‘—= ğ¹ğ‘—âŠ™(ğ‘†ğ‘—âˆ¨ğ‘…ğ‘—
),
so only areas that are spatially salient and safety-critical are transmitted. This concentrates communication on occluded
or risky regions that matter for decision-making, keeps privacy by avoiding raw points, and gracefully adapts to tighter
budgets by shrinking the selected area.
Jiaxi Liu: Preprint submitted to Elsevier
Page 9 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 4: Risk-aware communication pipeline executed on each partner vehicle ğ‘—. The shared feature map ğ¹ğ‘—is processed
by two light-weight heads: (i) Spatial-confidence map generator produces a spatial confidence map ğ¶ğ‘ ,ğ‘—that highlights
semantically important cells; an adaptive sampling module is used to select a sparse binary spatial mask based on scenario
ğ‘†ğ‘—for transmission. (ii) Risk-confidence map generator uses ğ¹ğ‘—together with the ego planned trajectory ğ‘ƒğ‘’and speed ğ‘£ğ‘’
to compute a risk map ğ¶ğ‘Ÿ,ğ‘—. Adaptive sampling converts it into a binary risk mask ğ‘…ğ‘—. Both masks (ğ‘†ğ‘—, ğ‘…ğ‘—
) are sent to the
ego vehicle, while a miniature Risk Decoder can optionally convert ğ¶ğ‘Ÿ,ğ‘—into a dense risk heat-map for supervision training.
Figure 5: Dual-attention feature fusion. Remote feature tensors ğ¹ğ‘˜and ğ¹ğ‘—are first filtered by a Safety-focused Feature
Selection block that combines each partnerâ€™s spatial mask ğ‘†ğ‘–and risk mask ğ‘…ğ‘–, yielding sparsified maps Ìƒğ¹ğ‘˜and Ìƒğ¹ğ‘—. The
ego map ğ¹ğ‘’and the sparsified partner maps are then fused by a location-wise multi-head attention module that performs
per-cell keyâ€“query interactions, producing an enriched representation Ìƒğ¹ğ‘’. This two-stage design discards bandwidth-hungry,
low-value regions before attention, so both communication and computation focus on areas that are simultaneously
safety-critical and semantically informative. During this process, only three low-bandwidth tensors ( Ìƒğ¹ğ‘—, ğ‘†ğ‘—, ğ‘…ğ‘—) leave the
vehicle, preserving privacy and saving channel capacity.
4.3.3. Dual-Attention Feature Fusion
At the ego agent, the masked partner maps { Ìƒğ¹ğ‘—} and the local map ğ¹ğ‘’are fused in two stages (Figure 5). First, a
safety-focused selector re-applies (ğ‘†ğ‘—, ğ‘…ğ‘—
) to suppress any residual clutter and enforce budget consistency. Second,
We fuse ego and partner features in a location-wise manner, for each BEV cell ğ®, the ego feature provides the query,
while only partners that selected this cell contribute keys and values. This yields an attention distribution over the
Jiaxi Liu: Preprint submitted to Elsevier
Page 10 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
relevant collaborators, ensuring that information is aggregated only where communication actually provided features.
A residual update then produces the fused representation Ìƒğ¹ğ‘’.
To handle small spatial misalignment, the module can optionally attend within a local window around ğ®, but still
restricts computation to cells indicated by partner selection. This keeps the complexity proportional to the number of
communicated cells, making the fusion efficient under sparse CP.
This kind of design limits computation to a small set of safety-relevant cells, improves alignment under occlusion,
and avoids flooding the decoder with low-value regions. When no partner data arrives, the module naturally falls back
to the ego features without architectural changes.
4.3.4. Budgeted Selection and Training Objective
The fused tensor is decoded as ( Ì‚ğ¶, Ì‚ğµ, Ì‚ğ‘…) = ğ‘“dec
( Ìƒğ¹ğ‘’
), where Ì‚ğ¶are class scores, Ì‚ğµare 3-D boxes, and Ì‚ğ‘…is the
refined risk heat-map.
Budgeted selection. Given a budget per link, Selective Information Sharing and Fusion Model ranks non-ego BEV
cells by the gain ğ‘”(ğ®) (Sec. 4.2) and selects the top ğ¾sel cells subject to the budget. Let the per-cell byte cost be
ğ‘cell=ğ‘idx+ğ¶ğ‘feat and header overhead â„hdr. For a byte budget ğµbytes, the capacity in cells is
ğ¾sel = max
(
0,
âŒŠğµbytes âˆ’â„hdr
ğ‘cell
âŒ‹)
,
(7)
where, â„hdr is a fixed header cost (bytes).
Budget-aware training. To make the bandwidthâ€“accuracy trade-off controllable at training time, we add a com-
munication regularizer that penalizes over-usage relative to a target budget; this does not change runtime budget, but
shapes the modelâ€™s selection behavior. The total loss denoted by îˆ¸total is as follows:
îˆ¸total = îˆ¸det + ğœ†risk â€– Ì‚ğ‘…âˆ’ğ‘…gtâ€–2
2 + ğœ†comm ğœ™(ğ‘ˆ; ğµbytes).
(8)
where:
â€¢ Detection loss. îˆ¸det = îˆ¸conf + îˆ¸reg is the standard detection loss. The classification term îˆ¸conf is a focal loss
with ğ›¼= 0.25, ğ›¾= 2.0, computed on BEV anchors and normalized by the number of positives. The regression
term îˆ¸reg is a weighted Smooth-L1 loss over 7 box codes per anchor.
â€¢ Risk regression. The risk loss is a mean-squared error between predicted and ground-truth BEV risk maps:
â€– Ì‚ğ‘…âˆ’ğ‘…gtâ€–2
2.
â€¢ Communication over-usage penalty. The term ğœ™(ğ‘ˆ; ğµbytes)
=
max (0, ğ‘ˆâˆ•ğµbytes âˆ’1). penalizes only
communication above the target budget, aligning learned masks with the desired budget without changing the
runtime protocol.
The usage definitions can be calculated from:
ğ‘ˆ= ğµğ‘ğ‘ğ‘¡ğ‘â„â‹…â„hdr +
( ğµğ‘ğ‘ğ‘¡ğ‘â„
âˆ‘
ğ‘=1
ğ¿ğ‘
âˆ‘
ğ‘™=2
âˆ‘
ğ‘–,ğ‘—
ğ‘€(ğ‘)
ğ‘™,ğ‘–,ğ‘—
)
â‹…(ğ‘idx + ğ¶â‹…ğ‘feat
),
(9)
where ğµğ‘ğ‘ğ‘¡ğ‘â„is the batch size of this training, ğ‘€(ğ‘)
ğ‘™,ğ‘–,ğ‘—âˆˆ{0, 1} is the non-ego mask (for all the masks of ego is
ğ‘™=1), â„hdr is a fixed header bytes cost, ğ‘idx is per-cell index bytes cost, ğ¶is the channel dimension, and ğ‘feat is
bytes per feature value.
5. Experimental Setup
5.1. Datasets
We use the OPV2V dataset [Xu et al., 2022b] as the base dataset. OPV2V is a synthetic multi-vehicle CP benchmark
generated by the OpenCDA co-simulation of SUMO [Krajzewicz et al., 2012] and CARLA [Dosovitskiy et al., 2017].
Jiaxi Liu: Preprint submitted to Elsevier
Page 11 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
OPV2V contains 73 scenarios (average âˆ¼25 s) across multiple CARLA towns, where 2 âˆ¼7 connected vehicles
record 64-channel LiDAR from their own viewpoints. We follow the standard frame-level counts of 6765/1981/2170
for train/val/test, respectively. Importantly, OPV2V natively covers a diverse set of driving situations without any
additional sampling from our side. The included situations comprise:
â€¢ Overtaking / Lane Change: fast lateral maneuvers with transient occlusions.
â€¢ Left-turn and Right-turn Intersections: cross-traffic under partial observability (pedestrians/cyclists may
emerge from blind zones).
â€¢ On-ramp Merging: gap selection and speed adjustment with strong temporal risk.
â€¢ Unprotected Crossroads: multiple agents with conflicting trajectories.
â€¢ Head-on Encounters: close-range opposing traffic forming highly critical regions.
â€¢ Straight Driving (Low-risk Baseline): low-complexity scenes for calibration.
â€¢ Multi-agent Cooperation: â‰¥3 vehicles jointly negotiating maneuvers.
To illustrate why risk-aware cooperation is meaningful, we provide illustrative exemplars from OPV2V for the above
situations in Figure 6. These thumbnails are for visualization only and do not change the dataset composition.
To strengthen generalization and avoid leakage, we keep the natural scenario composition of OPV2V, but ensure
that train/val/test have comparable proportions of each situation (e.g., intersections, merging, head-on). The unit
of assignment is the entire scenario (all its frames stay in one split), preventing temporal leakage while reducing
distributional drift between splits.
We control the number of agents per frame (2â€“7) by matching their histograms across splits within Â±5%. The
qualitative exemplars in Figure 6 shows the different scenarios that are inherently covered by the dataset organized by
us.
5.2. Risk label generation
To facilitate risk-aware CP using the OPV2V dataset [Xu et al., 2022b], we generate risk annotations based on
spatial, kinematic, and traffic-contextual information, further refined by expert domain knowledgeâ€”particularly in
complex environments such as intersections. The final risk score for each object is computed as a weighted combination
of three sub-components:
ğ‘…ğ‘–= ğ›¼ğ‘‘ğ‘…(ğ‘‘)
ğ‘–
+ ğ›¼ğ‘ ğ‘…(ğ‘ )
ğ‘–
+ ğ›¼ğ‘šğ‘…(ğ‘›)
ğ‘–,
(10)
where ğ‘…ğ‘–denotes the overall risk score for object ğ‘–, and ğ‘…(ğ‘‘)
ğ‘–, ğ‘…(ğ‘ )
ğ‘–, and ğ‘…(ğ‘›)
ğ‘–
correspond to distance-based, speed-based,
and intersection-based risk scores, respectively. The weights ğ›¼ğ‘‘= 0.5, ğ›¼ğ‘ = 0.3, and ğ›¼ğ‘›= 0.2 were selected based on
empirical tuning and expert input.
â€¢ Distance-Based Risk: Objects located closer to the ego vehicle are more likely to pose an immediate threat. We
quantify this via an exponential decay function of the Euclidean distance:
ğ‘…(ğ‘‘)
ğ‘–
= exp(âˆ’ğœ†ğ‘‘â‹…â€–ğ©ğ‘–âˆ’ğ©ğ‘’â€–2),
(11)
where ğ©ğ‘–and ğ©ğ‘’denote the positions of object ğ‘–and the ego vehicle, respectively. The parameter ğœ†ğ‘‘controls the
decay rate of risk with distance.
â€¢ Speed-Based Risk: Rapidly approaching vehicles or those with high relative speed introduce dynamic hazards.
We model this component as:
ğ‘…(ğ‘ )
ğ‘–
=
|ğ‘£ğ‘–âˆ’ğ‘£ğ‘’|
maxğ‘—|ğ‘£ğ‘—âˆ’ğ‘£ğ‘’| + ğœ–,
(12)
where ğ‘£ğ‘–is the velocity of object ğ‘–, ğ‘£ğ‘’is the ego vehicleâ€™s speed, and ğœ–is a small constant to avoid division by
zero. This formulation emphasizes relative speed normalized across the scene.
Jiaxi Liu: Preprint submitted to Elsevier
Page 12 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 6: Representative exemplars from OPV2V illustrating scenarios that are inherently covered by the dataset and
organized by us.
â€¢ Intersection-Based Risk: Intersections are inherently high-risk regions due to complex traffic flows, occlusion,
signal compliance issues, and the presence of vulnerable road users. We begin by measuring proximity to
intersections:
ğ‘…(ğ‘›)
ğ‘–
= exp
(
âˆ’ğœ†ğ‘¡â‹…min
ğªğ‘šâˆˆîˆ½â€–ğ©ğ‘–âˆ’ğªğ‘šâ€–2
)
,
(13)
where îˆ½= {ğªğ‘š} denotes known intersection coordinates and ğœ†ğ‘šadjusts the decay with distance to intersections.
Normalization: Finally, we clip the combined risk score to the range [0, 1] for stable learning:
Ì‚ğ‘…ğ‘–= min(1, max(0, ğ‘…ğ‘–)).
(14)
5.3. Baselines
To contextualize the standard AP results, we compare the following baselines under the same backbone, BEV grid,
IoU thresholds, synchronization window, and quantization:
â€¢ Where2Comm (Spatial-only baseline) [Hu et al., 2022]. A representative spatial-communication method that
learns where to communicate based solely on spatial saliency without explicit risk or task-aware weighting. Each
agent predicts a binary mask indicating informative BEV cells, and only those regions are transmitted for feature
fusion. This baseline captures the benefit of geometry-aware but task-agnostic cooperation.
â€¢ Upper Bound (fully connected). Fully connected communication with no budget, transmitting all partner
features for fusion; serves as a performance ceiling.
Jiaxi Liu: Preprint submitted to Elsevier
Page 13 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
â€¢ Lower Bound (single-agent). No cooperative communication. It measures the capability of the ego-only
detector.
â€¢ Fixed-Neighbor (equal-budget). The total communication budget is equally divided among all non-ego
neighbors. Within each neighbor, features (e.g., grid cells or point clusters) are uniformly sampled at random.
This baseline isolates the effect of adaptive link-wise budget allocation from uniform distribution.
â€¢ Random-Cell. A global uniform sampler randomly selects exactly ğ¾feature cells from all non-ego agents,
regardless of their spatial location or risk relevance. This baseline evaluates the effectiveness of our selective
content transmission compared to random feature selection under the same bandwidth constraint.
5.4. Implementation details
Model and feature encoding. We adopt a PointPillar BEV backbone. The PillarVFE uses 64 channels. The BEV
backbone has 3/5/8 blocks with filters 64/128/256 and deconvs of 128 channels; a shrink header downsamples to 256
channels for heads. We attach three lightweight heads: classification (per cell 2 anchors), regression (7 parameters per
anchor), and a risk head (1 per anchor) to produce dense risk heatmaps.
Voxel/grid and anchors. Voxel size is 0.4 Ã— 0.4 Ã— 4 m with LiDAR range [âˆ’140.8, âˆ’38.4, âˆ’3, 140.8, 38.4, 1] m.
The BEV grid is ğ»=192, ğ‘Š=704 (feature stride 4). Anchors follow (ğ‘™, ğ‘¤, â„)=(3.9, 1.6, 1.56) with yaw {0â—¦, 90â—¦};
ğ‘ğ‘€ğ‘†= 0.15, and the positive, negative thresholds are 0.6 and 0.45 separately.
Training setup. Optimizer: We select Adam with leanring rate=2 Ã— 10âˆ’4 as the optimizer and the selection of
weight_decay is 0.01 and eps=1e-10. For the learning rate schedule, we set cosine annealing for 50 epochs with 10-
epoch warmup (with the warmup learning rate=2 Ã— 10âˆ’5, and the minimal learning rate=5 Ã— 10âˆ’6). During the training
of all the models, we set the batch size as 8. In terms of connecting agent numbers, we cut the number of agents up to
5 CAVs. Data augmentation includes x-axis flip, random rotation (Â±45â—¦), and scaling (0.95â€“1.05). Voxelization caps
are 32 points/voxel, with train/test voxel maxima as 32k/70k separately.
Inference and post-processing. We decode detection and risk heatmaps after fusion. Evaluation uses IoU âˆˆ
{0.3, 0.5, 0.7}; risk-aware AP uses ğœâˆˆ{0.2, 0.3, 0.4}. We log per-frame communication rate and bytes for the report.
5.5. Evaluation protocols and metrics.
We use 3D Average Precision (3DAP) to assess object detection performance. Given a detection is considered
correct if the Intersection over Union (IoU) between the predicted and ground-truth 3D bounding box exceeds a
threshold ğœƒ, the AP is computed based on the precision-recall curve.
We report 3DAP under three IoU thresholds:
ğœƒâˆˆ{0.3, 0.5, 0.7},
corresponding to different levels of localization strictness.
Let ğ‘‡ğ‘ƒ(ğœƒ), ğ¹ğ‘ƒ(ğœƒ), and ğ¹ğ‘(ğœƒ) be the number of true positives, false positives, and false negatives under threshold
ğœƒ, respectively. Precision and recall are defined as:
Precision(ğœƒ) =
ğ‘‡ğ‘ƒ(ğœƒ)
ğ‘‡ğ‘ƒ(ğœƒ) + ğ¹ğ‘ƒ(ğœƒ),
Recall(ğœƒ) =
ğ‘‡ğ‘ƒ(ğœƒ)
ğ‘‡ğ‘ƒ(ğœƒ) + ğ¹ğ‘(ğœƒ).
(15)
3DAP is then computed as:
3DAP(ğœƒ) = âˆ«
1
0
Precision(ğœƒ, ğ‘Ÿ) ğ‘‘ğ‘Ÿ,
(16)
where Precision(ğœƒ, ğ‘Ÿ) is interpolated at recall level ğ‘Ÿ.
To assess the influence of risk understanding on perception, we compute 3DAP selectively over high-risk regions
determined by thresholding the predicted risk map.
Let îˆµğ‘Ÿğ‘–ğ‘ ğ‘˜(ğœ) = {ğ‘–| Ì‚ğ‘…ğ‘–> ğœ} be the set of objects or regions identified as risky with a risk threshold ğœ. We evaluate
detection performance on this subset, denoted as 3DAPğ‘Ÿğ‘–ğ‘ ğ‘˜(ğœƒ, ğœ):
3DAPğ‘Ÿğ‘–ğ‘ ğ‘˜(ğœƒ, ğœ) = 3DAP evaluated on îˆµğ‘Ÿğ‘–ğ‘ ğ‘˜(ğœ), with IoU threshold ğœƒ.
(17)
Jiaxi Liu: Preprint submitted to Elsevier
Page 14 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
We report results for:
ğœƒâˆˆ{0.3, 0.5, 0.7},
ğœâˆˆ{0.2, 0.3, 0.4}.
This metric captures how well the model perceives objects in scenarios that are potentially dangerous or require
immediate attention, reflecting the synergy between risk assessment and spatial awareness.
We evaluate under two protocols:
â€¢ P1: Fixed-bandwidth. Per-link budget ğµbytes âˆˆ{0.5, 0.7, 1, 2, 3, 5, 10} KB/frame. Each method is tuned per
ğµbytes; we report Risk-AP, Î”Risk-APâˆ•KB, and data transmission volume. This stresses efficiency at scarce
bandwidth.
â€¢ P2: Fixed-performance. Given a target Risk-AP (e.g., â‰¥ğ‘‹), we report minimal bytes and latency to reach it.
This answers how much bandwidth is necessary for a safety line.
6. Results and Discussion
6.1. Main Results (Standard AP)
We report standard detection AP at IoU 0.3/0.5/0.7 denoted as AP30, AP50 and AP70 separately, across
baselines and our method. See Table 2 for overall comparison. It should be noted that the communication budget
of Where2comm, Fixed-Neighbor, Random-Cell and ours are 20% of the fully connected situation like the settings of
the method Upper Bound. And there is no communication in Lower Bound method. As shown in the table, our model
achieves consistently competitive performance across all IoU thresholds, with only marginal differences compared
to the strongest baselines, while remaining close to the upper bound. This indicates that both our approach and the
baseline methods are able to effectively leverage the advantages of CP.
6.2. Risk-Aware Evaluation
We further evaluate Risk-Aware AP by filtering ground-truths above risk thresholds ğœâˆˆ{0.2, 0.3, 0.4}. Results are
summarized in Tables 3. Compared with the overall AP results, where our model and the baselines perform similarly
in Table 2, the risk-aware evaluation reveals a clearer distinction. As shown in Table 3, our method consistently
outperforms the baselines across all IoU thresholds, especially under higher risk conditions (ğœ= 0.3, 0.4). The
performance of our model remains close to the upper bound while the spatial-only baseline drops significantly as
risk increases. This demonstrates that our design better preserves detection robustness when encountering high-risk or
safety-critical objects, validating the effectiveness of the communication protocol and SRA-CP coordination. In other
words, although both methods achieve comparable aggregate perception accuracy, our framework exhibits stronger
risk sensitivity and resilience, which are essential for safety-oriented CP. We additionally visualize risk-aware example
heatmaps (Sec. 6.6).
6.3. P1: Pareto efficiency under fixed bandwidth
To further examine model performance under resource-constrained conditions, we plot AP30/AP50/AP70 vs.
communication cost (KB/frame) in Figure 7 and Risk-AP30/AP50/AP70 vs. communication cost (KB/frame) in
Figure 8. These plots provide a quantitative view of how perception accuracy scales with bandwidth usage.
Across the 0.5â€“10 KB/frame regime, our proposed SRA-CP configuration consistently dominates the Pareto
frontier, achieving higher safety-aware gains per byte compared to baseline methods. For example, at 5 KB/frame,
our approach yields approximately +4.7% Risk-AP50 improvement over the baseline while maintaining comparable
communication overhead. This demonstrates that the methodâ€™s communication sparsification and the inference fusion
jointly enable efficient and safety-preserving cooperation.
6.4. P2: Minimal cost to reach a safety line
Figure 9 reports the minimum bandwidth (KB/frame) required to achieve specific Risk-AP30/AP50/AP70 targets
under different risk thresholds. Across all nine subplots in Figure 9, our method consistently requires fewer bytes
per frame to reach the same Risk-AP target compared to the baseline, demonstrating superior efficiency across all
thresholds ğœâˆˆ{0.2, 0.3, 0.4}.
It is worth noting that our target values for Risk-AP were determined in a principled way: for each risk threshold
ğœ, we set the target AP values (for AP30, AP50, and AP70) to 0.9Ã—, 0.8Ã—, and 0.7Ã— of the upper bound performance,
Jiaxi Liu: Preprint submitted to Elsevier
Page 15 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 7: Comparison of perception accuracy (AP30, AP50, AP70) under varying communication costs (KB/frame) across
all objects.
Figure 8: Comparison of perception accuracy (AP@30, AP@50, AP@70) under varying communication costs (KB/frame)
for objects with different risk levels, defined by risk thresholds ğœâˆˆ{0.2, 0.3, 0.4}.
respectively. This provides a reasonable and balanced target scaleâ€”stringent enough to challenge the communication
strategy, yet attainable for well-designed cooperative frameworks.
For example, at ğœ= 0.2 and AP50=0.75, our method reaches the target Risk-AP using only 1.3 KB/frame, compared
to the baselineâ€™s 3.3 KB/frame. The advantage becomes even more pronounced under higher risk thresholds: at ğœ= 0.4
and AP70=0.42/0.38, the baseline fails to achieve the required AP target across all IoU levels (Figure 9(c, f)), while
our method maintains strong performance with 5.1 and 9 KB/frame. This indicates that when perception becomes
safety-critical, the baseline communication policy saturates its bandwidth without sufficient accuracy gain, whereas
our SRA-CP-driven policy continues to deliver usable, risk-aware perception outputs.
Jiaxi Liu: Preprint submitted to Elsevier
Page 16 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Figure 9: Comparison of different CP methods in terms of the minimum communication bandwidth (KB/frame) required
to achieve specific perception accuracy levels (AP30, AP50, AP70) for objects with varying risk levels, categorized by risk
thresholds ğœâˆˆ{0.2, 0.3, 0.4}.
Table 2
Detection performance comparison (the communication transmission volume of all the baselines are 20% of the Upper
Bound) (AP score higher is better).
Method
AP30
AP50
AP70
Upper Bound
0.9057
0.8955
0.7996
Ours
0.8920
0.8731
0.7979
Where2comm (spatial-only)
0.8902
0.8791
0.7928
Fixed-Neighbor (equal-budget, ours-union)
0.8341
0.8159
0.6857
Random-Cell (ours-union)
0.8337
0.8156
0.6861
Lower Bound
0.8190
0.7908
0.6263
6.5. Ablation Studies
We ablate key communication choices like gate type (S-only, R-only, Union), blind-zone estimation (on/off) to see
whether the modules of our method are actually working.
Gate Mode Analysis We compare three gate configurations under the same 5 kB/frame bandwidth: spatial-only (S-
only), risk-only (R-only), and our hybrid Union gate that integrates both spatial and risk cues as shown in Table 4. The
results in Table 4 report Risk-Aware AP at IoU=0.3/0.5/0.7 across ğœâˆˆ{0.2, 0.3, 0.4}.
Across all thresholds and IoU levels, the proposed Union gate consistently outperforms both S-only and R-only
variants. At ğœ=0.3, for instance, Union improves AP50 from 0.6636 (S-only) and 0.6722 (R-only) to 0.6959, while at
ğœ=0.4 the gap widens to over +4.2% compared with S-only. Similarly, the AP70 metric rises from 0.3302 (S-only)
Jiaxi Liu: Preprint submitted to Elsevier
Page 17 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Table 3
Risk-aware detection performance across risk thresholds (ğœ=0.2âˆ•0.3âˆ•0.4). Higher is better.
Method
Risk ğœ
AP30
AP50
AP70
Upper Bound
0.2
0.8461
0.8411
0.7745
0.3
0.7659
0.7632
0.6962
0.4
0.5003
0.4994
0.4704
Ours
0.2
0.8365
0.8315
0.7667
0.3
0.7642
0.7622
0.6998
0.4
0.4963
0.4955
0.4702
Where2comm (spatial-only)
0.2
0.8203
0.8136
0.7512
0.3
0.7412
0.7354
0.6807
0.4
0.4701
0.4553
0.4177
Fixed-Neighbor (equal-budget)
0.2
0.7644
0.7519
0.6519
0.3
0.6640
0.6553
0.5705
0.4
0.3610
0.3565
0.3171
Random-Cell
0.2
0.7641
0.7511
0.6505
0.3
0.6670
0.6578
0.5723
0.4
0.3737
0.3685
0.3238
Lower Bound
0.2
0.7531
0.7357
0.6191
0.3
0.6483
0.6374
0.5381
0.4
0.3631
0.3581
0.3111
Table 4
Risk-aware AP at IoU=0.3/0.5/0.7 for different gate modes (5k budget) across risk thresholds ğœ.
Gate
Metric
ğœ=0.2
ğœ=0.3
ğœ=0.4
S-only
AP30
0.7763
0.6714
0.3716
AP50
0.7608
0.6636
0.3661
AP70
0.6725
0.5963
0.3302
R-only
AP30
0.7861
0.6811
0.3959
AP50
0.7731
0.6722
0.3894
AP70
0.6795
0.6002
0.3544
Union (ours)
AP30
0.7981
0.7032
0.4128
AP50
0.7866
0.6959
0.4082
AP70
0.7097
0.6308
0.3742
and 0.3544 (R-only) to 0.3742. These gains demonstrate that combining spatial coverage with risk awareness yields
complementary benefitsâ€”risk-only gating favors safety-critical regions but may miss peripheral context, whereas
spatial-only gating ensures broader coverage but wastes bandwidth on low-risk areas.
By unifying both criteria, the Union gate adaptively allocates transmission priority based on spatial relevance and
estimated collision risk, effectively balancing perception completeness and communication efficiency. This hybrid
gating thus provides a more stable and risk-sensitive communication policy, enabling the system to maintain higher
detection performance even as ğœincreases.
Blind-Zone Estimation To examine whether the model benefits from explicitly prioritizing safety-critical blind
areas, we conduct an ablation study on the Union gating scheme with and without blind-zone weighting under a
fixed 5 kB/frame communication budget. The results in Table 5 report Risk-Aware AP at IoU=0.3/0.5/0.7 across risk
thresholds ğœâˆˆ{0.2, 0.3, 0.4}.
Jiaxi Liu: Preprint submitted to Elsevier
Page 18 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Table 5
Risk-aware AP at IoU=0.3/0.5/0.7 for Union gate with/without blind-zone weighting (5k budget) across ğœ.
Setting
Metric
ğœ=0.2
ğœ=0.3
ğœ=0.4
Union (no blind)
AP30
0.7912
0.6877
0.3973
AP50
0.7803
0.6778
0.3912
AP70
0.6948
0.6104
0.3542
Union (blind on, ours)
AP30
0.7981
0.7032
0.4128
AP50
0.7866
0.6959
0.4082
AP70
0.7097
0.6308
0.3742
Across all IoU and risk thresholds, enabling blind-zone weighting consistently improves detection performance.
Compared to the vanilla Union gate, our method achieves an average gain of +0.7%, +1.2%, and +1.6% for AP30, AP50
and AP70, respectively. The improvement becomes more pronounced as the risk threshold increases. For instance, at
ğœ=0.4, the Risk-AP70 rises from 0.3542 to 0.3742, representing a relative gain of +5.6%. This pattern suggests that the
proposed weighting mechanism effectively allocates communication bandwidth toward regions with higher occlusion
and potential collision risk.
Qualitatively, this mechanism acts as a â€œsafety amplifierâ€: when cooperative views overlap poorly or when agents
observe asymmetric blind spots, the weighting function adaptively increases the transmission priority of uncertain
spatial zones. As a result, even under the same bandwidth constraint, more informative features are propagated to
neighboring vehicles, enhancing risk-aware perception robustness in safety-critical scenarios.
6.6. Visualization & Case Study
Figure 10 illustrates a challenging unprotected left-turn scenario with dense cross-traffic. The ego vehicle intends
to turn left, yet its LiDAR alone cannot observe the incoming traffic hidden behind other vehiclesâ€™ occlusions. These
blind-zone regions coincide with locations where high-risk background vehicles are approaching, making the timely
restoration of occluded agents crucial for safe maneuver planning.
We compare four communication strategies: a random-cell baseline, spatial-only, risk-only, and our Union (SRA-
CP) method. The spatial-only, risk-only, and Union methods all operate under the same fixed communication budget,
whereas the random baseline uses a significantly higher budget, illustrating how communication volume alone does
not guarantee performance.
Ours vs. Spatial-only and Risk-only. Despite using the same byte budget, the three strategies prioritize cells
differently:
Spatial-only focuses solely on geometric visibility difficulty. It successfully identifies cells that are hard to perceive
but often fails to emphasize high-risk agents located in traffic-conflict regions. As a result, it may transmit cells that
are geometrically interesting yet irrelevant for imminent collision risk, while missing the truly dangerous ones.
Risk-only allocates nearly all bandwidth to the high-risk region. This improves awareness of hazardous agents but
ignores spatial fusion quality, often leading to incomplete or noisy reconstructions because difficult-to-fuse regions
receive insufficient coverage.
Union (Ours) balances both spatial fusion difficulty and collision risk. It means SRA-CP suppresses low-value
regions and forms a dense transmission corridor aligned with the egoâ€“background conflict path, precisely where the
occluded vehicle lies. As shown in the detection overlays, Union restores the hidden vehicle more reliably and aligns
closer with the ground truth than either single-objective method.
Ours vs. Random-cell Communication. Even with a much larger number of transmitted cells, the random-cell
method performs poorly. Because cells are sampled uniformly at random, it often allocates bandwidth to irrelevant
free-space areas while failing to cover the critical blind-zone region at the correct moment. Consequently, the recovered
detection remains incomplete or inconsistent despite the inflated budget.
In contrast, Union (SRA-CP) pinpoints and transmits only the essential cellsâ€”those that influence collision risk or
improve multi-agent fusion qualityâ€”and thus reconstructs the critical occluded vehicle with dramatically fewer bytes.
Jiaxi Liu: Preprint submitted to Elsevier
Page 19 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
What Gets Transmitted (Per-cell Transmission Maps). The transmission maps further confirm each methodâ€™s
behavior:
Spatial-only spreads bytes broadly across many cellsâ€”high coverage but low efficiency. Risk-only over-concentrates
in a compact regionâ€”high focus but weak contextual support. Random shows noisy, unstructured coverage even with a
large budgetâ€”no semantic prioritization. Union (Ours) exhibits an intelligent, elongated high-density band that tracks
the potential collision trajectory while maintaining minimal peripheral context.
This pattern matches the ablation findings in Table 4: combining spatial difficulty and risk factors yields the most
efficient allocation strategy.
In summary, our experiments show that SRA-CP consistently dominates existing cooperative-perception baselines
in the communicationâ€“safety trade-off. Under the same communication budget, SRA-CP matches or exceeds the
cutting-edge spatial-only selective method in perception accuracy, while delivering notably higher perception accuracy
for safety-critical objects. When sweeping the per-link budget, our method traces the Pareto frontier: for any given
bandwidth it attains the best risky-object detection, and for any target perception accuracy it requires substantially fewer
transmitted bytes than competing schemes. Qualitative case studies at unprotected intersections further illustrate that
SRA-CP automatically concentrates messages on risky blind-zone cells, allowing the ego vehicle to recover occluded,
dangerous agents earlier and more reliably during driving.
7. Conclusion
This paper presents a novel Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework to ad-
dress the scalability and bandwidth challenges of multi-agent cooperative perception in dynamic traffic environments.
We first design a protocol in which connected agents continuously broadcast their perception coverage with very low
communication cost and initiate on-demand handshakes when risk-relevant blind zones are detected. For a certain
connected agent, we propose a perceptual risk identification model to detect and quantify risk-critical occlusions,
a selective information sharing model to determine which features to transmit under bandwidth constraints, and a
dual-attention feature fusion model to integrate received features into the ego agentâ€™s perception output.
Extensive evaluations on a public dataset were conducted against five baseline methods, each targeting a different
aspect of the problem. These include a cutting-edge selective CP method, a fully connected CP setting as an upper
bound, a no-CP setup as a lower bound, and another 2 methods: fixed neighbor allocation and random feature
Sampling to evaluate the effects of communication target selection and content-level feature prioritization, respectively.
Experimental results show that SRA-CP achieves less than 1% loss for safety-critical objects compared to generic
CP, while using only 20% of the communication bandwidth. Moreover, compared to the cutting-edge selective CP
method, SRA-CP improves the AP for critical objects by 15% under the same bandwidth budget, demonstrating its
communication efficiency and risk-awareness advantage.
As future work, we are collecting real-world driving data using our labâ€™s connected vehicles. We plan to further
evaluate the framework on this in-house dataset and conduct field tests to assess its real-world applicability and
robustness.
References
Qi Chen, Xu Ma, Sihai Tang, Jingda Guo, Qing Yang, and Song Fu. F-cooper: Feature based cooperative perception for autonomous vehicle
edge computing system using 3d point clouds. In Proceedings of the 4th ACM/IEEE Symposium on Edge Computing (SEC), pages 88â€“100.
ACM/IEEE, 2019a.
Qi Chen, Sihai Tang, Qing Yang, and Song Fu. Cooper: Cooperative perception for connected autonomous vehicles based on 3d point clouds. In
Proceedings of the 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS), pages 514â€“524. IEEE, 2019b. doi:
10.1109/ICDCS.2019.00058.
Hsu-kuang Chiu, Chien-Yi Wang, Min-Hung Chen, and Stephen F. Smith. Probabilistic 3d multi-object cooperative tracking for autonomous driving
via differentiable multi-sensor kalman filter. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), pages
18458â€“18464. IEEE, 2024. doi: 10.1109/ICRA57147.2024.10610487.
Liang Dong, Zheng Yang, Xinjun Cai, Yi Zhao, Qiang Ma, and Xin Miao. Wave: Edge-device cooperated real-time object detection for open-air
applications. IEEE Transactions on Mobile Computing, 22(7):4347â€“4357, 2022.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference
on robot learning, pages 1â€“16. PMLR, 2017.
Brahim El Boukili, Mohammed-Hicham Zaggaf, and Lhoussain Bahatti. Cooperative lane keeping assist: Design and evaluation of a v2v lane
perception sharing approach. Journal of Robotics and Control, 6(5):2239â€“2248, 2025. doi: 10.18196/jrc.v6i5.26784.
Jiaxi Liu: Preprint submitted to Elsevier
Page 20 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Yiheng Feng, Chunhui Yu, and Henry X. Liu. Spatiotemporal intersection control in a connected and automated vehicle environment. Transportation
Research Part C: Emerging Technologies, 89:364â€“383, 2018. doi: 10.1016/j.trc.2018.02.001.
Bolin Gao, Jiaxi Liu, Hengduo Zou, Jiaxing Chen, Lei He, and Keqiang Li.
Vehicle-road-cloud collaborative perception framework and key
technologies: A review. IEEE Transactions on Intelligent Transportation Systems, 2024.
Kirin Godhwani, Adam S. R. Parker, Matthew E. Taylor, William Yeoh, and Reuth Mirsky. Towards spontaneous cooperation in multi-agent
reinforcement learning using explicit goal recognition. In RLC 2025 Workshop on Cooperative and Competitive Multi-Agent Reinforcement
Learning (CoCoMARL), 2025. Poster paper.
Yue Hu, Shaoheng Fang, Zixing Lei, Yiqi Zhong, and Siheng Chen. Where2comm: Communication-efficient collaborative perception via spatial
confidence maps. In Advances in Neural Information Processing Systems, volume 35, pages 4874â€“4886, 2022.
Lennart Lorenz Freimuth Jahn, Seongjeong Park, Yongseob Lim, Jinung An, and Gyeungho Choi. Enhancing lane detection with a lightweight
collaborative late fusion model. Robotics and Autonomous Systems, 175:104680, 2024.
Daniel Krajzewicz, Jakob Erdmann, Michael Behrisch, Laura Bieker, et al. Recent development and applications of sumo-simulation of urban
mobility. International journal on advances in systems and measurements, 5(3&4):128â€“138, 2012.
Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from
point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12697â€“12705, 2019.
Rongsong Li and Xin Pei. Multi-V2X: A large scale multi-modal multi-penetration-rate dataset for cooperative perception, 2024.
Jiaxi Liu, Bolin Gao, Wei Zhong, Yanbo Lu, and Shuo Han. Adaptive optimization strategy and evaluation of vehicle-road collaborative perception
algorithm in real-time settings. Computers and Electrical Engineering, 120:109785, 2024.
Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, and Zsolt Kira.
When2com: Multi-agent perception via communication graph grouping.
In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4106â€“4115, 2020.
Chengyuan Ma, Hangyu Li, Keke Long, Hang Zhou, Zhaohui Liang, Pei Li, Hongkai Yu, and Xiaopeng Li. Real-time identification of cooperative
perception necessity in road traffic scenarios. Available at SSRN 4973353, 2025.
Reuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot Fosong, William Macke, Mohan Sridharan, Peter Stone, and Stefano V. Albrecht. A survey
of ad hoc teamwork: Definitions, methods, and open problems. arXiv preprint arXiv:2202.10450, 2022.
Fenglian Pan, Yinwei Zhang, Jian Liu, Larry Head, Maria Elli, and Ignacio Alvarez. Reliability modeling for perception systems in autonomous
vehicles: A recursive event-triggering point process approach. Transportation Research Part C: Emerging Technologies, 169:104868, 2024. doi:
10.1016/j.trc.2024.104868.
Huan Qiu, Jian Zhou, Bijun Li, Qin Zou, Youchen Tang, and Man Luo. Map4comm: A map-aware collaborative perception framework with
efficient-bandwidth information fusion. Information Fusion, page 103567, 2025.
Ahmad Sarlak, Rahul Amin, and Abolfazl Razi. Extended visibility of autonomous vehicles via optimized cooperative perception under imperfect
communication. Transportation Research Part C: Emerging Technologies, 180:105350, 2025. doi: 10.1016/j.trc.2025.105350.
Jessica Van Brummelen, Marie Oâ€™Brien, Dominique Gruyer, and Homayoun Najjaran. Autonomous vehicle perception: The technology of today
and tomorrow. Transportation Research Part C: Emerging Technologies, 89:384â€“406, 2018. doi: 10.1016/j.trc.2018.02.012.
Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. V2VNet: Vehicle-to-vehicle commu-
nication for joint perception and prediction. In Computer Vision â€“ ECCV 2020, volume 12347 of Lecture Notes in Computer Science, pages
605â€“621. Springer, 2020.
Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian I. Kwon, Makoto Onizuka, Shaojie Tang, and Chuan Xiao. Shall we team
up: Exploring spontaneous cooperation of competing LLM agents. In Findings of the Association for Computational Linguistics: EMNLP 2024,
pages 5163â€“5186, Miami, Florida, USA, 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.297. URL
https://aclanthology.org/2024.findings-emnlp.297/.
Hao Xiang, Zhaoliang Zheng, Xin Xia, Runsheng Xu, Letian Gao, Zewei Zhou, Xu Han, Xinkai Ji, Mingxi Li, Zonglin Meng, Li Jin, Mingyue
Lei, Zhaoyang Ma, Zihang He, Haoxuan Ma, Yunshuang Yuan, Yingqian Zhao, and Jiaqi Ma. V2X-Real: A large-scale dataset for vehicle-to-
everything cooperative perception. In Computer Vision â€“ ECCV 2024, 2024.
Runsheng Xu, Hao Xiang, Zhengzhong Tu, Xin Xia, Ming-Hsuan Yang, and Jiaqi Ma. V2X-ViT: Vehicle-to-everything cooperative perception with
vision transformer. In Computer Vision â€“ ECCV 2022, volume 13699 of Lecture Notes in Computer Science, pages 107â€“124. Springer, 2022a.
Runsheng Xu, Hao Xiang, Xin Xia, Xu Han, Jinlong Li, and Jiaqi Ma. Opv2v: An open benchmark dataset and fusion pipeline for perception
with vehicle-to-vehicle communication. In 2022 IEEE International Conference on Robotics and Automation (ICRA), pages 2583â€“2589. IEEE,
2022b.
Runsheng Xu, Xin Xia, Jinlong Li, Hanzhao Li, Shuo Zhang, Zhengzhong Tu, Zonglin Meng, Hao Xiang, Xiaoyu Dong, Rui Song, Hongkai Yu,
Bolei Zhou, and Jiaqi Ma. V2V4Real: A real-world large-scale dataset for vehicle-to-vehicle cooperative perception. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13712â€“13722, 2023.
Dingkang Yang, Kun Yang, Yuzheng Wang, Jing Liu, Zhi Xu, Rongbin Yin, Peng Zhai, and Lihua Zhang. How2comm: Communication-efficient
and collaboration-pragmatic multi-agent perception. Advances in Neural Information Processing Systems, 36:25151â€“25164, 2023a.
Kun Yang, Dingkang Yang, Jingyu Zhang, Hanqi Wang, Peng Sun, and Liang Song. What2comm: Towards communication-efficient collaborative
perception via feature decoupling. In Proceedings of the 31st ACM international conference on multimedia, pages 7686â€“7695, 2023b.
Wenbin Yang, Hang Yu, Xiangfeng Luo, and Shaorong Xie. Density-aware early fusion for vehicle collaborative perception. IEEE Intelligent
Transportation Systems Magazine, 17(2):33â€“47, 2025.
Chunhui Yu, Yiheng Feng, Henry X. Liu, Wanjing Ma, and Xiaoguang Yang. Corridor level cooperative trajectory optimization with connected
and automated vehicles. Transportation Research Part C: Emerging Technologies, 105:405â€“421, 2019. doi: 10.1016/j.trc.2019.06.002.
Haibao Yu, Wenxian Yang, Hongzhi Ruan, Zhenwei Yang, Yingjuan Tang, Xu Gao, Xin Hao, Yifeng Shi, Yifeng Pan, Ning Sun, Juan Song, Jirui
Yuan, Ping Luo, and Zaiqing Nie. V2X-Seq: A large-scale sequential dataset for vehicle-infrastructure cooperative perception and forecasting.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.
Jiaxi Liu: Preprint submitted to Elsevier
Page 21 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
Yuanyuan Zha, Wei Shangguan, Junjie Chen, Linguo Chai, Weizhi Qiu, and Antonio M LÃ³pez. Heterogeneous multiscale cooperative perception
for connected autonomous vehicles via v2x interaction. IEEE Internet of Things Journal, 2025.
Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, and Haibao Yu. Cooptrack: Exploring end-to-end learning for efficient cooperative
sequential perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. doi: 10.48550/arXiv.2507.
19239. Highlight paper.
Walter Zimmer, Gerhard Arya Wardana, Suren Sritharan, Xingcheng Zhou, Rui Song, and Alois C. Knoll. Tumtraf V2X cooperative perception
dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.
Jiaxi Liu: Preprint submitted to Elsevier
Page 22 of 22

SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception
(a) Predictions vs. ground truth in BEV with risk overlay.
(b) Risk heatmap and per-cell transmission (Union vs. baselines).
Figure 10: Qualitative example at an unprotected intersection. Our method prioritizes risky blind-zone cells, recovering
occluded targets with fewer transmission bytes.
Jiaxi Liu: Preprint submitted to Elsevier
Page 23 of 22
