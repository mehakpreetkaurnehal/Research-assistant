RynnVLA-002: A Unified Vision-Language-Action
and World Model
Jun Cen1,2,3,âˆ—, Siteng Huang1,2,3,âˆ—, Yuqian Yuan1,3,âˆ—, Kehan Li1,2,âˆ—, Hangjie Yuan1,2,3, Chaohui Yu1,
Yuming Jiang1, Jiayan Guo1, Xin Li1,2, Hao Luo1,2, Fan Wang1, Deli Zhao1,2, Hao Chen3
1DAMO Academy, Alibaba Group 2Hupan Lab 3Zhejiang University
We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world
model leverages action and visual inputs to predict future image states, learning the underlying physics
of the environment to refine action generation. Conversely, the VLA model produces subsequent
actions from image observations, enhancing visual understanding and supporting the world modelâ€™s
image generation. The unified framework of RynnVLA-002 enables joint learning of environmental
dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA
and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both
simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO
simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated
world model boosts the overall success rate by 50%.
Date: November 24, 2025
Code: https://github.com/alibaba-damo-academy/RynnVLA-002
Correspondence: cenjun.cen@alibaba-inc.com
1
Introduction
The Vision-Language-Action (VLA) model has emerged as a promising paradigm for grounding language-
conditioned decision making in visual environments, enabling robots to map instructions and observations to
actions (Zitkovich et al., 2023, 2024). These models are constructed by augmenting large-scale pre-trained
Multimodal Large Language Models (MLLMs) (Liu et al., 2023b; Li et al., 2024a; Zhang et al., 2025a; Bai
et al., 2025) with either an action head or additional action expert module to generate actions. MLLMs
contribute robust capabilities in perception and decision making, enabling VLA models to exhibit enhanced
generalization across a wide range of robotic tasks (Black et al., 2024; Intelligence et al., 2025).
However, standard VLA architectures face three fundamental drawbacks. First, they cannot fully understand
actions because actions reside only on the output side, preventing the model from forming an explicit internal
representation of action dynamics. Second, they lack imagination: they do not predict how the world
might evolve given candidate actions, hindering foresight and counterfactual reasoning. Third, they have no
explicit understanding of physics. Without capturing physical dynamics, the model cannot internalize object
interactions, contact, or stability. World models directly address these limitations by learning to forecast
future observations conditioned on current images and actions, providing agents with action-aware internal
states, imagination, and a physics-informed representation of environment dynamics. (Ha and Schmidhuber,
2018; Wu et al., 2025a). Despite this advantage, world models are constrained by their inability to directly
generate action outputs, resulting in a functional gap that limits their application in scenarios requiring
explicit action planning.
To address the constraints inherent in both VLA models and world models, we introduce RynnVLA-002, an
autoregressive action world model for unified action and image understanding and generation. As depicted
in Fig. 1, RynnVLA-002 employs three separate tokenizers to encode images, text, and actions. The tokens
from different modalities are set to share the same vocabulary so that understanding and generation across
these modalities can be unified within a single LLM architecture. The world model component captures the
underlying physical dynamics of the environment by generating visual representations based on input actions.
This process of action interpretation and environmental physics learning is essential for enabling effective
arXiv:2511.17502v1  [cs.RO]  21 Nov 2025

VLA Model
Semantic 
Encoder
Text
Tokenizer
Image
Text
Action Head
Text
De-Tokenizer
Action
Text
Action World Model
Action
Tokenizer
Action
Action
De-Tokenizer
Action
World Model
Image
Tokenizer
Image
Image 
De-Tokenizer
Image
Action
Tokenizer
Action
(a) VLA Model
Image Understanding
Image Generation
Action Understanding
Action Generation
P
O
P
O
Image Understanding
Image Generation
Action Understanding
Action Generation
P
P
P
O
(b) World Model
Image Understanding
Image Generation
Action Understanding
P
P
P
Action Generation
P
(c) Action World Model
Text
Tokenizer
Text
Text
De-Tokenizer
Text
Image
Tokenizer
Image
Image 
De-Tokenizer
Image
Text
Tokenizer
Text
Text
De-Tokenizer
Text
ï¼ˆe.g., OpenVLA)
ï¼ˆe.g., iVideoGPT)
ï¼ˆe.g., WorldVLA)
Figure 1 (a) VLA model generates actions based on image understanding; (b) World model generates the image based on
image and action understanding; (c) Action World Model unifies both image and action understanding and generation.
decision making within the VLA model. At the same time, the VLA model embedded within RynnVLA-002
refines the understanding of visual data, thereby improving the precision of image generation performed by
the world model. This bidirectional enhancement creates a more robust and comprehensive model capable of
understanding and generating both actions and images.
In this work, we explore different action generation mechanisms. Our initial approach (i.e., WorldVLA (Cen
et al., 2025)) involved discretizing actions and unifying them with image and text tokens within a single
vocabulary. However, we find that this method struggles with generating coherent action chunks. The primary
reason for this is that pretrained multimodal language models have predominantly been exposed to images and
text rather than actions, resulting in limited action generalization capabilities. Furthermore, in autoregressive
models where subsequent actions are conditioned on preceding ones, error propagation becomes a critical issue,
as earlier incorrect predictions influence subsequent actions over time. To alleviate this issue, we proposed
an action attention masking strategy that selectively masks prior actions during the generation of current
actions. This approach effectively mitigates error accumulation and yields substantial improvements in the
task of action chunk generation in simulation.
However, in real-world robot experiments, this discrete design exhibits limited generalization capability
and slow inference. We attribute the poor generalization to the high-volume data requirement of discrete
autoregressive models (Kaplan et al., 2020), which is often unavailable in robotics. The slow inference,
meanwhile, stems from the sequential nature of the autoregressive generation process. To address these
issues, we evolve our architecture into a hybrid model that retains the original discrete joint modeling while
incorporating a continuous Action Transformer head (Zhao et al., 2023). This new head is significantly
smaller than the base LLM, which alleviates overfitting and improves generalization. Furthermore, the Action
Transformerâ€™s parallel decoding and bidirectional attention mechanism reduce the number of decoding steps,
accelerating inference, and generating smoother trajectories.
In summary, our contributions are as follows:
â€¢ We propose RynnVLA-002, an action world model that unifies VLA and World Model in a single
framework.
â€¢ We introduce an action attention masking strategy for the discrete action chunk generation, addressing
the challenge of action error accumulation when autoregressively generating action sequences. An
additional continuous Action Transformer head is added for stronger generalization ability and smoother
trajectory.
â€¢ Our experiments demonstrate that RynnVLA-002 outperforms the standalone VLA and world models,

[âˆ†ğ‘¥, âˆ†ğœƒ, âˆ†ğºğ‘Ÿğ‘–ğ‘]
Generate the 
next frame 
based on the 
current image 
and the action.
[âˆ†ğ‘¥, âˆ†ğœƒ, âˆ†ğºğ‘Ÿğ‘–ğ‘]
World 
Model
Action
De-Tokenizer
What action 
should the 
robot take to 
<task>?
Text
Tokenizer
Ã—ğ‘€
Ã—ğ¾
Image
Tokenizer
Text
Tokenizer
Action
Tokenizer
Image
De-Tokenizer
[ğ‘, ğ‘, ğ‘”]
State
Tokenizer
Image
Tokenizer
Action Transformer
[âˆ†ğ‘¥, âˆ†ğœƒ, âˆ†ğºğ‘Ÿğ‘–ğ‘] Ã—ğ¾
Continuous Actions 
Discrete Actions 
VLA 
Model
RynnVLA-002
Ã—ğ‘
Ã—ğ‘
Figure 2 Overview of RynnVLA-002. RynnVLA-002 involves VLA model data and world model data during the training
process.
highlighting the mutual enhancement between the world model and VLA model. Additionally, RynnVLA-
002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in
real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.
2
Related Work
2.1
Vision-Language-Action Models
VLM-based VLA. Vision-Language Model (VLM)-based VLA models (Brohan et al., 2023; Cheang et al., 2024;
Wen et al., 2025b; Li et al., 2023; Huang et al., 2024; Belkhale and Sadigh, 2024; Zhao et al., 2025b; Wang
et al., 2025a,b) map visualâ€“language inputs to actions. RT-2 (Zitkovich et al., 2023) first co-trained VLMs on
robotic trajectories and web-scale vision-language data, producing actions as discrete tokens. Subsequent
works (Wu et al., 2023; Zitkovich et al., 2024; Li et al., 2025b; Zhen et al., 2024; Pertsch et al., 2025) extend
this architecture to enhance generalization and representation efficiency. To address precision loss from discrete
tokens, LCB (Shentu et al., 2024) introduced a dual-system with a continuous policy head, inspiring variants
with different policy head models like diffusion transformers (Peebles and Xie, 2023), and incorporating
diverse training strategies across multiple embodiments (Zhang et al., 2024; Wen et al., 2024, 2025a; Zhou
et al., 2025; Li et al., 2024b). Recent frameworks such as Ï€0 (Black et al., 2024) leverage conditional flow
matching (Lipman et al., 2022), the open-source GR00T (Bjorck et al., 2025) scales it to complex humanoid
control, and Ï€0.5 (Intelligence et al., 2025) further improves generalization by leveraging large-scale multimodal
web and cross-embodiment data, enabling direct zero-shot deployment across robot platforms.
Visual Generation-based VLA. Beyond static perception, visual-generation approaches model dynamics by
predicting future visual states. UniPi (Du et al., 2023), DREAMGEN (Jang et al., 2025) and GeVRM (Zhang
et al., 2025b) generate future visual observations to guide action generation. Joint frameworks (Guo et al.,
2024; Zheng et al., 2025b; Li et al., 2025a) co-generate future frames and corresponding actions, enhancing
temporal consistency and policy learning. Others exploit future video prediction as a powerful pretraining
objective, including GR-2 (Cheang et al., 2024), VPP (Hu et al., 2024) and RynnVLA-001 (Jiang et al.,
2025). Collectively, these approaches highlight the potential of predictive visual modeling to bridge perception
and action, though challenges remain in visual fidelity, domain transfer, and computational efficiency. Our
RynnVLA-002 is built on Chameleon (Team, 2024), a unified model for image understanding and generation,
thus combining the benefits of both VLM and visual-generation-based approaches.

2.2
World Models
World models endow embodied AI with internal representations (Chen et al., 2022; Robine et al., 2023;
Wang et al., 2024) and predictive dynamics of the external world (Hafner et al., 2019, 2021; Okada and
Taniguchi, 2022), enabling physics-consistent interaction in dynamic environments (Xiang et al., 2023;
Mazzaglia et al., 2024; Ha and Schmidhuber, 2018). Recent advances have realized world models with
transformer-based architectures (Wu et al., 2025b; Robine et al., 2023; Micheli et al., 2022). Notably, Googleâ€™s
Genie framework (Bruce et al., 2024) constructs synthetic interactive environments through large-scale self-
supervised video pretraining. Nowadays, such world models are widely utilized to generate varied training
data (Agarwal et al., 2025), support model-based reinforcement learning algorithms (Wu et al., 2025a), and
aid in selecting the most suitable policies from a pool of generated options (Li et al., 2025a; Bar et al., 2024).
In this work, we show that world model and VLA could enhance each other.
3
Methods
3.1
Overview
The overall architecture of RynnVLA-002 is shown in Fig. 2. As can be seen, our RynnVLA-002 unifies two
foundational models in embodied AI. The first is the VLA model, where a policy Ï€ generates an action at
based on a language goal l, a proprioceptive state stâˆ’1, and an observation history otâˆ’h:t:
at âˆ¼Ï€(at | l, stâˆ’1, otâˆ’h:t).
(1)
The second is the world model, where the model f predicts the next observation ot from past observations
and actions:
Ë†ot âˆ¼f(ot | otâˆ’h:tâˆ’1, atâˆ’h:tâˆ’1).
(2)
We mix the VLA model data and the world model data to train the RynnVLA-002, an integrated model MÏˆ
that consolidates the capabilities of action prediction and world modeling. The dual nature of our model
is captured by its ability to be queried either as a VLA or as a world model, leveraging a shared group of
parameters Ïˆ.
3.2
Data Tokenization
Tokenizers. We initialize the model from Chameleon (Team, 2024) since it is a unified model for image
understanding and generation. Four tokenizers are involved, including an image tokenizer, a text tokenizer, a
state tokenizer, and an action tokenizer. The image tokenizer is a VQ-GAN model (Esser et al., 2021) with
additional perceptual losses to specific image regions, e.g., faces and salient objects (Gafni et al., 2022). The
compression ratio of the image tokenizer is 16 and the codebook size is 8192. The image tokenizer generates
256 tokens for 256 Ã— 256 images and 1024 tokens for 512 Ã— 512 images. The text tokenizer is a trained BPE
tokenizer (Sennrich et al., 2015). The image and text tokenizers are inherited from Chameleon. The state and
action tokenizer discretizes each dimension of continuous robot proprioceptive states and actions into one of
256 bins, with bin widths determined by the range of the training data (Zitkovich et al., 2024, 2023). All
image, text, action, and state tokens are in a single token vocabulary with the size of 65536. The continuous
actions generated by the Action Transformer are raw actions without tokenization.
VLA Model Data. The overall token sequence of VLA model data is:
{text} {state} {image-front-wrist}
|
{z
}
Ã—M
Ldis_action
z
}|
{
{action}
|
{z
}
Ã—K
The VLA model generates K actions based on the language instruction, proprioceptive state, and M historical
image observations. The text inputs are â€œWhat action should the robot take to + <task>+ ?â€. Ldis_action
refers to the cross-entropy loss of discrete action tokens.
World Model Data. World model generates the next image frame given the current image observation and
action. The overall token sequence is:

ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›!
ğ‘‡ğ‘’ğ‘¥ğ‘¡
ğ¼ğ‘šğ‘ğ‘”ğ‘’
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›"
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›!
ğ‘‡ğ‘’ğ‘¥ğ‘¡
ğ¼ğ‘šğ‘ğ‘”ğ‘’
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›"
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
ğ‘‡ğ‘’ğ‘¥ğ‘¡
ğ¼ğ‘šğ‘ğ‘”ğ‘’!
ğ¼ğ‘šğ‘ğ‘”ğ‘’"
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›
ğ‘‡ğ‘’ğ‘¥ğ‘¡
ğ¼ğ‘šğ‘ğ‘”ğ‘’!
ğ¼ğ‘šğ‘ğ‘”ğ‘’"
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›!
ğ‘‡ğ‘’ğ‘¥ğ‘¡
ğ¼ğ‘šğ‘ğ‘”ğ‘’
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›"
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›!
ğ‘‡ğ‘’ğ‘¥ğ‘¡
ğ¼ğ‘šğ‘ğ‘”ğ‘’
ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›"
(a) Default VLA model attention mask
(b) Our VLA model attention mask
(c) World model attention mask
Figure 3 Attention mask of (a) default VLA model, (2) our proposed VLA model, and (c) world model.
{text} {images-front-wrist}{action}
Limg
z
}|
{
{images-front-wrist}
|
{z
}
Ã—N
All of the training instances for world model share the same text prefix â€œGenerate the next frame based on
the current image and the action.â€ and there are no other task instructions since the action could totally
determine the next state of the world. The overall generation could repeat N times in an autoregressive
manner. Limg refers to the cross-entropy loss of discrete image tokens.
Training Objective. We mix the VLA model data and world model data to train our RynnVLA-002. The overall
loss function is Ldis = Ldis_action + Limg. In this way, RynnVLA-002 could behave as the VLA model or
world model depending on the user queries.
3.3
Action Chunk Generation
Attention Mask for Discrete Action Chunk. Generating multiple actions for execution is critical for efficiency
and a higher success rate (Kim et al., 2025). However, we find that naively generating consecutive actions in
the autoregressive model degrades the performance. Although the foundational MLLM demonstrates robust
generalization capabilities across the image and text domains, its capacity to generalize effectively in the
action domain is comparatively limited. Consequently, errors originating from earlier actions propagate to
subsequent actions under the default causal attention mask, resulting in performance degradation. To address
this limitation, we introduce an alternative attention mask tailored for action generation, depicted in Fig. 3
(b). This modified mask ensures that current actions rely solely on textual and visual input, while prohibiting
access to prior actions. Such design enables the autoregressive framework to generate multiple actions in
isolation, mitigating the error accumulation problem. The world model part adheres to the conventional
attention mask, as shown in Fig. 3 (c).
Action Transformer for Continuous Action Chunk. Although our discrete action chunking model performs well in
simulation, it rarely succeeds in real-world robot experiments. This discrepancy arises because real-world
applications demand significantly higher generalization to cope with dynamic variables like lighting and
object positioningâ€”factors not fully captured in simulation. The failure of our discrete model is rooted in
two key issues. First, its large autoregressive architecture is prone to severe overfitting when trained on
limited real-world dataset, leading to poor generalization. Second, our designed attention mask makes the
autoregressive model generate each action in isolation within the same chunk, which cannot ensure trajectory
continuity, resulting in severe shaking and non-smooth movements that drastically reduce the success rate.
To overcome these challenges, we propose to augment our architecture with a dedicated Action Transformer
to generate continuous action sequences (Zhao et al., 2023). This module processes the full contextâ€”including
language, image, and state tokensâ€”and utilizes learnable action queries to output an entire action chunk
in parallel. This design provides two distinct advantages. First, the Action Transformerâ€™s more compact

Table 1 Evaluation results on LIBERO benchmark. Pretraining means the model is pretrained on the large-scale robot
manipulation data.
Methods
Pretraining
Action Type
Spatial
Object
Goal
Long
Average
LAPA (Ye et al., 2024)
âœ—
Discrete
73.8
74.6
58.8
55.4
65.7
TraceVLA (Zheng et al., 2025a)
âœ“
Discrete
84.6
85.2
75.1
54.1
74.8
OpenVLA (Zitkovich et al., 2024)
âœ“
Discrete
84.7
88.4
79.2
53.7
76.5
SpatialVLA (Qu et al., 2025)
âœ“
Discrete
88.2
89.9
78.6
55.5
78.1
NORA (Hung et al., 2025)
âœ—
Discrete
85.6
89.4
80.0
63.0
79.5
CoT-VLA (Zhao et al., 2025a)
âœ“
Discrete
87.5
91.6
87.6
69.0
83.9
Ï€0-FAST (Black et al., 2024)
âœ“
Discrete
96.4
96.8
88.6
60.2
85.5
MolmoAct (Lee et al., 2025)
âœ“
Discrete
87.0
95.4
87.6
77.2
86.6
FlowVLA (Zhong et al., 2025)
âœ—
Discrete
93.2
95.0
91.6
72.6
88.1
UniVLA (Bu et al., 2025)
âœ“
Discrete
96.5
96.8
95.6
92.0
95.2
Diffusion Policy (Chi et al., 2025)
âœ—
Continuous
78.3
92.5
68.3
50.5
72.4
Octo (Team et al., 2024)
âœ“
Continuous
78.9
85.7
84.6
51.1
75.1
MDT (Reuss et al., 2024)
âœ—
Continuous
78.5
87.5
73.5
64.8
76.1
DiT Policy (Hou et al., 2024)
âœ“
Continuous
84.2
96.3
85.4
63.8
82.4
MaIL (Jia et al., 2024)
âœ—
Continuous
74.3
90.1
81.8
78.6
83.5
ThinkAct (Huang et al., 2025)
âœ“
Continuous
88.3
91.4
87.1
70.9
84.4
Ï€0 (Black et al., 2024)
âœ“
Continuous
90.0
86.0
95.0
73.0
86.0
SmolVLA (Shukor et al., 2025)
âœ—
Continuous
93.0
94.0
91.0
77.0
88.8
OpenVLA-OFT (Kim et al., 2025)
âœ“
Continuous
97.6
98.4
97.9
94.5
97.1
Seer (Tian et al., 2024)
âœ“
Continuous
â€“
â€“
â€“
87.7
â€“
UVA (Li et al., 2025a)
âœ—
Continuous
â€“
â€“
â€“
93.0
â€“
RynnVLA-002-Discrete
âœ—
Discrete
94.2
96.8
94.6
87.6
93.3
RynnVLA-002-Continuous
âœ—
Continuous
99.0
99.8
96.4
94.4
97.4
architecture is less prone to overfitting on limited data, thereby improving generalization and producing
fluid, stable actions. Second, by parallelly generating all actions in a single forward pass, it substantially
accelerates the inference speed compared to the autoregressive baselines that generate actions sequentially.
We use L1 regression loss Lconti_action to supervise the Action Transformer. The overall loss function is
L = Ldis + Î±Lconti = Ldis_action + Limg + Î±Lconti_action.
4
Experiments
4.1
Simulation Results
Benchmark. We evaluate our method on the LIBERO benchmark (Liu et al., 2023a). This benchmark is
composed of four distinct suites designed to test a range of robotic capabilities: (1) LIBERO-Spatial focuses
on spatial relationships by tasking the robot with placing a bowl in various locations; (2) LIBERO-Object
emphasizes object recognition and manipulation with unique objects; (3) LIBERO-Goal tests procedural
learning by varying task goals while using a fixed set of objects; (4) LIBERO-Long contains 10 complex
long-horizon manipulation tasks.
Dataset and Preprocessing. We first curate the dataset by removing unsuccessful trajectories and filtering
out â€œno-operationâ€ actions, a procedure also adopted by OpenVLA (Zitkovich et al., 2024). For world model
evaluation, which relies on ground-truth video-action pairs, we partition the cleaned data into a 90% training
set and a 10% validation set.
Hyperparameter Settings.
The VLA model takes M = 2 historical image frames as input.
We set
the action chunk size K = 10 for the longer LIBERO-Long and LIBERO-Spatial tasks and K = 5
for the shorter LIBERO-Object and LIBERO-Goal tasks.
For the world model, we use a single pre-
diction round (N = 1) to maintain computational efficiency.
The loss weighting parameter Î± = 10.

Table 2 Evaluation results on real-world SO100 robots. Success rate is reported.
Place the block inside the circle.
Pretraining
Single-Target
Multi-Target
w/ Distractors
GR00T N1.5 (Bjorck et al., 2025)
âœ“
90.0
60.0
50.0
Ï€0 (Black et al., 2024)
âœ“
100.0
70.0
50.0
RynnVLA-002
âœ—
90.0
90.0
80.0
Place the strawberries into the cup.
Pretraining
Single-Target
Multi-Target
w/ Distractors
GR00T N1.5 (Bjorck et al., 2025)
âœ“
50.0
50.0
70.0
Ï€0 (Black et al., 2024)
âœ“
80.0
70.0
40.0
RynnVLA-002
âœ—
80.0
80.0
50.0
(a)
(b)
(c)
Figure 4 Real-world robot settings. (a) Place the block
inside the circle. (b) Place the strawberries into the cup.
(c) Task with distractors.
Metrics. Our evaluation is twofold. To assess the
VLA model, we measure its success rate across 50
deployment rollouts per task, each initialized in a
different state. To assess the world model, we mea-
sure its video prediction accuracy on the held-out
validation set using four standard metrics: FrÃ©chet
Video Distance (FVD), Peak Signal-to-Noise Ra-
tio (PSNR), Structural Similarity Index (SSIM),
and Learned Perceptual Image Patch Similarity
(LPIPS).
Benchmark Results. We evaluate the performance
of discrete actions and continuous actions separately. As shown in Tab. 1, our RynnVLA-002 achieves high
success rates of 93.3% with discrete actions and 97.4% with continuous actions, demonstrating the effectiveness
of our core design principles: jointly learning the VLA modeling and world modeling, an attention mask
mechanism for discrete action generation, and the added continuous Action Transformer. Surprisingly, our
RynnVLA-002, without any pretraining, is still on par with strong baseline models pretrained on either
LIBERO-90 or massive real-robot datasets (Tian et al., 2024; Bu et al., 2025; Kim et al., 2025).
4.2
Real-World Robot Results
Datasets.
We curate a new real-world manipulation dataset collected with a LeRobot SO100 robotic
arm (Cadene et al., 2024). All trajectories are expert demonstrations obtained via human teleoperation. We
define two pick and place tasks for evaluation. (1) Place the block inside the circle: Emphasizing basic object
detection and grasp execution (248 demonstrations); (2) Place strawberries in the cup: Requiring fine-grained
localization and grasp-point prediction (249 demonstrations).
Baselines. We compare with two strong open-source baselines: GR00T N1.5 (Bjorck et al., 2025) and Ï€0 (Black
et al., 2024). For both methods, we initialize from the official pretrained checkpoints and finetune them on
the same SO100 dataset used for our model. We adopt the same recipe from the official codebases of these
baselines to do finetuning.
Evaluation. As shown in Fig. 4, our evaluation spans three scenarios: (1) Single-target manipulation, with
exactly one target object on the desktop; (2) Multi-target manipulation, with multiple target objects present;
and (3) Instruction-following with distractors, where both targets and distractors appear. A trial is deemed
successful if the robot places at least one target object into its designated location within a predefined time
budget. A trial fails if: (1) the time limit is exceeded; (2) the robot accrues more than five consecutive failed
grasp attempts on a target; (3) in the instruction-following with distractors setting, the agent attempts to
manipulate any distractor objects. Each task is tested for 10 times and we report the success rate.
Results. Tab. 2 shows the results of real-world robot experiments. Our RynnVLA-002 achieves competitive
results with GR00T N1.5 (Bjorck et al., 2025) and Ï€0 (Black et al., 2024) without pretraining. Notably,
RynnVLA-002 performs better than the baselines in cluttered environments. For instance, RynnVLA-002 has
over 80% success rate on both multi-target tasks and distractor-filled scenarios for the "Place the block" task,
surpassing the baselines by 10% to 30%.

Table 3 Ablation study of VLA model using discrete actions on LIBERO benchmark.
Index
VLA
Discrete
World
Model
Action
Chunking
Our VLA model
Attention Mask
Goal
Object
Spatial
Long
Average
1
âœ“
âœ—
âœ—
âœ—
67.3
82.9
77.8
23.0
62.8
2
âœ“
âœ“
âœ—
âœ—
73.1
88.0
80.2
27.3
67.2
3
âœ“
âœ—
âœ“
âœ—
79.6
82.9
36.7
16.9
54.0
4
âœ“
âœ—
âœ“
âœ“
84.4
90.9
81.8
49.3
76.6
5
âœ“
âœ“
âœ“
âœ“
85.1
90.9
84.0
52.4
78.1
Table 4 Ablation study of VLA model using continuous actions on LIBERO benchmark.
Index
VLA
Continuous
World
Model
Wrist
Camera
Proprioceptive
State
Goal
Object
Spatial
Long
Average
1
âœ“
âœ—
âœ—
âœ—
90.2
92.4
88.4
67.0
84.5
2
âœ“
âœ—
âœ“
âœ—
91.4
95.4
98.2
81.4
91.6
3
âœ“
âœ“
âœ“
âœ—
96.0
97.4
99.0
85.8
94.6
4
âœ“
âœ“
âœ“
âœ“
96.4
99.8
99.0
94.4
97.4
Figure 5 VLA model visualization on LIBERO. Task: put the cream cheese in the bowl. Top: w/o world model.
Bottom: w/ world model.
4.3
Ablation Study
World Model Benefits the VLA Model. On the LIBERO simulation benchmark, incorporating world model data
during training consistently improves performance. Specifically, in Tab. 3, the success rate for discrete actions
increases from 62.8% (Line 1) to 67.8% (Line 2), and from 76.6% (Line 4) to 78.1% (Line 5). A similar trend
is observed for continuous actions in Tab. 4, where the success rate increases from 91.6% (Line 2) to 94.6%
(Line 3). In real-world robot experiments, the benefit of world model data is even more pronounced. As shown
in Line 4 of Tab. 5, the model trained without world model data achieves a very low success rate on real-world
robot tasks, which is below 30%. In contrast, augmenting VLA training with world model significantly boosts
performance to over 80%. Fig. 5 shows that the model trained without world model data moves directly
toward the target location without successfully grasping the cheese or bottle, while the model jointly trained
with world model keeps retrying to grasp the target object when encountering failures. This behavior suggests
that the world model data helps the VLA model focus more on the manipulated objectsâ€”because the world
modelâ€™s training objective requires accurate prediction of object motion, thereby reinforcing attention to
object interaction dynamics.
VLA Model Enhances the World Model.
As shown in Tab. 6, the model trained on a mixture of VLA
and world model data achieves generation results that are comparable to or better than those of the
world model trained solely on world data.
Furthermore, Fig. 7 compares video generations from our
action world model with a baseline world model trained without VLA data. The baseline world model
fails to predict a successful grasp of the bowl from the front camera perspective in both two examples.

Table 5 Ablation study of VLA model on real-world robots.
Index
Action
Type
World
Model
Wrist
Camera
Proprioceptive
State
Single
Target
Multi
Target
with
Distractors
1
Discrete
âœ“
âœ“
âœ“
0
0
0
2
Continuous
âœ“
âœ“
âœ—
0
0
0
3
Continuous
âœ“
âœ—
âœ“
0
0
0
4
Continuous
âœ—
âœ“
âœ“
30.0
10.0
0
5
Continuous
âœ“
âœ“
âœ“
80.0
80.0
50.0
Figure 6 Ablation study of chunk length with discrete actions.
Table 6 Ablation study of world model on LIBERO validation
set.
Goal
FVDâ†“PSNRâ†‘SSIMâ†‘LPIPSâ†“
World Model
370.0
22.25
77.84
19.70
Action World Model
336.8
22.13
78.13
19.43
Object
FVDâ†“PSNRâ†‘SSIMâ†‘LPIPSâ†“
World Model
1141.6
20.31
59.59
27.30
Action World Model
877.2
22.18
65.03
22.60
Spatial
FVDâ†“PSNRâ†‘SSIMâ†‘LPIPSâ†“
World Model
405.4
22.32
79.15
20.28
Action World Model
373.1
23.88
82.41
16.33
Long
FVDâ†“PSNRâ†‘SSIMâ†‘LPIPSâ†“
World Model
557.73
18.24
69.16
31.60
Action World Model 427.86
19.36
72.19
27.78
In contrast, our action world model consistently
generates correct videos depicting a successful
grasp. Notably, the baseline world model also
exhibits a significant inconsistency: as seen in
Fig. 7 (a), the front camera shows a failed grasp
while the wrist camera shows a successful one.
This highlights a critical disalignment between
the modelâ€™s predictions for different viewpoints.
The visualization results validate that the im-
age understanding capabilities inherited from
the VLA model strengthen the world modelâ€™s
generation performance.
Attention Mask for Discrete Action Chunk Genera-
tion. Simultaneous generation of multiple actions
is essential for achieving effective and efficient
grasping. However, we observe that a vanilla
autoregressive approachâ€”where actions are gen-
erated sequentiallyâ€”can degrade model perfor-
mance, as evidenced by the results in row 3 of Table 3 and Fig. 6. The grasping success rate gradually
decreases with longer action chunks. This degradation arises because later actions become overly dependent
on preceding ones since they share the same space, rather than being grounded in visual input which is
a distinct modality. The generalization of the action is not that strong as this modality was not involved
during pretraining the MLLM. Consequently, errors tend to accumulate as the sequence of generated actions
increases. The proposed attention masking mechanism ensures that each action is generated independently
and solely determined by the visual input, thereby mitigating the issue of error propagation within the action
sequence. As illustrated in Fig. 6, the model incorporating the proposed attention mask demonstrates superior
performance compared to the vanilla attention mask, particularly under conditions of longer chunk lengths.
This highlights the efficacy of the introduced masking approach. If the length of the action chunk is excessively
prolonged, the robotâ€™s ability to timely adapt its policy becomes constrained, leading to a decline in overall
performance.
DiscreteActionsAcceleratetheConvergence. We retain discrete actions during training alongside the continuous
Action Transformer, as we find that this hybrid approach not only speeds up the convergence of VLA training

(a) Task: put the bowl on top of the cabinet. Top: our action world model. Bottom: world model.
(b) Task: pick up the black bowl and place it on the plate. Top: our action world model. Bottom: world model.
Figure 7 World model visualization.
Figure 8 Discrete action tokens accelerate the convergence of
continuous action generation.
but also improves the ultimate success rate. As
shown in Fig. 8, models trained with discrete
action tokens achieve a substantially higher suc-
cess rate than those trained without them, with
the advantage being most pronounced during the
initial stages of training.
Ablation Study of Wrist Camera and Proprioceptive
State. As shown in Line 1 of Tab. 4, our model
could achieve reasonable performance without
the wrist camera or proprioceptive state on the
LIBERO simulation benchmark. Incorporating these two sources of information brings in further performance

Table 7 Ablation study of VLA model on efficiency and action chunking on LIBERO benchmark. Frequency is measured
in Hz.
Chunk Size = 5
Chunk Size = 10
Index
Action
Type
Action
Chunking
Wrist
Camera
History
Length
Frequency
Goal
Object
Frequency
Spatial
Long
1
Discrete
âœ—
âœ—
0
2.50
60.0
71.0
2.50
77.0
10.4
2
Discrete
âœ“
âœ—
0
3.69
83.2
90.0
3.69
83.6
46.0
3
Discrete
âœ—
âœ—
1
1.88
74.6
72.8
1.88
78.6
17.0
4
Discrete
âœ“
âœ—
1
3.24
92.8
91.6
3.24
86.2
64.0
5
Discrete
âœ—
âœ“
1
1.25
82.6
89.6
1.25
96.6
61.2
6
Discrete
âœ“
âœ“
1
2.74
92.8
97.8
2.74
96.8
80.8
7
Continuous
âœ“
âœ—
0
24.94
80.0
89.6
48.20
84.0
48.6
8
Continuous
âœ“
âœ—
1
14.97
90.2
92.4
28.30
88.4
67.0
9
Continuous
âœ“
âœ“
1
7.75
91.4
99.4
15.78
98.2
81.4
Figure 9 Performance comparison between discrete action and continuous action.
gains (see Line 2 and Line 4 in Tab. 4). In real-world experiments (Line 2 and Line 3 in Tab. 5), the robot
consistently fails when the wrist camera or proprioceptive state is absent. On one hand, the wrist camera
provides crucial visual feedback on the relative pose between the gripper and the object, especially when the
robot is outside the field of view of the front camera. On the other hand, we find proprioceptive state is
essential for accurately timing gripper closure and object lifting during manipulation.
Efficiency Analysis. As shown in Tab. 7, incorporating additional input images, such as images from the wrist
camera or historical frames, improves performance but reduces speed. For discrete actions, action chunking
yields both higher inference speed and better performance compared to generating a single action per inference
step. Continuous action generation is significantly faster owing to its parallel generation nature, and frequency
scale almost linearly with chunk size as generating additional actions incurs negligible extra time.
Discrete Action and Continuous Action. Our model supports both discrete and continuous action generation,
and the experimental results reveal a clear advantage for the latter. Concretely, in the LIBERO simulation
benchmark, continuous actions result in significantly faster convergence (Fig. 9). Although the final per-
formances of these two models in simulation are comparable, their performance gap becomes considerably
significant in real-robot experiments. As shown in Table 5, our real-world experiments demonstrate a much
more substantial improvement when using continuous actions over discrete ones.
Table 8 Ablation study of world model pretraining.
Goal Object Spatial Long
w/o World Model Pretrain 67.3
82.9
77.8
23.0
w/ World Model Pretrain
73.1
84.0
79.8
30.2
World Model Pretraining for VLA Model.
Our
RynnVLA-002 unifies VLA model and world
model into a single training stage. We further
investigate the possibility of cold starting a VLA
model with world model pretraining. This form
of pretraining necessitates that the model de-
velop an understanding of visual inputs, actions,
and the underlying physical dynamics governing state transitions. We pretrain the model using the same
data source as the VLA model. As presented in Table 8, employing the world model for pretraining leads to

notable improvements in grasping performance. These findings highlight the potential of leveraging world
model pretraining in robotic applications, particularly in enhancing task-specific performance through prior
exposure to general world knowledge.
5
Conclusion
In this work, we propose RynnVLA-002, a unified framework that integrates the VLA and world model,
demonstrating that they enhance each other. Through this contribution, we aim to offer the embodied AI
research community a concrete methodology for enabling the synergistic interplay between the VLA and world
model. Furthermore, we believe this work helps lay the groundwork for a unified foundation for multi-modal
understanding and generation that spans text, vision, and action.
References
Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin
Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical AI. arXiv preprint
arXiv:2501.03575, 2025.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun
Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025.
Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models, 2024. https:
//arxiv.org/abs/2412.03572.
Suneel Belkhale and Dorsa Sadigh. MiniVLA: A better VLA with a smaller footprint, 2024. https://github.com/
Stanford-ILIAD/openvla-mini.
Johan Bjorck, Fernando CastaÃ±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox,
Fengyuan Hu, Spencer Huang, et al. GR00T N1: An open foundation model for generalist humanoid robots. arXiv
preprint arXiv:2503.14734, 2025.
Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom,
Karol Hausman, Brian Ichter, et al. Ï€0: A vision-language-action flow model for general robot control. arXiv
preprint arXiv:2410.24164, 2024.
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakr-
ishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. RT-1: Robotics transformer for real-world control at
scale. Robotics: Science and Systems, 2023.
Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi
Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first
International Conference on Machine Learning, 2024.
Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li.
Learning to act anywhere with task-centric latent actions. arXiv preprint arXiv:2502.14420, 2025.
Remi Cadene, Simon Alibert, Alexander Soare, Quentin Gallouedec, Adil Zouitine, Steven Palma, Pepijn Kooijmans,
Michel Aractingi, Mustafa Shukor, Dana Aubakirova, Martino Russi, Francesco Capuano, Caroline Pascal, Jade
Choghari, Jess Moss, and Thomas Wolf. LeRobot: State-of-the-art machine learning for real-world robotics in
Pytorch. https://github.com/huggingface/lerobot, 2024.
Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan
Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025.
Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu,
Yichu Yang, et al. GR-2: A generative video-language-action model with web-scale knowledge for robot manipulation.
arXiv preprint arXiv:2410.06158, 2024.
Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. TransDreamer: Reinforcement learning with transformer
world models. arXiv preprint arXiv:2202.09481, 2022.

Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song.
Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research,
pages 1684â€“1704, 2025.
Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel.
Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems,
36:9156â€“9172, 2023.
Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12873â€“12883, 2021.
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-A-Scene: Scene-based
text-to-image generation with human priors. In European Conference on Computer Vision, pages 89â€“106. Springer,
2022.
Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction
with action: Visual policy learning via joint denoising process. Advances in Neural Information Processing Systems,
37:112386â€“112410, 2024.
David Ha and JÃ¼rgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by
latent imagination. arXiv preprint arXiv:1912.01603, 2019.
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering Atari with discrete world models.
In International Conference on Learning Representations, 2021.
Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, and Yuntao
Chen. Diffusion transformer policy. arXiv preprint arXiv:2410.15959, 2024.
Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao
Lu, and Jianyu Chen. Video prediction policy: A generalist robot policy with predictive visual representations.
arXiv preprint arXiv:2412.14803, 2024.
Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. ThinkAct: Vision-language-
action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025.
Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu,
Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3D world. In International Conference on
Machine Learning, 2024.
Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, U Tan, Navonil Majumder, Soujanya Poria, et al. NORA:
A small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854,
2025.
Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail,
Michael Equi, Chelsea Finn, Niccolo Fusai, et al. Ï€0.5: a vision-language-action model with open-world generalization.
arXiv preprint arXiv:2504.16054, 2025.
Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil
Kundalia, Yen-Chen Lin, et al. DreamGen: Unlocking generalization in robot learning through neural trajectories.
arXiv preprint arXiv:2505.12705, 2025.
Xiaogang Jia, Qian Wang, Atalay Donat, Bowen Xing, Ge Li, Hongyi Zhou, Onur Celik, Denis Blessing, Rudolf
Lioutikov, and Gerhard Neumann. MaIL: Improving imitation learning with selective state space models. In
Conference on Robot Learning, 2024.
Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang,
Mingxiu Chen, et al. RynnVLA-001: Using human demonstrations to improve robot manipulation. arXiv preprint
arXiv:2509.15212, 2025.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361,
2020.
Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and
success. arXiv preprint arXiv:2502.19645, 2025.

Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang,
Sangho Lee, et al. MolmoAct: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917,
2025.
Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li,
Ziwei Liu, et al. LLaVA-OneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a.
Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong
Zhang, et al. CogACT: A foundational vision-language-action model for synergizing cognition and action in robotic
manipulation. arXiv preprint arXiv:2411.19650, 2024b.
Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200,
2025a.
Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe,
Ryan Burgert, Mu Cai, Yong Jae Lee, et al. LLaRA: Supercharging robot learning data for vision-language policy.
In International Conference on Learning Representations, 2025b.
Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang,
Huaping Liu, et al. Vision-language foundation models as effective robot imitators. arXiv preprint arXiv:2311.01378,
2023.
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative
modeling. arXiv preprint arXiv:2210.02747, 2022.
Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking
knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:44776â€“44791,
2023a.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information
Processing Systems, 36:34892â€“34916, 2023b.
Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and Sai Rajeswar. GenRL: Multimodal-foundation
world models for generalization in embodied agents. Advances in Neural Information Processing Systems, 37:
27529â€“27555, 2024.
Vincent Micheli, Eloi Alonso, and FranÃ§ois Fleuret. Transformers are sample-efficient world models. arXiv preprint
arXiv:2209.00588, 2022.
Masashi Okada and Tadahiro Taniguchi. Dreamingv2: Reinforcement learning with discrete world models without
reconstruction. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 985â€“991. IEEE,
2022.
William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 4195â€“4205, 2023.
Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn,
and Sergey Levine.
FAST: Efficient action tokenization for vision-language-action models.
arXiv preprint
arXiv:2501.09747, 2025.
Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao,
Dong Wang, et al. SpatialVLA: Exploring spatial representations for visual-language-action model. arXiv preprint
arXiv:2501.15830, 2025.
Moritz Reuss, Ã–mer ErdinÃ§ YaÄŸmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer:
Learning versatile behavior from multimodal goals. arXiv preprint arXiv:2407.05996, 2024.
Jan Robine, Marc HÃ¶ftmann, Tobias Uelwer, and Stefan Harmeling. Transformer-based world models are happy with
100k interactions. arXiv preprint arXiv:2303.07109, 2023.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units.
arXiv preprint arXiv:1508.07909, 2015.
Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From LLMs to actions: Latent codes as bridges
in hierarchical robot control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
8539â€“8546. IEEE, 2024.

Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel
Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. SmolVLA: A vision-language-action model for
affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025.
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.
Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna,
Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213,
2024.
Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics
models are scalable learners for robotic manipulation. arXiv preprint arXiv:2412.15109, 2024.
Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, and Jiwen Lu. WorldDreamer: Towards general
world models for video generation via predicting masked tokens. arXiv preprint arXiv:2401.09985, 2024.
Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, and Tong He. VQ-VLA: Improving vision-language-
action models via scaling vector-quantized action tokenizers. arXiv preprint arXiv:2507.01016, 2025a.
Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao,
Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, and Donglin Wang. VLA-Adapter:
An effective paradigm for tiny-scale vision-language-action model. In Proceedings of the 40th AAAI Conference on
Artificial Intelligence, 2025b.
Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin
Peng, Chaomin Shen, et al. Diffusion-VLA: Scaling robot foundation models via unified diffusion and autoregression.
arXiv preprint arXiv:2412.03293, 2024.
Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. DexVLA: Vision-language model
with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025a.
Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin
Shen, et al. TinyVLA: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE
Robotics and Automation Letters, 2025b.
Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and
Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. arXiv preprint
arXiv:2312.13139, 2023.
Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. iVideoGPT: Interactive
videogpts are scalable world models. Advances in Neural Information Processing Systems, 37:68082â€“68119, 2025a.
Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen, Lele Cheng, Yan Li, Tingting Gao, and Di Zhang.
Paragraph-to-image generation with information-enriched diffusion model. International Journal of Computer Vision,
pages 1â€“22, 2025b.
Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Language models meet
world models: Embodied experiences enhance language models. Advances in Neural Information Processing Systems,
36:75392â€“75412, 2023.
Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Sejune Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan,
Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758, 2024.
Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming
Jiang, Hang Zhang, Xin Li, et al. VideoLLaMA 3: Frontier multimodal foundation models for image and video
understanding. arXiv preprint arXiv:2501.13106, 2025a.
Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. GEVRM: Goal-expressive video
generation model for robust visual manipulation. In International Conference on Learning Representations, 2025b.
Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. HiRT:
Enhancing robotic control with hierarchical robot transformers. In Conference on Robot Learning, 2024.
Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han,
Chelsea Finn, et al. CoT-VLA: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings
of the Computer Vision and Pattern Recognition Conference, pages 1702â€“1713, 2025a.

Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with
low-cost hardware. Robotics: Science and Systems, 2023.
Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, and Donglin Wang. VLAS: Vision-
language-action model with speech instructions for customized robot manipulation. In International Conference on
Learning Representations, 2025b.
Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3D-VLA:
A 3D vision-language-action generative world model. In International Conference on Machine Learning, 2024.
Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal DaumÃ© III, Andrey Kolobov, Furong Huang, and
Jianwei Yang. TraceVLA: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies.
In International Conference on Learning Representations, 2025a.
Ruijie Zheng, Jing Wang, Scott Reed, Johan Bjorck, Yu Fang, Fengyuan Hu, Joel Jang, Kaushil Kundalia, Zongyu Lin,
Loic Magne, et al. FLARE: Robot learning with implicit world modeling. arXiv preprint arXiv:2505.15659, 2025b.
Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li.
FlowVLA: Thinking in motion with a visual chain of thought. arXiv preprint arXiv:2508.18269, 2025.
Zhongyi Zhou, Yichen Zhu, Minjie Zhu, Junjie Wen, Ning Liu, Zhiyuan Xu, Weibin Meng, Ran Cheng, Yaxin Peng,
Chaomin Shen, et al. ChatVLA: Unified multimodal understanding and robot control with vision-language-action
model. arXiv preprint arXiv:2502.14420, 2025.
Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker,
Ayzaan Wahid, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. In Conference
on Robot Learning, pages 2165â€“2183. PMLR, 2023.
Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker,
Ayzaan Wahid, et al. OpenVLA: An open-source vision-language-action model. In Conference on Robot Learning,
pages 2679â€“2713. PMLR, 2024.
