1â€“30
Harnessing Data from Clustered LQR Systems: Personalized and
Collaborative Policy Optimization
Vinay Kanakeri
VKANAKE@NCSU.EDU
Department of Electrical and Computer Engineering, North Carolina State University
Shivam Bajaj
BAJAJ41@PURDUE.EDU
The Elmore Family School of Electrical and Computer Engineering, Purdue University
Ashwin Verma
VERMA240@PURDUE.EDU
The Elmore Family School of Electrical and Computer Engineering, Purdue University
Vijay Gupta
GUPTA869@PURDUE.EDU
The Elmore Family School of Electrical and Computer Engineering, Purdue University
Aritra Mitra
AMITRA2@NCSU.EDU
Department of Electrical and Computer Engineering, North Carolina State University
Abstract
It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it
has been proposed that the learning algorithm utilize data from â€˜approximately similarâ€™ processes.
However, since the process models are unknown, identifying which other processes are similar poses
a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic
Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding
to a copy of a linear process to be controlled. The agentsâ€™ local processes can be partitioned into
clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination
and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous
clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable
notion of cluster separation that captures differences in closed-loop performance across systems,
we prove that our approach guarantees correct clustering with high probability. Furthermore, we
show that the sub-optimality gap of the policy learned for each cluster scales inversely with the
size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based
control. Our work is the first to reveal how clustering can be used in data-driven control to learn
personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality
due to inclusion of data from dissimilar processes. From a distributed implementation perspective,
our method is attractive as it incurs only a mild logarithmic communication overhead.
Keywords: Policy gradients for LQR; Collaborative Learning; Transfer/Multi-Task Learning.
1. Introduction
The last decade or so has seen a surge of interest in model-free data-driven control (Hu et al., 2023),
where control laws (policies) are learned directly from data, bypassing the need to estimate the
system model as an intermediate step. Although such a framework is promising, it relies on the
availability of adequate data to learn high-precision policies. Unfortunately, however, data from
physical processes (such as real-world robotic environments) could be scarce and/or difficult to
collect. Drawing inspiration from popular paradigms such as federated and meta-learning, some
recent papers (Zhang et al., 2023; Wang et al., 2023a; Toso et al., 2024) have attempted to mitigate
this challenge by exploring the idea of combining information generated by multiple environments,
where each environment represents a dynamical system with an associated cost performance metric
Â© V. Kanakeri, S. Bajaj, A. Verma, V. Gupta & A. Mitra.
arXiv:2511.17489v1  [cs.LG]  21 Nov 2025

KANAKERI BAJAJ VERMA GUPTA MITRA
that captures a task or a goal. The unifying theme in such papers is to learn a single common policy
that performs well across all environments by minimizing an average-cost performance metric. When
environments differ considerably in their tasks, such a single common policy might incur highly
sub-optimal performance on any given environment. More fundamentally, when environments differ
in dynamics, even the existence of a common stabilizing policy is unclear and difficult to verify in the
absence of models. Departing from the approach of learning a single common policy, in this paper,
we ask: (When) is it possible to learn personalized policies in a sample-efficient way by leveraging
data generated by potentially non-identical dynamical processes?
To formalize our study, we consider a scenario involving multiple agents that can be partitioned
into distinct clusters. We assume that all agents within a given cluster interact with the same physical
environment modeled as a linear time-variant (LTI) system with unknown dynamics; furthermore,
all agents within a cluster share the same quadratic cost function. However, the dynamics and cost
functions across clusters can be arbitrarily different. Thus, our setting captures both similarities
and differences in dynamics and tasks. As is common in collaborative and federated learning, we
allow agents to exchange information via a central aggregator. Concretely, our problem of interest
is to learn a personalized policy for each cluster that enjoys the benefits of collaboration, i.e., we
wish to show that such a policy can be learned faster (relative to a single-agent setting) by using the
collective samples available within the cluster. However, this is challenging, as we explain below.
Challenges. To make our setting realistic, we assume that the cluster structure is unknown a
priori. Since the system models associated with the clusters are also unknown, it becomes difficult
to decide how information should be exchanged between agents. In particular, care needs to be
taken to avoid misclustering, since transfer of information across clusters with arbitrarily different
LTI systems can lead to the learning of destabilizing policies; thus, in our setting, more data can
potentially hurt if not used judiciously. Additional subtleties arise as the agents in our setting access
only noisy zeroth-order information for both clustering and learning policies; we discuss them in
Sections 2 and 3. In light of these challenges, the main contributions of this paper are as follows.
â€¢ Problem Formulation. While clustering has been explored in federated learning (FL) for
static supervised learning tasks (Ghosh et al., 2020), our work provides the first principled study of
clustering in the context of model-free data-driven control, and shows how such a formalism can
enable learning personalized policies in a sample-efficient manner. As part of our formulation, we
identify a dissimilarity metric âˆ†(see (3)) that captures differences in optimal costs between clusters.
Our results reveal that a larger value of âˆ†leads to a faster separation of clusters.
â€¢ Novel Algorithm. The primary contribution of this paper is the development of a novel
model-free Personalized and Collaborative Policy Optimization (PO) algorithm (Algorithm 1) called
PCPO that combines ideas from sequential elimination in multi-armed bandits (Even-Dar et al.,
2006) and policy gradient algorithms in reinforcement learning (RL) (Agarwal et al., 2021). The lack
of prior knowledge of the cluster-separation gap âˆ†motivates the need for a sequential elimination
strategy to identify the clusters. Moreover, since it is non-trivial to decide when to stop clustering and
start collaborating, we propose an epoch-based approach that involves clustering and collaboration
in every epoch by requiring agents to maintain two separate sequences of policies: local policies
used purely for sequential clustering and global policies for collaboration. Another key feature of
our algorithm is that it only incurs a mild communication cost that scales logarithmically with the
number of agents and samples, making it particularly appealing for distributed implementation.
â€¢ Collaborative Gains. In Theorem 1, we prove that with high probability, our approach leads to
correct clustering, despite the absence of prior knowledge of models, cluster structure, and separation
2

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
gap âˆ†. This result also reveals that more heterogeneity can actually aid the learning process in that
a larger âˆ†incurs fewer noisy function evaluations for correct clustering. Building on Theorem 1,
our main result in Theorem 2 proves that by using PCPO, each agent can learn a near-optimal
personalized policy for its own system with a sub-optimality gap that scales inversely with the
number of agents in its cluster. In other words, PCPO prevents negative transfer of information across
clusters, while ensuring sample-complexity reductions via collaboration within each cluster. To our
knowledge, this is the first result to show how data from heterogeneous dynamical systems admitting
a cluster structure can be harnessed to expedite the learning of personalized policies.
Since this is a preliminary investigation of clustering in data-driven control, we restrict our
attention to the canonical Linear Quadratic Regulator (LQR) formalism. That said, we anticipate that
our general algorithmic template can be used in other supervised or RL problems.
Related Work. To put our contributions into perspective, we discuss relevant literature below.
â€¢ Policy Gradient for LQR. We build on the rich set of results on policy gradient methods for
the LQR problem (Fazel et al., 2018; Malik et al., 2020; Gravell et al., 2020; Zhang et al., 2021;
Mohammadi et al., 2019, 2020; Hu et al., 2023; Moghaddam et al., 2025). Generalizing these results
from the single system setting to our clustered multi-system formulation introduces various nuances
and challenges (outlined in Sections 2 and 3) that we address in this paper.
â€¢ Personalized Federated Learning. We draw inspiration from the work on clustering in
FL (Ghosh et al., 2020, 2022; Sattler et al., 2020) that aims to learn personalized models for groups of
agents that are similar in terms of their data distributions. Despite cosmetic similarities, the specifics
of our setting differ significantly in that we focus on control of dynamical systems where stability
plays a crucial role; no such stability concerns arise in the FL papers above on supervised learning.
â€¢ Collaborative System Identification. Our paper is related to a growing body of work that seeks
to leverage data from multiple dynamical systems to achieve statistical gains in estimation accuracy.
In this context, several papers (Wang et al., 2023b; Toso et al., 2023; Chen et al., 2023; Modi et al.,
2024; Rui and Dahleh, 2025; Tupe et al., 2025; Xin et al., 2025) have explored collaborative system
identification by combining trajectory data from multiple systems that share structural similarities.
In particular, Toso et al. (2023) and Rui and Dahleh (2025) assume a cluster structure like us. While
the above papers focus on using collective data for an open-loop estimation problem, namely system-
identification, our work focuses instead on data-efficient closed-loop control by directly learning
policies. As such, our notion of heterogeneity captures differences in closed-loop performance across
clusters as opposed to similarity metrics imposed on open-loop system matrices in the papers above.
â€¢ Meta, Multi-Task, and Transfer Learning in Control. Under the umbrella framework of
meta and transfer learning, various recent papers (Wang et al., 2023a; Toso et al., 2024; Aravind et al.,
2024; Stamouli et al., 2025) have used PO methods to study how information from multiple LTI
systems can be aggregated to learn policies that adapt across similar systems. Our formulation, which
seeks to find a personalized policy for every system, departs fundamentally from this line of work
which instead aims to learn a common policy for all systems. In this regard, we note that the closely
related papers of Wang et al. (2023a) and Toso et al. (2024) need to assume that all the systems are
sufficiently similar to admit a common stabilizing set. Even under this restrictive assumption, the
results in these papers indicate that the sub-optimality gap exhibits an additive heterogeneity-induced
bias term that might negate the speedups from collaboration. In contrast, our work does not require
a common stabilizing policy to exist for systems across clusters. Furthermore, our personalization
approach completely eliminates heterogeneity-induced biases. We also note that our approach incurs
a logarithmic (in agents and samples) communication cost as opposed to the linear cost in Wang
3

KANAKERI BAJAJ VERMA GUPTA MITRA
et al. (2023a). Finally, complementary to our clustering-based approach, ideas from representation
learning (Zhang et al., 2023; Guo et al., 2023; Lee et al., 2025) and domain randomization (Fujinami
et al., 2025) have also been recently used to improve data-efficiency in dynamic control tasks.
2. Problem Formulation
We consider a setting with N agents partitioned into H disjoint clusters {Mj}jâˆˆ[H]. With each
cluster j âˆˆ[H], we associate a tuple Sj = (Aj, Bj, Qj, Rj), comprising a system matrix Aj âˆˆRnÃ—n,
a control input matrix Bj âˆˆRnÃ—m, and two positive definite matrices Qj âˆˆRnÃ—n, Rj âˆˆRmÃ—m that
define the LQR cost function for cluster j. Each agent in cluster j interacts with the same instance
of the LQR problem specified by Sj, and aims to find a linear policy of the form ut = âˆ’Kxt that
minimizes the following infinite-horizon discounted cost:
Cj(K) = E
" âˆ
X
t=0
Î³t 
xâŠ¤
t Qjxt + uâŠ¤
t Rjut
#
subject to xt+1 = Ajxt + Bjut + zt,
(1)
where x0 = 0 and xt, ut, and zt are the state, control input (action), and exogenous process noise,
respectively, at time t, Î³ âˆˆ(0, 1) is a discount factor, and K is a control gain matrix. We make the
standard assumption that the pair (Aj, Bj) is controllable for every j âˆˆ[H]. Following Malik et al.
(2020), we assume that zt is sampled independently from a distribution D, such that:
E[zt] = 0, E[ztzâŠ¤
t ] = I, and âˆ¥ztâˆ¥2
2 â‰¤B, âˆ€t,
(2)
where B > 0 is some positive constant. For the LQR problem described in (1), it is well known (Bert-
sekas, 2015) that the optimal control law is a linear feedback policy of the form ut = âˆ’Kâˆ—
j xt, where
Kâˆ—
j is the optimal control gain matrix for cluster j. When Sj is known, each agent in Mj can obtain
Kâˆ—
j by solving the discrete-time algebraic Riccati equation (DARE) (Anderson and Moore, 2007).
However, our interest is in the learning scenario where the system matrices {(Aj, Bj)}jâˆˆ[H] are
unknown to the agents. Even in this setting, it is known that policy optimization (PO) algorithms that
treat the control gain as the optimization variable converge to the optimal policy (Fazel et al., 2018;
Malik et al., 2020). The implementation of such algorithms relies on noisy trajectory rollouts to
compute estimates of policy gradients.1 Specifically, given T independent rollouts from the tuple Sj,
each agent within Mj can generate a gain Ë†K such that with high probability, Cj( Ë†K) âˆ’Cj(Kâˆ—
j ) â‰¤
ËœO(1/
âˆš
T) (Malik et al., 2020). Our goal is to investigate whether this sample-complexity bound can
be improved by leveraging the cluster structure in our problem.
To achieve potential gains in sample-complexity via collaboration, we allow the agents to
communicate via a central server, and make the following assumption that is common in the literature
on collaborative/federated learning (KoneË‡cn`y et al., 2016; McMahan et al., 2017).
Assumption 1 The noise processes across agents are statistically independent, i.e., for all i1, i2 âˆˆ
[N] such that i1 Ì¸= i2, the noise stochastic processes {z(i1)
t
} and {z(i2)
t
} are mutually independent.
Here, with a slight overload of notation, we use {z(i)
t } to denote the noise process for agent i âˆˆ[N].
Although the above assumption suggests that exchange of information between agents can accelerate
the learning of an optimal policy, collaboration is complicated by the heterogeneity among clusters,
due to the difference in system dynamics (Aj, Bj) and in task objectives (Qj, Rj). To capture such
1. The notion of a rollout will be made precise later in this section.
4

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
heterogeneity across clusters, we take inspiration from the notion of â€œcluster separation gaps" in
supervised learning (Ghosh et al., 2022; Su et al., 2022; Sattler et al., 2020), and introduce
âˆ†:=
min
j1,j2âˆˆ[H]:j1Ì¸=j2
|Cj1(Kâˆ—
j1) âˆ’Cj2(Kâˆ—
j2)|.
(3)
We assume a non-zero separation across clusters, i.e., âˆ†> 0. While one can certainly formu-
late alternative notions of heterogeneity, we will show (in Theorem 2) that our metric of cluster-
dissimilarity, as captured by âˆ†in (3), can be suitably exploited to separate clusters and learn
personalized policies for each cluster. In particular, our results reveal that heterogeneity can be
helpful: a larger value of âˆ†leads to faster cluster separation using fewer rollouts. With the above
ideas in place, we are now ready to formally state our problem of interest. For each agent i âˆˆ[N],
let us use Ïƒ(i) to represent the index of the cluster to which it belongs.
Problem 1 (Clustered LQR Problem) Let Î´ âˆˆ(0, 1) be a given failure probability. Suppose
every agent i âˆˆ[N] has access to T independent rollouts from its corresponding system SÏƒ(i).
Develop an algorithm that returns { Ë†Ki}iâˆˆ[N] such that with probability at least 1 âˆ’Î´, the
following is true âˆ€i âˆˆ[N]:
CÏƒ(i)( Ë†Ki) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤ËœO
ï£«
ï£­
1
q
|MÏƒ(i)|T
ï£¶
ï£¸.
In simple words, our goal is to come up with an algorithm that generates a personalized control
policy for every agent that benefits from the collective information available within that agentâ€™s
cluster. This is quite non-trivial due to the following technical challenges.
â€¢ In our setting, the system dynamics and the cluster identities are both unknown a priori. Thus,
our problem requires learning the cluster identities and optimal policies simultaneously.
â€¢ The clustering process is complicated by two main issues. First, the information used for
clustering is based on noisy function evaluations that are insufficient for estimating the system models,
ruling out system-identification-based approaches in Toso et al. (2023) and Rui and Dahleh (2025).
Thus, we need to develop a model-free clustering algorithm. Second, the minimum separation gap âˆ†
in (3) is assumed to be unknown, ruling out the possibility of simple one-shot clustering approaches.
â€¢ Unlike supervised learning problems in Ghosh et al. (2020) and Su et al. (2022) where
misclustering only introduces a bias due to heterogeneity, the price of misclustering can be more
severe in our control setting. In particular, since the system tuples across clusters are allowed to be
arbitrarily different, transfer of information across clusters can lead to destabilizing policies.
In the next section, we will develop the PCPO algorithm that addresses the above challenges
and solves Problem 1, while incurring only a logarithmic (with respect to the number of agents and
rollouts) communication cost. In preparation for the next section, we now define the notion of a
rollout and a zeroth-order gradient estimator. Given a policy K, a rollout for an agent i âˆˆMj yields
a noisy sample of the infinite-horizon trajectory cost, defined as:
Cj(K; Z(i)) =
âˆ
X
t=0
Î³t 
xâŠ¤
t Qjxt + uâŠ¤
t Rjut

, where xt+1 = Ajxt + Bjut + zt, ut = âˆ’Kxt, (4)
5

KANAKERI BAJAJ VERMA GUPTA MITRA
x0 = 0 and Z(i) = {z(i)
t }. We will interpret each rollout as a sample. Using such noisy function
evaluations for a policy K run by an agent i âˆˆMj, we define the M-minibatched zeroth-order
gradient estimator with a smoothing radius r, as follows (Fazel et al., 2018; Malik et al., 2020):
gi(K) := 1
M
M
X
k=1
Cj(K + rUk; Z(i)
k )
D
r

Uk,
(5)
where D = mn, Uk is drawn independently from a uniform distribution over matrices with unit
Frobenius norm, and Z(i)
k
are independent copies of Z(i) for all k âˆˆ[M]. In the sequel, for an
agent i âˆˆMj, we use the shorthand ZOi(K, M, r) to refer to the M-minibatched zeroth-order
gradient estimator at policy K with smoothing radius r, as defined in (5). We use the notation c(p,_)
to denote problem-parameter-dependent constants and provide their expressions in Appendix A of
Kanakeri et al. (2025). We make the standard assumption (Fazel et al., 2018; Malik et al., 2020)
that each agent i has access to an initial controller K(0)
i
that lies within its respective stabilizing set:
{K âˆˆRmÃ—n : Ï(AÏƒ(i) âˆ’BÏƒ(i)K) < 1}, where Ï(X) is the spectral radius of a matrix X âˆˆRnÃ—n.
3. Description of the Algorithm
In this section, we present our proposed algorithm, Personalized and Collaborative Policy Opti-
mization (PCPO) (Algorithm 1), which effectively addresses Problem 1 by carefully accounting for
its inherent challenges. Since the cluster separation gap âˆ†is unknown, we propose a sequential
elimination strategy to identify the correct clusters. While the idea of sequential elimination has
been explored in the context of multi-armed bandits (Even-Dar et al., 2006), we show that a similar
approach can be used to effectively cluster LTI systems that satisfy the heterogeneity metric defined
in (3). The algorithm proceeds in epochs (indexed by l) of increasing duration, where in each epoch,
each agent i updates two sequences: a local sequence, {X(l)
i }lâ‰¥0, and a global sequence, { Ë†K(l)
i }lâ‰¥0.
The local sequence is updated using the zeroth-order policy optimization algorithm in Fazel et al.
(2018); Malik et al. (2020), and is used exclusively for clustering the agents. The global sequence is
updated by aggregating the gradient estimates from all the agents within an appropriately defined
neighborhood set. After each epoch, the neighborhood sets are updated based on the concentration
of the estimated cost around the optimal cost. As the number of rollouts increases across epochs, this
concentration becomes tighter, hence pruning out misclustered agents over successive epochs. The
various components of the algorithm are succinctly captured in Figure 1. In Section 4, we show that
the neighborhood sets eventually converge to the correct clusters with high probability, after which
the global sequence enjoys the collaborative gains without any heterogeneity bias. In what follows,
we elaborate on the rationale behind the design of the various components of PCPO.
Building intuition. Correctly identifying the agentsâ€™ clusters is crucial to reap any potential ben-
efits from collaboration, as collaborating with agents from a different cluster can lead to destabilizing
policies. In this regard, the major difficulty arises from the fact that the cluster separation gap âˆ†
in (3) is unknown a priori. To appreciate the associated challenges, as a thought experiment, let us
consider a simpler case where âˆ†is known. Under this scenario, each agent can locally run policy
optimization to obtain a policy in a sufficiently close neighborhood of the optimal policy. Then, each
agent can evaluate the cost at this policy and ensure that it is concentrated around the optimal cost.
If the neighborhood radius, which depends on âˆ†, is carefully chosen, it is easy to see that such a
one-shot approach leads to correct clustering. However, when âˆ†is unknown, one-shot clustering
may no longer work, motivating the sequential clustering idea in our proposed PCPO Algorithm.
6

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
Figure 1: Illustration of the epoch-based structure of PCPO, where each epoch involves three key
steps: local policy optimization (PO), cost estimation, and global PO.
Sequential elimination. In each epoch l of PCPO, for every agent i âˆˆ[N], the server maintains
a neighborhood set N (l)
i
as an estimate of the true cluster MÏƒ(i). All such neighborhood sets are
initialized from the set of all agents and sequentially pruned over epochs. For pruning, we start with
an initial estimate of âˆ†, denoted by âˆ†0, and update it by halving its value at the beginning of each
epoch l to obtain âˆ†l (Line 3 of Algo. 1). Our goal is to ensure that for all agents, the estimated cost
in epoch l is in the âˆ†l/4 neighborhood of its optimal cost. We achieve this in a two-step process,
where we first perform local policy optimization to obtain a policy that is âˆ†l/8-suboptimal (Line 4).
The localPO subroutine performs Ml-minibatched policy optimization for Rl iterations starting
with a controller X(lâˆ’1)
i
. In every iteration t âˆˆ[Rl], X(i,t), the t-th sub-iterate of localPO, is
updated as X(i,t+1) â†X(i,t) âˆ’Î·gi(X(i,t)), where gi(X(i,t)) = ZOi(X(i,t), Ml, r(loc)
l
). Then, we
estimate the cost at the policy X(l)
i
obtained from localPO with an error tolerance of âˆ†l/8 using
Ml rollouts (Line 5). Having achieved the desired cost-estimation accuracy of âˆ†l/4 for all agents,
we prune the neighborhood sets according to (7) in Line 13. Eventually, as âˆ†l â‰¤âˆ†, which happens
in O(log(âˆ†0/âˆ†)) epochs, correct clustering takes place, as elaborated in the next section.
Local and Global Sequences. To motivate the need for maintaining two sequences in PCPO, let
us again consider the case where âˆ†is known, where it would suffice for the agents to only maintain a
single sequence to run local PO until clustering, as discussed earlier in the one-shot clustering scheme.
After the clusters are identified, the same sequence can be used for collaboration. In our setting,
however, although the neighborhood sets eventually converge to the correct clusters in logarithmic
number of epochs with respect to 1/âˆ†, the number of such epochs cannot be determined a priori with-
out knowledge of âˆ†, making it difficult to decide when to initiate collaboration. Furthermore, with a
single sequence, collaborating with misclustered agents can lead to an undesirable scenario where the
sequence used to cluster is itself contaminated due to misclustering. PCPO carefully navigates this
difficulty by maintaining two sequences of policies at each agent. The local sequence, {X(l)
i }lâ‰¥0, is
used purely for clustering, and the global sequence, { Ë†K(l)
i }lâ‰¥0, is updated by aggregating gradients
from agents within the neighborhood set N (lâˆ’1)
i
from the previous epoch l âˆ’1; see (6) in Line 9.
Logarithmic communication. Since both local and global PO are performed for Rl iterations
with Ml rollouts per iteration, the overall sample complexity per epoch is Tl = 2RlMl + Ml, where
the additional Ml rollouts are due to the cost estimation step. Each iteration of global PO proceeds as
follows. First, for every agent, the server combines the minibatched zeroth-order gradient estimates
as per (6). Then, the agents update the iterates using the averaged gradient with an appropriately
chosen but fixed step size Î· (Line 10). Since the above essentially incurs O(Rl) communication steps
7

KANAKERI BAJAJ VERMA GUPTA MITRA
Algorithm 1 Personalized and Collaborative Policy Optimization (PCPO)
1: Initialization: âˆ†0; âˆ€i âˆˆ[N], Ë†K(0)
i
â†K(0)
i
, X(0)
i
â†K(0)
i
, N (0)
i
â†[N].
2: For l = 1, 2, . . . ,
3:
At Each Agent i: âˆ†l â†âˆ†lâˆ’1
2
, Î´l â†
Î´
2l2 , Î· â†c(p,1), Rl â†c(p,2) log
 c(p,3)N
âˆ†2
l

Ml â†
c(p,4)
âˆ†2
l
log

8DNRl
Î´l

, Ëœrl â†

c(p,5)
âˆšMl
r
log

8DNRl
Î´l
1/2
, r(loc)
l
â†min{c(p,6), Ëœrl}.
4:
Local Policy Optimization: X(l)
i
â†localPO(X(lâˆ’1)
i
, Ml, Rl, r(loc)
l
).
5:
Cost estimation: Ë†CÏƒ(i)(X(l)
i ) â†
1
Ml
PMl
j=1 CÏƒ(i)(X(l)
i , Z(i)
j ).
6:
Initialize Y (0)
i
â†Ë†K(lâˆ’1)
i
and set r(global)
(i,l)
â†min

c(p,6),
Ëœrl
|N (lâˆ’1)
i
|1/4

â–·For collaborative PO
7:
For k = 0, 1, . . . , Rl âˆ’1
8:
At Each Agent i: Transmit gi(Y (k)
i
) â†ZOi(Y (k)
i
, Ml, r(global)
(i,l)
) to the Server.
9:
At Server: Compute and transmit the averaged gradient estimate as follows:
Gi â†
1
|N (lâˆ’1)
i
|
X
jâˆˆN (lâˆ’1)
i
gj(Y (k)
j
).
(6)
10:
At Each Agent i: Y (k+1)
i
â†Y (k)
i
âˆ’Î·Gi.
â–·Global policy update via collaboration
11:
End For
12:
At Each Agent i: Update Ë†K(l)
i
â†Y (Rl)
i
. Transmit X(l)
i , Ë†CÏƒ(i)(X(l)
i ) to the server.
13:
At Server:
Update the neighborhood set as follows:
â–·Sequential elimination
N (l)
i
â†{j âˆˆN (lâˆ’1)
i
:
 Ë†CÏƒ(j)(X(l)
j ) âˆ’Ë†CÏƒ(i)(X(l)
i )
 â‰¤âˆ†l/2}
(7)
14:
If N (l)
i
Ì¸= N (lâˆ’1)
i
for some i âˆˆ[N]:
â–·Reinitialization to ensure stability
15:
For all agents i âˆˆ[N], update and transmit Ë†K(l)
i
as follows:
Ë†K(l)
i
â†
argmin
{X(l)
j :jâˆˆN (l)
i
}
{ Ë†CÏƒ(j)(X(l)
j ) : j âˆˆN (l)
i }.
(8)
16: End For
per epoch, the total communication complexity of PCPO is O(Rl Ã— Number of Epochs). Based on
our choice of parameters, both objects in the above product are logarithmic in the number of agents
N, the gap 1/âˆ†, and the number of total rollouts per agent, namely T.
Note on reinitialization. Since the server averages gradients in every epoch based on the
neighborhood set, agents inevitably collaborate across clusters until correct clustering is achieved.
This can lead to destabilizing policies in the global sequence. To mitigate this, at the end of each
epoch, we reinitialize the global policy sequences for all agents whenever any neighborhood set is
updated, as specified in (8). In the next section, we establish in Theorem 1 that the neighborhood
sets eventually converge to the correct clusters and cease to update. Consequently, reinitialization
8

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
ensures that the global sequences of agents within the same cluster evolve identically and achieve
collaborative gains once the correct clusters are identified.
4. Main Results
Our main results concern the two key components of the PCPO algorithm: (i) identifying the
correct clusters via sequential elimination, and (ii) performing collaborative policy optimization with
logarithmic communication. The following theorem captures the clustering component.
Theorem 1 (Clustering with sequential elimination) Define L = min{l âˆˆ1, 2, . . . : âˆ†l â‰¤âˆ†/2}.
Given a failure probability Î´ âˆˆ(0, 1), with probability at least 1 âˆ’Î´/2, the following statements
concerning the neighborhood sets from the PCPO algorithm hold for every agent i âˆˆ[N]:
1. In every epoch l, we have MÏƒ(i) âŠ†N (l)
i .
2. For any epoch l such that l â‰¥L, we have MÏƒ(i) = N (l)
i .
Discussion. The key technical contribution of Theorem 1 lies in showing that the sequential
elimination strategy successfully lets the neighborhood sets converge to the correct clusters with high
probability. In particular, we show that the true clusters are included in the neighborhood sets for all
agents in each epoch, and there exists an epoch L after which the neighborhood sets have converged
to the true clusters and remain fixed for all subsequent epochs (l â‰¥L). Later in this section, we
provide a proof sketch and discuss how these claims follow from the local policy optimization and
the cost estimation step, relying on our notion of the cluster separation gap as defined in (3). The
following theorem captures the key result concerning the collaborative optimization part in PCPO.
Theorem 2 (Collaborative Policy Optimization) Let the failure probability be Î´ âˆˆ(0, 1). Define
L = min{l âˆˆ1, 2, . . . : âˆ†l â‰¤âˆ†/2} and let Â¯L denote the last epoch. If the number of rollouts per
agent satisfies T â‰¥ËœO(1/âˆ†2), and Assumption 1 holds, then Â¯L > L and Ë†K(Â¯L)
i
satisfies the following
with probability at least 1 âˆ’Î´ for every agent i âˆˆ[N]:
CÏƒ(i)( Ë†K(Â¯L)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤O
ï£«
ï£­c(p,7)
q
log
  8DNT
Î´

q
T|MÏƒ(i)|
ï£¶
ï£¸.
(9)
Discussion. It was shown in Malik et al. (2020) that zeroth-order policy optimization provides a
ËœO(1/
âˆš
T) suboptimal policy using T rollouts for a single system LQR problem. In contrast, Wang
et al. (2023a) showed collaborative gains for a federated LQR setting while incurring additive bias
terms that depend on the heterogeneity gap. Moreover, the results in Wang et al. (2023a) apply only
to systems with bounded heterogeneity. Theorem 2 bridges this gap by showing that collaborative
gains (as evidenced by the
q
|MÏƒ(i)|-factor speedup for each agent i in (9)) can be achieved without
any additive bias through careful cluster identification and collaboration exclusively within clusters.
Furthermore, these gains hold for systems that can be arbitrarily different, as long as their optimal
costs are separated according to (3).
Corollary 3 (Logarithmic communication complexity) The PCPO algorithm guarantees a loga-
rithmic communication complexity with respect to the total number of rollouts T, the number of
agents N, and the inverse of the separation gap 1/âˆ†.
9

KANAKERI BAJAJ VERMA GUPTA MITRA
In the following, we provide proof sketches for both Theorem 1 and Theorem 2, while deferring
the detailed proofs to Kanakeri et al. (2025). The statements made in the proof sketches are proba-
bilistic in nature. However, to keep the exposition simpler, we omit specifying the success/failure
probability of the statements and refer the readers to Kanakeri et al. (2025) for such details.
Proof sketch for Theorem 1. We start by showing that the estimated cost for each agent is in the
âˆ†l/4 neighborhood of its optimal cost, i.e., in each epoch l, for each agent i, we prove that
| Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))| â‰¤âˆ†l/4.
(10)
Note that for any agent j âˆˆMÏƒ(i), since CÏƒ(i)(Kâˆ—
Ïƒ(i)) = CÏƒ(j)(Kâˆ—
Ïƒ(j)), in light of (10), it is
apparent that such an agent will pass the requirement in (7), and hence, never be eliminated from the
neighborhood sets of agent i. This explains the first claim of Theorem 1. As for the second claim, note
that for any j /âˆˆMÏƒ(i), in light of our dissimilarity metric âˆ†in (3), |CÏƒ(i)(Kâˆ—
Ïƒ(i))âˆ’CÏƒ(j)(Kâˆ—
Ïƒ(j))| â‰¥
âˆ†. Now for an epoch l such that âˆ†l â‰¤âˆ†/2, the above inequality can be combined with that in (10)
to see that | Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )| â‰¥3âˆ†/4, violating the requirement for inclusion in (7). It
remains to establish (10), which follows from two guarantees: (i) | Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )| â‰¤
âˆ†l/8, and (ii) |CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))| â‰¤âˆ†l/8. The second guarantee follows from an analysis
of the local PO sub-routine in Line 4 of Algo. 1, drawing on Malik et al. (2020); the first follows
from analyzing the cost estimation step in Line 5 based on a simple Hoeffding bound.
Proof sketch for Theorem 2. We prove Theorem 2 by conditioning on the event where the claims
made in Theorem 1 hold. As agents collaborate exclusively within their respective clusters and the
reinitialization step synchronizes their global sequences after correct clustering, the gradient estimate
obtained by averaging (see (6)) estimates of agents within a cluster enjoys a variance reduction effect
under Assumption 1. Using this, and the fact that the LQR cost satisfies a Ï•-smoothness and Âµ-PL
condition locally (Fazel et al., 2018; Malik et al., 2020), we establish that in each iteration k of the
final epoch Â¯L, the following recursion holds with probability 1 âˆ’Î´â€²:
Sk+1 â‰¤

1 âˆ’Î·Âµ
4

Sk + 3Î·
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
c2
(p,8)D2
(r(global)
(i,Â¯L)
)2|MÏƒ(i)|MÂ¯L
log
2D
Î´â€²

|
{z
}
s1
+ Ï•2(r(global)
(i,Â¯L)
)2
|
{z
}
s2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
(11)
where Sk := CÏƒ(i)(Y (k)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)). The term s1 is due to the concentration of the minibatched
gradient estimate around the gradient of a smoothed cost defined in Appendix A of Kanakeri et al.
(2025); this is the term that benefits from collaboration. The term s2 captures the bias that arises
when estimating gradients from noisy function evaluations. To ensure that this bias term does not
negate the collaborative speedup in s1, we choose the smoothing radius r(global)
(i,Â¯L)
to minimize the
sum s1 + s2. Unrolling the recursion for Rl iterations (with the choice of Rl in PCPO) provides
the per epoch convergence with rate ËœO

1/
q
|MÏƒ(i)|MÂ¯L

. Finally, using the fact that Ml increases
exponentially with epochs, we establish that MÂ¯L = Ëœâ„¦(T).
5. Conclusion
We developed a novel clustering-based approach for learning personalized control policies using data
from heterogeneous dynamical processes. As future work, we will explore (i) alternative measures of
dissimilarity across systems, (ii) more general dynamical processes, and (iii) online settings.
10

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning
Research, 22(98):1â€“76, 2021.
Brian DO Anderson and John B Moore. Optimal control: linear quadratic methods. Courier
Corporation, 2007.
Ashwin Aravind, Mohammad Taha Toghani, and CÃ©sar A Uribe. A moreau envelope approach for
LQR meta-policy estimation. In 2024 IEEE 63rd Conference on Decision and Control (CDC),
pages 415â€“420. IEEE, 2024.
Dimitri P Bertsekas. Dynamic programming and optimal control 4th edition, volume ii. Athena
Scientific, 2015.
Yiting Chen, Ana M Ospina, Fabio Pasqualetti, and Emiliano Dallâ€™Anese. Multi-task system
identification of similar linear time-invariant dynamical systems. In 2023 62nd IEEE Conference
on Decision and Control (CDC), pages 7342â€“7349. IEEE, 2023.
Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and
stopping conditions for the multi-armed bandit and reinforcement learning problems. Journal of
machine learning research, 7(6), 2006.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient
methods for the linear quadratic regulator. In Int. Conf. on Machine Learning, pages 1467â€“1476.
PMLR, 2018.
Tesshu Fujinami, Bruce D Lee, Nikolai Matni, and George J Pappas. Domain randomization is
sample efficient for linear quadratic control. arXiv preprint arXiv:2502.12310, 2025.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for
clustered federated learning. Advances in neural information processing systems, 33:19586â€“19597,
2020.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for
clustered federated learning. IEEE Transactions on Information Theory, 68(12):8076â€“8091, 2022.
Benjamin Gravell, Peyman Mohajerin Esfahani, and Tyler Summers. Learning optimal controllers
for linear systems with multiplicative noise via policy gradient. IEEE Transactions on Automatic
Control, 66(11):5283â€“5298, 2020.
Taosha Guo, Abed AlRahman Al Makdah, Vishaal Krishnan, and Fabio Pasqualetti. Imitation and
transfer learning for lqg control. IEEE Control Systems Letters, 7:2149â€“2154, 2023.
Bin Hu, Kaiqing Zhang, Na Li, Mehran Mesbahi, Maryam Fazel, and Tamer BaÂ¸sar. Toward a
theoretical foundation of policy optimization for learning control policies. Annual Review of
Control, Robotics, and Autonomous Systems, 6(1):123â€“158, 2023.
11

KANAKERI BAJAJ VERMA GUPTA MITRA
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. A short note
on concentration inequalities for random vectors with subgaussian norm.
arXiv preprint
arXiv:1902.03736, 2019.
Vinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, and Aritra Mitra. Harnessing data from
clustered LQR systems: Personalized and collaborative policy optimization. arXiv preprint, 2025.
Jakub KoneË‡cn`y, H Brendan McMahan, Felix X Yu, Peter RichtÃ¡rik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Bruce D Lee, Leonardo F Toso, Thomas T Zhang, James Anderson, and Nikolai Matni. Regret
analysis of multi-task representation learning for linear-quadratic adaptive control. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 39, pages 18062â€“18070, 2025.
Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L Bartlett, and Martin J
Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic
systems. Journal of Machine Learning Research, 21(21):1â€“51, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pages 1273â€“1282. PMLR, 2017.
Aditya Modi, Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Joint
learning of linear time-invariant dynamical systems. Automatica, 164:111635, 2024.
Amirreza Neshaei Moghaddam, Alex Olshevsky, and Bahman Gharesifard. Sample complexity
of the linear quadratic regulator: A reinforcement learning lens. Journal of Machine Learning
Research, 26(151):1â€“50, 2025.
Hesameddin Mohammadi, Armin Zare, Mahdi Soltanolkotabi, and Mihailo R JovanoviÂ´c. Global
exponential convergence of gradient methods over the nonconvex landscape of the linear quadratic
regulator. In 2019 IEEE 58th Conference on Decision and Control (CDC), pages 7474â€“7479.
IEEE, 2019.
Hesameddin Mohammadi, Mahdi Soltanolkotabi, and Mihailo R JovanoviÂ´c. On the linear conver-
gence of random search for discrete-time LQR. IEEE Control Systems Letters, 5(3):989â€“994,
2020.
Maryann Rui and Munther A Dahleh. Learning clusters of partially observed linear dynamical
systems. In 2025 American Control Conference (ACC), pages 3545â€“3550. IEEE, 2025.
Felix Sattler, Klaus-Robert MÃ¼ller, and Wojciech Samek. Clustered federated learning: Model-
agnostic distributed multitask optimization under privacy constraints. IEEE transactions on neural
networks and learning systems, 32(8):3710â€“3722, 2020.
Charis Stamouli, Leonardo F Toso, Anastasios Tsiamis, George J Pappas, and James Anderson.
Policy gradient bounds in multitask LQR. arXiv preprint arXiv:2509.19266, 2025.
12

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
Lili Su, Jiaming Xu, and Pengkun Yang. Global convergence of federated learning for mixed
regression. Advances in Neural Information Processing Systems, 35:29889â€“29902, 2022.
Leonardo F Toso, Han Wang, and James Anderson. Learning personalized models with clustered
system identification. In 2023 62nd IEEE Conference on Decision and Control (CDC), pages
7162â€“7169. IEEE, 2023.
Leonardo Felipe Toso, Donglin Zhan, James Anderson, and Han Wang. Meta-learning linear
quadratic regulators: a policy gradient maml approach for model-free LQR. In 6th Annual
Learning for Dynamics & Control Conference, pages 902â€“915. PMLR, 2024.
Omkar Tupe, Max Hartman, Lav R Varshney, and Saurav Prakash. Federated nonlinear system
identification. arXiv preprint arXiv:2508.15025, 2025.
Han Wang, Leonardo F Toso, Aritra Mitra, and James Anderson. Model-free learning with het-
erogeneous dynamical systems: A federated LQR approach. arXiv preprint arXiv:2308.11743,
2023a.
Han Wang, Leonardo Felipe Toso, and James Anderson.
Fedsysid: A federated approach to
sample-efficient system identification. In Learning for Dynamics and Control Conference, pages
1308â€“1320. PMLR, 2023b.
Lei Xin, Lintao Ye, George Chiu, and Shreyas Sundaram. Learning dynamical systems by leveraging
data from similar systems. IEEE Transactions on Automatic Control, 2025.
Kaiqing Zhang, Bin Hu, and Tamer Basar. Policy optimization for h2 linear control with hâˆ
robustness guarantee: Implicit regularization and global convergence. SIAM Journal on Control
and Optimization, 59(6):4081â€“4109, 2021.
Thomas T Zhang, Katie Kang, Bruce D Lee, Claire Tomlin, Sergey Levine, Stephen Tu, and Nikolai
Matni. Multi-task imitation learning for linear dynamical systems. In Learning for Dynamics and
Control Conference, pages 586â€“599. PMLR, 2023.
13

KANAKERI BAJAJ VERMA GUPTA MITRA
Appendix A. Properties of the LQR problem
Notation. For matrices A âˆˆRmÃ—n and B âˆˆRmÃ—n, we use âˆ¥Aâˆ¥to denote the Frobenius norm of
A which is defined as âˆ¥Aâˆ¥=
p
trace(AâŠ¤A). We use âŸ¨A, BâŸ©to denote the Frobenius inner-product
defined as âŸ¨A, BâŸ©= trace(AâŠ¤B).
In this section, we discuss some of the properties of the LQR cost that were established in Fazel
et al. (2018); Malik et al. (2020). In particular, the LQR cost in (1) is locally Lipschitz, locally
smooth, and enjoys a gradient-domination property over the set of stabilizing controllers. These key
properties aid in the convergence analysis of the model-free policy gradient algorithm, and we use
them in the proofs of Theorem 1 in Appendix B and Theorem 2 in Appendix C. However, to show
the convergence, it is crucial to ensure that the policy gradient iterates always lie within a restricted
subset of the stabilizing set with high probability. In Malik et al. (2020), such a restricted set is
chosen based on the initial suboptimality gap. In our setting, given access to a set of initial stabilizing
controllers for all agents, {K(0)
i
}iâˆˆ[N], and our initial guess for the cluster separation gap, âˆ†0, we
define Ëœâˆ†0 := max{maxiâˆˆ[N](CÏƒ(i)(K(0)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))), âˆ†0}, and the restricted sets as follows
for all j âˆˆ[H]:
G0
j := {K âˆˆRmÃ—n : Cj(K) âˆ’Cj(Kâˆ—
j ) â‰¤10 Ëœâˆ†0}.
(12)
Properties of the LQR cost. For each system j âˆˆ[H], on the restricted domain G0
j , Malik et al.
(2020) showed that the local properties hold uniformly, i.e., âˆƒÏ•j > 0, Î»j > 0, Ïj > 0, such that the
LQR cost in (1) is (Î»j, Ïj)-locally Lipschitz and (Ï•j, Ïj)-locally smooth for all policies K âˆˆG0
j .
Furthermore, it is known that the LQR cost satisfies the PL (gradient-domination) condition for all
policies in the stabilizing set (see Lemma 3 of Malik et al. (2020)). Denoting the parameter for the
PL condition for system j by Âµj > 0, we define Âµ := min{Âµ1, Âµ2, . . . , ÂµH}. Similarly, defining
Ï• := max{Ï•1, Ï•2, . . . , Ï•H}, Î» := max{Î»1, Î»2, . . . , Î»H}, and Ï := min{Ï1, Ï2, . . . , ÏH}, we can
ensure that for every system j âˆˆ[H], the cost in (1) is (Î», Ï)-locally Lipschitz and (Ï•, Ï)-locally
smooth for all policies in their respective restricted sets G0
j , and Âµ-PL in their respective stabilizing
sets. The following lemmas from Malik et al. (2020) capture these properties.
Lemma 4
(LQR cost is locally Lipschitz). For any system j âˆˆ[H], given a pair of policies
(K, Kâ€²) âˆˆ(G0
j Ã— G0
j ), if âˆ¥K âˆ’Kâ€²âˆ¥â‰¤Ï, we have
Cj(K) âˆ’Cj(Kâ€²)
 â‰¤Î»âˆ¥K âˆ’Kâ€²âˆ¥.
(13)
Lemma 5 (LQR cost has locally Lipschitz gradients.) For any system j âˆˆ[H], given a pair of
policies (K, Kâ€²) âˆˆ(G0
j Ã— G0
j ), if âˆ¥K âˆ’Kâ€²âˆ¥â‰¤Ï, we have
âˆ‡Cj(K) âˆ’âˆ‡Cj(Kâ€²)
 â‰¤Ï•âˆ¥K âˆ’Kâ€²âˆ¥.
(14)
Lemma 6 (LQR cost satisfies PL.) For any system j âˆˆ[H], given a stable policy K, we have
âˆ¥âˆ‡Cj(K)âˆ¥2 â‰¥Âµ(Cj(K) âˆ’Cj(Kâˆ—
j )).
(15)
Smoothed cost and the properties of the gradient estimate. The smoothed cost with a radius
r for a system j âˆˆ[H] is defined as Cj,r(K) := E[Cj(K + rv)], where v is uniformly distributed
over all matrices in RmÃ—n with the Frobenius norm of at most 1. It is shown in Fazel et al. (2018);
Malik et al. (2020) that the zeroth-order gradient estimate gi(Â·) as defined in (5) for an agent i is an
14

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
unbiased estimator of the gradient of the smoothed cost. In particular, for all systems j âˆˆ[H] and all
agents i âˆˆMj, we have the following properties for all K âˆˆG0
j and r âˆˆ(0, Ï):
E[gi(K)] = âˆ‡CÏƒ(i),r(K)
(16)
âˆ¥âˆ‡Cj,r(K) âˆ’âˆ‡Cj(K)âˆ¥â‰¤Ï•r.
(17)
Furthermore, it is known that the noisy rollout cost Cj(K + rU; Z(i)) is uniformly bounded
âˆ€K âˆˆG0
j , âˆ€r âˆˆ(0, Ï) and all U âˆˆRmÃ—n with âˆ¥Uâˆ¥= 1. In other words, there exists G(j)
âˆâ‰¥0 such
that Cj(K + rU; Z(i)) â‰¤G(j)
âˆ(see Lemma 11 of Malik et al. (2020) for the expression of G(j)
âˆ.) Let
us define Gâˆ:= max{G(1)
âˆ, G(2)
âˆ, . . . , G(H)
âˆ}. Hence, for any agent i âˆˆ[N], we have the following
âˆ€K âˆˆG0
j , âˆ€r âˆˆ(0, Ï) and all U âˆˆRmÃ—n with âˆ¥Uâˆ¥= 1:
CÏƒ(i)(K + rU; Z(i)) â‰¤Gâˆ.
(18)
We then have the following concentration result that will prove useful in establishing â€œvariance-
reduction" effects.
Lemma 7 (Concentration of the zeroth-order gradient estimates). For any system j âˆˆ[H], given
a policy K âˆˆG0
j , a smoothing radius r âˆˆ(0, Ï) and a failure probability Î´â€² âˆˆ(0, 1), the following
holds for the M-minibatched zeroth-order gradient estimate of an agent i âˆˆMj with probability at
least 1 âˆ’Î´â€²:
âˆ¥gi(K) âˆ’âˆ‡Cj,r(K)âˆ¥â‰¤

Gâˆ+ Î» Ï
D + Ï•Ï2
D

D
r
âˆš
M
s
log
2D
Î´â€²

.
(19)
Proof We have âˆ¥gi(K) âˆ’âˆ‡Cj,r(K)âˆ¥=
 1
M
PM
k=1(g(i,k)(K) âˆ’âˆ‡Cj,r(K))
, where we denoted
the k-th component of the minibatch as g(i,k)(K). Recall from (5) that this k-th component takes the
form
g(i,k)(K) = Cj(K + rUk; Z(i)
k )
D
r

Uk.
Hence, we have âˆ¥g(i,k)(K)âˆ¥â‰¤D
r Gâˆdue to (18) as âˆ¥Ukâˆ¥= 1. Let us define c(p,8) :=

Gâˆ+ Î» Ï
D + Ï•Ï2
D

.
We then have
âˆ¥g(i,k)(K) âˆ’âˆ‡Cj,r(K)âˆ¥= âˆ¥g(i,k)(K) âˆ’âˆ‡Cj,r(K) + âˆ‡Cj(K) âˆ’âˆ‡Cj(K)âˆ¥
(a)
â‰¤âˆ¥g(i,k)(K)âˆ¥+ âˆ¥âˆ‡Cj,r(K) âˆ’âˆ‡Cj(K)âˆ¥+ âˆ¥âˆ‡Cj(K)âˆ¥
(b)
â‰¤D
r Gâˆ+ Ï•r + Î»
= D
r

Gâˆ+ r2
D Ï• + r
DÎ»

(c)
â‰¤D
r

Gâˆ+ Ï2
D Ï• + Ï
DÎ»

= D
r c(p,8).
15

KANAKERI BAJAJ VERMA GUPTA MITRA
Table 1: Relevant notation and definitions
Notation
Definition
Ml
Minibatch size used to estimate zeroth-order gradients in the l-th epoch.
Rl
Number of steps/iterations of the local and global policy optimization in the l-th epoch.
Î·
Step size for both local and global policy optimization.
rl
Smoothing radius used in the zeroth-order gradient estimates in the l-th epoch.
âˆ†l
Estimate of âˆ†used to cluster the agents in the lth epoch.
N (l)
i
Neighborhood set corresponding to the i-th agent in the l-th epoch.
X(l)
i
Local policy for the i-th agent in the l-th epoch.
Ë†K(l)
i
Global policy for the i-th agent in the l-th epoch.
In the above, (a) follows from the triangle inequality, and (b) follows from the uniform-boundedness
of the noisy rollout together with (5), the bounded bias property as shown in (17), and the local-
Lipschitz property in Lemma 4. Finally, (c) follows from using r < Ï. Hence, g(i,k)(K)âˆ’âˆ‡Cj,r(K)
has a bounded norm and therefore belongs to a class of norm sub-Gaussian random matrices (Jin
et al., 2019). Furthermore, it has zero mean due to (16). Therefore, the concentration result follows
from a direct application of Corollary 7 from Jin et al. (2019) which provides a Hoeffding-type
inequality for norm sub-Gaussian random matrices.
Corollary 8 (Concentration of the collaborative zeroth-order gradient estimates.) Suppose As-
sumption 1 holds. For any system j âˆˆ[H], given a policy K âˆˆG0
j , a smoothing radius r âˆˆ(0, Ï),
define the collaborative zeroth-order gradient estimate as Gj(K) =
1
|Mj|
P
iâˆˆMj gi(K), where
gi(K) is the M-minibatched gradient estimate from agent i âˆˆMj. Let Î´â€² âˆˆ(0, 1). The following
holds with probability at least 1 âˆ’Î´â€²:
âˆ¥Gj(K) âˆ’âˆ‡Cj,r(K)âˆ¥â‰¤

Gâˆ+ Î» Ï
D + Ï•Ï2
D

D
r
p
|Mj|M
s
log
2D
Î´â€²

.
(20)
Proof Under Assumption 1, we note that the noise processes for all agents in Mj are independent.
The proof then follows from Lemma 7 as the collaborative zeroth-order gradient estimate can be
interpreted as a gradient estimate for system j with a |Mj|-fold increased minibatch size.
Note on the problem dependent constants. In Malik et al. (2020), the values of the constants
Î»j, Ï•j, Ïj, G(j)
âˆare first derived locally in terms of the local cost Cj(K), and then, the global
parameters are obtained by noting that the local cost is uniformly bounded over the restricted domain
as shown in Lemma 9 of Malik et al. (2020). Since we have a different definition of the restricted
domain, the values of our parameters vary from the ones provided in Malik et al. (2020). That said,
the global parameters in our setting can be derived exactly in the same way as in Malik et al. (2020)
by bounding the local cost as Cj(K) â‰¤10 Ëœâˆ†0 + Cj(Kâˆ—
j ).
For convenience, we compile all the relevant notation in Table 1.
In the main text, we used the notation c(p,_) to denote the problem-parameter-dependent constants
which are defined in the following.
16

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
c(p,8) =

Gâˆ+ Î» Ï
D + Ï•Ï2
D

c(p,9) = 12c(p,8)
Âµ

max
p
Ï•, 1
Ï
2
c(p,10) = max
n
âˆ†2
0, 256c2
(p,9)D2, c2
(p,8)D2âˆ†2
0, 36G2
âˆ
o
c(p,11) =
 
âˆ†2
0
c(p,10) log
  8DN
Î´

!
c(p,12) = 4
Î·Âµ
 
log
 
c(p,10)N Ëœâˆ†2
0
âˆ†2
0
!
+ log(4)
!
c(p,13) = 4 max{1, c(p,9)}
q
c(p,12) log(c(p,11)T)
c(p,1) = min
ï£±
ï£²
ï£³
8
Âµ, 1
4Ï•,
Ï
Î» + 2 max
nâˆšÏ•, 1
Ï
o
ï£¼
ï£½
ï£¾
c(p,2) = 4
Î·Âµ
c(p,3) = Ëœâˆ†0 max{16, 10c(p,10)}
c(p,4) = c(p,10)
c(p,5) = c(p,8)D
Ï•
c(p,6) = Ï
c(p,7) = Dc(p,13)
(21)
Based on the above definitions of the problem-parameter-dependent constants, we provide the
values used for the hyperparameters in the l-th epoch of the PCPO algorithm in Table 2.
17

KANAKERI BAJAJ VERMA GUPTA MITRA
Table 2: Hyperparameters with their values in the lth epoch
Hyperparameters
Values
âˆ†l
âˆ†0
2l
Î´l
Î´
2l2
Î·
c(p,1)
Rl
c(p,2) log
 c(p,3)N
âˆ†2
l

Ml
c(p,4)
âˆ†2
l
log

8DNRl
Î´l

Ëœrl

c(p,5)
âˆšMl
r
log

8DNRl
Î´l
1/2
r(loc)
l
min{c(p,6), Ëœrl}
r(global)
l
min

c(p,6),
Ëœrl
|N (lâˆ’1)
i
|1/4

Appendix B. Proof of Theorem 1
In this section, we provide the proof of Theorem 1 which concerns the clustering aspect of the PCPO
algorithm. In particular, we show that, with high probability, the true clusters are included in the
neighborhood sets for all agents in each epoch, and if epoch l â‰¥L = min{l âˆˆ1, 2, . . . : âˆ†l â‰¤âˆ†/2},
the neighborhood sets are identical to the clusters. More specifically, we show that for all agents
i âˆˆ[N], with probability at least 1 âˆ’Î´/2, MÏƒ(i) âŠ†N (l)
i
in every epoch l, and if l â‰¥L, then
MÏƒ(i) = N (l)
i .
To prove both claims, it suffices to show that with high probability, the estimated cost at a locally
optimized policy is concentrated in the âˆ†l/4-neighborhood of the optimal cost in every epoch l for
all agents i âˆˆ[N]. To see this, consider a â€œgoodâ€ event that occurs with probability 1 âˆ’Î´/2 where
the following holds for all agents i âˆˆ[N] in every epoch l (we will prove that such an event exists
later in this section):
| Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))| â‰¤âˆ†l/4.
(22)
On this â€œgoodâ€ event, in what follows, we show that the first claim of Theorem 1 holds. Accordingly,
fix an agent i and consider an agent j âˆˆMÏƒ(i). We now show by induction that j belongs to N (l)
i
in every epoch l. For the base case of induction, note that since we initialize the neighborhood sets
with all agents, j âˆˆN (0)
i
. Next, for an epoch l âˆ’1 â‰¥1, let us assume that j âˆˆN (lâˆ’1)
i
. Since
CÏƒ(i)(Kâˆ—
Ïƒ(i)) = CÏƒ(j)(Kâˆ—
Ïƒ(j)) as a consequence of j âˆˆMÏƒ(i), in the l-th epoch under the â€œgoodâ€
18

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
event where (22) holds, we have
| Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )| â‰¤| Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))| + | Ë†CÏƒ(j)(X(l)
j ) âˆ’CÏƒ(j)(Kâˆ—
Ïƒ(j))|
â‰¤âˆ†l/4 + âˆ†l/4 = âˆ†l/2,
implying that j âˆˆN (l)
i
based on the neighborhood set update rule in (7). Hence, by induction,
j âˆˆN (l)
i
in every epoch, therefore establishing the first claim of Theorem 1.
Next, we show that on the â€œgoodâ€ event where (22) holds for all agents in every epoch, the
second claim of Theorem 1 is also true. We prove this claim via contradiction. To proceed, suppose
that there exist an epoch l â‰¥L, an agent i, and an agent j /âˆˆMÏƒ(i) such that j âˆˆN (l)
i . Then, we
have the following in light of the heterogeneity metric defined in (3):
âˆ†â‰¤
CÏƒ(i)(Kâˆ—
Ïƒ(i)) âˆ’CÏƒ(j)(Kâˆ—
Ïƒ(j))

â‰¤
CÏƒ(i)(Kâˆ—
Ïƒ(i)) âˆ’Ë†CÏƒ(i)(X(l)
i ) + Ë†CÏƒ(j)(X(l)
j ) âˆ’CÏƒ(j)(Kâˆ—
Ïƒ(j)) + Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )

â‰¤
CÏƒ(i)(Kâˆ—
Ïƒ(i)) âˆ’Ë†CÏƒ(i)(X(l)
i )
 +
 Ë†CÏƒ(j)(X(l)
j ) âˆ’CÏƒ(j)(Kâˆ—
Ïƒ(j))
 +
 Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )

(a)
â‰¤âˆ†l/4 + âˆ†l/4 +
 Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )

(b)
â‰¤âˆ†/4 +
 Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )
 ,
where (a) holds due to (22), and (b) follows as âˆ†l â‰¤âˆ†/2 since l â‰¥L. The above set of inequalities
imply that
 Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )
 â‰¥(3/4)âˆ†â‰¥(3/2)âˆ†l, contradicting our assumption that
j âˆˆN (l)
i
which requires that
 Ë†CÏƒ(i)(X(l)
i ) âˆ’Ë†CÏƒ(j)(X(l)
j )
 â‰¤âˆ†l/2. Therefore, N (l)
i
= MÏƒ(i) for all
l â‰¥L, establishing the second claim of Theorem 1.
Now, it remains to prove that the â€œgoodâ€ event where (22) holds for all agents in every epoch
occurs with probability at least 1 âˆ’Î´/2. To do so, for an agent i âˆˆ[N] in epoch l, we have
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))
 â‰¤
CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))
 +
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )
 .
Therefore, to show (22), it suffices to show the following two guarantees for all agents in every
epoch:
(a)
CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))
 â‰¤âˆ†l/8
(b)
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )
 â‰¤âˆ†l/8.
(23)
Now, let us establish the claims in (23) via induction across epochs. Let us assume that for a fixed
epoch l âˆ’1 â‰¥1, the claims in (23) hold for all agents i âˆˆ[N] in every epoch k â‰¤{1, 2, . . . , l âˆ’1},
with probability at least (1 âˆ’Plâˆ’1
j=1
Î´j
2 ). Denoting this event as Elâˆ’1, in the following, we show that
the claims in (23) hold in epoch l, âˆ€i âˆˆ[N] with probability at least (1 âˆ’Î´l
2 ) conditioned on the
event Elâˆ’1.
In the following, fixing an agent i âˆˆ[N], we show: (a)
CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))
 â‰¤âˆ†l/8
with probability at least 1âˆ’Î´l/(4N) conditioned on the event Elâˆ’1, and (b)
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )
 â‰¤
19

KANAKERI BAJAJ VERMA GUPTA MITRA
âˆ†l/8 with probability at least 1 âˆ’Î´l/(4N) conditioned on the intersection of the events Elâˆ’1 and the
one where item (a) in (23) holds. The following lemma provides the convergence of the local policy
optimization sub-routine in epoch l which aids in establishing claim (a) from (23).
Lemma 9 (Local Policy Optimization.) For any agent i âˆˆMj, given a policy K0 âˆˆG0
j , let KR be
the output of the localPO(K0, M, R, r) subroutine with step size Î·. Then, for any Î´â€² âˆˆ(0, 1/R),
with probability at least 1 âˆ’Î´â€²R, KR âˆˆG0
j and we have the following:
Cj(KR) âˆ’Cj(Kâˆ—
j ) â‰¤

1 âˆ’Î·Âµ
4
R
(Cj(K0) âˆ’Cj(Kâˆ—
j )) +
 
c(p,9)D
âˆš
M
s
log
2D
Î´â€²
!
,
(24)
when Î· = c(p,1), M â‰¥
c(p,4)
âˆ†2
0 log(2D/Î´â€²), r = min{Ï, Ëœr}, where Ëœr =

c(p,5)
âˆš
M
q
log
  2D
Î´â€²
1/2
.
The proof of Lemma 9 is provided in Appendix B.1. We use Lemma 9 to analyze the local policy
optimization step in line 4 of the PCPO algorithm that helps in establishing
CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))
 â‰¤
âˆ†l/8 with probability at least 1 âˆ’Î´l/(4N). More precisely, the settings for the hyperparameters
(Î·, Ml, r(loc)
l
, Rl) from Table 2 meet the requirement for the corresponding hyperparameters in
Lemma 9. Furthermore, conditioned on the event Elâˆ’1, claim (a) in (23) implies that X(lâˆ’1)
i
âˆˆG0
Ïƒ(i).
Hence, the following holds due to Lemma 9 with probability at least 1âˆ’Î´â€²Rl for some Î´â€² âˆˆ(0, 1/Rl):
CÏƒ(i)(X(l)
i )âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤

1 âˆ’Î·Âµ
4
Rl (CÏƒ(i)(X(lâˆ’1)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)))
|
{z
}
s1
+
 
c(p,9)D
âˆšMl
s
log
2D
Î´â€²
!
|
{z
}
s2
.
Note that (CÏƒ(i)(X(lâˆ’1)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))) â‰¤âˆ†lâˆ’1/8 â‰¤âˆ†0 â‰¤Ëœâˆ†0 as a result of conditioning on the
event Elâˆ’1 where claim (a) of (23) holds. Hence, from Table 2, setting Rl = c(p,2) log
 c(p,3)N
âˆ†2
l

â‰¥
4
Î·Âµ log

16 Ëœâˆ†0
âˆ†l

ensures s1 â‰¤âˆ†l/16. Similarly, setting Ml =
c(p,4)
âˆ†2
l
log
  2D
Î´â€²

â‰¥
162c2
(p,9)D2
âˆ†2
l
log
  2D
Î´â€²

ensures s2 â‰¤âˆ†l/16. Finally, setting Î´â€² = Î´l/(4NRl) provides us with CÏƒ(i)(X(l)
i )âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤
âˆ†l/8, and hence X(l)
i
âˆˆG0
Ïƒ(i) with probability at least (1 âˆ’Î´l/(4N)), based on Lemma 9. Let us
denote this event by ËœE(l,1).
Now, we show that
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )
 â‰¤âˆ†l/8 with probability at least 1 âˆ’Î´l/(4N)
conditioned on the event ËœE(l,1). We have Ë†CÏƒ(i)(X(l)
i ) =
1
Ml
PMl
j=1 CÏƒ(i)(X(l)
i , Z(i)
j ) and E[CÏƒ(i)(X(l)
i , Z(i)
j )] =
CÏƒ(i)(X(l)
i ). Furthermore, since on event ËœE(l,1), X(l)
i
âˆˆG0
j , we have CÏƒ(i)(X(l)
i , Z(i)
j ) â‰¤Gâˆdue to
(18). Using Hoeffdingâ€™s inequality, the following holds for all s â‰¥0:
P
ï£«
ï£­

1
Ml
Ml
X
j=1
CÏƒ(i)(X(l)
i , Z(i)
j ) âˆ’CÏƒ(i)(X(l)
i )

â‰¥s
ï£¶
ï£¸â‰¤2 exp
âˆ’2s2Ml
G2âˆ

.
Setting s = âˆ†l/8, and requiring the failure probability on the R.H.S to be lesser than Î´l/(4N) leads
to the requirement: Ml â‰¥36G2
âˆ
âˆ†2
l
log

8N
Î´l

which is satisfied by our choice of Ml from Table 2.
20

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
Hence,
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )
 â‰¤âˆ†l/8 with probability at least 1 âˆ’Î´l/(4N). Let us denote
this event as ËœE(l,2).
Now, to show the two claims in (23), let us define an event ËœEl = ËœE(l,1) âˆ©ËœE(l,2).
Then,
P( ËœEl|Elâˆ’1) = P( ËœE(l,2)| ËœE(l,1), Elâˆ’1)P( ËœE(l,1)|Elâˆ’1) â‰¥(1 âˆ’Î´l/(4N))(1 âˆ’Î´l/(4N)) â‰¥1 âˆ’Î´l/(2N).
Therefore, on event ËœEl, both the guarantees: (a)
CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i))
 â‰¤âˆ†l/8 and (b)
 Ë†CÏƒ(i)(X(l)
i ) âˆ’CÏƒ(i)(X(l)
i )
 â‰¤âˆ†l/8 hold with probability at least 1 âˆ’Î´l/(2N). Union bounding
over all the agents, with probability at least 1 âˆ’Î´l/2, both claims in (23) hold for all agents i âˆˆ[N]
on the event ËœEl after conditioning on the event Elâˆ’1.
Finally, defining an event El = ËœEl âˆ©Elâˆ’1, we have,
P(El) = P( ËœEl|Elâˆ’1)P(Elâˆ’1) â‰¥(1 âˆ’Î´l/2)
ï£«
ï£­1 âˆ’
lâˆ’1
X
j=1
Î´j
2
ï£¶
ï£¸â‰¥
ï£«
ï£­1 âˆ’
l
X
j=1
Î´j
2
ï£¶
ï£¸.
Since Î´l = Î´/(2l2), we have Pl
j=1
Î´j
2 = Pl
j=1
Î´
4j2 â‰¤Î´/2. This completes the proof of Theorem 1.
B.1. Proof of Lemma 9
In this section, we prove Lemma 9 which provides the convergence of the localPO subroutine. Fix
a system j âˆˆ[H] and let Kt denote the controller in the t-th iteration of localPO âˆ€t = 0, 1, . . . , R.
Note that the localPO sub-routine proceeds as follows: starting with a controller K0 âˆˆG0
j , in
every iteration t, Kt is updated as Kt+1 = Kt âˆ’Î·g(Kt), where g(Kt) = ZO(Kt, M, r) is the
M-minibatched zeroth-order gradient estimate with a smoothing radius r. We prove the statement
via induction. Given the base case K0 âˆˆG0
j and Î´â€² âˆˆ(0, 1/R), let us assume that in the t-th iteration
the following holds for all Ï„ âˆˆ{1, 2, . . . , t} with probability at least (1 âˆ’Î´â€²t) :
KÏ„ âˆˆG0
j
Cj(KÏ„) âˆ’Cj(Kâˆ—
j ) â‰¤

1 âˆ’Î·Âµ
4

(Cj(KÏ„âˆ’1) âˆ’Cj(Kâˆ—
j )) + Î·Âµ
4
 
c(p,9)D
âˆš
M
s
log
2D
Î´â€²
!
.
(25)
Let us denote the event where both the claims in (25) hold by Et. Now, conditioned on the event Et,
in the following, we will show that with probability at least 1 âˆ’Î´â€², Kt+1 âˆˆG0
j and
Cj(Kt+1) âˆ’Cj(Kâˆ—
j ) â‰¤

1 âˆ’Î·Âµ
4

(Cj(Kt) âˆ’Cj(Kâˆ—
j )) + Î·Âµ
4
 
c(p,9)D
âˆš
M
s
log
2D
Î´â€²
!
.
In what follows, we omit the subscript notation j for convenience. Conditioned on the event Et,
we begin by analyzing the one-step progress in the (t + 1)-th iteration of localPO.
From Lemma 7, as the event Et ensures that Kt âˆˆG0, we have âˆ¥g(Kt) âˆ’âˆ‡Cr(Kt)âˆ¥â‰¤
c(p,8)D
r
âˆš
M
q
log
  2D
Î´â€²

with probability at least (1 âˆ’Î´â€²). Let us denote this event by ËœEt. Define et :=
21

KANAKERI BAJAJ VERMA GUPTA MITRA
g(Kt) âˆ’âˆ‡C(Kt). Conditioned on the event ËœEt âˆ©Et, we have
âˆ¥etâˆ¥= âˆ¥g(Kt) âˆ’âˆ‡Cr(Kt) + âˆ‡Cr(Kt) âˆ’âˆ‡C(Kt)âˆ¥
(a)
â‰¤âˆ¥g(Kt) âˆ’âˆ‡Cr(Kt)âˆ¥+ âˆ¥âˆ‡Cr(Kt) âˆ’âˆ‡C(Kt)âˆ¥
(b)
â‰¤c(p,8)D
r
âˆš
M
s
log
2D
Î´â€²

+ Ï•r,
(26)
where (a) follows from the triangle inequality and the (b) due to the event ËœEt âˆ©Et and (17). Let us
define cp =
c(p,8)D
r
âˆš
M
q
log
  2D
Î´â€²

+ Ï•r. Set Ëœr =

c(p,8)D
Ï•
âˆš
M
q
log
  2D
Î´â€²
1/2
, r = min{Ï, Ëœr}, and define
Z :=
c(p,8)D
âˆš
M
q
log
  2D
Î´â€²

. Based on the choice M â‰¥
c(p,4)
âˆ†2
0 log(2D/Î´â€²), we have Z â‰¤1, yielding the
following sequence of bounds on cp :
cp = Z
r + Ï•r
â‰¤max
Z
Ëœr + Ï•Ëœr, Z
Ï + Ï•Ï

(a)
â‰¤max
(
2
p
ZÏ•,
âˆš
Z
Ï
+ Ï•Ëœr
)
= max
(
2
p
ZÏ•,
âˆš
Z
Ï
+
p
ZÏ•
)
â‰¤2
âˆš
Z max
p
Ï•, 1
Ï

(27)
(b)
â‰¤2 max
p
Ï•, 1
Ï

,
(28)
where (a) and (b) follow from Z â‰¤1. Based on the above, we have
Î·âˆ¥g(Kt)âˆ¥â‰¤Î·(âˆ¥âˆ‡C(Kt)âˆ¥) + âˆ¥etâˆ¥
(a)
â‰¤Î·(Î» + cp)
(b)
â‰¤Î·

Î» + 2 max
p
Ï•, 1
Ï

,
where (a) follows from Lemma 4 and (b) follows from (28). Setting the RHS â‰¤Ï leads to
the requirement Î· â‰¤
Ï
Î»+2 max
nâˆšÏ•, 1
Ï
o which is satisfied by setting Î· = c(p,1). This ensures that
22

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
âˆ¥Kt+1 âˆ’Ktâˆ¥â‰¤Ï. Using the local smoothness property (Lemma 5), we then have
C(Kt+1) âˆ’C(Kt) â‰¤âŸ¨âˆ‡C(Kt), Kt+1 âˆ’KtâŸ©+ Ï•
2 âˆ¥Kt+1 âˆ’Ktâˆ¥2
= âˆ’Î·âŸ¨âˆ‡C(Kt), g(Kt)âŸ©+ Ï•Î·2
2 âˆ¥g(Kt)âˆ¥2
= âˆ’Î·âŸ¨âˆ‡C(Kt), âˆ‡C(Kt) + etâŸ©+ Ï•Î·2
2 âˆ¥âˆ‡C(Kt) + etâˆ¥2
(a)
â‰¤âˆ’Î·âˆ¥âˆ‡C(Kt)âˆ¥2 âˆ’Î·âŸ¨âˆ‡C(Kt), etâŸ©+ Ï•Î·2âˆ¥âˆ‡C(Kt)âˆ¥2 + Ï•Î·2âˆ¥etâˆ¥2
(b)
â‰¤âˆ’Î·(1 âˆ’Ï•Î·)âˆ¥âˆ‡C(Kt)âˆ¥2 + Î·
2âˆ¥âˆ‡C(Kt)âˆ¥2 + Î·
2âˆ¥etâˆ¥2 + Ï•Î·2âˆ¥etâˆ¥2
= âˆ’Î·
2(1 âˆ’2Ï•Î·)âˆ¥âˆ‡C(Kt)âˆ¥2 + Î·
2(1 + 2Ï•Î·)âˆ¥etâˆ¥2
(c)
â‰¤âˆ’Î·
4âˆ¥âˆ‡C(Kt)âˆ¥2 + 3Î·
4 c2
p.
In the above, we used âˆ¥A + Bâˆ¥2 â‰¤2âˆ¥Aâˆ¥2 + 2âˆ¥Bâˆ¥2 in (a), and âˆ’2âŸ¨A, BâŸ©â‰¤âˆ¥Aâˆ¥2 + âˆ¥Bâˆ¥2 in (b)
where A and B are any matrices in RmÃ—n. In (c), we used Î· â‰¤1/(4Ï•) (satisfied by our choice
Î· = c(p,1)) and âˆ¥etâˆ¥â‰¤cp. Denoting the suboptimality gap as St = C(Kt) âˆ’C(Kâˆ—), and using the
PL condition (15) in the above, we obtain the following with probability at least 1 âˆ’Î´â€²:
St+1 â‰¤

1 âˆ’Î·Âµ
4

St + 3Î·
4 c2
p
â‰¤

1 âˆ’Î·Âµ
4

St + Î·Âµ
4
 3
Âµc2
p

.
(29)
On event Et, since we have Kt âˆˆG0
j , St â‰¤10 Ëœâˆ†0. Furthermore, due to (27), we have
3
Âµc2
p â‰¤12
Âµ

max
p
Ï•, 1
Ï
2 c(p,8)D
âˆš
M
s
log
2D
Î´â€²

.
Defining c(p,9) :=
12c(p,8)
Âµ

max
nâˆšÏ•, 1
Ï
o2
and setting M â‰¥
c2
(p,9)D2
âˆ†2
0
log
  2D
Î´â€²

, which is satisfied
by M â‰¥
c(p,4)
âˆ†2
0 log(2D/Î´â€²) from the statement of Lemma 9, ensures that 3
Âµc2
p â‰¤âˆ†0 â‰¤10 Ëœâˆ†0. Hence,
based on (29), we have St+1 â‰¤
 1 âˆ’Î·Âµ
4

10 Ëœâˆ†0 + Î·Âµ
4 10 Ëœâˆ†0 â‰¤10 Ëœâˆ†0. Therefore, conditioned on the
event ËœEt âˆ©Et, Kt+1 âˆˆG0
j and
St+1 â‰¤

1 âˆ’Î·Âµ
4

St + Î·Âµ
4
 
c(p,9)D
âˆš
M
s
log
2D
Î´â€²
!
.
Now, let us define Et+1 := ËœEt âˆ©Et. We have P( ËœEt âˆ©Et) = P( ËœEt|Et)P(Et) â‰¥(1 âˆ’Î´â€²)(1 âˆ’Î´â€²t) â‰¥
1 âˆ’Î´â€²(t + 1). This completes the induction step. To prove the statement of Lemma 9, since
Î· = c(p,1) â‰¤8/Âµ, for any R â‰¥1, we can unroll the recursion on the event ER which occurs with
23

KANAKERI BAJAJ VERMA GUPTA MITRA
probability 1 âˆ’Î´â€²R. Doing so, we obtain the following which completes the proof:
SR â‰¤

1 âˆ’Î·Âµ
4
R
S0 +
Râˆ’1
X
k=0

1 âˆ’Î·Âµ
4
k Î·Âµ
4
 
c(p,9)D
âˆš
M
s
log
2D
Î´â€²
!
â‰¤

1 âˆ’Î·Âµ
4
R
S0 +
 
c(p,9)D
âˆš
M
s
log
2D
Î´â€²
!
.
(30)
24

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
Appendix C. Proof of Theorem 2
We prove Theorem 2 by conditioning on the event where the claims in (23) hold for all agents in
every epoch. In Appendix B, we showed that such an event occurs with probability at least 1 âˆ’Î´/2
and let us denote it by EThm1. Furthermore, under this event, the claims of Theorem 1 hold as shown
in Appendix B. In particular, the true clusters are always contained in the neighborhood sets and
correct clustering takes place at the latest during the L-th epoch, ensuring that the agents collaborate
solely within their own clusters from the (L + 1)-th epoch onward. With that in mind, we consider
the following approach to prove Theorem 2. First, we take for granted that the last epoch occurs
after the correct clustering takes place, i.e, Â¯L > L, and later show that this is indeed true if the total
number of rollouts T â‰¥ËœO(1/âˆ†2). Next, we show that for any l > L (note that at least one such
epoch exists in light of Â¯L > L,) the policy at the start of the collaborative policy optimization remains
in the corresponding restricted domain with high probability, i.e, Ë†K(lâˆ’1)
i
âˆˆG0
Ïƒ(i). Then, we focus
on the last epoch Â¯L and provide the convergence guarantee, and finally conclude by analyzing the
number of rollouts needed to ensure Â¯L > L.
Conditioned on the event EThm1, we follow an induction based argument to show that Ë†K(lâˆ’1)
i
âˆˆ
G0
Ïƒ(i) for all agents i âˆˆ[N] in every epoch l > L. Let us define ËœL as the first epoch where correct
clustering takes place. Due to Theorem 1, since the correct clustering takes place at the latest during
the Lth epoch, ËœL â‰¤L, and moreover, since the neighborhood sets are sequentially pruned with
no new agents getting added to the neighborhood sets, we have MÏƒ(i) = N (l)
i
for all agents in
every epoch l â‰¥ËœL. Furthermore, ËœL being the first epoch where correct clustering takes place,
N
ËœLâˆ’1
i
Ì¸= N ËœL
i = MÏƒ(i) for some agent i âˆˆ[N], hence causing reinitializtion as shown in (8). After
this reinitialization, the global sequences for all the agents are updated by collaborating within their
respective clusters, and hence the global sequences for two agents within a cluster evolve identically
in light of (6). In other words, for all agents i, j, if MÏƒ(i) = MÏƒ(j), then for all l â‰¥ËœL, we have the
following on the event EThm1:
Ë†K(l)
i
= Ë†K(l)
j
MÏƒ(i) = N (l)
i
= N (l)
j
= MÏƒ(j)
(31)
Taking this into account, we show that Ë†K(lâˆ’1)
i
âˆˆG0
Ïƒ(i) for all agents i âˆˆ[N] in every epoch l > ËœL
via induction across epochs. For the base case l = ËœL + 1, as a consequence of reinitialization during
the ËœLth epoch, and since X(ËœL)
i
âˆˆG0
Ïƒ(i) for all agents i âˆˆ[N] as a result of conditioning on the event
EThm1, we have Ë†K(ËœL)
i
âˆˆG0
Ïƒ(i) for all agents i âˆˆ[N]. Let us assume that for an epoch l â‰¥ËœL + 1,
with probability at least (1 âˆ’Plâˆ’1
j=1
Î´j
4 ), we have Ë†K(tâˆ’1)
i
âˆˆG0
Ïƒ(i) for all agents i âˆˆ[N] and for
all t âˆˆ{ËœL + 1, ËœL + 2, . . . , l}. With a slight abuse of notation, let us denote this event by Elâˆ’1.
Next, conditioned on the event EThm1 âˆ©Elâˆ’1, we show that Ë†K(l)
i
âˆˆG0
Ïƒ(i) for all agents i âˆˆ[N] with
probability at least 1 âˆ’Î´l/4.
In what follows we fix an agent i âˆˆ[N] and omit the notation i and Ïƒ(i) for convenience. Given
that K(lâˆ’1) âˆˆG0 on the event EThm1 âˆ©Elâˆ’1, we focus on analyzing the iterates {Y (k)}0â‰¤k<Rl in
the lth epoch. The iterates are updated as follows: Y (k+1) = Y (k) âˆ’Î·G(Y (k)) with Y (0) = Ë†K(lâˆ’1),
where we used G(Y (k)) to denote the averaged zeroth-order gradient estimate as shown in (6) in the
kth iteration. Note that in the light of Assumption 1, the averaged gradient estimate is an unbiased
25

KANAKERI BAJAJ VERMA GUPTA MITRA
estimate of âˆ‡Cr(Y (k)) with an effective minibatch size of Ml|M|. Therefore, we follow an approach
similar to the one from the proof of Lemma 9 in Appendix B.1 to analyze the one-step progress and
to show that Y (Rl) = Ë†Kl âˆˆG0. More specifically, we follow an induction based approach across
iterations and establish one-step recursion similar to (25) and finally unroll the recursion to obtain
something similar to (30). However, a key difference arises from the fact that the second term in the
RHS of both (25) and (30) will now enjoy a variance reduction effect due to collaboration in light of
Assumption 1 as shown in Corollary 8.
In particular, following the induction approach from the proof of Lemma 9 in Appendix B.1,
in the kth iteration, we have the following concentration with probability at least (1 âˆ’Î´â€²) after
conditioning on the event where the previous iterations satisfy similar guarantees as in (25):
âˆ¥G(Y (k)) âˆ’âˆ‡Cr(Y (k))âˆ¥â‰¤
c(p,8)D
r
p
Ml|M|
s
log
2D
Î´â€²

.
Conditioned on the event where the gradient estimate is concentrated as above, and defining ek :=
G(Y (k)) âˆ’âˆ‡C(Y (k)), we have âˆ¥ekâˆ¥â‰¤
c(p,8)D
râˆš
Ml|M|
q
log
  2D
Î´â€²

+ Ï•r following the arguments up to
(26). Now, let us define cp =
c(p,8)D
râˆš
Ml|M|
q
log
  2D
Î´â€²

+ Ï•r. Setting Ëœr =

c(p,8)D
Ï•âˆšMl
q
log
  2D
Î´â€²
1/2
and
r = min{Ï,
Ëœr
|M|1/4 }, we obtain the following bound on cp provided Z :=
c(p,8)D
âˆšMl
q
log
  2D
Î´â€²

â‰¤1
which is ensured by our setting for Ml in Table 2.
cp =
Z
r
p
|M|
+ Ï•r
â‰¤max

Z
Ëœr|M|1/4 + Ï•
Ëœr
|M|1/4 ,
Z
Ï|M|1/2 + Ï•Ï

â‰¤max
(
2
âˆšZÏ•
|M|1/4 ,
âˆš
Z
Ï|M|1/4 + Ï•
Ëœr
|M|1/4
)
â‰¤2
âˆš
Z
|M|1/4 max
p
Ï•, 1
Ï

(32)
â‰¤2 max
p
Ï•, 1
Ï

.
(33)
Based on the above, we choose Î· = c(p,1) â‰¤
Ï
Î»+2 max
nâˆšÏ•, 1
Ï
o to ensure that âˆ¥Y (k+1) âˆ’Y (k)âˆ¥â‰¤Ï.
Defining Sk = C(Y (k)) âˆ’C(Kâˆ—) and following the analysis from Appendix B.1 up to (29) and
using the bound on cp from (32), we obtain
Sk+1 â‰¤

1 âˆ’Î·Âµ
4

Sk
|
{z
}
s1
+ Î·Âµ
4
 
c(p,9)D
p
Ml|M|
s
log
2D
Î´â€²
!
|
{z
}
s2
,
with probability at least 1 âˆ’Î´â€². Note that since |M| â‰¥1, the term s2 is not greater than the
corresponding term from (25), and hence s2 â‰¤Î·Âµ
4 âˆ†0 â‰¤Î·Âµ
4 10 Ëœâˆ†0. Meanwhile, the term s1 â‰¤
26

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
 1 âˆ’Î·Âµ
4

10 Ëœâˆ†0 as we have conditioned on the event where Y (k) âˆˆG0 similar to the proof of
Lemma 9. This ensures that Y (k+1) âˆˆG0. Now, unrolling the recursion, we have with probability at
least 1 âˆ’Î´â€²Rl, Y (Rl) = Ë†K(l) âˆˆG0 and the following:
C( Ë†K(l)) âˆ’C(Kâˆ—) â‰¤

1 âˆ’Î·Âµ
4
Rl (C( Ë†K(lâˆ’1)) âˆ’C(Kâˆ—)) +
 
c(p,9)D
p
Ml|M|
s
log
2D
Î´â€²
!
. (34)
Setting Î´â€² = Î´l/(4RlN) and applying an union bound over all agents, we have the above guarantee
for all agents with probability at least 1 âˆ’Î´l/4.
Let us denote this event by ËœEl. Defining El = ËœEl âˆ©Elâˆ’1, we have the following:
P(El|EThm1) = P( ËœEl|Elâˆ’1, EThm1)P(Elâˆ’1|EThm1) â‰¥(1 âˆ’Î´l/4)
ï£«
ï£­1 âˆ’
lâˆ’1
X
j=1
Î´j
4
ï£¶
ï£¸â‰¥
ï£«
ï£­1 âˆ’
l
X
j=1
Î´j
4
ï£¶
ï£¸.
This completes the induction argument. Hence, we have established that Ë†K(lâˆ’1)
i
âˆˆG0
Ïƒ(i) and that
(34) holds for all agents i âˆˆ[N] in every epoch l â‰¥ËœL with probability at least

1 âˆ’Pl
j=1
Î´j
4

.
Next, we analyze the final convergence guarantee in the last epoch Â¯L. Conditioned on the event
EÂ¯Lâˆ’1 âˆ©EThm1, we obtain (34) as shown in the following with probability at least 1 âˆ’Î´Â¯L/4 for all
agents i âˆˆ[N]:
CÏƒ(i)( Ë†K(Â¯L)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤

1 âˆ’Î·Âµ
4
R Â¯L (CÏƒ(i)( Ë†K(Â¯Lâˆ’1)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)))
|
{z
}
s1
+
 
c(p,9)D
p
MÂ¯L|M|
s
log
8DNRÂ¯L
Î´Â¯L
!
|
{z
}
s2
.
In the above, the settings of RÂ¯L and MÂ¯L from Table 2 ensures the following: from RÂ¯L â‰¥
4
Î·Âµ log

c(p,4)N10 Ëœâˆ†0
âˆ†2
Â¯L

, note that the term
s1 â‰¤exp

âˆ’Î·ÂµRÂ¯L
4

S0 â‰¤exp

âˆ’Î·ÂµRÂ¯L
4

10 Ëœâˆ†0 â‰¤
âˆ†2Â¯L
c(p,4)N .
Using MÂ¯L =
c(p,4)
âˆ†2
Â¯L log

8DNR Â¯L
Î´ Â¯L

in the above, we have s1 â‰¤
log

8DNR Â¯L
Î´ Â¯L

M Â¯LN
. Furthermore, since
MÂ¯L =
c(p,4)
âˆ†2
Â¯L log

8DNR Â¯L
Î´ Â¯L

â‰¥log

8DNR Â¯L
Î´ Â¯L

, we have
s1 â‰¤
r
log

8DNR Â¯L
Î´ Â¯L

p
MÂ¯LN
â‰¤
r
log

8DNR Â¯L
Î´ Â¯L

p
MÂ¯LN
â‰¤
r
log

8DNR Â¯L
Î´ Â¯L

p
MÂ¯L|M|
.
27

KANAKERI BAJAJ VERMA GUPTA MITRA
Together with the term s2 we obtain the following with probability at least 1 âˆ’Î´Â¯L/4 for all agents
i âˆˆ[N]:
CÏƒ(i)( Ë†K(Â¯L)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤2 max{1, c(p,9)D}
q
MÂ¯L|MÏƒ(i)|
s
log
8DNRÂ¯L
Î´

.
(35)
The above holds on the event EÂ¯L conditioned on the event EThm1. We have P(EÂ¯L|EThm1) â‰¥

1 âˆ’PÂ¯L
j=1
Î´j
4

=

1 âˆ’PÂ¯L
j=1
Î´
8j2

â‰¥1 âˆ’Î´/4. Therefore,
P(EEÂ¯L âˆ©EThm1) = P(EEÂ¯L|EThm1)P(EThm1) â‰¥(1 âˆ’Î´/4)(1 âˆ’Î´/2) â‰¥1 âˆ’(Î´/4 + Î´/2) â‰¥1 âˆ’Î´.
Note that the guarantee in (35) provides a rate ËœO

1/
q
MÂ¯L|MÏƒ(i)|

. It remains to show that
MÂ¯L = Ëœâ„¦(T). Since âˆ†l = âˆ†0/4l and Rl â‰¥1 for all l âˆˆ{1, 2, . . . , Â¯L}, consider the following as
Ml â‰¤T
Ml â‰¤T =â‡’4l â‰¤
Tâˆ†2
0
c(p,10) log
  8DN
Î´

=â‡’l â‰¤log
 
Tâˆ†2
0
c(p,10) log
  8DN
Î´

!
= log(c(p,11)T),
(36)
where we defined c(p,11) :=

âˆ†2
0
c(p,10) log( 8DN
Î´ )

. Now, we use the upper bound on l to bound Rl as
follows:
Rl = 4
Î·Âµ
 
log
 
c(p,10)N Ëœâˆ†2
0
âˆ†2
0
!
+ l log(4)
!
(a)
â‰¤l
 
4
Î·Âµ
 
log
 
c(p,10)N Ëœâˆ†2
0
âˆ†2
0
!
+ log(4)
!!
(b)
â‰¤c(p,12) log(c(p,11)T),
28

HARNESSING DATA FROM CLUSTERED LQR SYSTEMS
where (a) follows as l â‰¥1, and (b) follows from (36) with c(p,12) :=
4
Î·Âµ

log

c(p,10)N Ëœâˆ†2
0
âˆ†2
0

+ log(4)

.
Therefore, the overall sample complexity has the following bound:
T =
Â¯L
X
l=1
(2MlRl + Ml)
(a)
â‰¤
Â¯L
X
l=1
(3MlRl)
(b)
â‰¤
Â¯L
X
l=1
3c(p,12) log(c(p,11)T)
 
c(p,10) log
  8DNT
Î´

âˆ†2
0
!
4l
= 4c(p,12) log(c(p,11)T)
 
c(p,10) log
  8DNT
Î´

âˆ†2
0
!
4
Â¯L
=â‡’
T
4c(p,12) log(c(p,11)T)

c(p,10) log( 8DNT
Î´
)
âˆ†2
0
 â‰¤4
Â¯L.
ï£¼
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£½
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´
ï£¾
(37)
In the above, (a) follows as Rl â‰¥1 and (b) as we used Rl â‰¤T in Ml. Using the lower bound
on 4Â¯L as obtained above in MÂ¯L, we obtain MÂ¯L â‰¥
T log
 8DNR Â¯L
Î´

4c(p,12) log(c(p,11)T) log( 8DNT
Î´
). Using this bound in
(35), we have the following with probability 1 âˆ’Î´:
CÏƒ(i)( Ë†K(Â¯L)
i
) âˆ’CÏƒ(i)(Kâˆ—
Ïƒ(i)) â‰¤
4D max{1, c(p,9)}
q
c(p,12) log(c(p,11)T) log
  8DNT
Î´

q
T|MÏƒ(i)|
=
Dc(p,13)
q
log
  8DNT
Î´

q
T|MÏƒ(i)|
,
(38)
where we defined c(p,13) := 4 max{1, c(p,9)}
q
c(p,12) log(c(p,11)T).
Finally, it remains to show that Â¯L > L when T â‰¥ËœO(1/âˆ†2). To ensure, Â¯L > L, consider the
number of rollouts required up to (L + 1)th epoch. From (37) with the summation from 1 to L + 1
we have:
L+1
X
l=1
(2MlRl + Ml) â‰¤4c(p,12) log(c(p,11)T)
 
c(p,10) log
  8DNT
Î´

âˆ†2
0
!
4L+1.
In the above, setting T â‰¥RHS to ensure Â¯L > L, we have
T â‰¥4c(p,12) log(c(p,11)T)
 
c(p,10) log
  8DNT
Î´

âˆ†2
0
!
4L+1
(a)
â‰¥4c(p,12) log(c(p,11)T)
 
c(p,10) log
  8DNT
Î´

âˆ†2
0
! âˆ†0
âˆ†
2
.
29

KANAKERI BAJAJ VERMA GUPTA MITRA
In the above, since L = min{l âˆˆ1, 2, . . . : âˆ†l â‰¤âˆ†/2}, (a) follows from the fact that âˆ†L+1 =
âˆ†0/(2L+1) â‰¤âˆ†/2. Hence, when T â‰¥ËœO(1/âˆ†2), we have Â¯L > L. This completes the proof of
Theorem 2.
Appendix D. Proof of Corollary 3
In this section, we analyze the total communication complexity of the PCPO algorithm. In every
epoch, each agent communicates with the server once in every iteration of the collaborative policy
optimization subroutine, and once to send the local policy to update the neighborhood sets. Hence,
the overall communication complexity is PÂ¯L
l=1(Rl + 1). Note that PÂ¯L
l=1(Rl + 1) â‰¤(RÂ¯L + 1)Â¯L
since RÂ¯L = c(p,2) log

2 Â¯Lc(p,3)N
âˆ†2
0

â‰¥c(p,2) log
 2lc(p,3)N
âˆ†2
0

= Rl.
From (37), Â¯L is logarithmic in T. Furthermore, RÂ¯L = c(p,2) log

2 Â¯Lc(p,3)N
âˆ†2
0

is logarithmic in
the number of agents N and T. Finally, since T â‰¥ËœO(1/âˆ†2), the overall communication complexity
is logarithmic in T, N and 1/âˆ†.
30
