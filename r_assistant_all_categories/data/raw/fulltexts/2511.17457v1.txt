GPR-OdomNet: Difference and Similarity-Driven
Odometry Estimation Network for Ground
Penetrating Radar-Based Localization
1st Huaichao Wang
the Department of Computer Science
Civil Aviation University of China
Tianjin, China
hc-wang@cauc.edu.cn
2nd Xuanxin Fan
the Department of Computer Science
Civil Aviation University of China
Tianjin, China
755824110@qq.com
3rd Ji Liu
Chengdu Textile College
Chengdu Textile College
Chengdu, China
liuji_haha@126.com
4th Dezhen Song
the Department of Robotics
Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI)
Abu Dhabi, UAE
dezhen.song@mbzuai.ac.ae
5th Haifeng Li
the Department of Computer Science
Civil Aviation University of China
Tianjin, China
hfli@cauc.edu.cn
Abstract—When performing robot/vehicle localization using
ground penetrating radar (GPR) to handle adverse weather and
environmental conditions, existing techniques often struggle to
accurately estimate distances when processing B-scan images
with minor distinctions. This study introduces a new neural
network-based odometry method that leverages the similarity and
difference features of GPR B-scan images for precise estimation
of the Euclidean distances traveled between the B-scan images.
The new custom neural network extracts multi-scale features
from B-scan images taken at consecutive moments and then
determines the Euclidean distance traveled by analyzing the
similarities and differences between these features. To evaluate
our method, an ablation study and comparison experiments have
been conducted using the publicly available CMU-GPR dataset.
The experimental results show that our method consistently
outperforms state-of-the-art counterparts in all tests. Specifically,
our method achieves a root mean square error (RMSE), and
achieves an overall weighted RMSE of 0.449 m across all data
sets, which is a 10.2% reduction in RMSE when compared to
the best state-of-the-art method.
Index Terms—Ground Penetrating Radar (GPR), Robot Lo-
calization, Deep Learning Odometry.
I. INTRODUCTION
Precise localization of robots and vehicles in challeng-
ing weather and environmental scenarios is essential for
autonomous driving. Popular localization methods rely on
onboard sensors such as the global positioning system (GPS)
receiver, cameras, lidars, and inertial measurement units (IMU)
[1]–[3]. However, these methods are significantly hampered in
environments such as urban canyons, tunnels, or under adverse
weather conditions, presenting significant safety challenges in
autonomous driving. In contrast, the subsurface structures and
features of an urban road remain stable and less affected by
these adverse conditions. Ground penetrating radar (GPR),
as a less-exploited sensor that is complementary to existing
sensors, can effectively detect subsurface features, providing
new opportunities to enhance the reliability and robustness
of localization under challenging weather or environmental
scenarios.
Subsurface
Distance
Difference Detection
Similarity Detection
Linear Regression
···
···
···
···
Feature Extraction
Subsurface
Ground
GPR-OdomNet
��−1
��
Butterworth
Bandpass
Filter
SEC 
Gain
Dewow 
Filter
Wavelet 
Transform
GPR Data Preprocessing
Fig. 1: A new GPR odometry network that estimates the
distance by comparing the differences and similarities between
two consecutive time step B-scan images.
Here, we introduce a novel deep learning-based GPR
odometry network (GPR-OdomNet) to capture the properties
of subsurface features. As shown in Fig. 1, GPR-OdomNet
uniquely exploits similarity and difference in consecutive GPR
B-scan images, achieving more accurate distance estimation by
capturing both high-level and subtle features. We conducted
extensive experiments using the publicly available CMU-GPR
dataset [4]. The experimental results show that our method
consistently outperforms state-of-the-art counterparts in all
tests. Specifically, our method reaches an overall absolute
trajectory error (ATE) of 0.449 meters, which is measured in
root mean square error (RMSE) aggregated over all scenes.
The result is a 10.2% reduction in RMSE compared to the
arXiv:2511.17457v1  [cs.CV]  21 Nov 2025

best state-of-the-art method.
II. RELATED WORK
The closely related work includes general GPR perception
applications, GPR-based localization development, and recent
deep learning-based approaches, in particular.
Due to its unique sensing modality, GPR finds many ap-
plication domains, including bridge inspection [5], [6], in-
frastructure assessment [7], [8], transportation infrastructure
diagnostics [9], [10], and extraterrestrial exploration [11],
[12]. These applications primarily utilize GPR’s subsurface
target detection capabilities. When mounted on a robot, GPR
enables 3D underground reconstruction through nondestructive
scanning [13]–[15], demonstrating GPR’s potential for high-
precision spatial mapping.
GPR-based localization research has gained a lot of research
attention recently. The pioneering localization GPR (LGPR)
system [16] proposes basic signal matching principles but suf-
fers from raw signal noise, limited sensitivity, and GPS depen-
dence. Skartados et al. [17] assume and utilize the existence of
a widespread pipeline structure, which also limits the scope of
practical application. Li et al. [18] propose a Dominant Energy
Curve (DEC) descriptor, enhancing accuracy via the metric
feature mapping. Subsequent work [19] developed a multi-
modal odometry system combining GPR with inertial and
wheel sensors, introducing subsurface feature matrix (SFM)
as a feature representation and employing factor graph-based
optimization to form a sensor fusion-based approach that is
more robust. Both of these methods are benefited from with
hand-crafted features. However, such methods rely on expert-
guided manual feature selection which introduces subjectivity
and computational complexity.
Despite the progress in using current deep learning solutions
with strong performance on GPR images with clear feature
disparities, more information can be extracted from adjacent
B-scans. Recognizing that precise displacement information is
embedded in these subtle variations between the B-scans, our
work designs a new neural network to capture the information,
which can significantly advance localization accuracy and
robustness.
III. PROBLEM DEFINITION
Before introducing the GPR localization problem, main
variables are defined as follows,
• L: the maximum width of the B-scan image, i.e., the
number of sampling points in the time dimension.
• Bt: the B-scan data which is the set of A-scan data from
time t −L to time t.
• Ot−1,t: the traveled distance of the GPR between time
t −1 and time t.
With notations defined, we formally define the problem as
follows:
Problem 1: Given Bt−1 and Bt, determine Ot−1,t.
IV. GPR-ODOMNET
We design a deep learning-based GPR odometry neural
network (GPR-OdomNet) using the properties of subsurface
features (see Fig. 3). GPR-OdomNet takes inputs from the
pre-processed B-scan images.
A. GPR Data Preprocessing
We apply the same pre-processing steps as in [4], [20]
to raw GPR inputs to enhance signal quality and reduce
noise, resulting in B-scan images for subsequent analysis. We
skip the details and focus on the below primary steps for
completeness purpose:
• A Butterworth bandpass filter is applied to eliminate high-
frequency noise and low-frequency drift.
• The spreading and exponential compensation (SEC) gain
function is utilized to compensate for signal propagation
loss.
• Background noise is reduced using Dewow filter.
• Wavelet transform is employed for denoising, thereby
improving the signal-to-noise ratio.
Figs. 3(a) and (b) illustrate two sample B-scan images
after the pre-processing step through data interpolation and
stitching, which serve as the input to the GPR-OdomNet.
B. GPR-OdomNet
The GPR-OdomNet estimates the distance traveled by the
GPR using two consecutive B-scan images. This network
is composed of four primary modules: feature extraction,
difference detection, similarity detection, and fully connected
regression, as depicted in Fig. 2.
1) Feature Extraction: Noise from the B-scan images is
inevitable, which may obscure critical reflection signals. A
key step is the suppression of noise and non-target reflections
to enhance the visibility of effective reflection signals. we
employ ResNet-50 with modified input channels for B-scan
processing,
Ft,1 = H(Bt) + F(Bt, Wl)
(1)
where H(·) denotes identity mapping, F(·) represents resid-
ual transformations with weights Wl, and Ft,1 is a four-
dimensional feature extracted by the first layer of ResNet-
50(which includes 3 residual blocks, as shown in Fig. 2),
including batch_size, channels, width and height.
After four layers of feature extraction using Resnet-50,
we obtained multi-scale features Ft,1, Ft,2, Ft,3 and Ft,4.
We compress the low-dimensional features Ft,1, Ft,2 and
Ft,3 containing detailed information to obtain Ft,d for dif-
ference detection, and flatten the high-dimensional features
Ft,4 containing global information to obtain Ft,s for similarity
detection. Figs. 3 (c) show some subgraphs of the extracted
features Ft−1,d and Ft,d. Figs. 3 (d) show the extracted
features Ft−1,s and Ft,s that have been flattened. Due to the
extracted feature dimension being 512, it is inconvenient to
illustrate. Hence, the result shown in Figs. 3 (c) and (d) are
randomly sampled for display. By the way, the result shown
in Figs. 3 (e) and (f) are also randomly sampled for display.

��−1
—
Conv
BN
ReLU
CBR×2
Conv
ReLU
Conv
AvgPool
Sigmoid
Conv
Sigmoid
×
AvgPool
Cosine
Similarity
CBR
AvgPool
��
LBRD
LBRD
Linear
D
C
Maxpool
ResBlock×3
ResBlock×4
ResBlock×6
ResBlock×3
CBR
Flatten
1×1 Conv
1×1 Conv
Up Sample
Squeeze
Excitation
CBR
C
Similarity 
Detection
Fully Connected
Feature Extraction 
CBR
Difference 
Detection
Fig. 2: System architecture of GPR-OdomNet. The network takes B-scan images from two consecutive time steps as input.
Initially, multi-scale features are extracted from each image separately. Subsequently, the difference tensor is obtained by
detecting difference between the two feature sets, and the similarity tensor is calculated through cosine similarity. Finally, the
fully connected layers perform regression on the difference tensor and similarity tensor to yield the final distance. c⃝means
tensor concatenation, ×⃝means tensor multiplication, -⃝means tensor subtraction, and D means distance.
Such feature extraction not only mitigates the interference
from noise, but also strengthens the feature expression of target
structures within the images, thereby enhancing the accuracy
of subsequent image analysis.
2) Difference Detection: After extracting features Ft,d,
Ft−1,d, Ft,s and Ft−1,s from the images Bt and Bt−1,
respectively, the next step is to use these four features for sim-
ilarity and difference detection. Similarity detection identifies
the common ground of the two features. However, adjacent
consecutive B-scans are very similar in general, as shown in
Figs. 3 (d). It is also important to identify their difference,
which contains important information about the GPR motion.
The absolute difference between the features of the two images
is computed, resulting in a difference vector that represents the
disparity in features,
∆Ft−1,t = |Ft,d −Ft−1,d|
(2)
Each element of ∆Ft−1,t reflects the magnitude of the dis-
crepancy between the two images along the corresponding
feature dimension. The absolute value is taken to disregard the
direction of the difference, focusing solely on the magnitude
of the disparity.
To further assist in the extraction of local patterns and struc-
tural information from ∆Ft−1,t, a convolutional operation is
applied. The convolutional layer filters are adapted to capture
local correlations within ∆Ft−1,t, such as identifying whether
the disparities along certain feature dimensions exhibit any
spatial regularity. The introduction of an activation function,
i.e. ReLU, adds non-linearity to the network, enabling it to
capture more complex patterns of feature differences,
CBR(x) = ReLU(BN(Conv(x)))
(3)
∆F Conv
t−1,t = CBR(CBR(∆Ft−1,t))
(4)
where Conv stands for convolution and BN represents batch
normalization.
To enhance the discriminative power of ∆Ft−1,t, we have
added spatial-channel attention mechanisms for the difference
features after convolution. The spatial-channel attention mod-
ule simultaneously captures both channel-wise interdependen-
cies and spatial importance, enhancing the network’s ability
to focus on semantically significant features. The attention
mechanism consists of two components:
• Channel Attention: Computes attention weights using
global average pooling and multi-layer perceptron with
sigmoid activation:
CA(x) = σ (Conv (ReLU (Conv (GAP(x)))))
(5)
where GAP denotes global average pooling, and σ is the
sigmoid function.
• Spatial Attention: Computes spatial attention weights
using spatial context aggregation with large receptive
field:
SA(x) = σ (Conv(x))
(6)
The final attention-enhanced difference representation is
obtained by applying both attention mechanisms sequentially.
The enhanced difference features are then aggregated through
global average pooling for subsequent processing:
Dt−1,t = GAP(∆F Conv
t−1,t⊗CA(∆F Conv
t−1,t)⊗SA(∆F Conv
t−1,t)) (7)
where ⊗denotes tensor multiplication, and Dt−1,t as shown
in Fig. 3(e) represents the feature vector capturing the differ-
ences between Ft−1,d and Ft,d, describing the variations in

0
25 50 75 100125150175200
0
25
50
75
100
125
150
175
200
(a) Bt−1
0
25 50 75 100125150175200
0
25
50
75
100
125
150
175
200
(b) Bt
(c) Ft−1,d and Ft,d
0
10
20
30
40
50
Feature Dimension
0.0
0.5
1.0
1.5
2.0
2.5
Feature Value
Ft-1,s
Ft,s
(d) Ft−1,s and Ft,s
0
10
20
30
40
50
Feature Dimension
0.0
0.5
1.0
1.5
2.0
Feature Value
(e) Dt−1,t
0
10
20
30
40
50
Feature Dimension
0.0
0.5
1.0
1.5
2.0
Feature Value
(f) St−1,t
Fig. 3: Examples of intermediate features of the GPR-
OdomNet. Due to the large feature dimension of Ft−1,d, Ft,d,
Ft−1,s, Ft,s, Dt−1,t and St−1,t, the illustration here is a
randomly sampled. The first eight subgraphs in (c) belong to
Ft−1,d, while the last eight belong to Ft,d
GPR signals between adjacent time windows. This combined
approach effectively captures both the magnitude and spatial-
channel significance of feature differences between adjacent
time windows.
3) Similarity Detection: In parallel to the difference de-
tection, the network evaluates the similarity features St−1,t
between the two B-scan images to enhance the accuracy of
distance estimation,
CS(x, y) =
x · y
∥x∥2 · ∥y∥2
(8)
St−1,t = GAP(CBR(CS(Ft−1,s, Ft,s)))
(9)
where St−1,t (see Fig. 3(f)) represents the feature vector
describing the similarities between Ft−1,s and Ft,s. And ∥·∥2
denotes the L2 norm (euclidean norm) of a vector, calculated
as the square root of the sum of its squared components.
Here, the cosine similarity (CS) between the extracted features
measures the similarity of the two images in the feature space.
The cosine similarity ranges from −1 to 1, with a value closer
PR
�t−1
��−1
��−1
��,�−1
��,�−1
��
��,�
��,�
WE
IM
BI
GO
��
��
Fig. 4: Overview of the factor graph model. Square nodes
represent factor nodes, and circular nodes represent variable
nodes. The prior (PR) node is a prior factor node. The IMU
(IM) node mainly constrains the pose and velocity between
two consecutive moments. The Wheel Encoder (WE) and
GPR-OdomNet (GO) nodes constrain the velocity of the next
moment t. The Bias (BI) node constrains the biases between
two consecutive moments.
to 1 indicating a higher similarity between the images in the
feature space, suggesting a smaller distance and vise versa.
Next, after convolution and pooling, overly similar features
are tended towards zero to highlight salient features and reduce
computational complexity. Comparing Fig. 3(e) and Fig. 3(f),
as analyzed in the previous section, we can see that due to
the excessive similarity between Ft and Ft−1, the similarity
detection result St−1,t (see Fig. 3(f)) tends to approach 0 in
all dimensions, which reduces the effectiveness of subsequent
regression. However, the difference detection results Dt−1,t
(see Fig. 3(e)) have more significant feature dimensions, which
provide a better basis for subsequent regression.
4) Fully Connected Regression:
Finally, the regression
module estimates the distance by concatenating the difference
features Dt−1,t and the similarity features St−1,t,
LBRD(x) = Dropout(ReLU(BN(Linear(x))))
(10)
Dt−1,t = Linear(LBRD(LBRD(Concat(Dt−1,t, St−1,t))))
(11)
where Dt−1,t signifies the predicted value by the network for
the (t −1, t)-th distance, Linear represents fully connected
layers, and Concat represents tensor concatenation.
The overall network is trained using the Root Mean Square
Error (RMSE) function with supervised ground truth data
obtained from a total station,
L =
v
u
u
t 1
n
T
X
t=1
(Ot−1,t −ˆOt−1,t)2
(12)
where T denotes the total number of samples.
V. EXPERIMENTS
We have implemented the proposed algorithm using Python
3.9 with packages such as torch 2.0.1, torchvision 0.15.2, and
Pillow 9.5.0 on a workstation with an Intel Xeon Silver 4216

processor, 256 GB random access memory (RAM), and an
NVIDIA GeForce RTX 3090 Graphics Processing Unit (GPU),
running on an Operating System of Ubuntu 22.04.1. Firstly,
we introduce the dataset used in the experiment. Secondly,
we evaluate GPR-OdomNet based relative distance estimation
when using adjacent GPR frames alone. Finally, we combine
GPR-OdomNet with other sensory inputs in a full-fledged
sensor-fusion-based odometry approach which allows us to
compare overall performance to that of the state-of-the-art
counterparts.
A. Dataset
We have evaluated our GPR-OdomNet using the widely
accepted public CMU-GPR dataset [4], which is often used in
GPR-assisted robot navigation. It covers three scenarios where
GPS signals are unavailable, including four trajectories in the
basement (nsh_b), three trajectories in the factory workshop
(nsh_h), and seven trajectories in the parking lot (gates_g).
For these three environments, the total trajectory lengths of the
gates_g, nsh_b, and nsh_h datasets are 365, 264, and
90 meters, respectively. For each trajectory, the collected data
include a single channel GPR, a camera, a Wheel Encoder, and
a total station, with the total station measurements serving as
the ground truth.
B. Ablation Study
To evaluate the effectiveness of each key module in GPR-
OdomNet, we have performed an ablation study. Tab. I shows
the results of the performance comparison of different net-
work variants, where the network performance is quantified
using the RMSE error defined in (12). A lower RMSE value
indicates a smaller deviation between the predicted distance
and the ground truth distance from the total station. Specif-
ically, Feature Concatenation serves as the baseline method,
employing a simple feature concatenation strategy; Difference
Only network focuses solely on image differences; Similarity
Only
network considers only image similarity; and our full
version of GPR-OdomNet.
TABLE I: RMSE (cm) of ablation studies
Network Configuration
Dataset
gates_g
nsh_b
nsh_h
Feature Concatenation
11.574
7.936
5.636
Similarity Only
5.214
3.007
2.802
Difference Only
3.749
1.912
2.195
GPR-OdomNet
3.579
1.808
1.810
As shown in the table, GPR-OdomNet performs better than
its degraded variants in all datasets, validating the need to
jointly exploit difference detection and similarity detection.
It should be noted that GPR-OdomNet reduces the average
RMSE by 69.1% compared to the worst-performing variant
(Feature Concatenation).
C. Comparison of GPR-Only Relative Position Estimation
Performance
We then compare our GPR-OdomNet with state-of-the-art
GPR methods to estimate distance between adjacent frames.
This is a non-cumulative relative position estimation from
frame matching. During the experiment, we have classified
the dataset based on the characteristics of trajectories (such
as smooth circular or sharp turns): 4 trajectories from the
gates_g dataset are selected for training and 2 for testing; 3
trajectories from the nsh_b dataset are used for training and
1 for testing; 2 trajectories from the nsh_h dataset are used
for training and 1 for testing. This partitioning method ensures
the diversity and representativeness of the samples, providing
a better comparison among all candidate methods.
Tab. II presents the odometry performance comparison
between our method and three state-of-the-art GPR odometry
methods.
TABLE II: RMSE (cm) of different GPR-based relative dis-
tance estimation methods
Model
Dataset
gates_g nsh_b nsh_h Overall
Learned GPR Method [20]
3.848
1.697 1.975
3.175
SFM [19]
5.582
5.138 3.000
5.270
CNN-LSTM [21]
5.975
3.528
-
-
GPR-OdomNet (Ours)
3.579
1.808 1.810
2.945
The CNN-LSTM method has not been tested under the
nsh_h dataset in [21], and therefore, the corresponding entry
is missing. It is clear that GPR-OdomNet and the Learned
GPR Method outperform SFM and CNN-LSTM. The GPR-
OdomNet outperforms the Learned GPR Method in two out
of three datasets. Overall, our method reduces RMSE by 7.2%
when compared with the best state-of-the-art method Learned
GPR Method.
D. Comparison of Odometry Performance in Sensor Fusion
Since GPR odometry is often used in combination with
other sensors such as IMU and Wheel Encoder, it is necessary
to compare the localization performance in the sensor fusion
setting. This study designs a set of comparative experiments:
a factor graph model in [19] that fuses the input of the IMU
and the Wheel Encoder is used as the baseline method. Then
GPR-OdomNet is incorporated using the actor graph model, as
shown in Fig. 4. Within the adopted factor graph optimization
framework, the IMU observations are processed using a pre-
integration method to constrain the robot’s position, orienta-
tion, and velocity, while the measurements from the Wheel
Encoder and the estimates from the GPR-OdomNet constrain
the robot’s velocity.
The experimental results are shown in Fig. 5, where the
Ground Truth represents data collected by the aforementioned
total station. The IMU & WheelEncoder does not include

-50
-40
-30
-20
-10
X(m)
-20
-15
-10
-5
0
5
Y(m)
Estimation Path
Start
Ground Truth
IMU & WheelEncoder
DEC
SFM
GPR-OdomNet
Fig. 5: Comparison of odometry performance in sensor fusion
using the gates_g dataset.
0
100
200
300
400
Time(s)
0.00
0.25
0.50
0.75
1.00
1.25
1.50
1.75
2.00
RMSE(m)
Absolute Trajectory Error (ATE)
IMU & WheelEncoder
DEC
SFM
GPR-OdomNet
Fig. 6: Analysis of ATE RMSE over time corresponding to
the trajectory in Fig. 5.
GPR input, which serves as the baseline method. The GPR-
OdomNet is our method here that employs GPR-OdomNet as
the GPR component of the sensor fusion. The SFM integrates
GPR odometry extracted using a subsurface feature matrix
(SFM) [19]. The DEC [18] is an absolute localization method
based on prior maps for DEC extraction and matching. The
results show that the odometer accuracy of the GPR-OdomNet
model is superior to other models.
To further compare odometry performance, Fig. 6 presents
RMSE values of Absolute Trajectory Error (ATE) [22] corre-
sponding to the trajectories in Fig. 5,
LAT E =
v
u
u
t 1
2T
T
X
t=1
((xt −ˆxt)2 + (yt −ˆyt)2)
(13)
where T represents the total duration of the robot’s movement,
xt and yt denote the horizontal and vertical coordinates
obtained from the total station at time t, while ˆxt and ˆyt
represent the estimated horizontal and vertical coordinates of
the robot at time t. This metric provides a global assessment
of the localization and odometry result, and hence is more
comprehensive. It is clear that the GPR-OdomNet method
outperforms others.
Tab. III details the RMSE for each dataset for each method.
All methods employ the factor graph in Fig. 4 with the same
input from IMU and Wheel Encoder. The only difference is
whether or what type of GPR odometry is included. Among
these, Learned GPR Method integrates the learning sensor
method [20]. Overall, the last column in the Tab. III, represents
ATE weighted by trajectory length across all three datasets,
LOverall =
Pn
i=1 Wi · LAT E,i
Pn
i=1 Wi
(14)
where Wi denotes the total travel distance of the ith trajectory,
and LAT E,i represents the ATE RMSE of the ith trajectory
calculated using (12).
TABLE III: ATE RMSE (m) comparison results across
datasets.
Model
Dataset
gates_g nsh_b nsh_h Overall
Learned GPR Method
1.228
0.332 0.251
0.590
SFM
0.468
0.734 0.439
0.568
DEC
0.47
0.52
0.57
0.50
IMU & WheelEncoder 0.588
1.074 1.015
0.743
GPR-OdomNet
0.353
0.751 0.380
0.449
The experimental results of the method DEC are reported
at two decimal places. Tab. III show that the proposed GPR-
OdomNet achieves the best overall performance. For specific
datasets, it performs best in the largest data set gates_g and
the second best in dataset nsh_h. However, the performance
on dataset nsh_b is average, which maybe due to the higher
proportion of regular steel bars and pipes in this dataset.
Further study is needed to analyze the issue. Nevertheless,
the overall performance show that our method reduce ATE
RMSE by at least by 10.2% when compared with the best
state-of-the-art method DEC. It is also worth noting that all
methods with GPR inputs outperforms the baseline No GPR
which means using GPR is necessary and helpful in odometry.
VI. CONCLUSION AND FUTURE WORK
A novel GPR odometry network based on 2D radar scan
image that extracts difference and similarity feature was
presented in the paper. The network estimated GPR travel-
ing distance by exploiting feature differences and similarity
information. Through systematic ablation experiments and
comparative experiments conducted on the publicly available
and widely-used CMU-GPR datasets, the results show that the
proposed network improved over the state-of-the-art method
by reducing odometry RMSE by at least 10.2%.
In the future, we will investigate deep learning-based multi-
sensor fusion methods to construct an end-to-end integrated

positioning system. This will further enhance the practicality
and robustness of the model.
REFERENCES
[1] Yanfei Zhu, Xuanyu Fang, and Chuanjiang Li. Multi-sensor information
fusion for mobile robot indoor-outdoor localization: A zonotopic set-
membership estimation approach. Electronics, 14(5):867, 2025.
[2] Kenny AQ Caldas, Roberto S Inoue, Marco H Terra, Vitor Guizilini, and
Fabio Ramos. Discrete-time markovian jump linear robust filtering for
visual and gps aided magneto-inertial navigation. IEEE Access, 2025.
[3] Olivier Aycard and Christophe Brouard. A new tool to initialize global
localization for a mobile robot.
In 2020 IEEE 32nd International
Conference on Tools with Artificial Intelligence (ICTAI), pages 1296–
1303. IEEE, 2020.
[4] Alexander Baikovitz, Paloma Sodhi, Michael Dille, and Michael Kaess.
Cmu-gpr dataset: Ground penetrating radar dataset for robot localization
and mapping. arXiv preprint arXiv:2107.07606, 2021.
[5] Ronny Salim Lim, Hung Manh La, and Weihua Sheng. A robotic crack
inspection and mapping system for bridge deck maintenance.
IEEE
Transactions on Automation Science and Engineering, 11(2):367–378,
2014.
[6] Habib Ahmed, Hung Manh La, and Nenad Gucunski. Review of non-
destructive civil infrastructure evaluation for bridges: State-of-the-art
robotic platforms, sensors and algorithms. Sensors, 20(14):3954, 2020.
[7] Huanhuan Chen and Anthony G Cohn. Buried utility pipeline mapping
based on multiple spatial data sources: A bayesian data fusion approach.
In Twenty-Second International Joint Conference on Artificial Intelli-
gence. Citeseer, 2011.
[8] Feng Yang, Xu Qiao, Yuanyuan Zhang, and Xianlei Xu.
Prediction
method of underground pipeline based on hyperbolic asymptote of gpr
image. In Proceedings of the 15th International Conference on Ground
Penetrating Radar, pages 674–678. IEEE, 2014.
[9] Haifeng Li, Nansha Li, Renbiao Wu, Huaichao Wang, Zhongcheng Gui,
and Dezhen Song. Gpr-rcnn: An algorithm of subsurface defect detection
for airport runway based on gpr. IEEE Robotics and Automation Letters,
6(2):3001–3008, 2021.
[10] Nansha Li, Renbiao Wu, Haifeng Li, Huaichao Wang, Zhongcheng Gui,
and Dezhen Song. M 2 fnet: Multi-modal fusion network for airport
runway subsurface defect detection using gpr data. IEEE Transactions
on Geoscience and Remote Sensing, 2023.
[11] Paul Furgale, Tim Barfoot, and Nadeem Ghafoor. Rover-based surface
and subsurface modeling for planetary exploration. In Field and Service
Robotics: Results of the 7th International Conference, pages 499–508.
Springer, 2010.
[12] Jialong Lai, Yi Xu, Roberto Bugiolacchi, Xu Meng, Long Xiao, Ming-
gang Xie, Bin Liu, Kaichang Di, Xiaoping Zhang, Bin Zhou, et al. First
look by the yutu-2 rover at the deep subsurface structure at the lunar
farside. Nature communications, 11(1):3426, 2020.
[13] Chieh Chou, Aaron Kingery, Di Wang, Haifeng Li, and Dezhen Song.
Encoder-camera-ground penetrating radar tri-sensor mapping for surface
and subsurface transportation infrastructure inspection. In 2018 IEEE
International Conference on Robotics and Automation (ICRA), pages
1452–1457. IEEE, 2018.
[14] Chieh Chou, Haifeng Li, and Dezhen Song.
Encoder-camera-ground
penetrating radar sensor fusion: Bimodal calibration and subsurface
mapping. IEEE Transactions on Robotics, 37(1):67–81, 2020.
[15] Haifeng Li, Chieh Chou, Longfei Fan, Binbin Li, Di Wang, and Dezhen
Song.
Toward automatic subsurface pipeline mapping by fusing a
ground-penetrating radar and a camera. IEEE Transactions on Automa-
tion Science and Engineering, 17(2):722–734, 2019.
[16] Matthew Cornick, Jeffrey Koechling, Byron Stanley, and Beijia Zhang.
Localizing ground penetrating radar: A step toward robust autonomous
ground vehicle localization.
Journal of field robotics, 33(1):82–102,
2016.
[17] Evangelos Skartados, Andreas Kargakos, Efthimios Tsiogas, Ioannis
Kostavelis, Dimitrios Giakoumis, and Dimitrios Tzovaras. Gpr antenna
localization based on a-scans. In 2019 27th European Signal Processing
Conference (EUSIPCO), pages 1–5. IEEE, 2019.
[18] Haifeng Li, Jiajun Guo, and Dezhen Song. Subsurface feature-based
ground robot/vehicle localization using a ground penetrating radar.
In 2024 IEEE International Conference on Robotics and Automation
(ICRA), pages 1716–1722. IEEE, 2024.
[19] Haifeng Li, Jiajun Guo, Xuanxin Fan, and Dezhen Song.
Ground
penetrating radar-assisted multimodal robot odometry using subsurface
feature matrix, 2025.
[20] Alexander Baikovitz, Paloma Sodhi, Michael Dille, and Michael Kaess.
Ground encoding: Learned factor graph-based models for localizing
ground penetrating radar. In 2021 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pages 5476–5483. IEEE, 2021.
[21] Sathira Wickramanayake, Karthick Thiyagarajan, and Sarath Kodagoda.
Deep learned ground penetrating radar subsurface features for robot
localization. In 2022 IEEE Sensors, pages 1–4. IEEE, 2022.
[22] Zichao Zhang and Davide Scaramuzza.
A tutorial on quantitative
trajectory evaluation for visual (-inertial) odometry. In 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS),
pages 7244–7251. IEEE, 2018.
